[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.12036v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v3",
                "updated": "2025-07-01T05:46:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    46,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Pacal Poupart"
                    },
                    {
                        "name": "Suraj Kothawade"
                    }
                ],
                "author_detail": {
                    "name": "Suraj Kothawade"
                },
                "author": "Suraj Kothawade",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v2",
                "updated": "2025-06-30T19:01:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    19,
                    1,
                    18,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24060v1",
                "updated": "2025-06-30T17:07:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:07:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    7,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combinatorial Multi-Access Coded Caching with Private Caches under\n  Intersecting Index Constraints"
                },
                "summary": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the coded caching system where each user, equipped with a private\ncache, accesses a distinct r-subset of access caches. A central server housing\na library of files populates both private and access caches using uncoded\nplacement. In this work, we focus on a constrained indexing regime, referred to\nas the intersection class, in which the sets used to index the demands of each\nuser must have a nonempty intersection. This regime models resource-limited IoT\nscenarios such as edge-assisted IoT systems, where devices with small private\ncaches connect to a small number of shared caches. We provide a necessary and\nsufficient condition under which the system parameters fall within this\nintersection class. Under this condition, we propose a centralized coded\ncaching scheme and characterize its rate-memory trade-off. Next, we define a\nuniform-intersection subclass and establish a condition under which the system\nbelongs to this subclass. Within this subclass, the proposed scheme has a\nregular structure, with each transmission benefiting the same number of users,\nand we characterize its rate-memory trade-off. Additionally, we derive an index\ncoding-based lower bound on the minimum achievable worst-case rate under\nuncoded placement. Finally, we provide numerical comparisons between the rate\nof the proposed scheme, the new lower bound, and bounds from the original work."
                },
                "authors": [
                    {
                        "name": "Dhruv Pratap Singh"
                    },
                    {
                        "name": "Anjana A. Mahesh"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages and 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v4",
                "updated": "2025-06-30T16:23:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    23,
                    35,
                    0,
                    181,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23809v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23809v1",
                "updated": "2025-06-30T12:55:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T12:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    55,
                    59,
                    0,
                    181,
                    0
                ],
                "title": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale Neural Network Quantum States for ab initio Quantum\n  Chemistry Simulations on Fugaku"
                },
                "summary": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving quantum many-body problems is one of the fundamental challenges in\nquantum chemistry. While neural network quantum states (NQS) have emerged as a\npromising computational tool, its training process incurs exponentially growing\ncomputational demands, becoming prohibitively expensive for large-scale\nmolecular systems and creating fundamental scalability barriers for real-world\napplications. To address above challenges, we present \\ours, a high-performance\nNQS training framework for \\textit{ab initio} electronic structure\ncalculations. First, we propose a scalable sampling parallelism strategy with\nmulti-layers workload division and hybrid sampling scheme, which break the\nscalability barriers for large-scale NQS training. Then, we introduce\nmulti-level parallelism local energy parallelism, enabling more efficient local\nenergy computation. Last, we employ cache-centric optimization for\ntransformer-based \\textit{ansatz} and incorporate it with sampling parallelism\nstrategy, which further speedup up the NQS training and achieve stable memory\nfootprint at scale. Experiments demonstrate that \\ours accelerate NQS training\nwith up to 8.41x speedup and attains a parallel efficiency up to 95.8\\% when\nscaling to 1,536 nodes."
                },
                "authors": [
                    {
                        "name": "Hongtao Xu"
                    },
                    {
                        "name": "Zibo Wu"
                    },
                    {
                        "name": "Mingzhen Li"
                    },
                    {
                        "name": "Weile Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weile Jia"
                },
                "author": "Weile Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23809v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23809v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02236v2",
                "updated": "2025-06-30T05:54:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    54,
                    40,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-04T03:18:56Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    3,
                    18,
                    56,
                    1,
                    63,
                    0
                ],
                "title": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-LLM: High-performance Code Generation for Vector Quantization\n  Augmented LLM Inference"
                },
                "summary": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we design and implement VQ-LLM, an efficient fused Vector\nQuantization (VQ) kernel generation framework. We first introduce a software\nabstraction called codebook cache to optimize codebook access efficiency and\nsupport the integration of VQ with various computations. The codebook cache\nadaptively stores different entries across the GPU's memory hierarchy,\nincluding off-chip global memory, on-chip shared memory, and registers.\nCentered around the codebook cache, we design an efficient computation engine\nthat optimizes memory traffic during computations involving codebooks. This\ncompute engine adopts the codebook-centric dataflow and fusion optimizations.\nAdditionally, we provide adaptive heuristics to tailor parameter selection in\nour optimizations to diverse VQ configurations. Our optimizations achieve an\naverage latency reduction of 46.13% compared to unoptimized versions. Compared\nto existing open-source implementations, our methods decrease latency by 64.36%\nto 99.1%. A final comparison with state-of-the-art element-wise quantization\nmethods like AWQ and KVQuant shows that our VQ-LLM is practically viable,\nachieving latencies close or even better latencies to those at equivalent\nbit-widths, potentially offering greater accuracy."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Xinhao Luo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Wentao Ni"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Yue Guan"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weihao Cui"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Yuhao Zhu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Chen Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chen Jin"
                },
                "author": "Chen Jin",
                "arxiv_doi": "10.1109/HPCA61900.2025.00112",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/HPCA61900.2025.00112",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.02236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12494v2",
                "updated": "2025-06-30T05:45:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    45,
                    43,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-14T13:16:31Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    16,
                    31,
                    5,
                    165,
                    0
                ],
                "title": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large\nlanguage model applications, with numerous existing frameworks offering a wide\nrange of functionalities to facilitate the development of RAG systems. However,\nwe have identified several persistent challenges in these frameworks, including\ndifficulties in algorithm reproduction and sharing, lack of new techniques, and\nhigh system overhead. To address these limitations, we introduce\n\\textbf{FlexRAG}, an open-source framework specifically designed for research\nand prototyping. FlexRAG supports text-based, multimodal, and network-based\nRAG, providing comprehensive lifecycle support alongside efficient asynchronous\nprocessing and persistent caching capabilities. By offering a robust and\nflexible solution, FlexRAG enables researchers to rapidly develop, deploy, and\nshare advanced RAG systems. Our toolkit and resources are available at\n\\href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}."
                },
                "authors": [
                    {
                        "name": "Zhuocheng Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Accepted by ACL 2025 Demo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v2",
                "updated": "2025-06-30T05:21:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    21,
                    58,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23488v1",
                "updated": "2025-06-30T03:22:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T03:22:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces"
                },
                "summary": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communication systems face significant challenges in meeting the\nincreasing demands for higher data rates and more reliable connectivity in\ncomplex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a\npromising technology for realizing wave-domain signal processing, with mobile\nSIMs offering superior communication performance compared to their fixed\ncounterparts. In this paper, we investigate a novel unmanned aerial vehicle\n(UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the\nlow-altitude economy (LAE) networks paradigm, where UAVs function as both base\nstations that cache SIM-processed data and mobile platforms that flexibly\ndeploy SIMs to enhance uplink communications from ground users. To maximize\nnetwork capacity, we formulate a UAV-SIM-based joint optimization problem\n(USBJOP) that comprehensively addresses three critical aspects: the association\nbetween UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and\nthe phase shifts across multiple SIM layers. Due to the inherent non-convexity\nand NP-hardness of USBJOP, we decompose it into three sub-optimization\nproblems, \\textit{i.e.}, association between UAV-SIMs and users optimization\nproblem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase\nshifts optimization problem (USPSOP), and solve them using an alternating\noptimization strategy. Specifically, we transform AUUOP and ULOP into convex\nforms solvable by the CVX tool, while addressing USPSOP through a generative\nartificial intelligence (GAI)-based hybrid optimization algorithm. Simulations\ndemonstrate that our proposed approach significantly outperforms benchmark\nschemes, achieving approximately 1.5 times higher network capacity compared to\nsuboptimal alternatives. Additionally, our proposed GAI method reduces the\nalgorithm runtime by 10\\% while maintaining solution quality."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Chuang Zhang"
                    },
                    {
                        "name": "Linyao Li"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This paper has been already submitted to TCCN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23405v1",
                "updated": "2025-06-29T21:55:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "published": "2025-06-29T21:55:58Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    21,
                    55,
                    58,
                    6,
                    180,
                    0
                ],
                "title": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors\n  upon GPGPU Platforms"
                },
                "summary": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In contemporary general-purpose graphics processing units (GPGPUs), the\ncontinued increase in raw arithmetic throughput is constrained by the\ncapabilities of the register file (single-cycle) and last-level cache (high\nbandwidth), which require the delivery of operands at a cadence demanded by\nwide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,\ndensity, or bandwidth of these memories can unlock substantial performance\ngains; however, the recent stagnation of SRAM bit-cell scaling leads to\ninequivalent losses in compute density.\n  To address the challenges posed by SRAM's scaling and leakage power\nconsumption, this paper explores the potential CMOS+X integration of amorphous\noxide semiconductor (AOS) transistors in capacitive, persistent memory\ntopologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in\nmulti-ported and high-bandwidth banked GPGPU memories. A detailed study of the\ndensity and energy tradeoffs of back-end-of-line (BEOL) integrated memories\nutilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while\naccounting for the macro-level limitations of integrating AOS candidate\nstructures proposed by the device community (an aspect often overlooked in\nprior work). By exploiting the short lifetime of register operands, we propose\na multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of\nthe footprint of SRAM with over 70% lower standby power, enabling enhancements\nto compute capacity, such as larger warp sizes or processor counts. Benchmarks\nrun on a validated NVIDIA Ampere-class GPU model, using a modified version of\nAccel-Sim, demonstrate improvements of up to 5.2x the performance per watt and\nan average 8% higher geometric mean instruction per cycle (IPC) on various\ncompute- and memory-bound tasks."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Ming-Yen Lee"
                    },
                    {
                        "name": "Seongwon Yoon"
                    },
                    {
                        "name": "Seongkwang Lim"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "14 pages, 18 figures, 4 tables, 4 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2; B.3.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22791v1",
                "updated": "2025-06-28T07:25:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "published": "2025-06-28T07:25:12Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    7,
                    25,
                    12,
                    5,
                    179,
                    0
                ],
                "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in\n  Large Language Models"
                },
                "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications."
                },
                "authors": [
                    {
                        "name": "Jianxin Yan"
                    },
                    {
                        "name": "Wangze Ni"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Zhan Qin"
                    },
                    {
                        "name": "Kui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Kui Ren"
                },
                "author": "Kui Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12593v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12593v4",
                "updated": "2025-06-28T06:24:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    6,
                    24,
                    44,
                    5,
                    179,
                    0
                ],
                "published": "2024-06-18T13:25:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    25,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document\n  Retrieval"
                },
                "summary": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentiable Search Index (DSI) utilizes pre-trained language models to\nperform indexing and document retrieval via end-to-end learning without relying\non external indexes. However, DSI requires full re-training to index new\ndocuments, causing significant computational inefficiencies. Continual learning\n(CL) offers a solution by enabling the model to incrementally update without\nfull re-training. Existing CL solutions in document retrieval rely on memory\nbuffers or generative models for rehearsal, which is infeasible when accessing\nprevious training data is restricted due to privacy concerns. To this end, we\nintroduce PromptDSI, a prompt-based, rehearsal-free continual learning approach\nfor document retrieval. PromptDSI follows the Prompt-based Continual Learning\n(PCL) framework, using learnable prompts to efficiently index new documents\nwithout accessing previous documents or queries. To improve retrieval latency,\nwe remove the initial forward pass of PCL, which otherwise greatly increases\ntraining and inference time, with a negligible trade-off in performance.\nAdditionally, we introduce a novel topic-aware prompt pool that employs neural\ntopic embeddings as fixed keys, eliminating the instability of prompt key\noptimization while maintaining competitive performance with existing PCL prompt\npools. In a challenging rehearsal-free continual learning setup, we demonstrate\nthat PromptDSI variants outperform rehearsal-based baselines, match the strong\ncache-based baseline in mitigating forgetting, and significantly improving\nretrieval performance on new corpora."
                },
                "authors": [
                    {
                        "name": "Tuan-Luc Huynh"
                    },
                    {
                        "name": "Thuy-Trang Vu"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Trung Le"
                    },
                    {
                        "name": "Dragan Gasevic"
                    },
                    {
                        "name": "Yuan-Fang Li"
                    },
                    {
                        "name": "Thanh-Toan Do"
                    }
                ],
                "author_detail": {
                    "name": "Thanh-Toan Do"
                },
                "author": "Thanh-Toan Do",
                "arxiv_comment": "ECML PKDD 2025 Research track. Camera-ready version. Code is\n  available at https://github.com/LouisDo2108/PromptDSI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12593v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12593v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v4",
                "updated": "2025-06-28T03:53:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    28,
                    3,
                    53,
                    17,
                    5,
                    179,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "17 pages, 12 figures, 9 tables",
                "arxiv_journal_ref": "International Conference on Machine Proceedings of the 42nd\n  International Conference on Machine Learning, Vancouver, Canada. PMLR 267,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22396v1",
                "updated": "2025-06-27T17:10:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T17:10:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    10,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,\n  KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization"
                },
                "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2)."
                },
                "authors": [
                    {
                        "name": "Danush Khanna"
                    },
                    {
                        "name": "Aditya Kumar Guru"
                    },
                    {
                        "name": "Srivarshinee Sridhar"
                    },
                    {
                        "name": "Zidan Ahmed"
                    },
                    {
                        "name": "Rubhav Bahirwani"
                    },
                    {
                        "name": "Meetu Malhotra"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Amitava Das"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Kripabandhu Ghosh"
                },
                "author": "Kripabandhu Ghosh",
                "arxiv_comment": "Preprint. Under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22033v1",
                "updated": "2025-06-27T09:27:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T09:27:04Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    27,
                    4,
                    4,
                    178,
                    0
                ],
                "title": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient\n  Pipeline-Parallel LLM Inference"
                },
                "summary": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As inference workloads for large language models (LLMs) scale to meet growing\nuser demand, pipeline parallelism (PP) has become a widely adopted strategy for\nmulti-GPU deployment, particularly in cross-node setups, to improve key-value\n(KV) cache capacity and inference throughput. However, PP suffers from inherent\ninefficiencies caused by three types of execution bubbles-load-imbalance,\nintra-stage, and inter-stage-which limit pipeline saturation. We present\nSiPipe, a heterogeneous pipeline design that improves throughput by leveraging\nunderutilized CPU resources to offload auxiliary computation and communication.\nSiPipe incorporates three key techniques-CPU sampling, a token-safe execution\nmodel, and structure-aware transmission-to mitigate pipeline bubbles and\nimprove execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1\ntimes higher throughput, 43% lower per-token latency, and up to 23% higher\naverage GPU utilization compared to the state-of-the-art vLLM under the same PP\nconfiguration, demonstrating its generality across LLMs and deployment\nscenarios."
                },
                "authors": [
                    {
                        "name": "Yongchao He"
                    },
                    {
                        "name": "Bohan Zhao"
                    },
                    {
                        "name": "Zheng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cao"
                },
                "author": "Zheng Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v3",
                "updated": "2025-06-27T09:14:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    9,
                    14,
                    2,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "41 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21901v1",
                "updated": "2025-06-27T04:38:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "published": "2025-06-27T04:38:20Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    4,
                    38,
                    20,
                    4,
                    178,
                    0
                ],
                "title": "A Survey of LLM Inference Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM Inference Systems"
                },
                "summary": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The past few years has witnessed specialized large language model (LLM)\ninference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside\nrapid LLM adoption via services like ChatGPT. Driving these system design\nefforts is the unique autoregressive nature of LLM request processing,\nmotivating new techniques for achieving high performance while preserving high\ninference quality over high-volume and high-velocity workloads. While many of\nthese techniques are discussed across the literature, they have not been\nanalyzed under the framework of a complete inference system, nor have the\nsystems themselves been analyzed and compared.\n  In this survey, we review these techniques, starting from operators and\nalgorithms for request processing, then moving on to techniques for model\noptimization and execution, including kernel design, batching, and scheduling,\nbefore ending with techniques for memory management, including paged memory,\neviction and offloading techniques, quantization, and cache persistence.\nThrough these discussions, we show that these techniques fundamentally rely on\nload prediction, adaptive mechanisms, and cost reduction in order to overcome\nthe challenges introduced by autoregressive generation and achieve the goals of\nthe system. We then discuss how these techniques can be combined to form\nsingle-replica and multi-replica inference systems, including disaggregated\ninference systems that offer more control over resource allocation and\nserverless systems that can be deployed over shared hardware infrastructure. We\nend with a discussion of remaining challenges."
                },
                "authors": [
                    {
                        "name": "James Pan"
                    },
                    {
                        "name": "Guoliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Guoliang Li"
                },
                "author": "Guoliang Li",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15294v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15294v3",
                "updated": "2025-06-27T03:43:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    27,
                    3,
                    43,
                    24,
                    4,
                    178,
                    0
                ],
                "published": "2025-02-21T08:40:07Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    8,
                    40,
                    7,
                    4,
                    52,
                    0
                ],
                "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate\n  LLM Inference"
                },
                "summary": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in large language models (LLMs) has\nimproved their ability to handle complex, long-text tasks. However, as the\nconversation rounds continue, it is required to store a large amount of KV\ncache in GPU memory, which significantly affects the efficiency and even\navailability of the model serving systems. This paper analyzes dialogue data\nfrom real users on the granularity of round and discovers that the LLM\ninference manifests a watershed layer, after which the distribution of\nround-level attention shows notable similarity. Based on this, we propose Round\nAttention - a novel round-level attention mechanism that selectively processes\nthe KV cache of top-k relevant rounds, where k is dynamically determined\nthrough the attention matrix in the watershed layer. Theoretical analysis\ndemonstrates that our method reduces memory usage by 54\\% to 82\\%, while\nexperimental results confirm that loading sparse critical-round KV cache\nmaintains answer accuracy without performance degradation."
                },
                "authors": [
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Zhicheng Hu"
                    },
                    {
                        "name": "Kun Cheng"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Qiheng Lv"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15294v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15294v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v1",
                "updated": "2025-06-26T18:51:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian Hger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v2",
                "updated": "2025-06-26T18:40:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    40,
                    55,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "arxiv_comment": "Accepted to Transactions of the Association for Computational\n  Linguistics (TACL 2025); Pre MIT Press version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19686v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19686v2",
                "updated": "2025-06-26T17:18:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    17,
                    18,
                    54,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-24T14:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Memories to Maps: Mechanisms of In-Context Reinforcement Learning\n  in Transformers"
                },
                "summary": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans and animals show remarkable learning efficiency, adapting to new\nenvironments with minimal experience. This capability is not well captured by\nstandard reinforcement learning algorithms that rely on incremental value\nupdates. Rapid adaptation likely depends on episodic memory -- the ability to\nretrieve specific past experiences to guide decisions in novel contexts.\nTransformers provide a useful setting for studying these questions because of\ntheir ability to learn rapidly in-context and because their key-value\narchitecture resembles episodic memory systems in the brain. We train a\ntransformer to in-context reinforcement learn in a distribution of planning\ntasks inspired by rodent behavior. We then characterize the learning algorithms\nthat emerge in the model. We first find that representation learning is\nsupported by in-context structure learning and cross-context alignment, where\nrepresentations are aligned across environments with different sensory stimuli.\nWe next demonstrate that the reinforcement learning strategies developed by the\nmodel are not interpretable as standard model-free or model-based planning.\nInstead, we show that in-context reinforcement learning is supported by caching\nintermediate computations within the model's memory tokens, which are then\naccessed at decision time. Overall, we find that memory may serve as a\ncomputational resource, storing both raw experience and cached computations to\nsupport flexible behavior. Furthermore, the representations developed in the\nmodel resemble computations associated with the hippocampal-entorhinal system\nin the brain, suggesting that our findings may be relevant for natural\ncognition. Taken together, our work offers a mechanistic hypothesis for the\nrapid adaptation that underlies in-context learning in artificial and natural\nsettings."
                },
                "authors": [
                    {
                        "name": "Ching Fang"
                    },
                    {
                        "name": "Kanaka Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Kanaka Rajan"
                },
                "author": "Kanaka Rajan",
                "arxiv_comment": "Updates: added other funding sources; formatted title correctly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19686v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19686v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21236v1",
                "updated": "2025-06-26T13:22:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T13:22:30Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    13,
                    22,
                    30,
                    3,
                    177,
                    0
                ],
                "title": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurements, simulations, and models of the point-spread function of\n  electron-beam lithography"
                },
                "summary": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When a sample is exposed using electron-beam lithography, the electrons\nscatter deep and far in the substrate, resulting in unwanted deposition of dose\nat both the nano- and the microscale. This proximity effect can be mitigated by\nproximity effect correction provided that accurate and validated models of the\npoint-spread function of the electron scattering are available. Most works so\nfar considered a double-Gaussian model of the electron point-spread function,\nwhich is very inaccurate for modern electron-beam writers with high\nacceleration voltages. We present measurements of the process point-spread\nfunction for chemically semi-amplified resist on silicon and indium phosphide\nsubstrates using a 150 kV electron-beam lithography system. We find that the\ndouble-Gaussian model deviates from experiments by up to four orders of\nmagnitude. We propose instead a model comprising the sum of a power-law and a\nGaussian, which is in excellent agreement with simulations of the electron\nscattering obtained by a Monte Carlo method. We apply the power-law plus\nGaussian model to quantify the electron scattering and proximity effect\ncorrection parameters across material stacks, processing, and voltages from 5\nkV to 150 kV. We find that the power-law term remains remarkably constant,\nwhereas the long-range dose contributions and the clearing dose are\nsignificantly affected by the substrate and the acceleration voltage."
                },
                "authors": [
                    {
                        "name": "Nikolaj B. Hougs"
                    },
                    {
                        "name": "Kristian S. Knudsen"
                    },
                    {
                        "name": "Marcus Albrechtsen"
                    },
                    {
                        "name": "Taichi Suhara"
                    },
                    {
                        "name": "Christian A. Rosiek"
                    },
                    {
                        "name": "Sren Stobbe"
                    }
                ],
                "author_detail": {
                    "name": "Sren Stobbe"
                },
                "author": "Sren Stobbe",
                "arxiv_comment": "Main; 15 pages, 7 figures. Supporting; 5 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21184v1",
                "updated": "2025-06-26T12:43:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T12:43:43Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    12,
                    43,
                    43,
                    3,
                    177,
                    0
                ],
                "title": "Task-Aware KV Compression For Cost-Effective Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Aware KV Compression For Cost-Effective Long Video Understanding"
                },
                "summary": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-video understanding (LVU) remains a severe challenge for existing\nmultimodal large language models (MLLMs), primarily due to the prohibitive\ncomputational cost. Recent approaches have explored KV compression to mitigate\nthis issue, but they often suffer from significant information loss at high\ncompression ratios. In this paper, we introduce Video-X^2L, which flexibly\npreserves critical video information for each LVU task. Video-X^2L involves two\nkey operations. The first one is called bi-level KV compression. During the\nMLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs:\nlow-compression KVs (L-KVs) to capture fine-grained video details and\nhigh-compression KVs (H-KVs) to offer compact video representations. The second\none is called selective KV re-loading. During the MLLM's decoding stage,\nVideo-X^2L selectively re-loads L-KVs for the most critical video chunks while\nusing H-KVs for other less important ones. This allows the MLLM to fully\nutilize task-specific information while maintaining the overall compactness.\nVideo-X^2L is simple yet effective: it is free from additional training and\ndirectly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L\nwith a variety of popular LVU benchmarks, including VideoMME, MLVU,\nLongVideoBench, and VNBench. Our experiment result shows that Video-X^2L\noutperforms existing KV-compression methods by a huge advantage while\nsubstantially saving the computation cost."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kun Lun"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "14 pages, 3 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v3",
                "updated": "2025-06-26T05:12:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    5,
                    12,
                    22,
                    3,
                    177,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimal infrastructure requirements make LoRa suitable for service delivery\nin remote areas. Additionally, web applications have become a de-facto standard\nfor modern service delivery. However, Long Range (LoRa) fails to enable HTTP\naccess due to its limited bandwidth, payload size limitations, and high\ncollisions in multi-user setups. We propose LoRaConnect to enable HTTP access\nover LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices\nconnect and access HTTP resources over LoRa backhaul. It implements caching and\nsynchronization mechanisms to address LoRa's aforementioned limitations. It\nalso implements a message-slicing method in the application layer to overcome\nLoRa's payload limitations. We evaluate the proposed system using actual\nhardware in three experimental setups to assess the baseline performance, ideal\nscenario, and practical application scenario with Frequency Hopping Spread\nSpectrum (FHSS). Additionally, it implements a ping operation to demonstrate\nInternet capability and extensible nature. LoRaWeb achieves an average\nthroughput of 1.18 KB/S approximately, with an access delay of only 1.3 S\napproximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves\nan access delay of approximately 6.7 S for a 10KB webpage in the ideal case and\nan average end-to-end delay of only 612 ms approximately in the FHSS-based\nsetup. Comparison with benchmark suggests multi-fold improvement."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20968v1",
                "updated": "2025-06-26T03:13:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "published": "2025-06-26T03:13:33Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    3,
                    13,
                    33,
                    3,
                    177,
                    0
                ],
                "title": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The electronic structures, magnetic transition and Fermi surface\n  instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O"
                },
                "summary": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnetism has recently emerged as a distinct and fundamental class of\nmagnetic order. Exploring its interplay with quantum phenomena such as\nunconventional superconductivity, density-wave instabilities, and many-body\neffects represents a compelling frontier. In this work, we theoretically\nconfirm the presence of high-temperature metallic altermagnetism in\nKV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal\ntransition arises from a Lifshitz transition associated with Fermi surface\nreconstruction. The previously reported spin-density wave gap is found to lie\nbelow the Fermi level in our study and is now recognized to be attributed to\nthe V-shaped density of states, originating from orbital-selective and\nsublattice-resolved half-metal-like behavior on a specific V atom. Furthermore,\nwe identify the instability from the nesting of spin-momentum-locked\ntwo-dimensional Fermi surfaces, which induces the SDW state. These findings\nposition KV$_2$Se$_2$O as a promising platform for investigating the interplay\namong altermagnetism, unconventional superconductivity, and density-wave order."
                },
                "authors": [
                    {
                        "name": "Yuanji Xu"
                    },
                    {
                        "name": "Huiyuan Zhang"
                    },
                    {
                        "name": "Maoyuan Feng"
                    },
                    {
                        "name": "Fuyang Tian"
                    }
                ],
                "author_detail": {
                    "name": "Fuyang Tian"
                },
                "author": "Fuyang Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v2",
                "updated": "2025-06-26T01:30:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    26,
                    1,
                    30,
                    43,
                    3,
                    177,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "We have withdrawn this manuscript due to a critical error in the\n  methodology which affects the validity of the main results. We are currently\n  working to address this issue and will resubmit once the correction is\n  complete",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20886v1",
                "updated": "2025-06-25T23:36:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T23:36:44Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    36,
                    44,
                    2,
                    176,
                    0
                ],
                "title": "Omniwise: Predicting GPU Kernels Performance with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Omniwise: Predicting GPU Kernels Performance with LLMs"
                },
                "summary": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows."
                },
                "authors": [
                    {
                        "name": "Zixian Wang"
                    },
                    {
                        "name": "Cole Ramos"
                    },
                    {
                        "name": "Muhammad A. Awad"
                    },
                    {
                        "name": "Keith Lowery"
                    }
                ],
                "author_detail": {
                    "name": "Keith Lowery"
                },
                "author": "Keith Lowery",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12942v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12942v3",
                "updated": "2025-06-25T23:03:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    23,
                    3,
                    54,
                    2,
                    176,
                    0
                ],
                "published": "2025-05-19T10:29:32Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    10,
                    29,
                    32,
                    0,
                    139,
                    0
                ],
                "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A3 : an Analytical Low-Rank Approximation Framework for Attention"
                },
                "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance."
                },
                "authors": [
                    {
                        "name": "Jeffrey T. H. Wong"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Xinye Cao"
                    },
                    {
                        "name": "Pedro Gimenes"
                    },
                    {
                        "name": "George A. Constantinides"
                    },
                    {
                        "name": "Wayne Luk"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12942v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12942v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20703v1",
                "updated": "2025-06-25T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    55,
                    2,
                    176,
                    0
                ],
                "title": "Generative Blocks World: Moving Things Around in Pictures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Blocks World: Moving Things Around in Pictures"
                },
                "summary": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization."
                },
                "authors": [
                    {
                        "name": "Vaibhav Vavilala"
                    },
                    {
                        "name": "Seemandhar Jain"
                    },
                    {
                        "name": "Rahul Vasanth"
                    },
                    {
                        "name": "D. A. Forsyth"
                    },
                    {
                        "name": "Anand Bhattad"
                    }
                ],
                "author_detail": {
                    "name": "Anand Bhattad"
                },
                "author": "Anand Bhattad",
                "arxiv_comment": "23 pages, 16 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20420v1",
                "updated": "2025-06-25T13:35:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T13:35:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    13,
                    35,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Semantic Caching for Improving Web Affordability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Caching for Improving Web Affordability"
                },
                "summary": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of web content has led to increasingly large webpages,\nposing significant challenges for Internet affordability, especially in\ndeveloping countries where data costs remain prohibitively high. We propose\nsemantic caching using Large Language Models (LLMs) to improve web\naffordability by enabling reuse of semantically similar images within webpages.\nAnalyzing 50 leading news and media websites, encompassing 4,264 images and\nover 40,000 image pairs, we demonstrate potential for significant data transfer\nreduction, with some website categories showing up to 37% of images as\nreplaceable. Our proof-of-concept architecture shows users can achieve\napproximately 10% greater byte savings compared to exact caching. We evaluate\nboth commercial and open-source multi-modal LLMs for assessing semantic\nreplaceability. GPT-4o performs best with a low Normalized Root Mean Square\nError of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA\n3.1 model shows comparable performance, highlighting its viability for\nlarge-scale applications. This approach offers benefits for both users and\nwebsite operators, substantially reducing data transmission. We discuss ethical\nconcerns and practical challenges, including semantic preservation, user-driven\ncache configuration, privacy concerns, and potential resistance from website\noperators"
                },
                "authors": [
                    {
                        "name": "Hafsa Akbar"
                    },
                    {
                        "name": "Danish Athar"
                    },
                    {
                        "name": "Muhammad Ayain Fida Rana"
                    },
                    {
                        "name": "Chaudhary Hammad Javed"
                    },
                    {
                        "name": "Zartash Afzal Uzmi"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.2, I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20283v1",
                "updated": "2025-06-25T09:44:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "published": "2025-06-25T09:44:25Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    9,
                    44,
                    25,
                    2,
                    176,
                    0
                ],
                "title": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do cell culturing influence the radiosensitizing effect of gold\n  nanoparticles part 2: scrutinizing the methodology producing recent evidence"
                },
                "summary": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When irradiation is performed with gold nanoparticles (AuNPs), a different\nshape of cells in suspension or adherent to walls may result in different\nprobability of cell survival. In a recent study, differences of up to a factor\nof 2 were found between the predicted survival of floating and adherent cells.\nThe present work aims to quantify the biases introduced by the simulation setup\nand the use of voxelized geometry in conjunction with the local effect model\nfor cell survival. The results show that simulated irradiation of a cell near\nthe surface with an incident beam matched to the cell dimensions results in\ndose values that are by a factor of about 50 lower than the dose to cells\ndeeper in the medium when irradiated with a Co-60 spectrum and lateral beam\ndimensions in the centimeter range. Furthermore, the number of ionizing photon\ninteractions in gold nanoparticles in a cell near the surface is lower by a\nfactor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose\nin voxels of size in the order of 200 nm for assessing cell survival with the\nlocal effect model (LEM) leads to an underestimation of the number of lesions\nfrom a single ionized AuNP by roughly two orders of magnitude and thus to an\noverestimation of cell survival. The effect of cell geometry on the survival\nrate was examined for approximate cell geometries and 100 kV x-ray irradiation,\nfor which the probability of photon interaction in gold nanoparticles is by\nmore than two orders of magnitude higher than for Co-60 irradiation. The\nresults show that the effects are negligible for 5 nm nanoparticles at the\nconcentration of AuNPs considered in preceding work. For 50 nm nanoparticles\nand thus a thousand times higher mass fraction of gold, significant reduction\nin cell survival is found, with a clear additional reduction predicted by the\nLEM as compared to the prediction based on mean dose to the nucleus."
                },
                "authors": [
                    {
                        "name": "Hans Rabus"
                    },
                    {
                        "name": "Oswald Msosa Mkanda"
                    }
                ],
                "author_detail": {
                    "name": "Oswald Msosa Mkanda"
                },
                "author": "Oswald Msosa Mkanda",
                "arxiv_comment": "16 pages, 6+1 Figs., 3+1 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20187v2",
                "updated": "2025-07-02T05:12:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    5,
                    12,
                    29,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-25T07:26:42Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    7,
                    26,
                    42,
                    2,
                    176,
                    0
                ],
                "title": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV\n  Management on a Single Commodity GPU"
                },
                "summary": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Large Language Models (LLMs) have achieved impressive performance\nacross a wide range of complex and long-context natural language tasks.\nHowever, performing long-context LLM inference locally on a commodity GPU (a\nPC) with privacy concerns remains challenging due to the increasing memory\ndemands of the key-value (KV) cache. Existing systems typically identify\nimportant tokens and selectively offload their KV data to GPU and CPU memory.\nThe KV data needs to be offloaded to disk due to the limited memory on a\ncommodity GPU, but the process is bottlenecked by token importance evaluation\noverhead and the disk's low bandwidth. In this paper, we present LeoAM, the\nfirst efficient importance-aware long-context LLM inference system for a single\ncommodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system\nemploys an adaptive KV management strategy that partitions KV data into\nvariable-sized chunks based on the skewed distribution of attention weights\nacross different layers to reduce computational and additional transmission\noverheads. Moreover, we propose a lightweight KV abstract method, which\nminimizes transmission latency by storing and extracting the KV abstract of\neach chunk on disk instead of the full KV data. LeoAM also leverages the\ndynamic compression and pipeline techniques to further accelerate inference.\nExperimental results demonstrate that LongInfer achieves an average inference\nlatency speedup of 3.46x, while maintaining comparable LLM response quality. In\nscenarios with larger batch sizes, it achieves up to a 5.47x speedup."
                },
                "authors": [
                    {
                        "name": "He Sun"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Mingjun Xiao"
                    },
                    {
                        "name": "Chengzhong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chengzhong Xu"
                },
                "author": "Chengzhong Xu",
                "arxiv_comment": "15 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20686v1",
                "updated": "2025-06-24T23:30:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T23:30:49Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    23,
                    30,
                    49,
                    1,
                    175,
                    0
                ],
                "title": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaFold: System-Level Optimizations for Accelerating Protein Structure\n  Prediction Models"
                },
                "summary": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein structure prediction models such as AlphaFold3 (AF3) push the\nfrontier of biomolecular modeling by incorporating science-informed\narchitectural changes to the transformer architecture. However, these advances\ncome at a steep system cost, introducing: compute- and memory-intensive\noperators, 2D attention mechanisms, and retrieval-augmented data pipelines,\nwhich collectively hinder the scalability of AF3 training. In this work, we\npresent MegaFold, a cross-platform system to accelerate AF3 training. MegaFold\ntackles key bottlenecks through ahead-of-time caching to eliminate GPU idle\ntime from the retrieval-augmented data pipeline, Triton-based kernels for\nmemory-efficient EvoAttention on heterogeneous devices, and deep fusion for\ncommon and critical small operators in AF3. Evaluation on both NVIDIA H200 and\nAMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by\nup to 1.23$\\times$ and improves per-iteration training time by up-to\n1.73$\\times$ and 1.62$\\times$ respectively. More importantly, MegaFold enables\ntraining on 1.35$\\times$ longer sequence lengths compared to PyTorch baselines\nwithout running out-of-memory, significantly improving the scalability of\nmodern protein folding models. We open source our code at\nhttps://github.com/Supercomputing-System-AI-Lab/MegaFold/."
                },
                "authors": [
                    {
                        "name": "Hoa La"
                    },
                    {
                        "name": "Ahan Gupta"
                    },
                    {
                        "name": "Alex Morehead"
                    },
                    {
                        "name": "Jianlin Cheng"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "arxiv_comment": "13 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v4",
                "updated": "2025-06-24T19:02:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    19,
                    2,
                    8,
                    1,
                    175,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: Application-Guided Profiling for Composing Heterogeneous\n  On-Chip Memories in AI Hardware Accelerators"
                },
                "summary": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive soaring memory requirements, higher-density on-chip\nmemory is needed for domain-specific accelerators beyond what current SRAM\ntechnology can provide. We motivate that algorithms and application behavior\nshould guide the composition of heterogeneous on-chip memories. However, little\nwork has incorporated dynamic application profiles into these design decisions,\nand no existing tools are expressly designed for this purpose. We present\nGainSight, a profiling framework that analyzes fine-grained memory access\npatterns and data lifetimes in domain-specific accelerators. By instrumenting\nretargetable architectural simulator backends with application- and\ndevice-agnostic analytical frontends, GainSight aligns workload-specific\ntraffic and lifetime metrics with mockups of emerging memory devices, informing\nsystem-level heterogeneous memory design. We also present a set of case studies\non MLPerf Inference and PolyBench workloads using simulated GPU and systolic\narray architectures, highlighting the utility of GainSight and the insights it\nprovides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic\narray scratchpad accesses across profiled workloads are short-lived and\nsuitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory\narrays that augment SRAM with GCRAM can reduce active energy consumption by up\nto 66.8%. To facilitate further research in this domain, GainSight is open\nsource at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hofeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19816v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19816v1",
                "updated": "2025-06-24T17:30:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T17:30:27Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    17,
                    30,
                    27,
                    1,
                    175,
                    0
                ],
                "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame\n  Prediction in Manipulation"
                },
                "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Yilun Chen"
                    },
                    {
                        "name": "Yang Tian"
                    },
                    {
                        "name": "Xiaoda Yang"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Feng Zhao"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "36 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19816v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19816v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19549v1",
                "updated": "2025-06-24T11:55:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T11:55:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    11,
                    55,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RCStat: A Statistical Framework for using Relative Contextualization in\n  Transformers"
                },
                "summary": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining."
                },
                "authors": [
                    {
                        "name": "Debabrata Mahapatra"
                    },
                    {
                        "name": "Shubham Agarwal"
                    },
                    {
                        "name": "Apoorv Saxena"
                    },
                    {
                        "name": "Subrata Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Subrata Mitra"
                },
                "author": "Subrata Mitra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19505v1",
                "updated": "2025-06-24T10:45:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T10:45:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    45,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in\n  Large Language Models"
                },
                "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73."
                },
                "authors": [
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Chuanfu Xiao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Mao Yang"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v2",
                "updated": "2025-06-24T09:27:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    27,
                    46,
                    1,
                    175,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (06/2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v1",
                "updated": "2025-06-24T09:00:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17338v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17338v2",
                "updated": "2025-06-24T06:44:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    44,
                    47,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-19T08:28:29Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    8,
                    28,
                    29,
                    3,
                    170,
                    0
                ],
                "title": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning"
                },
                "summary": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of multi-agent systems (MAS) in complex, dynamic\nenvironments necessitates robust and efficient mechanisms for managing shared\nknowledge. A critical challenge is ensuring that distributed memories remain\nsynchronized, relevant, and free from the accumulation of outdated or\ninconsequential data - a process analogous to biological forgetting. This paper\nintroduces the Co-Forgetting Protocol, a novel, comprehensive framework\ndesigned to address this challenge by enabling synchronized memory pruning in\nMAS. The protocol integrates three key components: (1) context-aware semantic\nvoting, where agents utilize a lightweight DistilBERT model to assess the\nrelevance of memory items based on their content and the current operational\ncontext; (2) multi-scale temporal decay functions, which assign diminishing\nimportance to memories based on their age and access frequency across different\ntime horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based\nconsensus mechanism, ensuring that decisions to retain or discard memory items\nare agreed upon by a qualified and fault-tolerant majority of agents, even in\nthe presence of up to f Byzantine (malicious or faulty) agents in a system of N\ngreater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient\ninter-agent communication and Pinecone for scalable vector embedding storage\nand similarity search, with SQLite managing metadata. Experimental evaluations\nin a simulated MAS environment with four agents demonstrate the protocol's\nefficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%\nvoting accuracy in forgetting decisions against human-annotated benchmarks, a\n92% PBFT consensus success rate under simulated Byzantine conditions, and an\n82% cache hit rate for memory access."
                },
                "authors": [
                    {
                        "name": "Duong Bach"
                    }
                ],
                "author_detail": {
                    "name": "Duong Bach"
                },
                "author": "Duong Bach",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17338v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17338v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19225v1",
                "updated": "2025-06-24T01:19:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "published": "2025-06-24T01:19:56Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    19,
                    56,
                    1,
                    175,
                    0
                ],
                "title": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV\n  Sparsification"
                },
                "summary": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds."
                },
                "authors": [
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Xiangrui Liu"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Huaying Yuan"
                    },
                    {
                        "name": "Juenjie Zhou"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "12 pages, 5 Figure, 3 Table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19175v1",
                "updated": "2025-06-23T22:33:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T22:33:58Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    22,
                    33,
                    58,
                    0,
                    174,
                    0
                ],
                "title": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices\n  and Tensors"
                },
                "summary": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrices and tensors are ubiquitous throughout multiple subfields of\ncomputing. The widespread usage of sparse data has inspired many in-memory and\non-disk storage formats, but the only widely adopted storage specifications are\nthe Matrix Market and FROSTT file formats, which both use ASCII text. Due to\nthe inefficiency of text storage, these files typically have larger file sizes\nand longer parsing times than binary storage formats, which directly store an\nin-memory representation to disk. This can be a major bottleneck; since sparse\ncomputation is often bandwidth-bound, the cost of loading or storing a matrix\nto disk often exceeds the cost of performing a sparse computation. While it is\ncommon practice for practitioners to develop their own, custom, non-portable\nbinary formats for high-performance sparse matrix storage, there is currently\nno cross-platform binary sparse matrix storage format. We present Binsparse, a\ncross-platform binary sparse matrix and tensor format specification. Binsparse\nis a modular, embeddable format, consisting of a JSON descriptor, which\ndescribes the matrix or tensor dimensions, type, and format, and a series of\nbinary arrays, which can be stored in all modern binary containers, such as\nHDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse\nspanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our\nBinsparse format on every matrix in the SuiteSparse Matrix Collection and a\nselection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format\nshows file size reductions of 2.4x on average without compression and 7.5x with\ncompression. We evaluate our parser's read/write performance against a\nstate-of-the-art Matrix Market parser, demonstrating warm cache mean read\nspeedups of 26.5x without compression and 2.6x with compression, and write\nspeedups of 31x without compression and 1.4x with compression."
                },
                "authors": [
                    {
                        "name": "Benjamin Brock"
                    },
                    {
                        "name": "Willow Ahrens"
                    },
                    {
                        "name": "Hameer Abbasi"
                    },
                    {
                        "name": "Timothy A. Davis"
                    },
                    {
                        "name": "Juni Kim"
                    },
                    {
                        "name": "James Kitchen"
                    },
                    {
                        "name": "Spencer Patty"
                    },
                    {
                        "name": "Isaac Virshup"
                    },
                    {
                        "name": "Erik Welch"
                    }
                ],
                "author_detail": {
                    "name": "Erik Welch"
                },
                "author": "Erik Welch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18879v1",
                "updated": "2025-06-23T17:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T17:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    17,
                    50,
                    11,
                    0,
                    174,
                    0
                ],
                "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommVQ: Commutative Vector Quantization for KV Cache Compression"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ."
                },
                "authors": [
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Muhammad Yusuf Hassan"
                    },
                    {
                        "name": "Talha Chafekar"
                    },
                    {
                        "name": "Tianle Cai"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Foroozan Karimzadeh"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "arxiv_comment": "ICML 2025 poster",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20722v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20722v3",
                "updated": "2025-06-23T07:59:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    7,
                    59,
                    17,
                    0,
                    174,
                    0
                ],
                "published": "2024-07-30T10:34:40Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    10,
                    34,
                    40,
                    1,
                    212,
                    0
                ],
                "title": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo"
                },
                "summary": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian\ninference but suffer from high computational costs due to their reliance on\nlarge particle ensembles for accurate estimates. We introduce persistent\nsampling (PS), an extension of SMC that systematically retains and reuses\nparticles from all prior iterations to construct a growing, weighted ensemble.\nBy leveraging multiple importance sampling and resampling from a mixture of\nhistorical distributions, PS mitigates the need for excessively large particle\ncounts, directly addressing key limitations of SMC such as particle\nimpoverishment and mode collapse. Crucially, PS achieves this without\nadditional likelihood evaluations-weights for persistent particles are computed\nusing cached likelihood values. This framework not only yields more accurate\nposterior approximations but also produces marginal likelihood estimates with\nsignificantly lower variance, enhancing reliability in model comparison.\nFurthermore, the persistent ensemble enables efficient adaptation of transition\nkernels by leveraging a larger, decorrelated particle pool. Experiments on\nhigh-dimensional Gaussian mixtures, hierarchical models, and non-convex targets\ndemonstrate that PS consistently outperforms standard SMC and related variants,\nincluding recycled and waste-free SMC, achieving substantial reductions in mean\nsquared error for posterior expectations and evidence estimates, all at reduced\ncomputational cost. PS thus establishes itself as a robust, scalable, and\nefficient alternative for complex Bayesian inference tasks."
                },
                "authors": [
                    {
                        "name": "Minas Karamanis"
                    },
                    {
                        "name": "Uro Seljak"
                    }
                ],
                "author_detail": {
                    "name": "Uro Seljak"
                },
                "author": "Uro Seljak",
                "arxiv_comment": "37 pages, 9 figures. Submitted to Statistics & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20722v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20722v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v3",
                "updated": "2025-06-23T03:20:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    20,
                    46,
                    0,
                    174,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill, a general-purpose fast generation\nmethod for any sequence prediction algorithm based on convolutional operators.\nFutureFill reduces generation time from quadratic to quasilinear in the context\nlength. Moreover, when generating from a prompt, it requires a prefill cache\nwhose size grows only with the number of tokens to be generated, often much\nsmaller than the caches required by standard convolutional or attention based\nmodels. We validate our theoretical claims with experiments on synthetic tasks\nand demonstrate substantial efficiency gains when generating from a deep\nconvolutional sequence prediction model."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Devan Shah"
                    },
                    {
                        "name": "Hubert Strauss"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20330v2",
                "updated": "2025-06-23T03:05:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    3,
                    5,
                    26,
                    0,
                    174,
                    0
                ],
                "published": "2025-02-27T17:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    59,
                    36,
                    3,
                    58,
                    0
                ],
                "title": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAPID: Long-Context Inference with Retrieval-Augmented Speculative\n  Decoding"
                },
                "summary": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context large language models (LLMs) offers a promising\nalternative to traditional retrieval-augmented generation (RAG) for processing\nextensive documents. However, the computational overhead of long-context\ninference presents significant efficiency challenges. While Speculative\nDecoding (SD) traditionally accelerates inference using smaller draft models,\nits effectiveness diminishes substantially in long-context scenarios due to\nmemory-bound KV cache operations. We introduce Retrieval-Augmented Speculative\nDecoding (RAPID), which leverages RAG for both accelerating and enhancing\ngeneration quality in long-context inference. RAPID introduces the RAG\ndrafter-a draft LLM operating on shortened retrieval contexts-to speculate on\nthe generation of long-context target LLMs. Our approach enables a new paradigm\nwhere same-scale or even larger LLMs can serve as RAG drafters while\nmaintaining computational efficiency. To fully leverage the potentially\nsuperior capabilities from stronger RAG drafters, we develop an inference-time\nknowledge transfer that enriches the target distribution by RAG. Extensive\nexperiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID\neffectively integrates the strengths of both RAG and long-context LLMs,\nachieving significant performance improvements (e.g., from 39.33 to 42.83 on\nInfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context\ninference. Our analyses also reveal the robustness of RAPID across various\ncontext lengths and retrieval quality."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Qilong Feng"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    }
                ],
                "author_detail": {
                    "name": "Michael Qizhe Shieh"
                },
                "author": "Michael Qizhe Shieh",
                "arxiv_comment": "ICML 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18226v1",
                "updated": "2025-06-23T01:27:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "published": "2025-06-23T01:27:06Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    1,
                    27,
                    6,
                    0,
                    174,
                    0
                ],
                "title": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make It Efficient: Dynamic Sparse Attention for Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive conditional image generation models have emerged as a dominant\nparadigm in text-to-image synthesis. These methods typically convert images\ninto one-dimensional token sequences and leverage the self-attention mechanism,\nwhich has achieved remarkable success in natural language processing, to\ncapture long-range dependencies, model global context, and ensure semantic\ncoherence. However, excessively long contexts during inference lead to\nsignificant memory overhead caused by KV-cache and computational delays. To\nalleviate these challenges, we systematically analyze how global semantics,\nspatial layouts, and fine-grained textures are formed during inference, and\npropose a novel training-free context optimization method called Adaptive\nDynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies\nhistorical tokens crucial for maintaining local texture consistency and those\nessential for ensuring global semantic coherence, thereby efficiently\nstreamlining attention computation. Additionally, we introduce a dynamic\nKV-cache update mechanism tailored for ADSA, reducing GPU memory consumption\nduring inference by approximately $50\\%$. Extensive qualitative and\nquantitative experiments demonstrate the effectiveness and superiority of our\napproach in terms of both generation quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Xunzhi Xiang"
                    },
                    {
                        "name": "Qi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Qi Fan"
                },
                "author": "Qi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v3",
                "updated": "2025-06-22T15:07:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    15,
                    7,
                    37,
                    6,
                    173,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17988v1",
                "updated": "2025-06-22T10:57:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "published": "2025-06-22T10:57:57Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    10,
                    57,
                    57,
                    6,
                    173,
                    0
                ],
                "title": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure User-friendly Blockchain Modular Wallet Design Using Android &\n  OP-TEE"
                },
                "summary": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging crypto economies still hemorrhage digital assets because legacy\nwallets leak private keys at almost every layer of the software stack, from\nuser-space libraries to kernel memory dumps. This paper solves that twin crisis\nof security and interoperability by re-imagining key management as a\nplatform-level service anchored in ARM TrustZone through OP-TEE. Our\narchitecture fractures the traditional monolithic Trusted Application into\nper-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's\nsingle-binary ceiling. A cryptographically sealed firmware-over-the-air\npipeline welds each TA set to an Android system image, enabling hot-swap\nupdates while Verified Boot enforces rollback protection. Every package carries\na chained signature developer first, registry second so even a compromised\nsupply chain cannot smuggle malicious code past the Secure World's RSA-PSS\ngatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and\nGP-compliant crypto APIs ensure secrets never bleed across trust boundaries or\ntiming domains. The Rich Execution Environment can interact only via\nhardware-mediated Secure Monitor Calls, collapsing the surface exposed to\nmalware in Android space. End-users enjoy a single polished interface yet can\ninstall or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap,\nshrinking both storage footprint and audit scope. For auditors, the composition\nmodel slashes duplicated verification effort by quarantining blockchain logic\ninside narrowly scoped modules that share formally specified interfaces. Our\nthreat analysis spans six adversary layers and shows how the design neutralizes\nREE malware sniffing, OTA injection, and cross-module side channels without\nexotic hardware. A reference implementation on AOSP exports a Wallet Manager\nHAL, custom SELinux domains, and a CI/CD pipeline that vet community modules\nbefore release. The result is not merely another hardware wallet but a\nprogrammable substrate that can evolve at the velocity of the blockchain\necosystem. By welding radical extensibility to hardware-anchored assurance, the\nplatform closes the security-usability gap that has long stymied mass-market\nself-custody. We posit that modular TEEs are the missing OS primitive for Web3,\nmuch as virtual memory unlocked multi-tasking in classical computing. Together,\nthese contributions sketch a blueprint for multi-chain asset management that is\nauditable, resilient, and poised for global deployment."
                },
                "authors": [
                    {
                        "name": "Seongjin Kim"
                    },
                    {
                        "name": "Sanguk Yun"
                    },
                    {
                        "name": "Jungho Jang"
                    }
                ],
                "author_detail": {
                    "name": "Jungho Jang"
                },
                "author": "Jungho Jang",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17331v2",
                "updated": "2025-06-22T03:46:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    22,
                    3,
                    46,
                    11,
                    6,
                    173,
                    0
                ],
                "published": "2025-05-22T22:54:21Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    22,
                    54,
                    21,
                    3,
                    142,
                    0
                ],
                "title": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training"
                },
                "summary": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to\nimprove both the training speed and inference throughput of LLaMA architectures\nwhile maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models\ninto shared KV caching across certain layers, significantly reducing KV\ncomputational complexity while maintaining or improving language performance.\nExperimental results demonstrate that ECHO-LLaMA achieves up to 77\\% higher\ntoken-per-second throughput during training, up to 16\\% higher Model FLOPs\nUtilization (MFU), and up to 14\\% lower loss when trained on an equal number of\ntokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\\%\nhigher test-time throughput compared to the baseline. By introducing a\ncomputationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable\nand cost-effective solution for pretraining and finetuning large language\nmodels, enabling faster and more resource-efficient training without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Maryam Dialameh"
                    },
                    {
                        "name": "Rezaul Karim"
                    },
                    {
                        "name": "Hossein Rajabzadeh"
                    },
                    {
                        "name": "Omar Mohamed Awad"
                    },
                    {
                        "name": "Hyock Ju Kwon"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Walid Ahmed"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10805v2",
                "updated": "2025-06-21T08:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    21,
                    8,
                    27,
                    10,
                    5,
                    172,
                    0
                ],
                "published": "2023-04-21T08:22:58Z",
                "published_parsed": [
                    2023,
                    4,
                    21,
                    8,
                    22,
                    58,
                    4,
                    111,
                    0
                ],
                "title": "RPLKG: Robust Prompt Learning with Knowledge Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPLKG: Robust Prompt Learning with Knowledge Graph"
                },
                "summary": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale pre-trained models surpass in transferability and robust\ngeneralization across diverse datasets. The emergence of multimodal pre-trained\nmodels like CLIP has significantly boosted performance in various experiments.\nHowever, generalizing to new datasets or domains remains challenging,\nespecially with limited labeled data. Also, existing methods often lack\ninterpretability and impose high computational costs. To address this, we\npropose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the\nknowledge graph to curate diverse, interpretable prompt sets automatically. Our\nmethod autonomously selects the optimal interpretable prompt based on dataset\ncharacteristics, achieving performance improvements over zero-shot learning and\ncompetitive performance compared to various prompt learning methods. Also,\nRPLKG efficiently reuses cached prompt embeddings from a single model pass and\noptimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast\ntraining. Moreover, RPLKG advances few-shot learning effectiveness while\nenhancing interpretability and efficiency in model adaptation. Our"
                },
                "authors": [
                    {
                        "name": "YongTaek Lim"
                    },
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Suho Kang"
                    },
                    {
                        "name": "Dokyung Yoon"
                    },
                    {
                        "name": "KyungWoo Song"
                    }
                ],
                "author_detail": {
                    "name": "KyungWoo Song"
                },
                "author": "KyungWoo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17121v1",
                "updated": "2025-06-20T16:21:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T16:21:12Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    16,
                    21,
                    12,
                    4,
                    171,
                    0
                ],
                "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context\n  LMs?"
                },
                "summary": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint."
                },
                "authors": [
                    {
                        "name": "Adithya Bhaskar"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "Tianyu Gao"
                    },
                    {
                        "name": "Yihe Dong"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16976v1",
                "updated": "2025-06-20T13:09:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "published": "2025-06-20T13:09:26Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    13,
                    9,
                    26,
                    4,
                    171,
                    0
                ],
                "title": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along"
                },
                "summary": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving."
                },
                "authors": [
                    {
                        "name": "Arthur Bernhardt"
                    },
                    {
                        "name": "Sajjad Tamimi"
                    },
                    {
                        "name": "Florian Stock"
                    },
                    {
                        "name": "Andreas Koch"
                    },
                    {
                        "name": "Ilia Petrov"
                    }
                ],
                "author_detail": {
                    "name": "Ilia Petrov"
                },
                "author": "Ilia Petrov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12708v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12708v3",
                "updated": "2025-06-19T12:27:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    12,
                    27,
                    10,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-15T03:41:34Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    3,
                    41,
                    34,
                    6,
                    166,
                    0
                ],
                "title": "Serving Large Language Models on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models on Huawei CloudMatrix384"
                },
                "summary": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\nper NPU even under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks."
                },
                "authors": [
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Huimin Lin"
                    },
                    {
                        "name": "Junbo Deng"
                    },
                    {
                        "name": "Nan Zou"
                    },
                    {
                        "name": "Xingkun Yang"
                    },
                    {
                        "name": "Yingyu Diao"
                    },
                    {
                        "name": "Weifeng Gao"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Zhangyu Chen"
                    },
                    {
                        "name": "Shirui Lu"
                    },
                    {
                        "name": "Zhao Qiu"
                    },
                    {
                        "name": "Peiyang Li"
                    },
                    {
                        "name": "Xianyu Chang"
                    },
                    {
                        "name": "Zhengzhong Yu"
                    },
                    {
                        "name": "Fangzheng Miao"
                    },
                    {
                        "name": "Jia Zheng"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Bei Wang"
                    },
                    {
                        "name": "Zaijian Zong"
                    },
                    {
                        "name": "Mosong Zhou"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Houjiang Chen"
                    },
                    {
                        "name": "Xingyu Liao"
                    },
                    {
                        "name": "Yipeng Li"
                    },
                    {
                        "name": "Wenxiao Zhang"
                    },
                    {
                        "name": "Ping Zhu"
                    },
                    {
                        "name": "Yinggang Wang"
                    },
                    {
                        "name": "Chuanjie Xiao"
                    },
                    {
                        "name": "Depeng Liang"
                    },
                    {
                        "name": "Dong Cao"
                    },
                    {
                        "name": "Juncheng Liu"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Huaguo Xie"
                    },
                    {
                        "name": "Huatao Wu"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Lv Chen"
                    },
                    {
                        "name": "Hu Liu"
                    },
                    {
                        "name": "Yujun Ding"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Jing Xia"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Heng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Heng Liao"
                },
                "author": "Heng Liao",
                "arxiv_comment": "59 pages, 24 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12708v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12708v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/1604.01713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/1604.01713v2",
                "updated": "2025-06-19T10:23:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    23,
                    50,
                    3,
                    170,
                    0
                ],
                "published": "2016-04-06T18:07:19Z",
                "published_parsed": [
                    2016,
                    4,
                    6,
                    18,
                    7,
                    19,
                    2,
                    97,
                    0
                ],
                "title": "A block Recycled GMRES method with investigations into aspects of solver\n  performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A block Recycled GMRES method with investigations into aspects of solver\n  performance"
                },
                "summary": "We propose a block Krylov subspace version of the GCRO-DR method proposed in\n[Parks et al.; SISC 2005], which is an iterative method allowing for the\nefficient minimization of the the residual over an augmented Krylov subspace.\nWe offer a clean derivation of our proposed method and discuss methods of\nselecting recycling subspaces at restart as well as implementation decisions in\nthe context of high-performance computing. Numerical experiments are split into\nthose demonstrating convergence properties and those demonstrating the data\nmovement and cache efficiencies of the dominant operations of the method,\nmeasured using processor monitoring code from Intel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a block Krylov subspace version of the GCRO-DR method proposed in\n[Parks et al.; SISC 2005], which is an iterative method allowing for the\nefficient minimization of the the residual over an augmented Krylov subspace.\nWe offer a clean derivation of our proposed method and discuss methods of\nselecting recycling subspaces at restart as well as implementation decisions in\nthe context of high-performance computing. Numerical experiments are split into\nthose demonstrating convergence properties and those demonstrating the data\nmovement and cache efficiencies of the dominant operations of the method,\nmeasured using processor monitoring code from Intel."
                },
                "authors": [
                    {
                        "name": "Michael L. Parks"
                    },
                    {
                        "name": "Kirk M. Soodhalter"
                    },
                    {
                        "name": "Daniel B. Szyld"
                    }
                ],
                "author_detail": {
                    "name": "Daniel B. Szyld"
                },
                "author": "Daniel B. Szyld",
                "arxiv_comment": "35 pages, 26 pages of manuscript text, 13 figures, 1 table, Temple\n  University Research Report 16-04-04",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/1604.01713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/1604.01713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16192v1",
                "updated": "2025-06-19T10:17:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T10:17:28Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    10,
                    17,
                    28,
                    3,
                    170,
                    0
                ],
                "title": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of discharge capillaries via benchmarked hydrodynamic\n  plasma simulations"
                },
                "summary": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plasma accelerators utilize strong electric fields in plasma waves to\naccelerate charged particles, making them a compact alternative to\nradiofrequency technologies. Discharge capillaries are plasma sources used in\nplasma accelerator research to provide acceleration targets, or as plasma\nlenses to capture or focus accelerated beams. They have applications for\nbeam-driven and laser-driven plasma accelerators and can sustain high\nrepetition rates for extended periods of time. Despite these advantages,\nhigh-fidelity simulations of discharge capillaries remain challenging due to\nthe range of mechanisms involved and the difficulty to diagnose them in\nexperiments. In this work, we utilize hydrodynamic plasma simulations to\nexamine the discharge process of a plasma cell and discuss implications for\nfuture accelerator systems. The simulation model is validated with experimental\nmeasurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV\ndischarge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is\nshown to deposit 178mJ of energy in the plasma. Potential difficulties with the\ncommon density measurement method using H{\\alpha} emission spectroscopy are\ndiscussed. This simulation model enables investigations of repeatability, heat\nflow management and fine tailoring of the plasma profile with discharges."
                },
                "authors": [
                    {
                        "name": "S. M. Mewes"
                    },
                    {
                        "name": "G. J. Boyle"
                    },
                    {
                        "name": "R. D'Arcy"
                    },
                    {
                        "name": "J. M. Garland"
                    },
                    {
                        "name": "M. Huck"
                    },
                    {
                        "name": "H. Jones"
                    },
                    {
                        "name": "G. Loisch"
                    },
                    {
                        "name": "A. R. Maier"
                    },
                    {
                        "name": "J. Osterhoff"
                    },
                    {
                        "name": "T. Parikh"
                    },
                    {
                        "name": "S. Wesch"
                    },
                    {
                        "name": "J. C. Wood"
                    },
                    {
                        "name": "M. Thvenet"
                    }
                ],
                "author_detail": {
                    "name": "M. Thvenet"
                },
                "author": "M. Thvenet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07575v2",
                "updated": "2025-06-19T07:29:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    7,
                    29,
                    9,
                    3,
                    170,
                    0
                ],
                "published": "2024-07-10T12:08:39Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    12,
                    8,
                    39,
                    2,
                    192,
                    0
                ],
                "title": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource Allocation for Twin Maintenance and Computing Task Processing\n  in Digital Twin Vehicular Edge Computing Network"
                },
                "summary": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising technology, vehicular edge computing (VEC) can provide\ncomputing and caching services by deploying VEC servers near vehicles. However,\nVEC networks still face challenges such as high vehicle mobility. Digital twin\n(DT), an emerging technology, can predict, estimate, and analyze real-time\nstates by digitally modeling objects in the physical world. By integrating DT\nwith VEC, a virtual vehicle DT can be created in the VEC server to monitor the\nreal-time operating status of vehicles. However, maintaining the vehicle DT\nmodel requires ongoing attention from the VEC server, which also needs to offer\ncomputing services for the vehicles. Therefore, effective allocation and\nscheduling of VEC server resources are crucial. This study focuses on a general\nVEC network with a single VEC service and multiple vehicles, examining the two\ntypes of delays caused by twin maintenance and computational processing within\nthe network. By transforming the problem using satisfaction functions, we\npropose an optimization problem aimed at maximizing each vehicle's resource\nutility to determine the optimal resource allocation strategy. Given the\nnon-convex nature of the issue, we employ multi-agent Markov decision processes\nto reformulate the problem. Subsequently, we propose the twin maintenance and\ncomputing task processing resource collaborative scheduling (MADRL-CSTC)\nalgorithm, which leverages multi-agent deep reinforcement learning. Through\nexperimental comparisons with alternative algorithms, it demonstrates that our\nproposed approach is effective in terms of resource allocation."
                },
                "authors": [
                    {
                        "name": "Yu Xie"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Jiangzhou Wang"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been accepted by IEEE Internet of Things Journal. The\n  source code has been released\n  at:https://github.com/qiongwu86/Resource-allocation-for-twin-maintenance-and-computing-tasks-in-digital-twin-mobile-edge-network",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v1",
                "updated": "2025-06-19T02:25:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced reasoning capabilities by\nemploying Chain-of-Thought (CoT). However, the extended reasoning sequences\nintroduce significant GPU memory overhead due to increased key-value (KV) cache\nsize, particularly in tasks requiring long reasoning sequences, such as\nmathematics and programming. Existing KV cache compression methods mitigate\nmemory bottlenecks but struggle in long reasoning tasks. In this paper, we\nanalyze attention patterns in reasoning tasks and reveal a Token Importance\nRecurrence phenomenon: a large proportion of tokens receive renewed attention\nafter multiple decoding steps, which is failed to capture by existing works and\nmay lead to unpredictable eviction on such periodically critical tokens. To\naddress this, we propose LazyEviction, a lagged KV eviction framework designed\nto maintain reasoning performance while reducing KV memory. LazyEviction is an\nObservation Window-based Lagged Eviction Mechanism retaining latent recurring\ntokens by performing lagged evictions across decoding steps, which contains two\nkey components: (1) Recurrence Interval Tracking for capturing temporal\nvariations in token importance, and (2) an Maximum Recurrence Interval-Centric\nEviction Policy that prioritizes eviction based on tokens' recurrence patterns.\nExtensive experiments demonstrate that LazyEviction reduces KV cache size by\n50% while maintaining comparable accuracy on mathematics reasoning datasets,\noutperforming state-of-the-art methods. Our findings highlight the importance\nof preserving recurring tokens, which are critical for maintaining knowledge\ncontinuity in multi-step reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v3",
                "updated": "2025-06-19T02:18:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    18,
                    16,
                    3,
                    170,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v4",
                "updated": "2025-06-18T22:51:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    22,
                    51,
                    6,
                    2,
                    169,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "igo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15682v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15682v2",
                "updated": "2025-07-01T21:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    21,
                    27,
                    40,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-18T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    59,
                    50,
                    2,
                    169,
                    0
                ],
                "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model"
                },
                "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad."
                },
                "authors": [
                    {
                        "name": "Anirud Aggarwal"
                    },
                    {
                        "name": "Abhinav Shrivastava"
                    },
                    {
                        "name": "Matthew Gwilliam"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Gwilliam"
                },
                "author": "Matthew Gwilliam",
                "arxiv_comment": "29 pages, 22 figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15682v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15682v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15645v1",
                "updated": "2025-06-18T17:14:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T17:14:07Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    14,
                    7,
                    2,
                    169,
                    0
                ],
                "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer."
                },
                "authors": [
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Hongyuan Hua"
                    },
                    {
                        "name": "Seoyoung Lee"
                    },
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15613v1",
                "updated": "2025-06-18T16:44:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T16:44:04Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    16,
                    44,
                    4,
                    2,
                    169,
                    0
                ],
                "title": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and\n  Instruction Annotation"
                },
                "summary": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores how Compute Express Link (CXL) can transform PCIe-based\nblock storage into a scalable, byte-addressable working memory. We address the\nchallenges of adapting block storage to CXL's memory-centric model by\nemphasizing cacheability as a key enabler and advocating for Type 3 endpoint\ndevices, referred to as CXL-SSDs. To validate our approach, we prototype a\nCXL-SSD on a custom FPGA platform and propose annotation mechanisms,\nDeterminism and Bufferability, to enhance performance while preserving data\npersistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves\n10.9x better performance than PCIe-based memory expanders and further reduces\nlatency by 5.4x with annotation enhancements. In workloads with high locality,\nCXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This\nwork highlights the feasibility of integrating block storage into CXL's\necosystem and provides a foundation for future memory-storage convergence."
                },
                "authors": [
                    {
                        "name": "Miryeong Kwon"
                    },
                    {
                        "name": "Donghyun Gouk"
                    },
                    {
                        "name": "Junhyeok Jang"
                    },
                    {
                        "name": "Jinwoo Baek"
                    },
                    {
                        "name": "Hyunwoo You"
                    },
                    {
                        "name": "Sangyoon Ji"
                    },
                    {
                        "name": "Hongjoo Jung"
                    },
                    {
                        "name": "Junseok Moon"
                    },
                    {
                        "name": "Seungkwan Kang"
                    },
                    {
                        "name": "Seungjun Lee"
                    },
                    {
                        "name": "Myoungsoo Jung"
                    }
                ],
                "author_detail": {
                    "name": "Myoungsoo Jung"
                },
                "author": "Myoungsoo Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16839v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16839v3",
                "updated": "2025-06-18T15:17:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    15,
                    17,
                    40,
                    2,
                    169,
                    0
                ],
                "published": "2025-05-22T16:07:12Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    16,
                    7,
                    12,
                    3,
                    142,
                    0
                ],
                "title": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaViDa: A Large Diffusion Language Model for Multimodal Understanding"
                },
                "summary": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Vision-Language Models (VLMs) can solve a wide range of tasks\nrequiring visual reasoning. In real-world scenarios, desirable properties for\nVLMs include fast inference and controllable generation (e.g., constraining\noutputs to adhere to a desired format). However, existing autoregressive (AR)\nVLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs)\noffer a promising alternative, enabling parallel decoding for faster inference\nand bidirectional context for controllable generation through text-infilling.\nWhile effective in language-only settings, DMs' potential for multimodal tasks\nis underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build\nLaViDa by equipping DMs with a vision encoder and jointly fine-tune the\ncombined parts for multimodal instruction following. To address challenges\nencountered, LaViDa incorporates novel techniques such as complementary masking\nfor effective training, prefix KV cache for efficient inference, and timestep\nshifting for high-quality sampling. Experiments show that LaViDa achieves\ncompetitive or superior performance to AR VLMs on multi-modal benchmarks such\nas MMMU, while offering unique advantages of DMs, including flexible\nspeed-quality tradeoff, controllability, and bidirectional reasoning. On COCO\ncaptioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x\nspeedup. On bidirectional tasks, it achieves +59% improvement on Constrained\nPoem Completion. These results demonstrate LaViDa as a strong alternative to AR\nVLMs. Code and models will be released in the camera-ready version."
                },
                "authors": [
                    {
                        "name": "Shufan Li"
                    },
                    {
                        "name": "Konstantinos Kallidromitis"
                    },
                    {
                        "name": "Hritik Bansal"
                    },
                    {
                        "name": "Akash Gokul"
                    },
                    {
                        "name": "Yusuke Kato"
                    },
                    {
                        "name": "Kazuki Kozuka"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Zhe Lin"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "26 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16839v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16839v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14168v2",
                "updated": "2025-06-18T09:44:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    9,
                    44,
                    9,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-17T04:08:18Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    8,
                    18,
                    1,
                    168,
                    0
                ],
                "title": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoMAR: Autoregressive Video Generatio with Continuous Tokens"
                },
                "summary": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked-based autoregressive models have demonstrated promising image\ngeneration capability in continuous space. However, their potential for video\ngeneration remains under-explored. In this paper, we propose \\textbf{VideoMAR},\na concise and efficient decoder-only autoregressive image-to-video model with\ncontinuous tokens, composing temporal frame-by-frame and spatial masked\ngeneration. We first identify temporal causality and spatial bi-directionality\nas the first principle of video AR models, and propose the next-frame diffusion\nloss for the integration of mask and video generation. Besides, the huge cost\nand difficulty of long sequence autoregressive modeling is a basic but crucial\nissue. To this end, we propose the temporal short-to-long curriculum learning\nand spatial progressive resolution training, and employ progressive temperature\nstrategy at inference time to mitigate the accumulation error. Furthermore,\nVideoMAR replicates several unique capacities of language models to video\ngeneration. It inherently bears high efficiency due to simultaneous\ntemporal-wise KV cache and spatial-wise parallel generation, and presents the\ncapacity of spatial and temporal extrapolation via 3D rotary embeddings. On the\nVBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos\nI2V) while requiring significantly fewer parameters ($9.3\\%$), training data\n($0.5\\%$), and GPU resources ($0.2\\%$)."
                },
                "authors": [
                    {
                        "name": "Hu Yu"
                    },
                    {
                        "name": "Biao Gong"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "DanDan Zheng"
                    },
                    {
                        "name": "Weilong Chai"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Kecheng Zheng"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21593v1",
                "updated": "2025-06-18T07:54:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    7,
                    54,
                    53,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T07:54:53Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    7,
                    54,
                    53,
                    2,
                    169,
                    0
                ],
                "title": "PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM\n  Applications"
                },
                "summary": "Enterprise deployments of large-language model (LLM) demand continuously\nchanging document collections with sub-second latency and predictable GPU cost\nrequirements that classical Retrieval-Augmented Generation (RAG) pipelines only\npartially satisfy. We present PentaRAG, a five-layer module that routes each\nquery through two instant caches (fixed key-value and semantic), a\nmemory-recall mode that exploits the LLM's own weights, an adaptive session\nmemory, and a conventional retrieval-augmentation layer. Implemented with\nMistral-8B, Milvus and vLLM, the system can answer most repeated or\nsemantically similar questions from low-latency caches while retaining full\nretrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined\nwith the memory-recall layer raises answer similarity by approximately 8% and\nfactual correctness by approximately 16% over the base model. Under a\nnine-session runtime simulation, cache warming reduces mean latency from\nseveral seconds to well below one second and shifts traffic toward the fast\npaths. Resource-efficiency tests show that PentaRAG cuts average GPU time to\n0.248 seconds per query, roughly half that of a naive RAG baseline, and\nsustains an aggregate throughput of approximately 100,000 queries per second on\nour setup. These results demonstrate that a layered routing strategy can\ndeliver freshness, speed, and efficiency simultaneously in production-grade RAG\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise deployments of large-language model (LLM) demand continuously\nchanging document collections with sub-second latency and predictable GPU cost\nrequirements that classical Retrieval-Augmented Generation (RAG) pipelines only\npartially satisfy. We present PentaRAG, a five-layer module that routes each\nquery through two instant caches (fixed key-value and semantic), a\nmemory-recall mode that exploits the LLM's own weights, an adaptive session\nmemory, and a conventional retrieval-augmentation layer. Implemented with\nMistral-8B, Milvus and vLLM, the system can answer most repeated or\nsemantically similar questions from low-latency caches while retaining full\nretrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined\nwith the memory-recall layer raises answer similarity by approximately 8% and\nfactual correctness by approximately 16% over the base model. Under a\nnine-session runtime simulation, cache warming reduces mean latency from\nseveral seconds to well below one second and shifts traffic toward the fast\npaths. Resource-efficiency tests show that PentaRAG cuts average GPU time to\n0.248 seconds per query, roughly half that of a naive RAG baseline, and\nsustains an aggregate throughput of approximately 100,000 queries per second on\nour setup. These results demonstrate that a layered routing strategy can\ndeliver freshness, speed, and efficiency simultaneously in production-grade RAG\nsystems."
                },
                "authors": [
                    {
                        "name": "Abu Hanif Muhammad Syarubany"
                    },
                    {
                        "name": "Chang Dong Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Chang Dong Yoo"
                },
                "author": "Chang Dong Yoo",
                "arxiv_comment": "Annual Conference of The Institute of Electronics and Information\n  Engineers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15174v1",
                "updated": "2025-06-18T06:41:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T06:41:35Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    6,
                    41,
                    35,
                    2,
                    169,
                    0
                ],
                "title": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in\n  GPUs"
                },
                "summary": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse data structures are commonly used in neural networks to reduce the\nmemory footprint. These data structures are compact but cause irregularities\nsuch as random memory accesses, which prevent efficient use of the memory\nhierarchy. GPUs are a common platform for machine learning practitioners, but\nrunning compact data structures on these devices often leads to slow-downs due\nto inefficient use of computing and memory resources. This paper proposes a new\ncompiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse\nmatrix-matrix multiplication (SPMM) on GPU devices. The transformation\nincreases data reuse in registers and caches while creating more balanced\nworkloads for GPU computing resources. The transformation is tested on sparse\nneural networks in convolutional and transformer models. On an A100 GPU and\nacross a columns of matrix B (bCols) in $ A \\times B = C$ from range of 32 to\n128, the transformation yields a geometric mean speedup of 1.84$\\times$ to\n2.27$\\times$ compared to cuBLAS and cuSPARSE baselines, respectively."
                },
                "authors": [
                    {
                        "name": "Hossein Albakri"
                    },
                    {
                        "name": "Kazem Cheshmi"
                    }
                ],
                "author_detail": {
                    "name": "Kazem Cheshmi"
                },
                "author": "Kazem Cheshmi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15155v1",
                "updated": "2025-06-18T05:56:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T05:56:01Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    56,
                    1,
                    2,
                    169,
                    0
                ],
                "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving"
                },
                "summary": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly being deployed in datacenters. Serving\nthese models requires careful memory management, as their memory usage includes\nstatic weights, dynamic activations, and key-value caches. While static weights\nare constant and predictable, dynamic components such as activations and KV\ncaches change frequently during runtime, presenting significant challenges for\nefficient memory management. Modern LLM serving systems typically handle\nruntime memory and KV caches at distinct abstraction levels: runtime memory\nmanagement relies on static tensor abstractions, whereas KV caches utilize a\npage table-based virtualization layer built on top of the tensor abstraction.\nThis virtualization dynamically manages KV caches to mitigate memory\nfragmentation. However, this dual-level approach fundamentally isolates runtime\nmemory and KV cache management, resulting in suboptimal memory utilization\nunder dynamic workloads, which can lead to a nearly 20% drop in throughput.\n  To address these limitations, we propose eLLM, an elastic memory management\nframework inspired by the classical memory ballooning mechanism in operating\nsystems. The core components of eLLM include: (1) Virtual Tensor Abstraction,\nwhich decouples the virtual address space of tensors from the physical GPU\nmemory, creating a unified and flexible memory pool; (2) an Elastic Memory\nMechanism that dynamically adjusts memory allocation through runtime memory\ninflation and deflation, leveraging CPU memory as an extensible buffer; and (3)\na Lightweight Scheduling Strategy employing SLO-aware policies to optimize\nmemory utilization and effectively balance performance trade-offs under\nstringent SLO constraints. Comprehensive evaluations demonstrate that eLLM\nsignificantly outperforms state-of-the-art systems, 2.32x higher decoding\nthroughput, and supporting 3x larger batch sizes for 128K-token inputs."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Yangjie Zhou"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Yongjie Yuan"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21590v1",
                "updated": "2025-06-18T05:07:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T05:07:47Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "title": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation"
                },
                "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Salim I. Amoukou"
                    },
                    {
                        "name": "Francesco Leofante"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22463v1",
                "updated": "2025-06-18T03:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    3,
                    31,
                    53,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T03:31:53Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    3,
                    31,
                    53,
                    2,
                    169,
                    0
                ],
                "title": "Modulated Diffusion: Accelerating Generative Modeling with Modulated\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modulated Diffusion: Accelerating Generative Modeling with Modulated\n  Quantization"
                },
                "summary": "Diffusion models have emerged as powerful generative models, but their high\ncomputation cost in iterative sampling remains a significant bottleneck. In\nthis work, we present an in-depth and insightful study of state-of-the-art\nacceleration techniques for diffusion models, including caching and\nquantization, revealing their limitations in computation error and generation\nquality. To break these limits, this work introduces Modulated Diffusion\n(MoDiff), an innovative, rigorous, and principled framework that accelerates\ngenerative modeling through modulated quantization and error compensation.\nMoDiff not only inherents the advantages of existing caching and quantization\nmethods but also serves as a general framework to accelerate all diffusion\nmodels. The advantages of MoDiff are supported by solid theoretical insight and\nanalysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate\nthat MoDiff significant reduces activation quantization from 8 bits to 3 bits\nwithout performance degradation in post-training quantization (PTQ). Our code\nimplementation is available at https://github.com/WeizhiGao/MoDiff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as powerful generative models, but their high\ncomputation cost in iterative sampling remains a significant bottleneck. In\nthis work, we present an in-depth and insightful study of state-of-the-art\nacceleration techniques for diffusion models, including caching and\nquantization, revealing their limitations in computation error and generation\nquality. To break these limits, this work introduces Modulated Diffusion\n(MoDiff), an innovative, rigorous, and principled framework that accelerates\ngenerative modeling through modulated quantization and error compensation.\nMoDiff not only inherents the advantages of existing caching and quantization\nmethods but also serves as a general framework to accelerate all diffusion\nmodels. The advantages of MoDiff are supported by solid theoretical insight and\nanalysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate\nthat MoDiff significant reduces activation quantization from 8 bits to 3 bits\nwithout performance degradation in post-training quantization (PTQ). Our code\nimplementation is available at https://github.com/WeizhiGao/MoDiff."
                },
                "authors": [
                    {
                        "name": "Weizhi Gao"
                    },
                    {
                        "name": "Zhichao Hou"
                    },
                    {
                        "name": "Junqi Yin"
                    },
                    {
                        "name": "Feiyi Wang"
                    },
                    {
                        "name": "Linyu Peng"
                    },
                    {
                        "name": "Xiaorui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaorui Liu"
                },
                "author": "Xiaorui Liu",
                "arxiv_comment": "26 pages, accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v1",
                "updated": "2025-06-18T02:22:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time--quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and two streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy--even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15057v1",
                "updated": "2025-06-18T01:37:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "published": "2025-06-18T01:37:55Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    1,
                    37,
                    55,
                    2,
                    169,
                    0
                ],
                "title": "Compatibility of trapped ions and dielectrics at cryogenic temperatures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compatibility of trapped ions and dielectrics at cryogenic temperatures"
                },
                "summary": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the impact of an unshielded dielectric $\\unicode{x2013}$ here, a\nbare optical fiber $\\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several\nhundred $\\mu$m away in a cryogenic surface electrode trap. We observe\ndistance-dependent stray electric fields of up to a few kV/m due to the\ndielectric, which drift on average less than 10% per month and can be fully\ncompensated with reasonable voltages on the trap electrodes. We observe ion\nmotional heating rates attributable to the dielectric of $\\approx$30 quanta per\nsecond at an ion-fiber distance of 215(4) $\\mu$m and $\\approx$1.5 MHz motional\nfrequency. These results demonstrate the viability of using unshielded,\ntrap-integrated dielectric objects such as miniature optical cavities or other\noptical elements in cryogenic surface electrode ion traps."
                },
                "authors": [
                    {
                        "name": "M. Bruff"
                    },
                    {
                        "name": "L. Sonderhouse"
                    },
                    {
                        "name": "K. N. David"
                    },
                    {
                        "name": "J. Stuart"
                    },
                    {
                        "name": "D. H. Slichter"
                    },
                    {
                        "name": "D. Leibfried"
                    }
                ],
                "author_detail": {
                    "name": "D. Leibfried"
                },
                "author": "D. Leibfried",
                "arxiv_comment": "MB and LS contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14769v1",
                "updated": "2025-06-17T17:59:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T17:59:12Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    17,
                    59,
                    12,
                    1,
                    168,
                    0
                ],
                "title": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal\n  Diffusion"
                },
                "summary": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy (DP) enables robots to learn complex behaviors by imitating\nexpert demonstrations through action diffusion. However, in practical\napplications, hardware limitations often degrade data quality, while real-time\nconstraints restrict model inference to instantaneous state and scene\nobservations. These limitations seriously reduce the efficacy of learning from\nexpert demonstrations, resulting in failures in object localization, grasp\nplanning, and long-horizon task execution. To address these challenges, we\npropose Causal Diffusion Policy (CDP), a novel transformer-based diffusion\nmodel that enhances action prediction by conditioning on historical action\nsequences, thereby enabling more coherent and context-aware visuomotor policy\nlearning. To further mitigate the computational cost associated with\nautoregressive inference, a caching mechanism is also introduced to store\nattention key-value pairs from previous timesteps, substantially reducing\nredundant computations during execution. Extensive experiments in both\nsimulated and real-world environments, spanning diverse 2D and 3D manipulation\ntasks, demonstrate that CDP uniquely leverages historical action sequences to\nachieve significantly higher accuracy than existing methods. Moreover, even\nwhen faced with degraded input observation quality, CDP maintains remarkable\nprecision by reasoning through temporal continuity, which highlights its\npractical robustness for robotic control under realistic, imperfect conditions."
                },
                "authors": [
                    {
                        "name": "Jiahua Ma"
                    },
                    {
                        "name": "Yiran Qin"
                    },
                    {
                        "name": "Yixiong Li"
                    },
                    {
                        "name": "Xuanqi Liao"
                    },
                    {
                        "name": "Yulan Guo"
                    },
                    {
                        "name": "Ruimao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ruimao Zhang"
                },
                "author": "Ruimao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14630v1",
                "updated": "2025-06-17T15:25:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T15:25:11Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    15,
                    25,
                    11,
                    1,
                    168,
                    0
                ],
                "title": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a\n  Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)"
                },
                "summary": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Keigo, a concurrency- and workload-aware storage middleware that\nenhances the performance of log-structured merge key-value stores (LSM KVS)\nwhen they are deployed on a hierarchy of storage devices. The key observation\nbehind Keigo is that there is no one-size-fits-all placement of data across the\nstorage hierarchy that optimizes for all workloads. Hence, to leverage the\nbenefits of combining different storage devices, Keigo places files across\ndifferent devices based on their parallelism, I/O bandwidth, and capacity. We\nintroduce three techniques - concurrency-aware data placement, persistent\nread-only caching, and context-based I/O differentiation. Keigo is portable\nacross different LSMs, is adaptable to dynamic workloads, and does not require\nextensive profiling. Our system enables established production KVS such as\nRocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We\nevaluate Keigo using synthetic and realistic workloads, showing that it\nimproves the throughput of production-grade LSMs up to 4x for write- and 18x\nfor read-heavy workloads when compared to general-purpose storage systems and\nspecialized LSM KVS."
                },
                "authors": [
                    {
                        "name": "Rben Ado"
                    },
                    {
                        "name": "Zhongjie Wu"
                    },
                    {
                        "name": "Changjun Zhou"
                    },
                    {
                        "name": "Oana Balmau"
                    },
                    {
                        "name": "Joo Paulo"
                    },
                    {
                        "name": "Ricardo Macedo"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Macedo"
                },
                "author": "Ricardo Macedo",
                "arxiv_comment": "This is an extended version of the full paper to appear in VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17421v2",
                "updated": "2025-06-17T05:58:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    5,
                    58,
                    1,
                    1,
                    168,
                    0
                ],
                "published": "2025-02-24T18:53:31Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    18,
                    53,
                    31,
                    0,
                    55,
                    0
                ],
                "title": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongSpec: Long-Context Lossless Speculative Decoding with Efficient\n  Drafting and Verification"
                },
                "summary": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) can now process extremely long contexts,\nefficient inference over these extended inputs has become increasingly\nimportant, especially for emerging applications like LLM agents that highly\ndepend on this capability. Speculative decoding (SD) offers a promising\nlossless acceleration technique compared to lossy alternatives such as\nquantization and model cascades. However, most state-of-the-art SD methods are\ntrained on short texts (typically fewer than 4k tokens), making them unsuitable\nfor long-context scenarios. Specifically, adapting these methods to long\ncontexts presents three key challenges: (1) the excessive memory demands posed\nby draft models due to large Key-Value (KV) cache; (2) performance degradation\nresulting from the mismatch between short-context training and long-context\ninference; and (3) inefficiencies in tree attention mechanisms when managing\nlong token sequences. This work introduces LongSpec, a framework that addresses\nthese challenges through three core innovations: a memory-efficient draft model\nwith a constant-sized KV cache; novel position indices that mitigate the\ntraining-inference mismatch; and an attention aggregation strategy that\ncombines fast prefix computation with standard tree attention to enable\nefficient decoding. Experimental results confirm the effectiveness of LongSpec,\nachieving up to a 3.26x speedup over strong Flash Attention baselines across\nfive long-context understanding datasets, as well as a 2.25x reduction in\nwall-clock time on the AIME24 long reasoning task with the QwQ model,\ndemonstrating significant latency improvements for long-context applications.\nThe code is available at https://github.com/sail-sg/LongSpec."
                },
                "authors": [
                    {
                        "name": "Penghui Yang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14852v1",
                "updated": "2025-06-17T04:42:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "published": "2025-06-17T04:42:30Z",
                "published_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    42,
                    30,
                    1,
                    168,
                    0
                ],
                "title": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching"
                },
                "summary": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agentic applications have shown increasingly remarkable\ncapabilities in complex workflows but incur substantial costs due to extensive\nplanning and reasoning requirements. Existing LLM caching techniques (like\ncontext caching and semantic caching), primarily designed for serving chatbots,\nare insufficient for agentic applications where outputs depend on external data\nor environmental contexts. We propose agentic plan caching, a novel approach\nthat extracts, stores, adapts, and reuses structured plan templates from\nplanning stages of agentic applications across semantically similar tasks to\nreduce the cost of serving. Unlike traditional semantic caching, our system\nextracts plan templates from completed agent executions at test-time, employs\nkeyword extraction to match new requests against cached plans, and utilizes\nlightweight models to adapt these templates to task-specific plans with\ncontexts. Evaluation across multiple real-world agentic applications shows that\nour system can reduce costs by 46.62% on average while maintaining performance,\noffering a more efficient solution for serving LLM-based agents that\ncomplements existing LLM serving infrastructures."
                },
                "authors": [
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Michael Wornow"
                    },
                    {
                        "name": "Kunle Olukotun"
                    }
                ],
                "author_detail": {
                    "name": "Kunle Olukotun"
                },
                "author": "Kunle Olukotun",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.06153v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.06153v2",
                "updated": "2025-06-17T04:00:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    4,
                    0,
                    42,
                    1,
                    168,
                    0
                ],
                "published": "2023-03-10T04:37:07Z",
                "published_parsed": [
                    2023,
                    3,
                    10,
                    4,
                    37,
                    7,
                    4,
                    69,
                    0
                ],
                "title": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim: A pure software simulated CXL.mem for performance\n  characterization"
                },
                "summary": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXLMemSim is a fast, lightweight simulation framework that enables\nperformance characterization of memory systems based on Compute Express Link\n(CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to\nmitigate memory stranding (underutilized memory trapped on fully loaded\nservers) in cloud and datacenter environments. However, CXL-attached memory\nintroduces additional latency and bandwidth constraints compared to local DRAM,\nand real CXL .mem hardware is not yet widely available for empirical\nevaluation. CXLMemSim addresses this gap by attaching to unmodified\napplications and simulating CXL-based memory pools in software. It operates by\ntracing memory allocations and accesses using efficient kernel probes and\nhardware performance counters, dividing execution into epochs, and injecting\ntiming delays to emulate various CXL .mem latency/bandwidth characteristics.\nThis approach incurs modest runtime overhead while preserving realistic\nload/store memory access patterns. We implement CXLMemSim on commodity hardware\nwithout special devices, and our evaluation shows that it runs orders of\nmagnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world\nworkloads, while accurately modeling the performance impact of CXL .mem. We\ndemonstrate use cases where CXLMemSim enables experimentation with memory\npooling configurations, scheduling policies, data migration strategies, and\ncaching techniques that were previously infeasible to evaluate at scale. Key\nfindings include the viability of software-based CXL .mem emulation with low\noverhead, insights into latency and congestion effects in memory pools, and\nguidance for system designers to optimize memory disaggregation. Overall,\nCXLMemSim provides a practical and extensible platform for researchers and\npractitioners to explore CXL.mem innovations before real hardware becomes\ncommonplace."
                },
                "authors": [
                    {
                        "name": "Yiwei Yang"
                    },
                    {
                        "name": "Brian Zhao"
                    },
                    {
                        "name": "Yusheng Zheng"
                    },
                    {
                        "name": "Pooneh Safayenikoo"
                    },
                    {
                        "name": "Tanvir Ahmed Khan"
                    },
                    {
                        "name": "Andi Quinn"
                    }
                ],
                "author_detail": {
                    "name": "Andi Quinn"
                },
                "author": "Andi Quinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.06153v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.06153v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v2",
                "updated": "2025-06-17T02:24:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    2,
                    24,
                    51,
                    1,
                    168,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v2",
                "updated": "2025-06-17T00:26:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    17,
                    0,
                    26,
                    21,
                    1,
                    168,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13991v1",
                "updated": "2025-06-16T20:46:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T20:46:20Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    20,
                    46,
                    20,
                    0,
                    167,
                    0
                ],
                "title": "glass: ordered set data structure for client-side order books",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "glass: ordered set data structure for client-side order books"
                },
                "summary": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"ordered set\" abstract data type with operations \"insert\", \"erase\",\n\"find\", \"min\", \"max\", \"next\" and \"prev\" is ubiquitous in computer science. It\nis usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We\npresent our implementation of ordered set based on a trie. It only supports\ninteger keys (as opposed to keys of any strict weakly ordered type) and is\noptimized for market data, namely for what we call sequential locality. The\nfollowing is the list of what we believe to be novelties:\n  * Cached path to exploit sequential locality, and fast truncation thereof on\nerase operation;\n  * A hash table (or, rather, a cache table) with hard O(1) time guarantees on\nany operation to speed up key lookup (up to a pre-leaf node);\n  * Hardware-accelerated \"find next/previous set bit\" operations with BMI2\ninstruction set extension on x86-64;\n  * Order book-specific features: the preemption principle and the tree\nrestructure operation that prevent the tree from consuming too much memory.\n  We achieve the following speedups over C++'s standard std::map container:\n6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market\ndata, and a more modest 2x-3x speedup on iteration. In this paper, we discuss\nour implementation."
                },
                "authors": [
                    {
                        "name": "Viktor Krapivensky"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Krapivensky"
                },
                "author": "Viktor Krapivensky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.13184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.13184v2",
                "updated": "2025-06-16T17:17:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    17,
                    38,
                    0,
                    167,
                    0
                ],
                "published": "2023-06-22T19:58:48Z",
                "published_parsed": [
                    2023,
                    6,
                    22,
                    19,
                    58,
                    48,
                    3,
                    173,
                    0
                ],
                "title": "Cache-Aided Variable-Length Coding with Perfect Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided Variable-Length Coding with Perfect Privacy"
                },
                "summary": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A cache-aided compression problem with perfect privacy is studied, where a\nserver has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$\nbits. The server is connected to $K$ users through a shared link, where each\nuser has access to a local cache of size $MF$ bits. In the placement phase, the\nserver fills the users$'$ caches without prior knowledge of their future\ndemands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file $Y_i$ is arbitrarily correlated\nwith a private attribute $X$, and an adversary is assumed to have access to the\nshared link. The users and the server have access to a shared secret key $W$.\nThe goal is to design the cache contents and the delivered message $\\cal C$\nsuch that the average length of $\\mathcal{C}$ is minimized, while satisfying:\ni. The response $\\cal C$ does not disclose any information about $X$, i.e., $X$\nand $\\cal C$ are statistically independent yielding $I(X;\\mathcal{C})=0$, which\ncorresponds to the perfect privacy constraint; ii. User $i$ is able to decode\nits demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\\cal\nC$, and the shared secret key $W$. Due to the correlation of database with the\nprivate attribute, existing codes for cache-aided delivery do not fulfill the\nperfect privacy constraint. Indeed, in this work, we propose a lossless\nvariable-length coding scheme that combines privacy-aware compression with\ncoded caching techniques. In particular, we use two-part code construction and\nFunctional Representation Lemma. Furthermore, we propose an alternative coding\nscheme based on the minimum entropy coupling concept and a greedy entropy-based\nalgorithm. We show that the proposed scheme improves the previous results\nobtained by Functional Representation Lemma."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.13184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.13184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13541v1",
                "updated": "2025-06-16T14:30:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T14:30:17Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    30,
                    17,
                    0,
                    167,
                    0
                ],
                "title": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Weight-shared Heterogeneous Group Attention Experts for\n  Dynamic Token-wise KV Optimization"
                },
                "summary": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models face scalability challenges in causal language modeling\n(CLM) due to inefficient memory allocation for growing key-value (KV) caches,\nwhich strains compute and storage resources. Existing methods like Grouped\nQuery Attention (GQA) and token-level KV optimization improve efficiency but\nrely on rigid resource allocation, often discarding \"low-priority\" tokens or\nstatically grouping them, failing to address the dynamic spectrum of token\nimportance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that\ndynamically optimizes token-wise computation and memory allocation. Unlike\nprior approaches, mixSGA retains all tokens while adaptively routing them to\nspecialized experts with varying KV group sizes, balancing granularity and\nefficiency. Our key novelties include: (1) a token-wise expert-choice routing\nmechanism guided by learned importance scores, enabling proportional resource\nallocation without token discard; (2) weight-sharing across grouped attention\nprojections to minimize parameter overhead; and (3) an auxiliary loss to ensure\none-hot routing decisions for training-inference consistency in CLMs. Extensive\nevaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show\nmixSGA's superiority over static baselines. On instruction-following and\ncontinued pretraining tasks, mixSGA achieves higher ROUGE-L and lower\nperplexity under the same KV budgets."
                },
                "authors": [
                    {
                        "name": "Guanghui Song"
                    },
                    {
                        "name": "Dongping Liao"
                    },
                    {
                        "name": "Yiren Zhao"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Cheng-zhong Xu"
                    },
                    {
                        "name": "Xitong Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xitong Gao"
                },
                "author": "Xitong Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13456v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13456v1",
                "updated": "2025-06-16T13:14:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T13:14:58Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    13,
                    14,
                    58,
                    0,
                    167,
                    0
                ],
                "title": "Block-wise Adaptive Caching for Accelerating Diffusion Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-wise Adaptive Caching for Accelerating Diffusion Policy"
                },
                "summary": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policy has demonstrated strong visuomotor modeling capabilities,\nbut its high computational cost renders it impractical for real-time robotic\ncontrol. Despite huge redundancy across repetitive denoising steps, existing\ndiffusion acceleration techniques fail to generalize to Diffusion Policy due to\nfundamental architectural and data divergences. In this paper, we propose\nBlock-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by\ncaching intermediate action features. BAC achieves lossless action generation\nacceleration by adaptively updating and reusing cached features at the block\nlevel, based on a key observation that feature similarities vary non-uniformly\nacross timesteps and locks. To operationalize this insight, we first propose\nthe Adaptive Caching Scheduler, designed to identify optimal update timesteps\nby maximizing the global feature similarities between cached and skipped\nfeatures. However, applying this scheduler for each block leads to signiffcant\nerror surges due to the inter-block propagation of caching errors, particularly\nwithin Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop\nthe Bubbling Union Algorithm, which truncates these errors by updating the\nupstream blocks with signiffcant caching errors before downstream FFNs. As a\ntraining-free plugin, BAC is readily integrable with existing transformer-based\nDiffusion Policy and vision-language-action models. Extensive experiments on\nmultiple robotic benchmarks demonstrate that BAC achieves up to 3x inference\nspeedup for free."
                },
                "authors": [
                    {
                        "name": "Kangye Ji"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Hanyun Cui"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Shengjia Hua"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13456v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13456v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13246v1",
                "updated": "2025-06-16T08:43:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T08:43:56Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    43,
                    56,
                    0,
                    167,
                    0
                ],
                "title": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed\n  Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains"
                },
                "summary": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a formalised architecture for synthetic agents designed\nto retain immutable memory, verifiable reasoning, and constrained epistemic\ngrowth. Traditional AI systems rely on mutable, opaque statistical models prone\nto epistemic drift and historical revisionism. In contrast, we introduce the\nconcept of the Merkle Automaton, a cryptographically anchored, deterministic\ncomputational framework that integrates formal automata theory with\nblockchain-based commitments. Each agent transition, memory fragment, and\nreasoning step is committed within a Merkle structure rooted on-chain,\nrendering it non-repudiable and auditably permanent. To ensure selective access\nand confidentiality, we derive symmetric encryption keys from ECDH exchanges\ncontextualised by hierarchical privilege lattices. This enforces cryptographic\naccess control over append-only DAG-structured knowledge graphs. Reasoning is\nconstrained by formal logic systems and verified through deterministic\ntraversal of policy-encoded structures. Updates are non-destructive and\nhistoried, preserving epistemic lineage without catastrophic forgetting.\nZero-knowledge proofs facilitate verifiable, privacy-preserving inclusion\nattestations. Collectively, this architecture reframes memory not as a cache\nbut as a ledger - one whose contents are enforced by protocol, bound by\ncryptography, and constrained by formal logic. The result is not an intelligent\nagent that mimics thought, but an epistemic entity whose outputs are provably\nderived, temporally anchored, and impervious to post hoc revision. This design\nlays foundational groundwork for legal, economic, and high-assurance\ncomputational systems that require provable memory, unforgeable provenance, and\nstructural truth."
                },
                "authors": [
                    {
                        "name": "Craig Steven Wright"
                    }
                ],
                "author_detail": {
                    "name": "Craig Steven Wright"
                },
                "author": "Craig Steven Wright",
                "arxiv_comment": "47 pages, includes formal automata specifications, cryptographic\n  constructions, and epistemic architecture schema",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70,\n  68P25, 68T37",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.3; D.4.6; E.3; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02969v2",
                "updated": "2025-06-16T06:38:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    6,
                    38,
                    23,
                    0,
                    167,
                    0
                ],
                "published": "2025-03-04T19:51:29Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    19,
                    51,
                    29,
                    1,
                    63,
                    0
                ],
                "title": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniSST: Simultaneous Translation of Unbounded Speech with Large\n  Language Model"
                },
                "summary": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous translation of unbounded streaming speech remains a challenging\nproblem due to the need for effectively processing the history speech context\nand past translations so that quality and latency, including computation\noverhead, can be balanced. Most prior works assume pre-segmented speech,\nlimiting their real-world applicability. In this paper, we propose InfiniSST, a\nnovel approach that formulates SST as a multi-turn dialogue task, enabling\nseamless translation of unbounded speech. We construct translation trajectories\nand robust segments from MuST-C with multi-latency augmentation during training\nand develop a key-value (KV) cache management strategy to facilitate efficient\ninference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that\nInfiniSST reduces computation-aware latency by 0.5 to 1 second while\nmaintaining the same translation quality compared to baselines. Ablation\nstudies further validate the contributions of our data construction and cache\nmanagement strategy. We release the code and demo at\nhttps://github.com/LeiLiLab/InfiniSST"
                },
                "authors": [
                    {
                        "name": "Siqi Ouyang"
                    },
                    {
                        "name": "Xi Xu"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13059v1",
                "updated": "2025-06-16T03:00:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-16T03:00:40Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "title": "Multipole Attention for Efficient Long Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multipole Attention for Efficient Long Context Reasoning"
                },
                "summary": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on\ncomplex problem-solving tasks. While these models have attained high accuracy\nby leveraging additional computation at test time, they need to generate long\nchain-of-thought reasoning in order to think before answering, which requires\ngenerating thousands of tokens. While sparse attention methods can help reduce\nthe KV cache pressure induced by this long autoregressive reasoning, these\nmethods can introduce errors which disrupt the reasoning process. Additionally,\nprior methods often pre-process the input to make it easier to identify the\nimportant prompt tokens when computing attention during generation, and this\npre-processing is challenging to perform online for newly generated reasoning\ntokens. Our work addresses these challenges by introducing Multipole Attention,\nwhich accelerates autoregressive reasoning by only computing exact attention\nfor the most important tokens, while maintaining approximate representations\nfor the remaining tokens. Our method first performs clustering to group\ntogether semantically similar key vectors, and then uses the cluster centroids\nboth to identify important key vectors and to approximate the remaining key\nvectors in order to retain high accuracy. We design a fast cluster update\nprocess to quickly re-cluster the input and previously generated tokens,\nthereby allowing for accelerating attention to the previous output tokens. We\nevaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our\napproach can maintain accuracy on complex reasoning tasks even with aggressive\nattention sparsity settings. We also provide kernel implementations to\ndemonstrate the practical efficiency gains from our method, achieving up to\n4.5$\\times$ speedup for attention in long-context reasoning applications. Our\ncode is available at https://github.com/SqueezeAILab/MultipoleAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09342v2",
                "updated": "2025-06-16T02:57:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    16,
                    2,
                    57,
                    37,
                    0,
                    167,
                    0
                ],
                "published": "2025-06-11T02:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    48,
                    16,
                    2,
                    162,
                    0
                ],
                "title": "Latent Multi-Head Attention for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Multi-Head Attention for Small Language Models"
                },
                "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "arxiv_comment": "6 pages, 1 figure. 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v2",
                "updated": "2025-06-15T13:04:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    13,
                    4,
                    14,
                    6,
                    166,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. In this paper, we first show that the data plane can be made fast with\nno or O(1) caching by loading parameters through the compute network between\nGPUs because: (1) its speed is comparable to host cache and is underutilized,\nand (2) scaling multiple instances requires no or O(1) caching with\nnetwork-optimized multicast. Second, autoscaling can be made live by breaking\nthe scaling abstraction for inference from a coarse-grained instance-level to a\nfine-grained layer-level. This allows us to offload the layer computation from\nthe overloaded serving instances to the scaled ones without waiting for the\nparameters to be fully loaded. Under real-world workloads, our system\nBLITZSCALE achieves up to 94 % lower tail latency reductions compared to\nstate-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU\ntime used for serving by 49 % when compared with serving systems that do not\nsupport autoscaling like DistServe and vLLM with the same\nservice-level-agreement."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "In proceedings of OSDI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v3",
                "updated": "2025-06-15T08:41:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    8,
                    41,
                    9,
                    6,
                    166,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025, revised under shepherding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v1",
                "updated": "2025-06-15T07:19:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13814v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13814v1",
                "updated": "2025-06-14T20:17:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:17:43Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    17,
                    43,
                    5,
                    165,
                    0
                ],
                "title": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering"
                },
                "summary": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics rendering applications increasingly leverage neural networks in\ntasks such as denoising, supersampling, and frame extrapolation to improve\nimage quality while maintaining frame rates. The temporal coherence inherent in\nthese tasks presents an opportunity to reuse intermediate results from previous\nframes and avoid redundant computations. Recent work has shown that caching\nintermediate features to be reused in subsequent inferences is an effective\nmethod to reduce latency in diffusion models. We extend this idea to real-time\nrendering and present ReFrame, which explores different caching policies to\noptimize trade-offs between quality and performance in rendering workloads.\nReFrame can be applied to a variety of encoder-decoder style networks commonly\nfound in rendering pipelines. Experimental results show that we achieve 1.4x\nspeedup on average with negligible quality loss in three real-time rendering\ntasks. Code available:\nhttps://ubc-aamodt-group.github.io/reframe-layer-caching/"
                },
                "authors": [
                    {
                        "name": "Lufei Liu"
                    },
                    {
                        "name": "Tor M. Aamodt"
                    }
                ],
                "author_detail": {
                    "name": "Tor M. Aamodt"
                },
                "author": "Tor M. Aamodt",
                "arxiv_comment": "Published at ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13814v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13814v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12616v1",
                "updated": "2025-06-14T20:00:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T20:00:53Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    20,
                    0,
                    53,
                    5,
                    165,
                    0
                ],
                "title": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Agile Software Management for Edge and Fog Computing Based\n  Smart City Infrastructure"
                },
                "summary": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of smart cities demands scalable, secure, and energy-efficient\narchitectures for real-time data processing. With the number of IoT devices\nexpected to exceed 40 billion by 2030, traditional cloud-based systems are\nincreasingly constrained by bandwidth, latency, and energy limitations. This\npaper leverages the ROOF (Real-time Onsite Operations Facilitation) framework\nwith decentralized computing at intermediary fog and peripheral edge network\nlayers to reduce latency by processing data near its point of origin. ROOF\nfeatures fog caching to avoid redundancy, ultra-low-power wireless transmission\nfor energy savings, and AI-driven resource allocation for efficiency. Security\nis enhanced through TLS encryption, blockchain-based authentication, and\nedge-level access control. Case studies from Bhubaneswar, Barcelona and\nCopenhagen validate the use of ROOF in traffic systems and environmental\nmonitoring. The paper concludes by outlining key challenges and prospects of\nAI-driven analytics in smart urban infrastructure."
                },
                "authors": [
                    {
                        "name": "Debasish Jana"
                    },
                    {
                        "name": "Pinakpani Pal"
                    },
                    {
                        "name": "Pawan Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Pawan Kumar"
                },
                "author": "Pawan Kumar",
                "arxiv_comment": "The paper has been published at the Fifth International Conference on\n  Computing and Communication Networks (ICCCN 2025), Volume 1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12370v1",
                "updated": "2025-06-14T06:36:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-14T06:36:54Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    36,
                    54,
                    5,
                    165,
                    0
                ],
                "title": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Unified Caching for Accelerating Heterogeneous AI Workloads"
                },
                "summary": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern AI clusters, which host diverse workloads like data pre-processing,\ntraining and inference, often store the large-volume data in cloud storage and\nemploy caching frameworks to facilitate remote data access. To avoid\ncode-intrusion complexity and minimize cache space wastage, it is desirable to\nmaintain a unified cache shared by all the workloads. However, existing cache\nmanagement strategies, designed for specific workloads, struggle to handle the\nheterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous\naccess patterns and item storage granularities. In this paper, we propose\nIGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache\nleverages a hierarchical access abstraction, AccessStreamTree, to organize the\nrecent data accesses in a tree structure, facilitating access pattern detection\nat various granularities. Using this abstraction, IGTCache applies hypothesis\ntesting to categorize data access patterns as sequential, random, or skewed.\nBased on these detected access patterns and granularities, IGTCache tailors\noptimal cache management strategies including prefetching, eviction, and space\nallocation accordingly. Experimental results show that IGTCache increases the\ncache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the\noverall job completion time by 52.2%."
                },
                "authors": [
                    {
                        "name": "Tianze Wang"
                    },
                    {
                        "name": "Yifei Liu"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Pengfei Zuo"
                    },
                    {
                        "name": "Jiawei Zhang"
                    },
                    {
                        "name": "Qizhen Weng"
                    },
                    {
                        "name": "Yin Chen"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "15 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v2",
                "updated": "2025-06-14T06:17:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    6,
                    17,
                    33,
                    5,
                    165,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency. Our code is available at\nhttps://github.com/sjtu-zhao-lab/ClusterKV."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04593v3",
                "updated": "2025-06-14T00:52:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    0,
                    52,
                    10,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-05T03:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    16,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM"
                },
                "summary": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods."
                },
                "authors": [
                    {
                        "name": "Xun Li"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been submitted to IEEE letters. The source code has\n  been released at:\n  https://github.com/qiongwu86/Federated-Learning-Assisted-Edge-Caching-Scheme-Based-on-Lightweight-Architecture-DDPM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v3",
                "updated": "2025-06-13T21:01:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    21,
                    1,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06266v3",
                "updated": "2025-06-13T17:58:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    58,
                    55,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-06T17:48:23Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    17,
                    48,
                    23,
                    4,
                    157,
                    0
                ],
                "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study"
                },
                "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining."
                },
                "authors": [
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "Emily Liu"
                    },
                    {
                        "name": "Will Tennien"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    },
                    {
                        "name": "Christopher Re"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Re"
                },
                "author": "Christopher Re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11970v1",
                "updated": "2025-06-13T17:28:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:28:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation"
                },
                "summary": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM."
                },
                "authors": [
                    {
                        "name": "Chris S. Lin"
                    },
                    {
                        "name": "Jeonghyun Woo"
                    },
                    {
                        "name": "Prashant J. Nair"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "8 pages, including appendices. The paper is presented at DRAMSec\n  2025. (see https://dramsec.ethz.ch/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11886v1",
                "updated": "2025-06-13T15:35:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:35:54Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache"
                },
                "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Siyang He"
                    },
                    {
                        "name": "Qiqi Wang"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "10 pages, 7 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04065v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04065v4",
                "updated": "2025-06-13T08:32:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    32,
                    26,
                    4,
                    164,
                    0
                ],
                "published": "2024-05-07T07:14:38Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    7,
                    14,
                    38,
                    1,
                    128,
                    0
                ],
                "title": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference"
                },
                "summary": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost."
                },
                "authors": [
                    {
                        "name": "Runheng Liu"
                    },
                    {
                        "name": "Xingchen Xiao"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Zhijing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Wu"
                },
                "author": "Zhijing Wu",
                "arxiv_comment": "ACL 2025 Findings, 14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04065v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04065v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.16571v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16571v2",
                "updated": "2025-07-01T17:51:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    51,
                    47,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-19T19:52:53Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    19,
                    52,
                    53,
                    3,
                    170,
                    0
                ],
                "title": "Capturing Visualization Design Rationale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing Visualization Design Rationale"
                },
                "summary": "Prior natural language datasets for data visualization have focused on tasks\nsuch as visualization literacy assessment, insight generation, and\nvisualization generation from natural language instructions. These studies\noften rely on controlled setups with purpose-built visualizations and\nartificially constructed questions. As a result, they tend to prioritize the\ninterpretation of visualizations, focusing on decoding visualizations rather\nthan understanding their encoding. In this paper, we present a new dataset and\nmethodology for probing visualization design rationale through natural\nlanguage. We leverage a unique source of real-world visualizations and natural\nlanguage narratives: literate visualization notebooks created by students as\npart of a data visualization course. These notebooks combine visual artifacts\nwith design exposition, in which students make explicit the rationale behind\ntheir design decisions. We also use large language models (LLMs) to generate\nand categorize question-answer-rationale triples from the narratives and\narticulations in the notebooks. We then carefully validate the triples and\ncurate a dataset that captures and distills the visualization design choices\nand corresponding rationales of the students.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior natural language datasets for data visualization have focused on tasks\nsuch as visualization literacy assessment, insight generation, and\nvisualization generation from natural language instructions. These studies\noften rely on controlled setups with purpose-built visualizations and\nartificially constructed questions. As a result, they tend to prioritize the\ninterpretation of visualizations, focusing on decoding visualizations rather\nthan understanding their encoding. In this paper, we present a new dataset and\nmethodology for probing visualization design rationale through natural\nlanguage. We leverage a unique source of real-world visualizations and natural\nlanguage narratives: literate visualization notebooks created by students as\npart of a data visualization course. These notebooks combine visual artifacts\nwith design exposition, in which students make explicit the rationale behind\ntheir design decisions. We also use large language models (LLMs) to generate\nand categorize question-answer-rationale triples from the narratives and\narticulations in the notebooks. We then carefully validate the triples and\ncurate a dataset that captures and distills the visualization design choices\nand corresponding rationales of the students."
                },
                "authors": [
                    {
                        "name": "Maeve Hutchinson"
                    },
                    {
                        "name": "Radu Jianu"
                    },
                    {
                        "name": "Aidan Slingsby"
                    },
                    {
                        "name": "Jo Wood"
                    },
                    {
                        "name": "Pranava Madhyastha"
                    }
                ],
                "author_detail": {
                    "name": "Pranava Madhyastha"
                },
                "author": "Pranava Madhyastha",
                "arxiv_comment": "To be presented at IEEE VIS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16571v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16571v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10685v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10685v3",
                "updated": "2025-07-01T17:49:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    49,
                    9,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-12T13:30:01Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    13,
                    30,
                    1,
                    3,
                    163,
                    0
                ],
                "title": "Defensive Adversarial CAPTCHA: A Semantics-Driven Framework for Natural\n  Adversarial Example Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defensive Adversarial CAPTCHA: A Semantics-Driven Framework for Natural\n  Adversarial Example Generation"
                },
                "summary": "Traditional CAPTCHA (Completely Automated Public Turing Test to Tell\nComputers and Humans Apart) schemes are increasingly vulnerable to automated\nattacks powered by deep neural networks (DNNs). Existing adversarial attack\nmethods often rely on the original image characteristics, resulting in\ndistortions that hinder human interpretation and limit their applicability in\nscenarios where no initial input images are available. To address these\nchallenges, we propose the Unsourced Adversarial CAPTCHA (DAC), a novel\nframework that generates high-fidelity adversarial examples guided by\nattacker-specified semantics information. Leveraging a Large Language Model\n(LLM), DAC enhances CAPTCHA diversity and enriches the semantic information. To\naddress various application scenarios, we examine the white-box targeted attack\nscenario and the black box untargeted attack scenario. For target attacks, we\nintroduce two latent noise variables that are alternately guided in the\ndiffusion step to achieve robust inversion. The synergy between gradient\nguidance and latent variable optimization achieved in this way ensures that the\ngenerated adversarial examples not only accurately align with the target\nconditions but also achieve optimal performance in terms of distributional\nconsistency and attack effectiveness. In untargeted attacks, especially for\nblack-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA\n(BP-DAC), a two-step optimization strategy employing multimodal gradients and\nbi-path optimization for efficient misclassification. Experiments show that the\ndefensive adversarial CAPTCHA generated by BP-DAC is able to defend against\nmost of the unknown models, and the generated CAPTCHA is indistinguishable to\nboth humans and DNNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional CAPTCHA (Completely Automated Public Turing Test to Tell\nComputers and Humans Apart) schemes are increasingly vulnerable to automated\nattacks powered by deep neural networks (DNNs). Existing adversarial attack\nmethods often rely on the original image characteristics, resulting in\ndistortions that hinder human interpretation and limit their applicability in\nscenarios where no initial input images are available. To address these\nchallenges, we propose the Unsourced Adversarial CAPTCHA (DAC), a novel\nframework that generates high-fidelity adversarial examples guided by\nattacker-specified semantics information. Leveraging a Large Language Model\n(LLM), DAC enhances CAPTCHA diversity and enriches the semantic information. To\naddress various application scenarios, we examine the white-box targeted attack\nscenario and the black box untargeted attack scenario. For target attacks, we\nintroduce two latent noise variables that are alternately guided in the\ndiffusion step to achieve robust inversion. The synergy between gradient\nguidance and latent variable optimization achieved in this way ensures that the\ngenerated adversarial examples not only accurately align with the target\nconditions but also achieve optimal performance in terms of distributional\nconsistency and attack effectiveness. In untargeted attacks, especially for\nblack-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA\n(BP-DAC), a two-step optimization strategy employing multimodal gradients and\nbi-path optimization for efficient misclassification. Experiments show that the\ndefensive adversarial CAPTCHA generated by BP-DAC is able to defend against\nmost of the unknown models, and the generated CAPTCHA is indistinguishable to\nboth humans and DNNs."
                },
                "authors": [
                    {
                        "name": "Xia Du"
                    },
                    {
                        "name": "Xiaoyuan Liu"
                    },
                    {
                        "name": "Jizhe Zhou"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Chi-man Pun"
                    },
                    {
                        "name": "Cong Wu"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wei Ni"
                    },
                    {
                        "name": "Jun Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jun Luo"
                },
                "author": "Jun Luo",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10685v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10685v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11801v2",
                "updated": "2025-07-01T17:41:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    41,
                    45,
                    1,
                    182,
                    0
                ],
                "published": "2025-03-14T18:42:29Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    18,
                    42,
                    29,
                    4,
                    73,
                    0
                ],
                "title": "Diffuse-CLoC: Guided Diffusion for Physics-based Character Look-ahead\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffuse-CLoC: Guided Diffusion for Physics-based Character Look-ahead\n  Control"
                },
                "summary": "We present Diffuse-CLoC, a guided diffusion framework for physics-based\nlook-ahead control that enables intuitive, steerable, and physically realistic\nmotion generation. While existing kinematics motion generation with diffusion\nmodels offer intuitive steering capabilities with inference-time conditioning,\nthey often fail to produce physically viable motions. In contrast, recent\ndiffusion-based control policies have shown promise in generating physically\nrealizable motion sequences, but the lack of kinematics prediction limits their\nsteerability. Diffuse-CLoC addresses these challenges through a key insight:\nmodeling the joint distribution of states and actions within a single diffusion\nmodel makes action generation steerable by conditioning it on the predicted\nstates. This approach allows us to leverage established conditioning techniques\nfrom kinematic motion generation while producing physically realistic motions.\nAs a result, we achieve planning capabilities without the need for a high-level\nplanner. Our method handles a diverse set of unseen long-horizon downstream\ntasks through a single pre-trained model, including static and dynamic obstacle\navoidance, motion in-betweening, and task-space control. Experimental results\nshow that our method significantly outperforms the traditional hierarchical\nframework of high-level motion diffusion and low-level tracking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Diffuse-CLoC, a guided diffusion framework for physics-based\nlook-ahead control that enables intuitive, steerable, and physically realistic\nmotion generation. While existing kinematics motion generation with diffusion\nmodels offer intuitive steering capabilities with inference-time conditioning,\nthey often fail to produce physically viable motions. In contrast, recent\ndiffusion-based control policies have shown promise in generating physically\nrealizable motion sequences, but the lack of kinematics prediction limits their\nsteerability. Diffuse-CLoC addresses these challenges through a key insight:\nmodeling the joint distribution of states and actions within a single diffusion\nmodel makes action generation steerable by conditioning it on the predicted\nstates. This approach allows us to leverage established conditioning techniques\nfrom kinematic motion generation while producing physically realistic motions.\nAs a result, we achieve planning capabilities without the need for a high-level\nplanner. Our method handles a diverse set of unseen long-horizon downstream\ntasks through a single pre-trained model, including static and dynamic obstacle\navoidance, motion in-betweening, and task-space control. Experimental results\nshow that our method significantly outperforms the traditional hierarchical\nframework of high-level motion diffusion and low-level tracking."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Huang"
                    },
                    {
                        "name": "Takara Truong"
                    },
                    {
                        "name": "Yunbo Zhang"
                    },
                    {
                        "name": "Fangzhou Yu"
                    },
                    {
                        "name": "Jean Pierre Sleiman"
                    },
                    {
                        "name": "Jessica Hodgins"
                    },
                    {
                        "name": "Koushil Sreenath"
                    },
                    {
                        "name": "Farbod Farshidian"
                    }
                ],
                "author_detail": {
                    "name": "Farbod Farshidian"
                },
                "author": "Farbod Farshidian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22663v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22663v2",
                "updated": "2025-07-01T17:38:27Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    38,
                    27,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-28T17:59:57Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    17,
                    59,
                    57,
                    2,
                    148,
                    0
                ],
                "title": "Training Free Stylized Abstraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Free Stylized Abstraction"
                },
                "summary": "Stylized abstraction synthesizes visually exaggerated yet semantically\nfaithful representations of subjects, balancing recognizability with perceptual\ndistortion. Unlike image-to-image translation, which prioritizes structural\nfidelity, stylized abstraction demands selective retention of identity cues\nwhile embracing stylistic divergence, especially challenging for\nout-of-distribution individuals. We propose a training-free framework that\ngenerates stylized abstractions from a single image using inference-time\nscaling in vision-language models (VLLMs) to extract identity-relevant\nfeatures, and a novel cross-domain rectified flow inversion strategy that\nreconstructs structure based on style-dependent priors. Our method adapts\nstructural restoration dynamically through style-aware temporal scheduling,\nenabling high-fidelity reconstructions that honor both subject and style. It\nsupports multi-round abstraction-aware generation without fine-tuning. To\nevaluate this task, we introduce StyleBench, a GPT-based human-aligned metric\nsuited for abstract styles where pixel-level similarity fails. Experiments\nacross diverse abstraction (e.g., LEGO, knitted dolls, South Park) show strong\ngeneralization to unseen identities and styles in a fully open-source setup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stylized abstraction synthesizes visually exaggerated yet semantically\nfaithful representations of subjects, balancing recognizability with perceptual\ndistortion. Unlike image-to-image translation, which prioritizes structural\nfidelity, stylized abstraction demands selective retention of identity cues\nwhile embracing stylistic divergence, especially challenging for\nout-of-distribution individuals. We propose a training-free framework that\ngenerates stylized abstractions from a single image using inference-time\nscaling in vision-language models (VLLMs) to extract identity-relevant\nfeatures, and a novel cross-domain rectified flow inversion strategy that\nreconstructs structure based on style-dependent priors. Our method adapts\nstructural restoration dynamically through style-aware temporal scheduling,\nenabling high-fidelity reconstructions that honor both subject and style. It\nsupports multi-round abstraction-aware generation without fine-tuning. To\nevaluate this task, we introduce StyleBench, a GPT-based human-aligned metric\nsuited for abstract styles where pixel-level similarity fails. Experiments\nacross diverse abstraction (e.g., LEGO, knitted dolls, South Park) show strong\ngeneralization to unseen identities and styles in a fully open-source setup."
                },
                "authors": [
                    {
                        "name": "Aimon Rahman"
                    },
                    {
                        "name": "Kartik Narayan"
                    },
                    {
                        "name": "Vishal M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Vishal M. Patel"
                },
                "author": "Vishal M. Patel",
                "arxiv_comment": "Project Page: https://kartik-3004.github.io/TF-SA/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22663v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01868v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01868v2",
                "updated": "2025-07-01T17:22:45Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    22,
                    45,
                    1,
                    182,
                    0
                ],
                "published": "2024-08-03T21:39:43Z",
                "published_parsed": [
                    2024,
                    8,
                    3,
                    21,
                    39,
                    43,
                    5,
                    216,
                    0
                ],
                "title": "Meta-Posterior Consistency for the Bayesian Inference of Metastable\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Posterior Consistency for the Bayesian Inference of Metastable\n  System"
                },
                "summary": "The vast majority of the literature on learning dynamical systems or\nstochastic processes from time series has focused on stable or ergodic systems,\nfor both Bayesian and frequentist inference procedures. However, most\nreal-world systems are only metastable, that is, the dynamics appear to be\nstable on some time scale, but are in fact unstable over longer time scales.\nConsistency of inference for metastable systems may not be possible, but one\ncan ask about metaconsistency: Do inference procedures converge when\nobservations are taken over a large but finite time interval, but diverge on\nlonger time scales? In this paper we introduce, discuss, and quantify\nmetaconsistency in a Bayesian framework. We discuss how metaconsistency can be\nexploited to efficiently infer a model for a sub-system of a larger system,\nwhere inference on the global behavior may require much more data, or there is\nno theoretical guarantee as to the asymptotic success of inference procedures.\nWe also discuss the relation between metaconsistency and the spectral\nproperties of the model dynamical system in the case of uniformly ergodic and\nnon-ergodic diffusions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vast majority of the literature on learning dynamical systems or\nstochastic processes from time series has focused on stable or ergodic systems,\nfor both Bayesian and frequentist inference procedures. However, most\nreal-world systems are only metastable, that is, the dynamics appear to be\nstable on some time scale, but are in fact unstable over longer time scales.\nConsistency of inference for metastable systems may not be possible, but one\ncan ask about metaconsistency: Do inference procedures converge when\nobservations are taken over a large but finite time interval, but diverge on\nlonger time scales? In this paper we introduce, discuss, and quantify\nmetaconsistency in a Bayesian framework. We discuss how metaconsistency can be\nexploited to efficiently infer a model for a sub-system of a larger system,\nwhere inference on the global behavior may require much more data, or there is\nno theoretical guarantee as to the asymptotic success of inference procedures.\nWe also discuss the relation between metaconsistency and the spectral\nproperties of the model dynamical system in the case of uniformly ergodic and\nnon-ergodic diffusions."
                },
                "authors": [
                    {
                        "name": "Zachary P Adams"
                    },
                    {
                        "name": "Sayan Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Sayan Mukherjee"
                },
                "author": "Sayan Mukherjee",
                "arxiv_comment": "32 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01868v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01868v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F15, 60J70",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22773v2",
                "updated": "2025-07-01T17:12:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    12,
                    12,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-28T06:26:06Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    6,
                    26,
                    6,
                    5,
                    179,
                    0
                ],
                "title": "Not All Water Consumption Is Equal: A Water Stress Weighted Metric for\n  Sustainable Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Water Consumption Is Equal: A Water Stress Weighted Metric for\n  Sustainable Computing"
                },
                "summary": "Water consumption is an increasingly critical dimension of computing\nsustainability, especially as AI workloads rapidly scale. However, current\nwater impact assessment often overlooks where and when water stress is more\nsevere. To fill in this gap, we present SCARF, the first general framework that\nevaluates water impact of computing by factoring in both spatial and temporal\nvariations in water stress. SCARF calculates an Adjusted Water Impact (AWI)\nmetric that considers both consumption volume and local water stress over time.\nThrough three case studies on LLM serving, datacenters, and semiconductor\nfabrication plants, we show the hidden opportunities for reducing water impact\nby optimizing location and time choices, paving the way for water-sustainable\ncomputing. The code is available at https://github.com/jojacola/SCARF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water consumption is an increasingly critical dimension of computing\nsustainability, especially as AI workloads rapidly scale. However, current\nwater impact assessment often overlooks where and when water stress is more\nsevere. To fill in this gap, we present SCARF, the first general framework that\nevaluates water impact of computing by factoring in both spatial and temporal\nvariations in water stress. SCARF calculates an Adjusted Water Impact (AWI)\nmetric that considers both consumption volume and local water stress over time.\nThrough three case studies on LLM serving, datacenters, and semiconductor\nfabrication plants, we show the hidden opportunities for reducing water impact\nby optimizing location and time choices, paving the way for water-sustainable\ncomputing. The code is available at https://github.com/jojacola/SCARF."
                },
                "authors": [
                    {
                        "name": "Yanran Wu"
                    },
                    {
                        "name": "Inez Hua"
                    },
                    {
                        "name": "Yi Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yi Ding"
                },
                "author": "Yi Ding",
                "arxiv_comment": "7 pages, 9 figures, The 4th Workshop on Sustainable Computer Systems\n  (HotCarbon'25), Cambridge, MA, July 10-11th, 2025",
                "arxiv_journal_ref": "ACM SIGEnergy Energy Informatics Review (EIR), Volume 5 Issue 2,\n  July 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04370v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04370v4",
                "updated": "2025-07-01T17:12:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    12,
                    1,
                    1,
                    182,
                    0
                ],
                "published": "2024-06-01T02:08:44Z",
                "published_parsed": [
                    2024,
                    6,
                    1,
                    2,
                    8,
                    44,
                    5,
                    153,
                    0
                ],
                "title": "Large Language Model Confidence Estimation via Black-Box Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Confidence Estimation via Black-Box Access"
                },
                "summary": "Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\\&A tasks as well\nas of Pegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\\&A tasks as well\nas of Pegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset."
                },
                "authors": [
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    },
                    {
                        "name": "Soumya Ghosh"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Prasanna Sattigeri"
                    }
                ],
                "author_detail": {
                    "name": "Prasanna Sattigeri"
                },
                "author": "Prasanna Sattigeri",
                "arxiv_comment": "Accepted to TMLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04370v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04370v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18597v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18597v3",
                "updated": "2025-07-01T17:06:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    6,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2024-05-28T21:19:15Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    21,
                    19,
                    15,
                    1,
                    149,
                    0
                ],
                "title": "Nonparametric causal inference for optogenetics: sequential excursion\n  effects for dynamic regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric causal inference for optogenetics: sequential excursion\n  effects for dynamic regimes"
                },
                "summary": "Optogenetics is a powerful neuroscience technique for studying how neural\ncircuit manipulation affects behavior. Standard analysis conventions discard\ninformation and severely limit the scope of the causal questions that can be\nprobed. To address this gap, we 1) draw connections to the causal inference\nliterature on sequentially randomized experiments, 2) propose a non-parametric\nframework for analyzing \"open-loop\" (static regime) optogenetics behavioral\nexperiments, 3) derive extensions of history-restricted marginal structural\nmodels for dynamic treatment regimes with positivity violations for\n\"closed-loop\" designs, and 4) propose a taxonomy of identifiable causal effects\nthat encompass a far richer collection of scientific questions compared to\nstandard methods. From another view, our work extends \"excursion effect\"\nmethods, popularized recently in the mobile health literature, to enable\nestimation of causal contrasts for treatment sequences in the presence of\npositivity violations. We describe sufficient conditions for identifiability of\nthe proposed causal estimands, and provide asymptotic statistical guarantees\nfor a proposed inverse probability-weighted estimator, a multiply-robust\nestimator (for two intervention timepoints), a framework for hypothesis\ntesting, and a computationally scalable implementation. Finally, we apply our\nframework to data from a recent neuroscience study and show how it provides\ninsight into causal effects of optogenetics on behavior that are obscured by\nstandard analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optogenetics is a powerful neuroscience technique for studying how neural\ncircuit manipulation affects behavior. Standard analysis conventions discard\ninformation and severely limit the scope of the causal questions that can be\nprobed. To address this gap, we 1) draw connections to the causal inference\nliterature on sequentially randomized experiments, 2) propose a non-parametric\nframework for analyzing \"open-loop\" (static regime) optogenetics behavioral\nexperiments, 3) derive extensions of history-restricted marginal structural\nmodels for dynamic treatment regimes with positivity violations for\n\"closed-loop\" designs, and 4) propose a taxonomy of identifiable causal effects\nthat encompass a far richer collection of scientific questions compared to\nstandard methods. From another view, our work extends \"excursion effect\"\nmethods, popularized recently in the mobile health literature, to enable\nestimation of causal contrasts for treatment sequences in the presence of\npositivity violations. We describe sufficient conditions for identifiability of\nthe proposed causal estimands, and provide asymptotic statistical guarantees\nfor a proposed inverse probability-weighted estimator, a multiply-robust\nestimator (for two intervention timepoints), a framework for hypothesis\ntesting, and a computationally scalable implementation. Finally, we apply our\nframework to data from a recent neuroscience study and show how it provides\ninsight into causal effects of optogenetics on behavior that are obscured by\nstandard analyses."
                },
                "authors": [
                    {
                        "name": "Gabriel Loewinger"
                    },
                    {
                        "name": "Alexander W. Levis"
                    },
                    {
                        "name": "Francisco Pereira"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Pereira"
                },
                "author": "Francisco Pereira",
                "arxiv_comment": "62 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18597v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18597v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19955v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19955v2",
                "updated": "2025-07-01T17:01:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    1,
                    12,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-26T13:18:37Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    13,
                    18,
                    37,
                    0,
                    146,
                    0
                ],
                "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research"
                },
                "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery."
                },
                "authors": [
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Miao Xiong"
                    },
                    {
                        "name": "Yujie Lu"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Ailin Deng"
                    },
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Yibo Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "42 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19955v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19955v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17336v2",
                "updated": "2025-07-01T16:41:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    41,
                    35,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-19T07:13:30Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    7,
                    13,
                    30,
                    3,
                    170,
                    0
                ],
                "title": "Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought\n  Reasoning and Homomorphically Encrypted Vector Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought\n  Reasoning and Homomorphically Encrypted Vector Databases"
                },
                "summary": "Large language models (LLMs) are increasingly used as personal agents,\naccessing sensitive user data such as calendars, emails, and medical records.\nUsers currently face a trade-off: They can send private records, many of which\nare stored in remote databases, to powerful but untrusted LLM providers,\nincreasing their exposure risk. Alternatively, they can run less powerful\nmodels locally on trusted devices. We bridge this gap. Our Socratic\nChain-of-Thought Reasoning first sends a generic, non-private user query to a\npowerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and\ndetailed sub-queries without accessing user data. Next, we embed these\nsub-queries and perform encrypted sub-second semantic search using our\nHomomorphically Encrypted Vector Database across one million entries of a\nsingle user's private data. This represents a realistic scale of personal\ndocuments, emails, and records accumulated over years of digital activity.\nFinally, we feed the CoT prompt and the decrypted records to a local language\nmodel and generate the final response. On the LoCoMo long-context QA benchmark,\nour hybrid framework, combining GPT-4o with a local Llama-3.2-1B model,\noutperforms using GPT-4o alone by up to 7.1 percentage points. This\ndemonstrates a first step toward systems where tasks are decomposed and split\nbetween untrusted strong LLMs and weak local ones, preserving user privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used as personal agents,\naccessing sensitive user data such as calendars, emails, and medical records.\nUsers currently face a trade-off: They can send private records, many of which\nare stored in remote databases, to powerful but untrusted LLM providers,\nincreasing their exposure risk. Alternatively, they can run less powerful\nmodels locally on trusted devices. We bridge this gap. Our Socratic\nChain-of-Thought Reasoning first sends a generic, non-private user query to a\npowerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and\ndetailed sub-queries without accessing user data. Next, we embed these\nsub-queries and perform encrypted sub-second semantic search using our\nHomomorphically Encrypted Vector Database across one million entries of a\nsingle user's private data. This represents a realistic scale of personal\ndocuments, emails, and records accumulated over years of digital activity.\nFinally, we feed the CoT prompt and the decrypted records to a local language\nmodel and generate the final response. On the LoCoMo long-context QA benchmark,\nour hybrid framework, combining GPT-4o with a local Llama-3.2-1B model,\noutperforms using GPT-4o alone by up to 7.1 percentage points. This\ndemonstrates a first step toward systems where tasks are decomposed and split\nbetween untrusted strong LLMs and weak local ones, preserving user privacy."
                },
                "authors": [
                    {
                        "name": "Yubeen Bae"
                    },
                    {
                        "name": "Minchan Kim"
                    },
                    {
                        "name": "Jaejin Lee"
                    },
                    {
                        "name": "Sangbum Kim"
                    },
                    {
                        "name": "Jaehyung Kim"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    }
                ],
                "author_detail": {
                    "name": "Niloofar Mireshghallah"
                },
                "author": "Niloofar Mireshghallah",
                "arxiv_comment": "29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13030v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13030v5",
                "updated": "2025-07-01T16:36:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    48,
                    1,
                    182,
                    0
                ],
                "published": "2025-02-18T16:46:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    46,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Conformal Inference under High-Dimensional Covariate Shifts via\n  Likelihood-Ratio Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Inference under High-Dimensional Covariate Shifts via\n  Likelihood-Ratio Regularization"
                },
                "summary": "We consider the problem of conformal prediction under covariate shift. Given\nlabeled data from a source domain and unlabeled data from a covariate shifted\ntarget domain, we seek to construct prediction sets with valid marginal\ncoverage in the target domain. Most existing methods require estimating the\nunknown likelihood ratio function, which can be prohibitive for\nhigh-dimensional data such as images. To address this challenge, we introduce\nthe likelihood ratio regularized quantile regression (LR-QR) algorithm, which\ncombines the pinball loss with a novel choice of regularization in order to\nconstruct a threshold function without directly estimating the unknown\nlikelihood ratio. We show that the LR-QR method has coverage at the desired\nlevel in the target domain, up to a small error term that we can control. Our\nproofs draw on a novel analysis of coverage via stability bounds from learning\ntheory. Our experiments demonstrate that the LR-QR algorithm outperforms\nexisting methods on high-dimensional prediction tasks, including a regression\ntask for the Communities and Crime dataset, an image classification task from\nthe WILDS repository, and an LLM question-answering task on the MMLU benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of conformal prediction under covariate shift. Given\nlabeled data from a source domain and unlabeled data from a covariate shifted\ntarget domain, we seek to construct prediction sets with valid marginal\ncoverage in the target domain. Most existing methods require estimating the\nunknown likelihood ratio function, which can be prohibitive for\nhigh-dimensional data such as images. To address this challenge, we introduce\nthe likelihood ratio regularized quantile regression (LR-QR) algorithm, which\ncombines the pinball loss with a novel choice of regularization in order to\nconstruct a threshold function without directly estimating the unknown\nlikelihood ratio. We show that the LR-QR method has coverage at the desired\nlevel in the target domain, up to a small error term that we can control. Our\nproofs draw on a novel analysis of coverage via stability bounds from learning\ntheory. Our experiments demonstrate that the LR-QR algorithm outperforms\nexisting methods on high-dimensional prediction tasks, including a regression\ntask for the Communities and Crime dataset, an image classification task from\nthe WILDS repository, and an LLM question-answering task on the MMLU benchmark."
                },
                "authors": [
                    {
                        "name": "Sunay Joshi"
                    },
                    {
                        "name": "Shayan Kiyani"
                    },
                    {
                        "name": "George Pappas"
                    },
                    {
                        "name": "Edgar Dobriban"
                    },
                    {
                        "name": "Hamed Hassani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Hassani"
                },
                "author": "Hamed Hassani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13030v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13030v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24625v2",
                "updated": "2025-07-01T16:26:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    26,
                    47,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-30T14:16:41Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    16,
                    41,
                    4,
                    150,
                    0
                ],
                "title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors"
                },
                "summary": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method, the Video-3D Geometry Large\nLanguage Model (VG LLM). Our approach employs a 3D visual geometry encoder that\nextracts 3D prior information from video sequences. This information is\nintegrated with visual tokens and fed into the MLLM. Extensive experiments have\nshown that our method has achieved substantial improvements in various tasks\nrelated to 3D scene understanding and spatial reasoning, all directly learned\nfrom video sources. Impressively, our 4B model, which does not rely on explicit\n3D data inputs, achieves competitive results compared to existing\nstate-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method, the Video-3D Geometry Large\nLanguage Model (VG LLM). Our approach employs a 3D visual geometry encoder that\nextracts 3D prior information from video sequences. This information is\nintegrated with visual tokens and fed into the MLLM. Extensive experiments have\nshown that our method has achieved substantial improvements in various tasks\nrelated to 3D scene understanding and spatial reasoning, all directly learned\nfrom video sources. Impressively, our 4B model, which does not rely on explicit\n3D data inputs, achieves competitive results compared to existing\nstate-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations."
                },
                "authors": [
                    {
                        "name": "Duo Zheng"
                    },
                    {
                        "name": "Shijia Huang"
                    },
                    {
                        "name": "Yanyang Li"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23986v2",
                "updated": "2025-07-01T16:23:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    23,
                    28,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T15:50:08Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    50,
                    8,
                    0,
                    181,
                    0
                ],
                "title": "StreamFlow: Streaming Flow Matching with Block-wise Guided Attention\n  Mask for Speech Token Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamFlow: Streaming Flow Matching with Block-wise Guided Attention\n  Mask for Speech Token Decoding"
                },
                "summary": "Recent advancements in discrete token-based speech generation have\nhighlighted the importance of token-to-waveform generation for audio quality,\nparticularly in real-time interactions. Traditional frameworks integrating\nsemantic tokens with flow matching (FM) struggle with streaming capabilities\ndue to their reliance on a global receptive field. Additionally, directly\nimplementing token-by-token streaming speech generation often results in\ndegraded audio quality. To address these challenges, we propose StreamFlow, a\nnovel neural architecture that facilitates streaming flow matching with\ndiffusion transformers (DiT). To mitigate the long-sequence extrapolation\nissues arising from lengthy historical dependencies, we design a local\nblock-wise receptive field strategy. Specifically, the sequence is first\nsegmented into blocks, and we introduce block-wise attention masks that enable\nthe current block to receive information from the previous or subsequent block.\nThese attention masks are combined hierarchically across different DiT-blocks\nto regulate the receptive field of DiTs. Both subjective and objective\nexperimental results demonstrate that our approach achieves performance\ncomparable to non-streaming methods while surpassing other streaming methods in\nterms of speech quality, all the while effectively managing inference time\nduring long-sequence generation. Furthermore, our method achieves a notable\nfirst-packet latency of only 180 ms.\\footnote{Speech samples:\nhttps://dukguo.github.io/StreamFlow/}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in discrete token-based speech generation have\nhighlighted the importance of token-to-waveform generation for audio quality,\nparticularly in real-time interactions. Traditional frameworks integrating\nsemantic tokens with flow matching (FM) struggle with streaming capabilities\ndue to their reliance on a global receptive field. Additionally, directly\nimplementing token-by-token streaming speech generation often results in\ndegraded audio quality. To address these challenges, we propose StreamFlow, a\nnovel neural architecture that facilitates streaming flow matching with\ndiffusion transformers (DiT). To mitigate the long-sequence extrapolation\nissues arising from lengthy historical dependencies, we design a local\nblock-wise receptive field strategy. Specifically, the sequence is first\nsegmented into blocks, and we introduce block-wise attention masks that enable\nthe current block to receive information from the previous or subsequent block.\nThese attention masks are combined hierarchically across different DiT-blocks\nto regulate the receptive field of DiTs. Both subjective and objective\nexperimental results demonstrate that our approach achieves performance\ncomparable to non-streaming methods while surpassing other streaming methods in\nterms of speech quality, all the while effectively managing inference time\nduring long-sequence generation. Furthermore, our method achieves a notable\nfirst-packet latency of only 180 ms.\\footnote{Speech samples:\nhttps://dukguo.github.io/StreamFlow/}"
                },
                "authors": [
                    {
                        "name": "Dake Guo"
                    },
                    {
                        "name": "Jixun Yao"
                    },
                    {
                        "name": "Linhan Ma"
                    },
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18325v2",
                "updated": "2025-07-01T16:14:42Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    14,
                    42,
                    1,
                    182,
                    0
                ],
                "published": "2025-02-25T16:20:10Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    16,
                    20,
                    10,
                    1,
                    56,
                    0
                ],
                "title": "A Unified Bayesian Perspective for Conventional and Robust Adaptive\n  Filters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Bayesian Perspective for Conventional and Robust Adaptive\n  Filters"
                },
                "summary": "In this work, we present a new perspective on the origin and interpretation\nof adaptive filters. By applying Bayesian principles of recursive inference\nfrom the state-space model and using a series of simplifications regarding the\nstructure of the solution, we can present, in a unified framework, derivations\nof many adaptive filters that depend on the probabilistic model of the\nmeasurement noise. In particular, under a Gaussian model, we obtain solutions\nwell-known in the literature (such as LMS, NLMS, or Kalman filter), while using\nnon-Gaussian noise, we derive new adaptive algorithms. Notably, under the\nassumption of Laplacian noise, we obtain a family of robust filters of which\nthe sign-error algorithm is a well-known member, while other algorithms,\nderived effortlessly in the proposed framework, are entirely new. Numerical\nexamples are shown to illustrate the properties and provide a better insight\ninto the performance of the derived adaptive filters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present a new perspective on the origin and interpretation\nof adaptive filters. By applying Bayesian principles of recursive inference\nfrom the state-space model and using a series of simplifications regarding the\nstructure of the solution, we can present, in a unified framework, derivations\nof many adaptive filters that depend on the probabilistic model of the\nmeasurement noise. In particular, under a Gaussian model, we obtain solutions\nwell-known in the literature (such as LMS, NLMS, or Kalman filter), while using\nnon-Gaussian noise, we derive new adaptive algorithms. Notably, under the\nassumption of Laplacian noise, we obtain a family of robust filters of which\nthe sign-error algorithm is a well-known member, while other algorithms,\nderived effortlessly in the proposed framework, are entirely new. Numerical\nexamples are shown to illustrate the properties and provide a better insight\ninto the performance of the derived adaptive filters."
                },
                "authors": [
                    {
                        "name": "Leszek Szczecinski"
                    },
                    {
                        "name": "Jacob Benesty"
                    },
                    {
                        "name": "Eduardo Vinicius Kuhn"
                    }
                ],
                "author_detail": {
                    "name": "Eduardo Vinicius Kuhn"
                },
                "author": "Eduardo Vinicius Kuhn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03628v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03628v2",
                "updated": "2025-07-01T16:02:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    2,
                    21,
                    1,
                    182,
                    0
                ],
                "published": "2025-02-05T21:34:02Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    21,
                    34,
                    2,
                    2,
                    36,
                    0
                ],
                "title": "The Hidden Life of Tokens: Reducing Hallucination of Large\n  Vision-Language Models via Visual Information Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hidden Life of Tokens: Reducing Hallucination of Large\n  Vision-Language Models via Visual Information Steering"
                },
                "summary": "Large Vision-Language Models (LVLMs) can reason effectively over both textual\nand visual inputs, but they tend to hallucinate syntactically coherent yet\nvisually ungrounded contents. In this paper, we investigate the internal\ndynamics of hallucination by examining the tokens logits ranking throughout the\ngeneration process, revealing three key patterns in how LVLMs process\ninformation: (1) gradual visual information loss - visually grounded tokens\ngradually become less favored throughout generation, and (2) early excitation -\nsemantically meaningful tokens achieve peak activation in the layers earlier\nthan the final layer. (3) hidden genuine information - visually grounded tokens\nthough not being eventually decoded still retain relatively high rankings at\ninference. Based on these insights, we propose VISTA (Visual Information\nSteering with Token-logit Augmentation), a training-free inference-time\nintervention framework that reduces hallucination while promoting genuine\ninformation. VISTA works by combining two complementary approaches: reinforcing\nvisual information in activation space and leveraging early layer activations\nto promote semantically meaningful decoding. Compared to existing methods,\nVISTA requires no external supervision and is applicable to various decoding\nstrategies. Extensive experiments show that VISTA on average reduces\nhallucination by about 40% on evaluated open-ended generation task, and it\nconsistently outperforms existing methods on four benchmarks across four\narchitectures under three decoding strategies. Code is available at\nhttps://github.com/LzVv123456/VISTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) can reason effectively over both textual\nand visual inputs, but they tend to hallucinate syntactically coherent yet\nvisually ungrounded contents. In this paper, we investigate the internal\ndynamics of hallucination by examining the tokens logits ranking throughout the\ngeneration process, revealing three key patterns in how LVLMs process\ninformation: (1) gradual visual information loss - visually grounded tokens\ngradually become less favored throughout generation, and (2) early excitation -\nsemantically meaningful tokens achieve peak activation in the layers earlier\nthan the final layer. (3) hidden genuine information - visually grounded tokens\nthough not being eventually decoded still retain relatively high rankings at\ninference. Based on these insights, we propose VISTA (Visual Information\nSteering with Token-logit Augmentation), a training-free inference-time\nintervention framework that reduces hallucination while promoting genuine\ninformation. VISTA works by combining two complementary approaches: reinforcing\nvisual information in activation space and leveraging early layer activations\nto promote semantically meaningful decoding. Compared to existing methods,\nVISTA requires no external supervision and is applicable to various decoding\nstrategies. Extensive experiments show that VISTA on average reduces\nhallucination by about 40% on evaluated open-ended generation task, and it\nconsistently outperforms existing methods on four benchmarks across four\narchitectures under three decoding strategies. Code is available at\nhttps://github.com/LzVv123456/VISTA."
                },
                "authors": [
                    {
                        "name": "Zhuowei Li"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Yunhe Gao"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Yuxiao Chen"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Long Zhao"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Dimitris N. Metaxas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris N. Metaxas"
                },
                "author": "Dimitris N. Metaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03628v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03628v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18710v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18710v3",
                "updated": "2025-07-01T15:49:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    49,
                    58,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-23T14:49:01Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    49,
                    1,
                    0,
                    174,
                    0
                ],
                "title": "Benchmarking the Pedagogical Knowledge of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the Pedagogical Knowledge of Large Language Models"
                },
                "summary": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions."
                },
                "authors": [
                    {
                        "name": "Maxime Lelivre"
                    },
                    {
                        "name": "Amy Waldock"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Natalia Valds Aspillaga"
                    },
                    {
                        "name": "Alasdair Mackintosh"
                    },
                    {
                        "name": "Mara Jos Ogando Portela"
                    },
                    {
                        "name": "Jared Lee"
                    },
                    {
                        "name": "Paul Atherton"
                    },
                    {
                        "name": "Robin A. A. Ince"
                    },
                    {
                        "name": "Oliver G. B. Garrod"
                    }
                ],
                "author_detail": {
                    "name": "Oliver G. B. Garrod"
                },
                "author": "Oliver G. B. Garrod",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18710v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18710v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12514v2",
                "updated": "2025-07-01T15:33:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    33,
                    56,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-18T18:36:53Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    18,
                    36,
                    53,
                    6,
                    138,
                    0
                ],
                "title": "Reasoning by Superposition: A Theoretical Perspective on Chain of\n  Continuous Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning by Superposition: A Theoretical Perspective on Chain of\n  Continuous Thought"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in many\napplications, including challenging reasoning problems via chain-of-thoughts\n(CoTs) techniques that generate ``thinking tokens'' before answering the\nquestions. While existing theoretical works demonstrate that CoTs with discrete\ntokens boost the capability of LLMs, recent work on continuous CoTs lacks a\ntheoretical understanding of why it outperforms discrete counterparts in\nvarious reasoning tasks such as directed graph reachability, a fundamental\ngraph reasoning problem that includes many practical domain applications as\nspecial cases. In this paper, we prove that a two-layer transformer with $D$\nsteps of continuous CoTs can solve the directed graph reachability problem,\nwhere $D$ is the diameter of the graph, while the best known result of\nconstant-depth transformers with discrete CoTs requires $O(n^2)$ decoding steps\nwhere $n$ is the number of vertices ($D<n$). In our construction, each\ncontinuous thought vector is a superposition state that encodes multiple search\nfrontiers simultaneously (i.e., parallel breadth-first search (BFS)), while\ndiscrete CoTs must choose a single path sampled from the superposition state,\nwhich leads to sequential search that requires many more steps and may be\ntrapped into local solutions. We also performed extensive experiments to verify\nthat our theoretical construction aligns well with the empirical solution\nobtained via training dynamics. Notably, encoding of multiple search frontiers\nas a superposition state automatically emerges in training continuous CoTs,\nwithout explicit supervision to guide the model to explore multiple paths\nsimultaneously.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance in many\napplications, including challenging reasoning problems via chain-of-thoughts\n(CoTs) techniques that generate ``thinking tokens'' before answering the\nquestions. While existing theoretical works demonstrate that CoTs with discrete\ntokens boost the capability of LLMs, recent work on continuous CoTs lacks a\ntheoretical understanding of why it outperforms discrete counterparts in\nvarious reasoning tasks such as directed graph reachability, a fundamental\ngraph reasoning problem that includes many practical domain applications as\nspecial cases. In this paper, we prove that a two-layer transformer with $D$\nsteps of continuous CoTs can solve the directed graph reachability problem,\nwhere $D$ is the diameter of the graph, while the best known result of\nconstant-depth transformers with discrete CoTs requires $O(n^2)$ decoding steps\nwhere $n$ is the number of vertices ($D<n$). In our construction, each\ncontinuous thought vector is a superposition state that encodes multiple search\nfrontiers simultaneously (i.e., parallel breadth-first search (BFS)), while\ndiscrete CoTs must choose a single path sampled from the superposition state,\nwhich leads to sequential search that requires many more steps and may be\ntrapped into local solutions. We also performed extensive experiments to verify\nthat our theoretical construction aligns well with the empirical solution\nobtained via training dynamics. Notably, encoding of multiple search frontiers\nas a superposition state automatically emerges in training continuous CoTs,\nwithout explicit supervision to guide the model to explore multiple paths\nsimultaneously."
                },
                "authors": [
                    {
                        "name": "Hanlin Zhu"
                    },
                    {
                        "name": "Shibo Hao"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Stuart Russell"
                    },
                    {
                        "name": "Yuandong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yuandong Tian"
                },
                "author": "Yuandong Tian",
                "arxiv_comment": "26 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23427v2",
                "updated": "2025-07-01T15:27:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    27,
                    37,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-29T23:24:33Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    23,
                    24,
                    33,
                    6,
                    180,
                    0
                ],
                "title": "Probing the detectability of electromagnetic signatures from Galactic\n  isolated black holes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the detectability of electromagnetic signatures from Galactic\n  isolated black holes"
                },
                "summary": "Context: A large number of isolated black holes (IBHs) are expected to\npopulate the Galaxy. However, only one has been confirmed by the analysis of a\nmicrolensing event, and no confirmed emission detection from an IBH has been\nreported so far.\n  Aims: We analysed the detectability of electromagnetic signatures from IBHs\nmoving in the Galaxy. Methods: We considered accretion from the interstellar\nmedium onto an IBH and assumed the formation of an outflow. We modelled the\naccretion process and the interaction of the outflow with the surrounding\nmedium on large scales, including mechanical feedback on the accretion process.\nFurthermore, we also calculated the emission from three different regions: the\naccretion region, the radiation from the outflow medium interaction structure,\nand the emission of relativistic particles that diffuse in the surrounding\nmedium.\n  Results: Multiwavelength emission associated with Galactic IBHs can be\ndetected in systems moving through a very dense medium. Thermal emission from\naccretion could be observed in the mid infrared and in hard X rays with current\nand forthcoming observatories. Thermal and non thermal emission from the\noutflow medium shock could also be detected in the radio and millimetre ranges.\nMoreover, detection of the emission from particles diffusing in a dense medium\ncould be feasible in gamma rays. Applying our model to the IBH associated with\nthe gravitational microlensing event MOA2011BLG191 OGLE2011BLG0462, we inferred\nthat radio and infrared detection of the IBH is plausible. Also, we derived\nthat IBHs could be modest Galactic cosmic ray contributors, potentially\nreaching a 1% contribution at 1 PeV. Finally, by extending our model to\nprimordial black holes, we conclude that efficient leptonic acceleration in\ntheir outflow medium interactions would rule them out as a major dark matter\ncomponent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: A large number of isolated black holes (IBHs) are expected to\npopulate the Galaxy. However, only one has been confirmed by the analysis of a\nmicrolensing event, and no confirmed emission detection from an IBH has been\nreported so far.\n  Aims: We analysed the detectability of electromagnetic signatures from IBHs\nmoving in the Galaxy. Methods: We considered accretion from the interstellar\nmedium onto an IBH and assumed the formation of an outflow. We modelled the\naccretion process and the interaction of the outflow with the surrounding\nmedium on large scales, including mechanical feedback on the accretion process.\nFurthermore, we also calculated the emission from three different regions: the\naccretion region, the radiation from the outflow medium interaction structure,\nand the emission of relativistic particles that diffuse in the surrounding\nmedium.\n  Results: Multiwavelength emission associated with Galactic IBHs can be\ndetected in systems moving through a very dense medium. Thermal emission from\naccretion could be observed in the mid infrared and in hard X rays with current\nand forthcoming observatories. Thermal and non thermal emission from the\noutflow medium shock could also be detected in the radio and millimetre ranges.\nMoreover, detection of the emission from particles diffusing in a dense medium\ncould be feasible in gamma rays. Applying our model to the IBH associated with\nthe gravitational microlensing event MOA2011BLG191 OGLE2011BLG0462, we inferred\nthat radio and infrared detection of the IBH is plausible. Also, we derived\nthat IBHs could be modest Galactic cosmic ray contributors, potentially\nreaching a 1% contribution at 1 PeV. Finally, by extending our model to\nprimordial black holes, we conclude that efficient leptonic acceleration in\ntheir outflow medium interactions would rule them out as a major dark matter\ncomponent."
                },
                "authors": [
                    {
                        "name": "Javier Rodrigo Martinez"
                    },
                    {
                        "name": "Valenti Bosch-Ramon"
                    },
                    {
                        "name": "Florencia Laura Vieyro"
                    },
                    {
                        "name": "Santiago del Palacio"
                    }
                ],
                "author_detail": {
                    "name": "Santiago del Palacio"
                },
                "author": "Santiago del Palacio",
                "arxiv_doi": "10.1051/0004-6361/202554910",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202554910",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.23427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 7 figures. Accepted for publication in the section \"2.\n  Astrophysical processes\" of Astronomy & Astrophysics",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22698v2",
                "updated": "2025-07-01T15:26:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    26,
                    29,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-28T00:31:14Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    0,
                    31,
                    14,
                    5,
                    179,
                    0
                ],
                "title": "Text Production and Comprehension by Human and Artificial Intelligence:\n  Interdisciplinary Workshop Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Production and Comprehension by Human and Artificial Intelligence:\n  Interdisciplinary Workshop Report"
                },
                "summary": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration."
                },
                "authors": [
                    {
                        "name": "Emily Dux Speltz"
                    }
                ],
                "author_detail": {
                    "name": "Emily Dux Speltz"
                },
                "author": "Emily Dux Speltz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11163v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11163v4",
                "updated": "2025-07-01T15:17:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    17,
                    41,
                    1,
                    182,
                    0
                ],
                "published": "2024-09-17T13:17:25Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    13,
                    17,
                    25,
                    1,
                    261,
                    0
                ],
                "title": "Fast radio bursts as a probe of gravity on cosmological scales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast radio bursts as a probe of gravity on cosmological scales"
                },
                "summary": "We explore the potential for improving constraints on gravity by leveraging\ncorrelations in the dispersion measure derived from Fast Radio Bursts (FRBs) in\ncombination with cosmic shear. Specifically, we focus on Horndeski gravity,\ninferring the kinetic braiding and Planck mass run rate from a stage-4 cosmic\nshear mock survey alongside a survey comprising $10^4$ FRBs. For the inference\npipeline, we utilise the Boltzmann code hi_class to predict the linear matter\npower spectrum in modified gravity scenarios, while non-linear corrections are\nobtained from the halo-model employed in HMcode, including feedback mechanisms.\nOur findings indicate that FRBs can disentangle degeneracies between baryonic\nfeedback and cosmological parameters, as well as the mass of massive neutrinos.\nSince these parameters are also degenerate with modified gravity parameters,\nthe inclusion of FRBs can enhance constraints on Horndeski parameters by up to\n$40$ percent, despite being a less significant measurement. Additionally, we\napply our model to current FRB data and use the uncertainty in the\n$\\mathrm{DM}-z$ relation to impose limits on gravity. However, due to the\nlimited sample size of current data, constraints are predominantly influenced\nby theoretical priors. Despite this, our study demonstrates that FRBs will\nsignificantly augment the limited set of cosmological probes available, playing\na critical role in providing alternative tests of feedback, cosmology, and\ngravity. All codes used in this work are made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the potential for improving constraints on gravity by leveraging\ncorrelations in the dispersion measure derived from Fast Radio Bursts (FRBs) in\ncombination with cosmic shear. Specifically, we focus on Horndeski gravity,\ninferring the kinetic braiding and Planck mass run rate from a stage-4 cosmic\nshear mock survey alongside a survey comprising $10^4$ FRBs. For the inference\npipeline, we utilise the Boltzmann code hi_class to predict the linear matter\npower spectrum in modified gravity scenarios, while non-linear corrections are\nobtained from the halo-model employed in HMcode, including feedback mechanisms.\nOur findings indicate that FRBs can disentangle degeneracies between baryonic\nfeedback and cosmological parameters, as well as the mass of massive neutrinos.\nSince these parameters are also degenerate with modified gravity parameters,\nthe inclusion of FRBs can enhance constraints on Horndeski parameters by up to\n$40$ percent, despite being a less significant measurement. Additionally, we\napply our model to current FRB data and use the uncertainty in the\n$\\mathrm{DM}-z$ relation to impose limits on gravity. However, due to the\nlimited sample size of current data, constraints are predominantly influenced\nby theoretical priors. Despite this, our study demonstrates that FRBs will\nsignificantly augment the limited set of cosmological probes available, playing\na critical role in providing alternative tests of feedback, cosmology, and\ngravity. All codes used in this work are made publicly available."
                },
                "authors": [
                    {
                        "name": "Dennis Neumann"
                    },
                    {
                        "name": "Robert Reischke"
                    },
                    {
                        "name": "Steffen Hagstotz"
                    },
                    {
                        "name": "Hendrik Hildebrandt"
                    }
                ],
                "author_detail": {
                    "name": "Hendrik Hildebrandt"
                },
                "author": "Hendrik Hildebrandt",
                "arxiv_doi": "10.33232/001c.140864",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.33232/001c.140864",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.11163v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11163v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 11 figures, 5 tables; The newest version is a correction to\n  the previous version accepted for publication by the Open Journal of\n  Astrophysics. Due to a bug, the matter power spectrum was generated with 6\n  ultra-relativistic species instead of 3, leading to power suppression on\n  small scales. This bug has been fixed and all results updated",
                "arxiv_journal_ref": "Open J. of Astrophysics 8 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04568v2",
                "updated": "2025-07-01T15:16:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    16,
                    39,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-07T17:03:22Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    3,
                    22,
                    2,
                    127,
                    0
                ],
                "title": "Conformal Survival Bands for Risk Screening under Right-Censoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Survival Bands for Risk Screening under Right-Censoring"
                },
                "summary": "We propose a method to quantify uncertainty around individual survival\ndistribution estimates using right-censored data, compatible with any survival\nmodel. Unlike classical confidence intervals, the survival bands produced by\nthis method offer predictive rather than population-level inference, making\nthem useful for personalized risk screening. For example, in a low-risk\nscreening scenario, they can be applied to flag patients whose survival band at\n12 months lies entirely above 50\\%, while ensuring that at least half of\nflagged individuals will survive past that time on average. Our approach builds\non recent advances in conformal inference and integrates ideas from inverse\nprobability of censoring weighting and multiple testing with false discovery\nrate control. We provide asymptotic guarantees and show promising performance\nin finite samples with both simulated and real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method to quantify uncertainty around individual survival\ndistribution estimates using right-censored data, compatible with any survival\nmodel. Unlike classical confidence intervals, the survival bands produced by\nthis method offer predictive rather than population-level inference, making\nthem useful for personalized risk screening. For example, in a low-risk\nscreening scenario, they can be applied to flag patients whose survival band at\n12 months lies entirely above 50\\%, while ensuring that at least half of\nflagged individuals will survive past that time on average. Our approach builds\non recent advances in conformal inference and integrates ideas from inverse\nprobability of censoring weighting and multiple testing with false discovery\nrate control. We provide asymptotic guarantees and show promising performance\nin finite samples with both simulated and real data."
                },
                "authors": [
                    {
                        "name": "Matteo Sesia"
                    },
                    {
                        "name": "Vladimir Svetnik"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Svetnik"
                },
                "author": "Vladimir Svetnik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.15032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.15032v2",
                "updated": "2025-07-01T15:14:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    14,
                    9,
                    1,
                    182,
                    0
                ],
                "published": "2023-09-26T16:01:54Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    16,
                    1,
                    54,
                    1,
                    269,
                    0
                ],
                "title": "SOFARI: High-Dimensional Manifold-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOFARI: High-Dimensional Manifold-Based Inference"
                },
                "summary": "Multi-task learning is a widely used technique for harnessing information\nfrom various tasks. Recently, the sparse orthogonal factor regression (SOFAR)\nframework, based on the sparse singular value decomposition (SVD) within the\ncoefficient matrix, was introduced for interpretable multi-task learning,\nenabling the discovery of meaningful latent feature-response association\nnetworks across different layers. However, conducting precise inference on the\nlatent factor matrices has remained challenging due to the orthogonality\nconstraints inherited from the sparse SVD constraints. In this paper, we\nsuggest a novel approach called the high-dimensional manifold-based SOFAR\ninference (SOFARI), drawing on the Neyman near-orthogonality inference while\nincorporating the Stiefel manifold structure imposed by the SVD constraints. By\nleveraging the underlying Stiefel manifold structure that is crucial to\nenabling inference, SOFARI provides easy-to-use bias-corrected estimators for\nboth latent left factor vectors and singular values, for which we show to enjoy\nthe asymptotic mean-zero normal distributions with estimable variances. We\nintroduce two SOFARI variants to handle strongly and weakly orthogonal latent\nfactors, where the latter covers a broader range of applications. We illustrate\nthe effectiveness of SOFARI and justify our theoretical results through\nsimulation examples and a real data application in economic forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-task learning is a widely used technique for harnessing information\nfrom various tasks. Recently, the sparse orthogonal factor regression (SOFAR)\nframework, based on the sparse singular value decomposition (SVD) within the\ncoefficient matrix, was introduced for interpretable multi-task learning,\nenabling the discovery of meaningful latent feature-response association\nnetworks across different layers. However, conducting precise inference on the\nlatent factor matrices has remained challenging due to the orthogonality\nconstraints inherited from the sparse SVD constraints. In this paper, we\nsuggest a novel approach called the high-dimensional manifold-based SOFAR\ninference (SOFARI), drawing on the Neyman near-orthogonality inference while\nincorporating the Stiefel manifold structure imposed by the SVD constraints. By\nleveraging the underlying Stiefel manifold structure that is crucial to\nenabling inference, SOFARI provides easy-to-use bias-corrected estimators for\nboth latent left factor vectors and singular values, for which we show to enjoy\nthe asymptotic mean-zero normal distributions with estimable variances. We\nintroduce two SOFARI variants to handle strongly and weakly orthogonal latent\nfactors, where the latter covers a broader range of applications. We illustrate\nthe effectiveness of SOFARI and justify our theoretical results through\nsimulation examples and a real data application in economic forecasting."
                },
                "authors": [
                    {
                        "name": "Zemin Zheng"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Yingying Fan"
                    },
                    {
                        "name": "Jinchi Lv"
                    }
                ],
                "author_detail": {
                    "name": "Jinchi Lv"
                },
                "author": "Jinchi Lv",
                "arxiv_comment": "208 pages, 2 figures, to appear in Journal of the American\n  Statistical Association",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.15032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.15032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10747v2",
                "updated": "2025-07-01T15:13:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    13,
                    39,
                    1,
                    182,
                    0
                ],
                "published": "2024-02-16T15:13:30Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    15,
                    13,
                    30,
                    4,
                    47,
                    0
                ],
                "title": "Fully Differentiable Lagrangian Convolutional Neural Network for\n  Physics-Informed Precipitation Nowcasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Differentiable Lagrangian Convolutional Neural Network for\n  Physics-Informed Precipitation Nowcasting"
                },
                "summary": "This paper presents a convolutional neural network model for precipitation\nnowcasting that combines data-driven learning with physics-informed domain\nknowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed\nNowcasting, that draws from existing extrapolation-based nowcasting methods. It\nconsists of a U-Net that dynamically produces mesoscale advection motion\nfields, a differentiable semi-Lagrangian extrapolation operator, and an\nadvection-free U-Net capturing the growth and decay of precipitation over time.\nUsing our approach, we successfully implement the Lagrangian convolutional\nneural network for precipitation nowcasting in a fully differentiable and\nGPU-accelerated manner. This allows for end-to-end training and inference,\nincluding the data-driven Lagrangian coordinate system transformation of the\ndata at runtime. We evaluate the model and compare it with other related\nAI-based models both quantitatively and qualitatively in an extreme event case\nstudy. Based on our evaluation, LUPIN matches and even exceeds the performance\nof the chosen benchmarks, opening the door for other Lagrangian machine\nlearning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a convolutional neural network model for precipitation\nnowcasting that combines data-driven learning with physics-informed domain\nknowledge. We propose LUPIN, a Lagrangian Double U-Net for Physics-Informed\nNowcasting, that draws from existing extrapolation-based nowcasting methods. It\nconsists of a U-Net that dynamically produces mesoscale advection motion\nfields, a differentiable semi-Lagrangian extrapolation operator, and an\nadvection-free U-Net capturing the growth and decay of precipitation over time.\nUsing our approach, we successfully implement the Lagrangian convolutional\nneural network for precipitation nowcasting in a fully differentiable and\nGPU-accelerated manner. This allows for end-to-end training and inference,\nincluding the data-driven Lagrangian coordinate system transformation of the\ndata at runtime. We evaluate the model and compare it with other related\nAI-based models both quantitatively and qualitatively in an extreme event case\nstudy. Based on our evaluation, LUPIN matches and even exceeds the performance\nof the chosen benchmarks, opening the door for other Lagrangian machine\nlearning models."
                },
                "authors": [
                    {
                        "name": "Peter Pavlk"
                    },
                    {
                        "name": "Martin Vboh"
                    },
                    {
                        "name": "Anna Bou Ezzeddine"
                    },
                    {
                        "name": "Viera Rozinajov"
                    }
                ],
                "author_detail": {
                    "name": "Viera Rozinajov"
                },
                "author": "Viera Rozinajov",
                "arxiv_comment": "Submitted to Applied Computing and Geosciences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04707v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04707v3",
                "updated": "2025-07-01T15:12:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    12,
                    22,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-07T18:01:01Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    18,
                    1,
                    1,
                    2,
                    127,
                    0
                ],
                "title": "Physical Conditions of the Ionized Superwind in NGC 253 with VLT/MUSE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Conditions of the Ionized Superwind in NGC 253 with VLT/MUSE"
                },
                "summary": "We present an analysis of the H$\\alpha$-emitting ionized gas in the warm\nphase of the NGC 253 outflow using integral field spectroscopy from the Multi\nUnit Spectroscopic Explorer (MUSE). In each spaxel, we decompose H$\\alpha$, [N\nII], and [S II] emission lines into a system of up to 3 Gaussian components,\naccounting for the velocity contributions due to the disk and both intercepted\nwalls of an outflow cone. In the approaching southern lobe of the outflow, we\nfind maximum deprojected outflow velocities down to ~ -500 km/s. Velocity\ngradients of this outflowing gas range from ~ -350 to -550 km/s/kpc with\nincreasing distance from the nucleus. Additionally, [N II]/H$\\alpha$ and [S\nII]/H$\\alpha$ integrated line ratios are suggestive of shocks as the dominant\nionization source throughout the wind. Electron densities, inferred from the [S\nII] doublet, peak at 2100 cm$^{-3}$ near the nucleus and reach $\\lesssim 50\n$cm$^{-3}$ in the wind. Finally, at an uncertainty of 0.3 dex on the inferred\nmass of $4\\times10^{5}$ M$_{\\odot}$, the mass-outflow rate of the\nH$\\alpha$-emitting gas in the southern outflow lobe is ~ 0.4 M$_{\\odot}$/year.\nThis yields a mass-loading factor of $\\eta$ ~ 0.1 and a ~ 2% starburst energy\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an analysis of the H$\\alpha$-emitting ionized gas in the warm\nphase of the NGC 253 outflow using integral field spectroscopy from the Multi\nUnit Spectroscopic Explorer (MUSE). In each spaxel, we decompose H$\\alpha$, [N\nII], and [S II] emission lines into a system of up to 3 Gaussian components,\naccounting for the velocity contributions due to the disk and both intercepted\nwalls of an outflow cone. In the approaching southern lobe of the outflow, we\nfind maximum deprojected outflow velocities down to ~ -500 km/s. Velocity\ngradients of this outflowing gas range from ~ -350 to -550 km/s/kpc with\nincreasing distance from the nucleus. Additionally, [N II]/H$\\alpha$ and [S\nII]/H$\\alpha$ integrated line ratios are suggestive of shocks as the dominant\nionization source throughout the wind. Electron densities, inferred from the [S\nII] doublet, peak at 2100 cm$^{-3}$ near the nucleus and reach $\\lesssim 50\n$cm$^{-3}$ in the wind. Finally, at an uncertainty of 0.3 dex on the inferred\nmass of $4\\times10^{5}$ M$_{\\odot}$, the mass-outflow rate of the\nH$\\alpha$-emitting gas in the southern outflow lobe is ~ 0.4 M$_{\\odot}$/year.\nThis yields a mass-loading factor of $\\eta$ ~ 0.1 and a ~ 2% starburst energy\nefficiency."
                },
                "authors": [
                    {
                        "name": "Serena A. Cronin"
                    },
                    {
                        "name": "Alberto D. Bolatto"
                    },
                    {
                        "name": "Enrico Congiu"
                    },
                    {
                        "name": "Keaton Donaghue"
                    },
                    {
                        "name": "Kathryn Kreckel"
                    },
                    {
                        "name": "Adam K. Leroy"
                    },
                    {
                        "name": "Rebecca C. Levy"
                    },
                    {
                        "name": "Sylvain Veilleux"
                    },
                    {
                        "name": "Fabian Walter"
                    },
                    {
                        "name": "Lenin Nolasco"
                    }
                ],
                "author_detail": {
                    "name": "Lenin Nolasco"
                },
                "author": "Lenin Nolasco",
                "arxiv_doi": "10.3847/1538-4357/add738",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/add738",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.04707v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04707v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, published 1 July 2025",
                "arxiv_journal_ref": "ApJ 987 92 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13759v2",
                "updated": "2025-07-01T15:08:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    8,
                    58,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-16T17:59:08Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    59,
                    8,
                    0,
                    167,
                    0
                ],
                "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete Diffusion in Large Language and Multimodal Models: A Survey"
                },
                "summary": "In this work, we provide a systematic survey of Discrete Diffusion Language\nModels (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).\nUnlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,\nparallel decoding paradigm using full attention and a denoising-based\ngeneration strategy. This paradigm naturally enables parallel generation,\nfine-grained output controllability, and dynamic, response-aware perception.\nThese capabilities are previously difficult to achieve with AR models.\nRecently, a growing number of industrial-scale proprietary d(M)LLMs, as well as\na large number of open-source academic d(M)LLMs, have demonstrated performance\ncomparable to their autoregressive counterparts, while achieving up to 10x\nacceleration in inference speed.\n  The advancement of discrete diffusion LLMs and MLLMs has been largely driven\nby progress in two domains. The first is the development of autoregressive LLMs\nand MLLMs, which has accumulated vast amounts of data, benchmarks, and\nfoundational infrastructure for training and inference. The second contributing\ndomain is the evolution of the mathematical models underlying discrete\ndiffusion. Together, these advancements have catalyzed a surge in dLLMs and\ndMLLMs research in early 2025.\n  In this work, we present a comprehensive overview of the research in the dLLM\nand dMLLM domains. We trace the historical development of dLLMs and dMLLMs,\nformalize the underlying mathematical frameworks, and categorize representative\nmodels. We further analyze key techniques for training and inference, and\nsummarize emerging applications across language, vision-language, and\nbiological domains. We conclude by discussing future directions for research\nand deployment.\n  Paper collection: https://github.com/LiQiiiii/DLLM-Survey",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we provide a systematic survey of Discrete Diffusion Language\nModels (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).\nUnlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,\nparallel decoding paradigm using full attention and a denoising-based\ngeneration strategy. This paradigm naturally enables parallel generation,\nfine-grained output controllability, and dynamic, response-aware perception.\nThese capabilities are previously difficult to achieve with AR models.\nRecently, a growing number of industrial-scale proprietary d(M)LLMs, as well as\na large number of open-source academic d(M)LLMs, have demonstrated performance\ncomparable to their autoregressive counterparts, while achieving up to 10x\nacceleration in inference speed.\n  The advancement of discrete diffusion LLMs and MLLMs has been largely driven\nby progress in two domains. The first is the development of autoregressive LLMs\nand MLLMs, which has accumulated vast amounts of data, benchmarks, and\nfoundational infrastructure for training and inference. The second contributing\ndomain is the evolution of the mathematical models underlying discrete\ndiffusion. Together, these advancements have catalyzed a surge in dLLMs and\ndMLLMs research in early 2025.\n  In this work, we present a comprehensive overview of the research in the dLLM\nand dMLLM domains. We trace the historical development of dLLMs and dMLLMs,\nformalize the underlying mathematical frameworks, and categorize representative\nmodels. We further analyze key techniques for training and inference, and\nsummarize emerging applications across language, vision-language, and\nbiological domains. We conclude by discussing future directions for research\nand deployment.\n  Paper collection: https://github.com/LiQiiiii/DLLM-Survey"
                },
                "authors": [
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13859v2",
                "updated": "2025-07-01T15:08:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    8,
                    43,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-16T18:00:01Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    18,
                    0,
                    1,
                    0,
                    167,
                    0
                ],
                "title": "R-symmetries, anomalies and non-invertible defects from non-BPS branes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-symmetries, anomalies and non-invertible defects from non-BPS branes"
                },
                "summary": "We propose a holographic realization of symmetry operators for symmetries\nassociated to isometries in terms of non-BPS Kaluza-Klein monopoles. Their\nexistence is supported by dualities, and their action can be inferred by\nconsistency with well-known String Theory constructions. We use our proposal to\ndescribe the $U(1)$ superconformal R-symmetry of the Klebanov-Witten 4d\n$\\mathcal{N}=1$ theory dual to Type IIB String Theory on $AdS_5\\times T^{1,1}$.\nWe precisely reproduce the expectations from field theory, including the 't\nHooft self-anomaly of the R-symmetry and the mixed 't Hooft anomaly with the\nbaryonic symmetry. For a choice of boundary conditions, the R-symmetry becomes\nnon-invertible and the worldvolume action for the non-BPS Kaluza-Klein monopole\nprecisely accounts for this.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a holographic realization of symmetry operators for symmetries\nassociated to isometries in terms of non-BPS Kaluza-Klein monopoles. Their\nexistence is supported by dualities, and their action can be inferred by\nconsistency with well-known String Theory constructions. We use our proposal to\ndescribe the $U(1)$ superconformal R-symmetry of the Klebanov-Witten 4d\n$\\mathcal{N}=1$ theory dual to Type IIB String Theory on $AdS_5\\times T^{1,1}$.\nWe precisely reproduce the expectations from field theory, including the 't\nHooft self-anomaly of the R-symmetry and the mixed 't Hooft anomaly with the\nbaryonic symmetry. For a choice of boundary conditions, the R-symmetry becomes\nnon-invertible and the worldvolume action for the non-BPS Kaluza-Klein monopole\nprecisely accounts for this."
                },
                "authors": [
                    {
                        "name": "Hugo Calvo"
                    },
                    {
                        "name": "Francesco Mignosa"
                    },
                    {
                        "name": "Diego Rodriguez-Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Diego Rodriguez-Gomez"
                },
                "author": "Diego Rodriguez-Gomez",
                "arxiv_comment": "28 pages, 2 figures; v2: references added. Version to appear on JHEP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16889v2",
                "updated": "2025-07-01T15:08:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    8,
                    41,
                    1,
                    182,
                    0
                ],
                "published": "2025-02-24T06:40:18Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    40,
                    18,
                    0,
                    55,
                    0
                ],
                "title": "Beyond Diagnostic Performance: Revealing and Quantifying Ethical Risks\n  in Pathology Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Diagnostic Performance: Revealing and Quantifying Ethical Risks\n  in Pathology Foundation Models"
                },
                "summary": "Pathology foundation models (PFMs), as large-scale pre-trained models\ntailored for computational pathology, have significantly advanced a wide range\nof applications. Their ability to leverage prior knowledge from massive\ndatasets has streamlined the development of intelligent pathology models.\nHowever, we identify several critical and interrelated ethical risks that\nremain underexplored, yet must be addressed to enable the safe translation of\nPFMs from lab to clinic. These include the potential leakage of\npatient-sensitive attributes, disparities in model performance across\ndemographic and institutional subgroups, and the reliance on\ndiagnosis-irrelevant features that undermine clinical reliability. In this\nstudy, we pioneer the quantitative analysis for ethical risks in PFMs,\nincluding privacy leakage, clinical reliability, and group fairness.\nSpecifically, we propose an evaluation framework that systematically measures\nkey dimensions of ethical concern: the degree to which patient-sensitive\nattributes can be inferred from model representations, the extent of\nperformance disparities across demographic and institutional subgroups, and the\ninfluence of diagnostically irrelevant features on model decisions. We further\ninvestigate the underlying causes of these ethical risks in PFMs and\nempirically validate our findings. Then we offer insights into potential\ndirections for mitigating such risks, aiming to inform the development of more\nethically robust PFMs. This work provides the first quantitative and systematic\nevaluation of ethical risks in PFMs. Our findings highlight the urgent need for\nethical safeguards in PFMs and offer actionable insights for building more\ntrustworthy and clinically robust PFMs. To facilitate future research and\ndeployment, we will release the assessment framework as an online toolkit to\nsupport the development, auditing, and deployment of ethically robust PFMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pathology foundation models (PFMs), as large-scale pre-trained models\ntailored for computational pathology, have significantly advanced a wide range\nof applications. Their ability to leverage prior knowledge from massive\ndatasets has streamlined the development of intelligent pathology models.\nHowever, we identify several critical and interrelated ethical risks that\nremain underexplored, yet must be addressed to enable the safe translation of\nPFMs from lab to clinic. These include the potential leakage of\npatient-sensitive attributes, disparities in model performance across\ndemographic and institutional subgroups, and the reliance on\ndiagnosis-irrelevant features that undermine clinical reliability. In this\nstudy, we pioneer the quantitative analysis for ethical risks in PFMs,\nincluding privacy leakage, clinical reliability, and group fairness.\nSpecifically, we propose an evaluation framework that systematically measures\nkey dimensions of ethical concern: the degree to which patient-sensitive\nattributes can be inferred from model representations, the extent of\nperformance disparities across demographic and institutional subgroups, and the\ninfluence of diagnostically irrelevant features on model decisions. We further\ninvestigate the underlying causes of these ethical risks in PFMs and\nempirically validate our findings. Then we offer insights into potential\ndirections for mitigating such risks, aiming to inform the development of more\nethically robust PFMs. This work provides the first quantitative and systematic\nevaluation of ethical risks in PFMs. Our findings highlight the urgent need for\nethical safeguards in PFMs and offer actionable insights for building more\ntrustworthy and clinically robust PFMs. To facilitate future research and\ndeployment, we will release the assessment framework as an online toolkit to\nsupport the development, auditing, and deployment of ethically robust PFMs."
                },
                "authors": [
                    {
                        "name": "Weiping Lin"
                    },
                    {
                        "name": "Shen Liu"
                    },
                    {
                        "name": "Runchen Zhu"
                    },
                    {
                        "name": "Yixuan Lin"
                    },
                    {
                        "name": "Baoshun Wang"
                    },
                    {
                        "name": "Liansheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liansheng Wang"
                },
                "author": "Liansheng Wang",
                "arxiv_comment": "33 pages,5 figure,23 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12571v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12571v3",
                "updated": "2025-07-01T15:02:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    2,
                    55,
                    1,
                    182,
                    0
                ],
                "published": "2024-08-22T17:39:26Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    39,
                    26,
                    3,
                    235,
                    0
                ],
                "title": "Deep-learning-based continuous attacks on quantum key distribution\n  protocols",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep-learning-based continuous attacks on quantum key distribution\n  protocols"
                },
                "summary": "The most important characteristic of a Quantum Key Distribution (QKD)\nprotocol is its security against third-party attacks, and the potential\ncountermeasures available. While new types of attacks are regularly developed\nin the literature, they rarely involve the use of weak continuous measurement\nand more specifically machine learning to infer the qubit states. In this\npaper, we design a new individual attack scheme called\n\\textit{Deep-learning-based continuous attack} (DLCA) that exploits continuous\nmeasurement together with the powerful pattern recognition capacities of deep\nrecurrent neural networks. We show that, when applied to the BB84 protocol, our\nattack increases only slightly the Quantum Bit Error Rate (QBER) of a noisy\nchannel and allows the spy to infer a significant part of the sifted key. In\naddition, we show it yields similar performances in terms of information gain\nwhen compared to an optimal individual attack, namely the phase-covariant\nquantum cloner. Our individual attack scheme demonstrates\ndeep-learning-enhanced quantum state tomography applied to QKD and could be\ngeneralized in many different ways,",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most important characteristic of a Quantum Key Distribution (QKD)\nprotocol is its security against third-party attacks, and the potential\ncountermeasures available. While new types of attacks are regularly developed\nin the literature, they rarely involve the use of weak continuous measurement\nand more specifically machine learning to infer the qubit states. In this\npaper, we design a new individual attack scheme called\n\\textit{Deep-learning-based continuous attack} (DLCA) that exploits continuous\nmeasurement together with the powerful pattern recognition capacities of deep\nrecurrent neural networks. We show that, when applied to the BB84 protocol, our\nattack increases only slightly the Quantum Bit Error Rate (QBER) of a noisy\nchannel and allows the spy to infer a significant part of the sifted key. In\naddition, we show it yields similar performances in terms of information gain\nwhen compared to an optimal individual attack, namely the phase-covariant\nquantum cloner. Our individual attack scheme demonstrates\ndeep-learning-enhanced quantum state tomography applied to QKD and could be\ngeneralized in many different ways,"
                },
                "authors": [
                    {
                        "name": "Tho Lejeune"
                    },
                    {
                        "name": "Franois Damanet"
                    }
                ],
                "author_detail": {
                    "name": "Franois Damanet"
                },
                "author": "Franois Damanet",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12571v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12571v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09310v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09310v2",
                "updated": "2025-07-01T14:55:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    55,
                    5,
                    1,
                    182,
                    0
                ],
                "published": "2025-01-16T05:54:59Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    54,
                    59,
                    3,
                    16,
                    0
                ],
                "title": "A Study of In-Context-Learning-Based Text-to-SQL Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study of In-Context-Learning-Based Text-to-SQL Errors"
                },
                "summary": "Large language models (LLMs) have been adopted to perform text-to-SQL tasks,\nutilizing their in-context learning (ICL) capability to translate natural\nlanguage questions into structured query language (SQL). However, such a\ntechnique faces correctness problems and requires efficient repairing\nsolutions. In this paper, we conduct the first comprehensive study of\ntext-to-SQL errors. Our study covers four representative ICL-based techniques,\nfive basic repairing methods, two benchmarks, and two LLM settings. We find\nthat text-to-SQL errors are widespread and summarize 29 error types of 7\ncategories. We also find that existing repairing attempts have limited\ncorrectness improvement at the cost of high computational overhead with many\nmis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL\nerror detection and repairing framework. The evaluation demonstrates that\nMapleRepair outperforms existing solutions by repairing 13.8% more queries with\nneglectable mis-repairs and 67.4% less overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been adopted to perform text-to-SQL tasks,\nutilizing their in-context learning (ICL) capability to translate natural\nlanguage questions into structured query language (SQL). However, such a\ntechnique faces correctness problems and requires efficient repairing\nsolutions. In this paper, we conduct the first comprehensive study of\ntext-to-SQL errors. Our study covers four representative ICL-based techniques,\nfive basic repairing methods, two benchmarks, and two LLM settings. We find\nthat text-to-SQL errors are widespread and summarize 29 error types of 7\ncategories. We also find that existing repairing attempts have limited\ncorrectness improvement at the cost of high computational overhead with many\nmis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL\nerror detection and repairing framework. The evaluation demonstrates that\nMapleRepair outperforms existing solutions by repairing 13.8% more queries with\nneglectable mis-repairs and 67.4% less overhead."
                },
                "authors": [
                    {
                        "name": "Jiawei Shen"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Ruoyi Qiao"
                    },
                    {
                        "name": "Jiazhen Zou"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Yuchen Shao"
                    },
                    {
                        "name": "Yueling Zhang"
                    },
                    {
                        "name": "Weikai Miao"
                    },
                    {
                        "name": "Geguang Pu"
                    }
                ],
                "author_detail": {
                    "name": "Geguang Pu"
                },
                "author": "Geguang Pu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09310v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09310v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20170v2",
                "updated": "2025-07-01T14:53:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    53,
                    43,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-26T16:12:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    12,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "Program of Equations Thoughts to Solve Algebra Word Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Program of Equations Thoughts to Solve Algebra Word Problems"
                },
                "summary": "Solving algebraic word problems (AWPs) has recently emerged as an important\nnatural language processing task. Recently, large language models (LLMs) have\ndemonstrated powerful mathematical capabilities, and the Chain-of-Thought\ntechnique, which guides LLMs through step-by-step reasoning, has yielded\nimpressive results. However, this reasoning ability is limited by the\ncomputational weaknesses of LLMs themselves, where calculation errors can\naccumulate, leading to incorrect final answers. To address this, we propose\nProgram of Equations Thoughts (POET), which transforms the task of generating\nstep-by-step reasoning answers into a two-stage task of predicting equations\nand generating code, offloading complex computations to a Python interpreter to\navoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which\nutilizes a manually designed template to enable LLMs to directly generate\nPython code for one-step solving. Our method achieves accuracies of 95.3% and\n98.0% on the PEN and ALG514 datasets, respectively, setting a new\nstate-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5%\non the DRAW-1K dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving algebraic word problems (AWPs) has recently emerged as an important\nnatural language processing task. Recently, large language models (LLMs) have\ndemonstrated powerful mathematical capabilities, and the Chain-of-Thought\ntechnique, which guides LLMs through step-by-step reasoning, has yielded\nimpressive results. However, this reasoning ability is limited by the\ncomputational weaknesses of LLMs themselves, where calculation errors can\naccumulate, leading to incorrect final answers. To address this, we propose\nProgram of Equations Thoughts (POET), which transforms the task of generating\nstep-by-step reasoning answers into a two-stage task of predicting equations\nand generating code, offloading complex computations to a Python interpreter to\navoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which\nutilizes a manually designed template to enable LLMs to directly generate\nPython code for one-step solving. Our method achieves accuracies of 95.3% and\n98.0% on the PEN and ALG514 datasets, respectively, setting a new\nstate-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5%\non the DRAW-1K dataset."
                },
                "authors": [
                    {
                        "name": "Yunze Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yunze Lin"
                },
                "author": "Yunze Lin",
                "arxiv_comment": "Withdrawn pending institutional authorization and core revisions to\n  address methodological inconsistencies in Sections 3-4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08271v2",
                "updated": "2025-07-01T14:52:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    52,
                    33,
                    1,
                    182,
                    0
                ],
                "published": "2025-03-11T10:40:39Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    10,
                    40,
                    39,
                    1,
                    70,
                    0
                ],
                "title": "LangTime: A Language-Guided Unified Model for Time Series Forecasting\n  with Proximal Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LangTime: A Language-Guided Unified Model for Time Series Forecasting\n  with Proximal Policy Optimization"
                },
                "summary": "Recent research has shown an increasing interest in utilizing pre-trained\nlarge language models (LLMs) for a variety of time series applications.\nHowever, there are three main challenges when using LLMs as foundational models\nfor time series forecasting: (1) Cross-domain generalization. (2)\nCross-modality alignment. (3) Error accumulation in autoregressive frameworks.\nTo address these challenges, we proposed LangTime, a language-guided unified\nmodel for time series forecasting that incorporates cross-domain pre-training\nwith reinforcement learning-based fine-tuning. Specifically, LangTime\nconstructs Temporal Comprehension Prompts (TCPs), which include dataset-wise\nand channel-wise instructions, to facilitate domain adaptation and condense\ntime series into a single token, enabling LLMs to understand better and align\ntemporal data. To improve autoregressive forecasting, we introduce TimePPO, a\nreinforcement learning-based fine-tuning algorithm. TimePPO mitigates error\naccumulation by leveraging a multidimensional rewards function tailored for\ntime series and a repeat-based value estimation strategy. Extensive experiments\ndemonstrate that LangTime achieves state-of-the-art cross-domain forecasting\nperformance, while TimePPO fine-tuning effectively enhances the stability and\naccuracy of autoregressive forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown an increasing interest in utilizing pre-trained\nlarge language models (LLMs) for a variety of time series applications.\nHowever, there are three main challenges when using LLMs as foundational models\nfor time series forecasting: (1) Cross-domain generalization. (2)\nCross-modality alignment. (3) Error accumulation in autoregressive frameworks.\nTo address these challenges, we proposed LangTime, a language-guided unified\nmodel for time series forecasting that incorporates cross-domain pre-training\nwith reinforcement learning-based fine-tuning. Specifically, LangTime\nconstructs Temporal Comprehension Prompts (TCPs), which include dataset-wise\nand channel-wise instructions, to facilitate domain adaptation and condense\ntime series into a single token, enabling LLMs to understand better and align\ntemporal data. To improve autoregressive forecasting, we introduce TimePPO, a\nreinforcement learning-based fine-tuning algorithm. TimePPO mitigates error\naccumulation by leveraging a multidimensional rewards function tailored for\ntime series and a repeat-based value estimation strategy. Extensive experiments\ndemonstrate that LangTime achieves state-of-the-art cross-domain forecasting\nperformance, while TimePPO fine-tuning effectively enhances the stability and\naccuracy of autoregressive forecasting."
                },
                "authors": [
                    {
                        "name": "Wenzhe Niu"
                    },
                    {
                        "name": "Zongxia Xie"
                    },
                    {
                        "name": "Yanru Sun"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Man Xu"
                    },
                    {
                        "name": "Chao Hao"
                    }
                ],
                "author_detail": {
                    "name": "Chao Hao"
                },
                "author": "Chao Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11194v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11194v3",
                "updated": "2025-07-01T14:41:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    41,
                    35,
                    1,
                    182,
                    0
                ],
                "published": "2024-11-17T22:58:28Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    22,
                    58,
                    28,
                    6,
                    322,
                    0
                ],
                "title": "Careless Whisper: Exploiting Silent Delivery Receipts to Monitor Users\n  on Mobile Instant Messengers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Careless Whisper: Exploiting Silent Delivery Receipts to Monitor Users\n  on Mobile Instant Messengers"
                },
                "summary": "With over 3 billion users globally, mobile instant messaging apps have become\nindispensable for both personal and professional communication. Besides plain\nmessaging, many services implement additional features such as delivery and\nread receipts informing a user when a message has successfully reached its\ntarget. This paper highlights that delivery receipts can pose significant\nprivacy risks to users. We use specifically crafted messages that trigger\ndelivery receipts allowing any user to be pinged without their knowledge or\nconsent. By using this technique at high frequency, we demonstrate how an\nattacker could extract private information such as the online and activity\nstatus of a victim, e.g., screen on/off. Moreover, we can infer the number of\ncurrently active user devices and their operating system, as well as launch\nresource exhaustion attacks, such as draining a user's battery or data\nallowance, all without generating any notification on the target side. Due to\nthe widespread adoption of vulnerable messengers (WhatsApp and Signal) and the\nfact that any user can be targeted simply by knowing their phone number, we\nargue for a design change to address this issue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With over 3 billion users globally, mobile instant messaging apps have become\nindispensable for both personal and professional communication. Besides plain\nmessaging, many services implement additional features such as delivery and\nread receipts informing a user when a message has successfully reached its\ntarget. This paper highlights that delivery receipts can pose significant\nprivacy risks to users. We use specifically crafted messages that trigger\ndelivery receipts allowing any user to be pinged without their knowledge or\nconsent. By using this technique at high frequency, we demonstrate how an\nattacker could extract private information such as the online and activity\nstatus of a victim, e.g., screen on/off. Moreover, we can infer the number of\ncurrently active user devices and their operating system, as well as launch\nresource exhaustion attacks, such as draining a user's battery or data\nallowance, all without generating any notification on the target side. Due to\nthe widespread adoption of vulnerable messengers (WhatsApp and Signal) and the\nfact that any user can be targeted simply by knowing their phone number, we\nargue for a design change to address this issue."
                },
                "authors": [
                    {
                        "name": "Gabriel K. Gegenhuber"
                    },
                    {
                        "name": "Maximilian Gnther"
                    },
                    {
                        "name": "Markus Maier"
                    },
                    {
                        "name": "Aljosha Judmayer"
                    },
                    {
                        "name": "Florian Holzbauer"
                    },
                    {
                        "name": "Philipp . Frenzel"
                    },
                    {
                        "name": "Johanna Ullrich"
                    }
                ],
                "author_detail": {
                    "name": "Johanna Ullrich"
                },
                "author": "Johanna Ullrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11194v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11194v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14649v2",
                "updated": "2025-07-01T14:41:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    41,
                    8,
                    1,
                    182,
                    0
                ],
                "published": "2024-10-18T17:46:37Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    46,
                    37,
                    4,
                    292,
                    0
                ],
                "title": "EvoPress: Accurate Dynamic Model Compression via Evolutionary Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoPress: Accurate Dynamic Model Compression via Evolutionary Search"
                },
                "summary": "The high computational costs of large language models (LLMs) have led to a\nflurry of research on LLM compression, via methods such as quantization,\nsparsification, or structured pruning. A new frontier in this area is given by\ndynamic, non-uniform compression methods, which adjust the compression levels\n(e.g., sparsity) per-block or even per-layer in order to minimize accuracy\nloss, while guaranteeing a global compression threshold. Yet, current methods\nrely on estimating the importance of a given layer, implicitly assuming that\nlayers contribute independently to the overall compression error. We begin from\nthe motivating observation that this independence assumption does not generally\nhold for LLM compression: pruning a model further may even significantly\nrecover performance. To address this, we propose EvoPress, a novel evolutionary\nframework for dynamic LLM compression. By formulating dynamic compression as a\ngeneral optimization problem, EvoPress identifies optimal compression profiles\nin a highly efficient manner, and generalizes across diverse models and\ncompression techniques. Via EvoPress, we achieve state-of-the-art performance\nfor dynamic compression of Llama, Mistral, and Phi models, setting new\nbenchmarks for structural pruning (block/layer dropping), unstructured\nsparsity, and quantization with dynamic bitwidths. Our code is available at\nhttps://github.com/IST-DASLab/EvoPress}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high computational costs of large language models (LLMs) have led to a\nflurry of research on LLM compression, via methods such as quantization,\nsparsification, or structured pruning. A new frontier in this area is given by\ndynamic, non-uniform compression methods, which adjust the compression levels\n(e.g., sparsity) per-block or even per-layer in order to minimize accuracy\nloss, while guaranteeing a global compression threshold. Yet, current methods\nrely on estimating the importance of a given layer, implicitly assuming that\nlayers contribute independently to the overall compression error. We begin from\nthe motivating observation that this independence assumption does not generally\nhold for LLM compression: pruning a model further may even significantly\nrecover performance. To address this, we propose EvoPress, a novel evolutionary\nframework for dynamic LLM compression. By formulating dynamic compression as a\ngeneral optimization problem, EvoPress identifies optimal compression profiles\nin a highly efficient manner, and generalizes across diverse models and\ncompression techniques. Via EvoPress, we achieve state-of-the-art performance\nfor dynamic compression of Llama, Mistral, and Phi models, setting new\nbenchmarks for structural pruning (block/layer dropping), unstructured\nsparsity, and quantization with dynamic bitwidths. Our code is available at\nhttps://github.com/IST-DASLab/EvoPress}."
                },
                "authors": [
                    {
                        "name": "Oliver Sieberling"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "ICML camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04075v2",
                "updated": "2025-07-01T14:39:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    39,
                    29,
                    1,
                    182,
                    0
                ],
                "published": "2024-06-06T13:43:17Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    13,
                    43,
                    17,
                    3,
                    158,
                    0
                ],
                "title": "The Mass of the Vela Pulsar Progenitor and the Age of the Vela-Puppis\n  Complex",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mass of the Vela Pulsar Progenitor and the Age of the Vela-Puppis\n  Complex"
                },
                "summary": "The association of the Vela Pulsar with the Vela Supernova Remnant has long\nsupported the hypothesis that core-collapse supernovae yield neutron stars, but\nits surrounding stellar population now offers new insights into progenitor\nevolution. By age-dating stars within 150 pc of the Vela Pulsar, we infer\nproperties of its progenitor. These stars belong to the Vela-Puppis complex,\nrevealing the region's star formation history. While stellar population models\nwith standard assumptions suggest a likely progenitor age and mass, these\npredictions are internally inconsistent with the observed population,\nindicating that something is missing in the standard modeling approach. With\nthose assumptions, there is very weak support for a $\\lesssim$10 Myr old\npopulation, moderate support for a 40 Myr old population, and strong support\nfor an intermediate age population around 65-100 Myrs old. The $\\lesssim$10 Myr\nsignal hinges on two peculiar O stars, which are unlike any others in the\nVela-Puppis complex and imply nearly three times more main sequence stars than\nare observed. The 40 Myr-old population is supported by 6 red supergiants\n(RSGs) and several Be stars; but this population is again marginally\ninconsistent with the observed distribution of main sequence stars. The red\ngiant (RG) and MS distributions are consistent with a 65-100 Myr old\npopulation. We discuss several possible resolutions, emphasizing how binary\nevolution and/or very rapid rotation could resolve these discrepancies. Gaia\nparallaxes and {\\it Stellar Ages} enable these results; {\\it Stellar Ages} is a\nnovel stellar population modeling algorithm that combines individual and\npopulation-level age inferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The association of the Vela Pulsar with the Vela Supernova Remnant has long\nsupported the hypothesis that core-collapse supernovae yield neutron stars, but\nits surrounding stellar population now offers new insights into progenitor\nevolution. By age-dating stars within 150 pc of the Vela Pulsar, we infer\nproperties of its progenitor. These stars belong to the Vela-Puppis complex,\nrevealing the region's star formation history. While stellar population models\nwith standard assumptions suggest a likely progenitor age and mass, these\npredictions are internally inconsistent with the observed population,\nindicating that something is missing in the standard modeling approach. With\nthose assumptions, there is very weak support for a $\\lesssim$10 Myr old\npopulation, moderate support for a 40 Myr old population, and strong support\nfor an intermediate age population around 65-100 Myrs old. The $\\lesssim$10 Myr\nsignal hinges on two peculiar O stars, which are unlike any others in the\nVela-Puppis complex and imply nearly three times more main sequence stars than\nare observed. The 40 Myr-old population is supported by 6 red supergiants\n(RSGs) and several Be stars; but this population is again marginally\ninconsistent with the observed distribution of main sequence stars. The red\ngiant (RG) and MS distributions are consistent with a 65-100 Myr old\npopulation. We discuss several possible resolutions, emphasizing how binary\nevolution and/or very rapid rotation could resolve these discrepancies. Gaia\nparallaxes and {\\it Stellar Ages} enable these results; {\\it Stellar Ages} is a\nnovel stellar population modeling algorithm that combines individual and\npopulation-level age inferences."
                },
                "authors": [
                    {
                        "name": "Jeremiah W. Murphy"
                    },
                    {
                        "name": "Andres F. Barrientos"
                    },
                    {
                        "name": "Rene Andrae"
                    },
                    {
                        "name": "Joseph Guzman"
                    },
                    {
                        "name": "Benjamin F. Williams"
                    },
                    {
                        "name": "Julianne J. Dalcanton"
                    },
                    {
                        "name": "Brad Koplitz"
                    }
                ],
                "author_detail": {
                    "name": "Brad Koplitz"
                },
                "author": "Brad Koplitz",
                "arxiv_comment": "26 pages, 16 figures, 1 table. accepted in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14341v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14341v3",
                "updated": "2025-07-01T14:39:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    39,
                    28,
                    1,
                    182,
                    0
                ],
                "published": "2024-12-18T21:23:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    23,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Inferring protein folding mechanisms from natural sequence diversity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring protein folding mechanisms from natural sequence diversity"
                },
                "summary": "Protein sequences serve as a natural record of the evolutionary constraints\nthat shape their functional structures. We show that it is possible to use only\nsequence information to go beyond predicting native structures and global\nstability to infer the folding mechanisms of globular proteins. The one- and\ntwo-body evolutionary energy fields at the amino-acid level are mapped to a\ncoarse-grained description of folding, where proteins are divided into\ncontiguous folding elements, commonly referred to as foldons. For 15 diverse\nprotein families, we calculated the folding mechanisms of hundreds of proteins\nby simulating an Ising chain of foldons, with their energetics determined by\nthe amino acid sequences. We show that protein topology imposes limits on the\nvariability of folding cooperativity within a family. While most beta and\nalpha/beta structures exhibit only a few possible mechanisms despite high\nsequence diversity, alpha topologies allow for diverse folding scenarios among\nfamily members. We show that both the stability and cooperativity changes\ninduced by mutations can be computed directly using sequence-based evolutionary\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protein sequences serve as a natural record of the evolutionary constraints\nthat shape their functional structures. We show that it is possible to use only\nsequence information to go beyond predicting native structures and global\nstability to infer the folding mechanisms of globular proteins. The one- and\ntwo-body evolutionary energy fields at the amino-acid level are mapped to a\ncoarse-grained description of folding, where proteins are divided into\ncontiguous folding elements, commonly referred to as foldons. For 15 diverse\nprotein families, we calculated the folding mechanisms of hundreds of proteins\nby simulating an Ising chain of foldons, with their energetics determined by\nthe amino acid sequences. We show that protein topology imposes limits on the\nvariability of folding cooperativity within a family. While most beta and\nalpha/beta structures exhibit only a few possible mechanisms despite high\nsequence diversity, alpha topologies allow for diverse folding scenarios among\nfamily members. We show that both the stability and cooperativity changes\ninduced by mutations can be computed directly using sequence-based evolutionary\nmodels."
                },
                "authors": [
                    {
                        "name": "Ezequiel A. Galpern"
                    },
                    {
                        "name": "Ernesto A. Roman"
                    },
                    {
                        "name": "Diego U. Ferreiro"
                    }
                ],
                "author_detail": {
                    "name": "Diego U. Ferreiro"
                },
                "author": "Diego U. Ferreiro",
                "arxiv_comment": "21 pages, 5 figures and Supplementary material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14341v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14341v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15162v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15162v2",
                "updated": "2025-07-01T14:31:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    31,
                    58,
                    1,
                    182,
                    0
                ],
                "published": "2025-04-21T15:08:01Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    15,
                    8,
                    1,
                    0,
                    111,
                    0
                ],
                "title": "To Offload or Not To Offload: Model-driven Comparison of Edge-native and\n  On-device Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Offload or Not To Offload: Model-driven Comparison of Edge-native and\n  On-device Processing"
                },
                "summary": "Computational offloading is a promising approach for overcoming resource\nconstraints on client devices by moving some or all of an application's\ncomputations to remote servers. With the advent of specialized hardware\naccelerators, client devices are now able to perform fast local processing of\nspecific tasks, such as machine learning inference, reducing the need for\noffloading computations. However, edge servers with accelerators also offer\nfaster processing for offloaded tasks than was previously possible. In this\npaper, we present an analytic and experimental comparison of on-device\nprocessing and edge offloading for a range of accelerator, network, and\napplication workload scenarios, with the goal of understanding when to use\nlocal on-device processing and when to offload computations. We present models\nthat leverage analytical queuing results to capture the effects of dynamic\nfactors such as the performance gap between the device and edge server, network\nvariability, server load, and multi-tenancy on the edge server. We\nexperimentally demonstrate the accuracy of our models for a range of hardware\nand application scenarios and show that our models achieve a mean absolute\npercentage error of 2.2% compared to observed latencies. We use our models to\ndevelop an adaptive resource manager for intelligent offloading and show its\nefficacy in the presence of variable network conditions and dynamic\nmulti-tenant edge settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational offloading is a promising approach for overcoming resource\nconstraints on client devices by moving some or all of an application's\ncomputations to remote servers. With the advent of specialized hardware\naccelerators, client devices are now able to perform fast local processing of\nspecific tasks, such as machine learning inference, reducing the need for\noffloading computations. However, edge servers with accelerators also offer\nfaster processing for offloaded tasks than was previously possible. In this\npaper, we present an analytic and experimental comparison of on-device\nprocessing and edge offloading for a range of accelerator, network, and\napplication workload scenarios, with the goal of understanding when to use\nlocal on-device processing and when to offload computations. We present models\nthat leverage analytical queuing results to capture the effects of dynamic\nfactors such as the performance gap between the device and edge server, network\nvariability, server load, and multi-tenancy on the edge server. We\nexperimentally demonstrate the accuracy of our models for a range of hardware\nand application scenarios and show that our models achieve a mean absolute\npercentage error of 2.2% compared to observed latencies. We use our models to\ndevelop an adaptive resource manager for intelligent offloading and show its\nefficacy in the presence of variable network conditions and dynamic\nmulti-tenant edge settings."
                },
                "authors": [
                    {
                        "name": "Nathan Ng"
                    },
                    {
                        "name": "David Irwin"
                    },
                    {
                        "name": "Ananthram Swami"
                    },
                    {
                        "name": "Don Towsley"
                    },
                    {
                        "name": "Prashant Shenoy"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Shenoy"
                },
                "author": "Prashant Shenoy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15162v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15162v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04037v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04037v4",
                "updated": "2025-07-01T14:22:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    22,
                    24,
                    1,
                    182,
                    0
                ],
                "published": "2024-11-06T16:34:59Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    16,
                    34,
                    59,
                    2,
                    311,
                    0
                ],
                "title": "Investigating the heterogenous effects of a massive content moderation\n  intervention via Difference-in-Differences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating the heterogenous effects of a massive content moderation\n  intervention via Difference-in-Differences"
                },
                "summary": "In today's online environments, users encounter harm and abuse on a daily\nbasis. Therefore, content moderation is crucial to ensure their safety and\nwell-being. However, the effectiveness of many moderation interventions is\nstill uncertain. Here, we apply a causal inference approach to shed light on\nthe effectiveness of The Great Ban, a massive social media deplatforming\nintervention on Reddit. We analyze 53M comments shared by nearly 34K users,\nproviding in-depth results on both the intended and unintended consequences of\nthe ban. Our causal analyses reveal that 15.6% of the moderated users abandoned\nthe platform while the remaining ones decreased their overall toxicity by 4.1%.\nNonetheless, a small subset of users exhibited marked increases in both the\nintensity and volume of toxic behavior, particularly among those whose activity\nlevels changed after the intervention. However, these reactions were not\naccompanied by greater activity or engagement, suggesting that even the most\ntoxic users maintained a limited overall impact. Our findings bring to light\nnew insights on the effectiveness of deplatforming moderation interventions.\nFurthermore, they also contribute to informing future content moderation\nstrategies and regulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In today's online environments, users encounter harm and abuse on a daily\nbasis. Therefore, content moderation is crucial to ensure their safety and\nwell-being. However, the effectiveness of many moderation interventions is\nstill uncertain. Here, we apply a causal inference approach to shed light on\nthe effectiveness of The Great Ban, a massive social media deplatforming\nintervention on Reddit. We analyze 53M comments shared by nearly 34K users,\nproviding in-depth results on both the intended and unintended consequences of\nthe ban. Our causal analyses reveal that 15.6% of the moderated users abandoned\nthe platform while the remaining ones decreased their overall toxicity by 4.1%.\nNonetheless, a small subset of users exhibited marked increases in both the\nintensity and volume of toxic behavior, particularly among those whose activity\nlevels changed after the intervention. However, these reactions were not\naccompanied by greater activity or engagement, suggesting that even the most\ntoxic users maintained a limited overall impact. Our findings bring to light\nnew insights on the effectiveness of deplatforming moderation interventions.\nFurthermore, they also contribute to informing future content moderation\nstrategies and regulations."
                },
                "authors": [
                    {
                        "name": "Lorenzo Cima"
                    },
                    {
                        "name": "Benedetta Tessa"
                    },
                    {
                        "name": "Stefano Cresci"
                    },
                    {
                        "name": "Amaury Trujillo"
                    },
                    {
                        "name": "Marco Avvenuti"
                    }
                ],
                "author_detail": {
                    "name": "Marco Avvenuti"
                },
                "author": "Marco Avvenuti",
                "arxiv_doi": "10.1016/j.osnem.2025.100320",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.osnem.2025.100320",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.04037v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04037v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2401.11254 This work is an\n  extension of this conference paper: Cima, L., Trujillo, A., Avvenuti, M., &\n  Cresci, S. (2024, May). The Great Ban: Efficacy and Unintended Consequences\n  of a Massive Deplatforming Operation on Reddit. In Companion Publication of\n  the 16th ACM Web Science Conference (pp. 85-93)",
                "arxiv_journal_ref": "Online Social Networks and Media (OSNEM), 2025",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09438v2",
                "updated": "2025-07-01T14:16:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    16,
                    43,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-14T14:46:32Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    46,
                    32,
                    2,
                    134,
                    0
                ],
                "title": "Evaluating GPT- and Reasoning-based Large Language Models on Physics\n  Olympiad Problems: Surpassing Human Performance and Implications for\n  Educational Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating GPT- and Reasoning-based Large Language Models on Physics\n  Olympiad Problems: Surpassing Human Performance and Implications for\n  Educational Assessment"
                },
                "summary": "Large language models (LLMs) are now widely accessible, reaching learners at\nall educational levels. This development has raised concerns that their use may\ncircumvent essential learning processes and compromise the integrity of\nestablished assessment formats. In physics education, where problem solving\nplays a central role in instruction and assessment, it is therefore essential\nto understand the physics-specific problem-solving capabilities of LLMs. Such\nunderstanding is key to informing responsible and pedagogically sound\napproaches to integrating LLMs into instruction and assessment. This study\ntherefore compares the problem-solving performance of a general-purpose LLM\n(GPT-4o, using varying prompting techniques) and a reasoning-optimized model\n(o1-preview) with that of participants of the German Physics Olympiad, based on\na set of well-defined Olympiad problems. In addition to evaluating the\ncorrectness of the generated solutions, the study analyzes characteristic\nstrengths and limitations of LLM-generated solutions. The findings of this\nstudy indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate\nadvanced problem-solving capabilities on Olympiad-type physics problems, on\naverage outperforming the human participants. Prompting techniques had little\neffect on GPT-4o's performance, while o1-preview almost consistently\noutperformed both GPT-4o and the human benchmark. Based on these findings, the\nstudy discusses implications for the design of summative and formative\nassessment in physics education, including how to uphold assessment integrity\nand support students in critically engaging with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are now widely accessible, reaching learners at\nall educational levels. This development has raised concerns that their use may\ncircumvent essential learning processes and compromise the integrity of\nestablished assessment formats. In physics education, where problem solving\nplays a central role in instruction and assessment, it is therefore essential\nto understand the physics-specific problem-solving capabilities of LLMs. Such\nunderstanding is key to informing responsible and pedagogically sound\napproaches to integrating LLMs into instruction and assessment. This study\ntherefore compares the problem-solving performance of a general-purpose LLM\n(GPT-4o, using varying prompting techniques) and a reasoning-optimized model\n(o1-preview) with that of participants of the German Physics Olympiad, based on\na set of well-defined Olympiad problems. In addition to evaluating the\ncorrectness of the generated solutions, the study analyzes characteristic\nstrengths and limitations of LLM-generated solutions. The findings of this\nstudy indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate\nadvanced problem-solving capabilities on Olympiad-type physics problems, on\naverage outperforming the human participants. Prompting techniques had little\neffect on GPT-4o's performance, while o1-preview almost consistently\noutperformed both GPT-4o and the human benchmark. Based on these findings, the\nstudy discusses implications for the design of summative and formative\nassessment in physics education, including how to uphold assessment integrity\nand support students in critically engaging with LLMs."
                },
                "authors": [
                    {
                        "name": "Paul Tschisgale"
                    },
                    {
                        "name": "Holger Maus"
                    },
                    {
                        "name": "Fabian Kieser"
                    },
                    {
                        "name": "Ben Kroehs"
                    },
                    {
                        "name": "Stefan Petersen"
                    },
                    {
                        "name": "Peter Wulff"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wulff"
                },
                "author": "Peter Wulff",
                "arxiv_doi": "10.1103/6fmx-bsnl",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/6fmx-bsnl",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.09438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24852v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24852v2",
                "updated": "2025-07-01T13:51:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    51,
                    1,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-30T17:49:30Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    17,
                    49,
                    30,
                    4,
                    150,
                    0
                ],
                "title": "Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for\n  End-to-End Few-Shot and Continual Learning from Sequential Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for\n  End-to-End Few-Shot and Continual Learning from Sequential Data"
                },
                "summary": "On-device learning at the edge enables low-latency, private personalization\nwith improved long-term robustness and reduced maintenance costs. Yet,\nachieving scalable, low-power end-to-end on-chip learning, especially from\nreal-world sequential data with a limited number of examples, is an open\nchallenge. Indeed, accelerators supporting error backpropagation optimize for\nlearning performance at the expense of inference efficiency, while simplified\nlearning algorithms often fail to reach acceptable accuracy targets. In this\nwork, we present Chameleon, leveraging three key contributions to solve these\nchallenges. (i) A unified learning and inference architecture supports few-shot\nlearning (FSL), continual learning (CL) and inference at only 0.5% area\noverhead to the inference logic. (ii) Long temporal dependencies are\nefficiently captured with temporal convolutional networks (TCNs), enabling the\nfirst demonstration of end-to-end on-chip FSL and CL on sequential data and\ninference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free\ncompute array allows either matching the power consumption of state-of-the-art\ninference-only keyword spotting (KWS) accelerators or enabling $4.3\\times$\nhigher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records\non Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way\n5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots),\nwhile maintaining an inference accuracy of 93.3% on the 12-class Google Speech\nCommands dataset at an extreme-edge power budget of 3.1 $\\mu$W.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device learning at the edge enables low-latency, private personalization\nwith improved long-term robustness and reduced maintenance costs. Yet,\nachieving scalable, low-power end-to-end on-chip learning, especially from\nreal-world sequential data with a limited number of examples, is an open\nchallenge. Indeed, accelerators supporting error backpropagation optimize for\nlearning performance at the expense of inference efficiency, while simplified\nlearning algorithms often fail to reach acceptable accuracy targets. In this\nwork, we present Chameleon, leveraging three key contributions to solve these\nchallenges. (i) A unified learning and inference architecture supports few-shot\nlearning (FSL), continual learning (CL) and inference at only 0.5% area\noverhead to the inference logic. (ii) Long temporal dependencies are\nefficiently captured with temporal convolutional networks (TCNs), enabling the\nfirst demonstration of end-to-end on-chip FSL and CL on sequential data and\ninference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free\ncompute array allows either matching the power consumption of state-of-the-art\ninference-only keyword spotting (KWS) accelerators or enabling $4.3\\times$\nhigher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records\non Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way\n5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots),\nwhile maintaining an inference accuracy of 93.3% on the 12-class Google Speech\nCommands dataset at an extreme-edge power budget of 3.1 $\\mu$W."
                },
                "authors": [
                    {
                        "name": "Douwe den Blanken"
                    },
                    {
                        "name": "Charlotte Frenkel"
                    }
                ],
                "author_detail": {
                    "name": "Charlotte Frenkel"
                },
                "author": "Charlotte Frenkel",
                "arxiv_comment": "14 pages, 7 figures; added FSL power consumption measurements at 100\n  kHz clock speed, fixed typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24852v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; B.6.0; B.7.0; I.2.6; B.5.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07844v2",
                "updated": "2025-07-01T13:45:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    45,
                    21,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-09T15:08:41Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    15,
                    8,
                    41,
                    0,
                    160,
                    0
                ],
                "title": "Conditional Local Independence Testing for Dynamic Causal Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Local Independence Testing for Dynamic Causal Discovery"
                },
                "summary": "Inferring causal relationships from dynamical systems is the central interest\nof many scientific inquiries. Conditional Local Independence (CLI), which\ndescribes whether the evolution of one process is influenced by another process\ngiven additional processes, is important for causal learning in such systems.\nHowever, existing CLI tests were limited to counting processes. In this paper,\nwe propose a nonparametric CLT test for It\\^o processes. Specifically, we first\nintroduce a testing statistic based on the Local Covariance Measure (LCM) by\nconstructing a martingale from the conditional expectation of the process of\ninterest. For estimation, we propose an efficient estimator based on the\noptimal filtering equation, which can achieve root-N consistency. To establish\nthe asymptotic level and power of the test, we relax the restrictive\nboundedness condition to a moment bound condition, which is practical for It\\^o\nprocesses. We verify the proposed test in synthetic and real-world experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring causal relationships from dynamical systems is the central interest\nof many scientific inquiries. Conditional Local Independence (CLI), which\ndescribes whether the evolution of one process is influenced by another process\ngiven additional processes, is important for causal learning in such systems.\nHowever, existing CLI tests were limited to counting processes. In this paper,\nwe propose a nonparametric CLT test for It\\^o processes. Specifically, we first\nintroduce a testing statistic based on the Local Covariance Measure (LCM) by\nconstructing a martingale from the conditional expectation of the process of\ninterest. For estimation, we propose an efficient estimator based on the\noptimal filtering equation, which can achieve root-N consistency. To establish\nthe asymptotic level and power of the test, we relax the restrictive\nboundedness condition to a moment bound condition, which is practical for It\\^o\nprocesses. We verify the proposed test in synthetic and real-world experiments."
                },
                "authors": [
                    {
                        "name": "Mingzhou Liu"
                    },
                    {
                        "name": "Xinwei Sun"
                    },
                    {
                        "name": "Yizhou Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Wang"
                },
                "author": "Yizhou Wang",
                "arxiv_comment": "Working paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08904v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08904v2",
                "updated": "2025-07-01T13:40:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    40,
                    16,
                    1,
                    182,
                    0
                ],
                "published": "2025-03-11T21:32:28Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    21,
                    32,
                    28,
                    1,
                    70,
                    0
                ],
                "title": "Towards Efficient Parametric State Estimation in Circulating Fuel\n  Reactors with Shallow Recurrent Decoder Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Parametric State Estimation in Circulating Fuel\n  Reactors with Shallow Recurrent Decoder Networks"
                },
                "summary": "The recent developments in data-driven methods have paved the way to new\nmethodologies to provide accurate state reconstruction of engineering systems;\nnuclear reactors represent particularly challenging applications for this task\ndue to the complexity of the strongly coupled physics involved and the\nextremely harsh and hostile environments, especially for new technologies such\nas Generation-IV reactors. Data-driven techniques can combine different sources\nof information, including computational proxy models and local noisy\nmeasurements on the system, to robustly estimate the state. This work leverages\nthe novel Shallow Recurrent Decoder architecture to infer the entire state\nvector (including neutron fluxes, precursors concentrations, temperature,\npressure and velocity) of a reactor from three out-of-core time-series neutron\nflux measurements alone. In particular, this work extends the standard\narchitecture to treat parametric time-series data, ensuring the possibility of\ninvestigating different accidental scenarios and showing the capabilities of\nthis approach to provide an accurate state estimation in various operating\nconditions. This paper considers as a test case the Molten Salt Fast Reactor\n(MSFR), a Generation-IV reactor concept, characterised by strong coupling\nbetween the neutronics and the thermal hydraulics due to the liquid nature of\nthe fuel. The promising results of this work are further strengthened by the\npossibility of quantifying the uncertainty associated with the state\nestimation, due to the considerably low training cost. The accurate\nreconstruction of every characteristic field in real-time makes this approach\nsuitable for monitoring and control purposes in the framework of a reactor\ndigital twin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent developments in data-driven methods have paved the way to new\nmethodologies to provide accurate state reconstruction of engineering systems;\nnuclear reactors represent particularly challenging applications for this task\ndue to the complexity of the strongly coupled physics involved and the\nextremely harsh and hostile environments, especially for new technologies such\nas Generation-IV reactors. Data-driven techniques can combine different sources\nof information, including computational proxy models and local noisy\nmeasurements on the system, to robustly estimate the state. This work leverages\nthe novel Shallow Recurrent Decoder architecture to infer the entire state\nvector (including neutron fluxes, precursors concentrations, temperature,\npressure and velocity) of a reactor from three out-of-core time-series neutron\nflux measurements alone. In particular, this work extends the standard\narchitecture to treat parametric time-series data, ensuring the possibility of\ninvestigating different accidental scenarios and showing the capabilities of\nthis approach to provide an accurate state estimation in various operating\nconditions. This paper considers as a test case the Molten Salt Fast Reactor\n(MSFR), a Generation-IV reactor concept, characterised by strong coupling\nbetween the neutronics and the thermal hydraulics due to the liquid nature of\nthe fuel. The promising results of this work are further strengthened by the\npossibility of quantifying the uncertainty associated with the state\nestimation, due to the considerably low training cost. The accurate\nreconstruction of every characteristic field in real-time makes this approach\nsuitable for monitoring and control purposes in the framework of a reactor\ndigital twin."
                },
                "authors": [
                    {
                        "name": "Stefano Riva"
                    },
                    {
                        "name": "Carolina Introini"
                    },
                    {
                        "name": "J. Nathan Kutz"
                    },
                    {
                        "name": "Antonio Cammi"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Cammi"
                },
                "author": "Antonio Cammi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08904v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11620v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11620v3",
                "updated": "2025-07-01T12:15:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    15,
                    44,
                    1,
                    182,
                    0
                ],
                "published": "2025-02-17T10:03:01Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    10,
                    3,
                    1,
                    0,
                    48,
                    0
                ],
                "title": "Assessing Correctness in LLM-Based Code Generation via Uncertainty\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Correctness in LLM-Based Code Generation via Uncertainty\n  Estimation"
                },
                "summary": "In this work, we explore uncertainty estimation as a proxy for correctness in\nLLM-generated code. To this end, we adapt two state-of-the-art techniques from\nnatural language generation -- one based on entropy and another on mutual\ninformation -- to the domain of code generation. Given the distinct semantic\nproperties of code, we introduce modifications, including a semantic\nequivalence check based on symbolic execution. Our findings indicate a strong\ncorrelation between the uncertainty computed through these techniques and\ncorrectness, highlighting the potential of uncertainty estimation for quality\nassessment. Additionally, we propose a simplified version of the entropy-based\nmethod that assumes a uniform distribution over the LLM's responses,\ndemonstrating comparable effectiveness. Using these techniques, we develop an\nabstention policy that prevents the model from making predictions when\nuncertainty is high, reducing incorrect outputs to near zero. Our evaluation on\nthe LiveCodeBench shows that our approach significantly outperforms a baseline\nrelying solely on LLM-reported log-probabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explore uncertainty estimation as a proxy for correctness in\nLLM-generated code. To this end, we adapt two state-of-the-art techniques from\nnatural language generation -- one based on entropy and another on mutual\ninformation -- to the domain of code generation. Given the distinct semantic\nproperties of code, we introduce modifications, including a semantic\nequivalence check based on symbolic execution. Our findings indicate a strong\ncorrelation between the uncertainty computed through these techniques and\ncorrectness, highlighting the potential of uncertainty estimation for quality\nassessment. Additionally, we propose a simplified version of the entropy-based\nmethod that assumes a uniform distribution over the LLM's responses,\ndemonstrating comparable effectiveness. Using these techniques, we develop an\nabstention policy that prevents the model from making predictions when\nuncertainty is high, reducing incorrect outputs to near zero. Our evaluation on\nthe LiveCodeBench shows that our approach significantly outperforms a baseline\nrelying solely on LLM-reported log-probabilities."
                },
                "authors": [
                    {
                        "name": "Arindam Sharma"
                    },
                    {
                        "name": "Cristina David"
                    }
                ],
                "author_detail": {
                    "name": "Cristina David"
                },
                "author": "Cristina David",
                "arxiv_comment": "18 pages and 3 References Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11620v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11620v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02007v2",
                "updated": "2025-07-01T11:37:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    11,
                    37,
                    52,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-25T09:25:39Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    9,
                    25,
                    39,
                    6,
                    145,
                    0
                ],
                "title": "eACGM: Non-instrumented Performance Tracing and Anomaly Detection\n  towards Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "eACGM: Non-instrumented Performance Tracing and Anomaly Detection\n  towards Machine Learning Systems"
                },
                "summary": "We present eACGM, a full-stack AI/ML system monitoring framework based on\neBPF. eACGM collects real-time performance data from key hardware components,\nincluding the GPU and network communication layer, as well as from key software\nstacks such as CUDA, Python, and PyTorch, all without requiring any code\ninstrumentation or modifications. Additionally, it leverages libnvml to gather\nprocess-level GPU resource usage information. By applying a Gaussian Mixture\nModel (GMM) to the collected multidimensional performance metrics for\nstatistical modeling and clustering analysis, eACGM effectively identifies\ncomplex failure modes, such as latency anomalies, hardware failures, and\ncommunication inefficiencies, enabling rapid diagnosis of system bottlenecks\nand abnormal behaviors.\n  To evaluate eACGM's effectiveness and practicality, we conducted extensive\nempirical studies and case analyses in multi-node distributed training\nscenarios. The results demonstrate that eACGM, while maintaining a\nnon-intrusive and low-overhead profile, successfully captures critical\nperformance anomalies during model training and inference. Its stable anomaly\ndetection performance and comprehensive monitoring capabilities validate its\napplicability and scalability in real-world production environments, providing\nstrong support for performance optimization and fault diagnosis in large-scale\nAI/ML systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present eACGM, a full-stack AI/ML system monitoring framework based on\neBPF. eACGM collects real-time performance data from key hardware components,\nincluding the GPU and network communication layer, as well as from key software\nstacks such as CUDA, Python, and PyTorch, all without requiring any code\ninstrumentation or modifications. Additionally, it leverages libnvml to gather\nprocess-level GPU resource usage information. By applying a Gaussian Mixture\nModel (GMM) to the collected multidimensional performance metrics for\nstatistical modeling and clustering analysis, eACGM effectively identifies\ncomplex failure modes, such as latency anomalies, hardware failures, and\ncommunication inefficiencies, enabling rapid diagnosis of system bottlenecks\nand abnormal behaviors.\n  To evaluate eACGM's effectiveness and practicality, we conducted extensive\nempirical studies and case analyses in multi-node distributed training\nscenarios. The results demonstrate that eACGM, while maintaining a\nnon-intrusive and low-overhead profile, successfully captures critical\nperformance anomalies during model training and inference. Its stable anomaly\ndetection performance and comprehensive monitoring capabilities validate its\napplicability and scalability in real-world production environments, providing\nstrong support for performance optimization and fault diagnosis in large-scale\nAI/ML systems."
                },
                "authors": [
                    {
                        "name": "Ruilin Xu"
                    },
                    {
                        "name": "Zongxuan Xie"
                    },
                    {
                        "name": "Pengfei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Chen"
                },
                "author": "Pengfei Chen",
                "arxiv_comment": "IWQoS 2025 (Camera-Ready Version)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17080v2",
                "updated": "2025-07-01T11:26:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    11,
                    26,
                    7,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-20T08:49:18Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    8,
                    49,
                    18,
                    1,
                    140,
                    0
                ],
                "title": "Not Minds, but Signs: Reframing LLMs through Semiotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not Minds, but Signs: Reframing LLMs through Semiotics"
                },
                "summary": "This paper challenges the prevailing tendency to frame Large Language Models\n(LLMs) as cognitive systems, arguing instead for a semiotic perspective that\nsituates these models within the broader dynamics of sign manipulation and\nmeaning-making. Rather than assuming that LLMs understand language or simulate\nhuman thought, we propose that their primary function is to recombine,\nrecontextualize, and circulate linguistic forms based on probabilistic\nassociations. By shifting from a cognitivist to a semiotic framework, we avoid\nanthropomorphism and gain a more precise understanding of how LLMs participate\nin cultural processes, not by thinking, but by generating texts that invite\ninterpretation. Through theoretical analysis and practical examples, the paper\ndemonstrates how LLMs function as semiotic agents whose outputs can be treated\nas interpretive acts, open to contextual negotiation and critical reflection.\nWe explore applications in literature, philosophy, education, and cultural\nproduction, emphasizing how LLMs can serve as tools for creativity, dialogue,\nand critical inquiry. The semiotic paradigm foregrounds the situated,\ncontingent, and socially embedded nature of meaning, offering a more rigorous\nand ethically aware framework for studying and using LLMs. Ultimately, this\napproach reframes LLMs as technological participants in an ongoing ecology of\nsigns. They do not possess minds, but they alter how we read, write, and make\nmeaning, compelling us to reconsider the foundations of language,\ninterpretation, and the role of artificial systems in the production of\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper challenges the prevailing tendency to frame Large Language Models\n(LLMs) as cognitive systems, arguing instead for a semiotic perspective that\nsituates these models within the broader dynamics of sign manipulation and\nmeaning-making. Rather than assuming that LLMs understand language or simulate\nhuman thought, we propose that their primary function is to recombine,\nrecontextualize, and circulate linguistic forms based on probabilistic\nassociations. By shifting from a cognitivist to a semiotic framework, we avoid\nanthropomorphism and gain a more precise understanding of how LLMs participate\nin cultural processes, not by thinking, but by generating texts that invite\ninterpretation. Through theoretical analysis and practical examples, the paper\ndemonstrates how LLMs function as semiotic agents whose outputs can be treated\nas interpretive acts, open to contextual negotiation and critical reflection.\nWe explore applications in literature, philosophy, education, and cultural\nproduction, emphasizing how LLMs can serve as tools for creativity, dialogue,\nand critical inquiry. The semiotic paradigm foregrounds the situated,\ncontingent, and socially embedded nature of meaning, offering a more rigorous\nand ethically aware framework for studying and using LLMs. Ultimately, this\napproach reframes LLMs as technological participants in an ongoing ecology of\nsigns. They do not possess minds, but they alter how we read, write, and make\nmeaning, compelling us to reconsider the foundations of language,\ninterpretation, and the role of artificial systems in the production of\nknowledge."
                },
                "authors": [
                    {
                        "name": "Davide Picca"
                    }
                ],
                "author_detail": {
                    "name": "Davide Picca"
                },
                "author": "Davide Picca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12787v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12787v3",
                "updated": "2025-07-01T11:18:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    11,
                    18,
                    54,
                    1,
                    182,
                    0
                ],
                "published": "2024-11-19T11:03:09Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    3,
                    9,
                    1,
                    324,
                    0
                ],
                "title": "From Holistic to Localized: Local Enhanced Adapters for Efficient Visual\n  Instruction Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Holistic to Localized: Local Enhanced Adapters for Efficient Visual\n  Instruction Fine-Tuning"
                },
                "summary": "Efficient Visual Instruction Fine-Tuning (EVIT) seeks to adapt Multimodal\nLarge Language Models (MLLMs) to downstream tasks with minimal computational\noverhead. However, as task diversity and complexity increase, EVIT faces\nsignificant challenges in resolving data conflicts. To address this limitation,\nwe propose the Dual Low-Rank Adaptation (Dual-LoRA), a holistic-to-local\nframework that enhances the adapter's capacity to address data conflict through\ndual structural optimization. Specifically, we utilize two subspaces: a skill\nspace for stable, holistic knowledge retention, and a rank-rectified task space\nthat locally activates the holistic knowledge. Additionally, we introduce\nVisual Cue Enhancement (VCE), a multi-level local feature aggregation module\ndesigned to enrich the vision-language projection with local details. Our\napproach is both memory- and time-efficient, requiring only 1.16$\\times$ the\ninference time of the standard LoRA method (with injection into the query and\nvalue projection layers), and just 73\\% of the inference time of a 4-expert\nLoRA-MoE. Extensive experiments on various downstream tasks and general MLLM\nbenchmarks validate the effectiveness of our proposed methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Visual Instruction Fine-Tuning (EVIT) seeks to adapt Multimodal\nLarge Language Models (MLLMs) to downstream tasks with minimal computational\noverhead. However, as task diversity and complexity increase, EVIT faces\nsignificant challenges in resolving data conflicts. To address this limitation,\nwe propose the Dual Low-Rank Adaptation (Dual-LoRA), a holistic-to-local\nframework that enhances the adapter's capacity to address data conflict through\ndual structural optimization. Specifically, we utilize two subspaces: a skill\nspace for stable, holistic knowledge retention, and a rank-rectified task space\nthat locally activates the holistic knowledge. Additionally, we introduce\nVisual Cue Enhancement (VCE), a multi-level local feature aggregation module\ndesigned to enrich the vision-language projection with local details. Our\napproach is both memory- and time-efficient, requiring only 1.16$\\times$ the\ninference time of the standard LoRA method (with injection into the query and\nvalue projection layers), and just 73\\% of the inference time of a 4-expert\nLoRA-MoE. Extensive experiments on various downstream tasks and general MLLM\nbenchmarks validate the effectiveness of our proposed methods."
                },
                "authors": [
                    {
                        "name": "Pengkun Jiao"
                    },
                    {
                        "name": "Bin Zhu"
                    },
                    {
                        "name": "Jingjing Chen"
                    },
                    {
                        "name": "Chong-Wah Ngo"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12787v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12787v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14001v2",
                "updated": "2025-07-01T11:08:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    11,
                    8,
                    55,
                    1,
                    182,
                    0
                ],
                "published": "2025-04-18T18:00:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    18,
                    0,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "Spatially resolved [CII]-gas conversion factor in early galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatially resolved [CII]-gas conversion factor in early galaxies"
                },
                "summary": "Determining how efficiently gas collapses into stars at high-redshift is key\nto understanding galaxy evolution in the Epoch of Reionization (EoR). Globally,\nthis process is quantified by the gas depletion time ($t_{dep}$); on resolved\nscales, by the slope and normalization of the Kennicutt-Schmidt (KS) relation.\nThis work explores the global ($\\alpha_{[CII]}$) and spatially resolved\n($W_{[CII]}$) [CII]-to-gas conversion factors at high-$z$ and their role in\ninferring reliable gas masses, surface densities, and $t_{dep}$ in the EoR. We\nselect galaxies at 4<z<9 from the SERRA cosmological zoom-in simulation, that\nfeatures on-the-fly radiative transfer and resolves interstellar medium\nproperties down to $\\approx$30 pc. The [CII] emission modelling from\nphotodissociation regions allow us to derive global $\\alpha_{ [CII]}$, and maps\nof $W_{[CII]}$. We study their dependence on gas metallicity (Z), density (n),\nMach number (M), and burstiness parameter ($k_s$), and provide best fit\nrelations. The $\\alpha_{[CII]}$ decreases with increasing $Z$ and galaxy\ncompactness, while the resolved $W_{[CII]}$ shows two regimes: at $Z< 0.2\nZ_\\odot$, it anticorrelates with n and Z, but not with $k_s$; above this\nthreshold, it also depends on $k_s$, with more bursty regions showing lower\nconversion factors. This implies $W_{[CII]}\\propto \\Sigma_{[CII]}^{-0.5}$, as\ndense, metal-rich, and bursty regions exhibit higher [CII] surface brightness.\nApplying a constant $\\alpha_{[CII]}$ overestimates $\\Sigma_{gas}$ in bright\n$\\Sigma_{[CII]}$ patches, thus flattening the KS slope and overestimating\n$t_{dep}$ by a factor of $\\approx$4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining how efficiently gas collapses into stars at high-redshift is key\nto understanding galaxy evolution in the Epoch of Reionization (EoR). Globally,\nthis process is quantified by the gas depletion time ($t_{dep}$); on resolved\nscales, by the slope and normalization of the Kennicutt-Schmidt (KS) relation.\nThis work explores the global ($\\alpha_{[CII]}$) and spatially resolved\n($W_{[CII]}$) [CII]-to-gas conversion factors at high-$z$ and their role in\ninferring reliable gas masses, surface densities, and $t_{dep}$ in the EoR. We\nselect galaxies at 4<z<9 from the SERRA cosmological zoom-in simulation, that\nfeatures on-the-fly radiative transfer and resolves interstellar medium\nproperties down to $\\approx$30 pc. The [CII] emission modelling from\nphotodissociation regions allow us to derive global $\\alpha_{ [CII]}$, and maps\nof $W_{[CII]}$. We study their dependence on gas metallicity (Z), density (n),\nMach number (M), and burstiness parameter ($k_s$), and provide best fit\nrelations. The $\\alpha_{[CII]}$ decreases with increasing $Z$ and galaxy\ncompactness, while the resolved $W_{[CII]}$ shows two regimes: at $Z< 0.2\nZ_\\odot$, it anticorrelates with n and Z, but not with $k_s$; above this\nthreshold, it also depends on $k_s$, with more bursty regions showing lower\nconversion factors. This implies $W_{[CII]}\\propto \\Sigma_{[CII]}^{-0.5}$, as\ndense, metal-rich, and bursty regions exhibit higher [CII] surface brightness.\nApplying a constant $\\alpha_{[CII]}$ overestimates $\\Sigma_{gas}$ in bright\n$\\Sigma_{[CII]}$ patches, thus flattening the KS slope and overestimating\n$t_{dep}$ by a factor of $\\approx$4."
                },
                "authors": [
                    {
                        "name": "L. Vallini"
                    },
                    {
                        "name": "A. Pallottini"
                    },
                    {
                        "name": "M. Kohandel"
                    },
                    {
                        "name": "L. Sommovigo"
                    },
                    {
                        "name": "A. Ferrara"
                    },
                    {
                        "name": "M. Bethermin"
                    },
                    {
                        "name": "R. Herrera-Camus"
                    },
                    {
                        "name": "S. Carniani"
                    },
                    {
                        "name": "A. Faisst"
                    },
                    {
                        "name": "A. Zanella"
                    },
                    {
                        "name": "F. Pozzi"
                    },
                    {
                        "name": "M. Dessauges-Zavadsky"
                    },
                    {
                        "name": "C. Gruppioni"
                    },
                    {
                        "name": "E. Veraldi"
                    },
                    {
                        "name": "C. Accard"
                    }
                ],
                "author_detail": {
                    "name": "C. Accard"
                },
                "author": "C. Accard",
                "arxiv_comment": "12 pages, 8 figures. Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23800v2",
                "updated": "2025-07-01T10:16:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    10,
                    16,
                    28,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T12:44:47Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    12,
                    44,
                    47,
                    0,
                    181,
                    0
                ],
                "title": "Towards the Training of Deeper Predictive Coding Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards the Training of Deeper Predictive Coding Neural Networks"
                },
                "summary": "Predictive coding networks trained with equilibrium propagation are neural\nmodels that perform inference through an iterative energy minimization process.\nPrevious studies have demonstrated their effectiveness in shallow\narchitectures, but show significant performance degradation when depth exceeds\nfive to seven layers. In this work, we show that the reason behind this\ndegradation is due to exponentially imbalanced errors between layers during\nweight updates, and predictions from the previous layer not being effective in\nguiding updates in deeper layers. We address the first issue by introducing two\nnovel methods to optimize the latent variables that use precision-weighting to\nre-balance the distribution of energy among layers during the `relaxation\nphase', and the second issue by proposing a novel weight update mechanism that\nreduces error accumulation in deeper layers. Empirically, we test our methods\non a large number of image classification tasks, resulting in large\nimprovements in test accuracy across networks with more than seven layers, with\nperformances comparable to those of backprop on similar models. These findings\nsuggest that a better understanding of the relaxation phase is important to\ntrain models using equilibrium propagation at scale, and open new possibilities\nfor their application in complex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive coding networks trained with equilibrium propagation are neural\nmodels that perform inference through an iterative energy minimization process.\nPrevious studies have demonstrated their effectiveness in shallow\narchitectures, but show significant performance degradation when depth exceeds\nfive to seven layers. In this work, we show that the reason behind this\ndegradation is due to exponentially imbalanced errors between layers during\nweight updates, and predictions from the previous layer not being effective in\nguiding updates in deeper layers. We address the first issue by introducing two\nnovel methods to optimize the latent variables that use precision-weighting to\nre-balance the distribution of energy among layers during the `relaxation\nphase', and the second issue by proposing a novel weight update mechanism that\nreduces error accumulation in deeper layers. Empirically, we test our methods\non a large number of image classification tasks, resulting in large\nimprovements in test accuracy across networks with more than seven layers, with\nperformances comparable to those of backprop on similar models. These findings\nsuggest that a better understanding of the relaxation phase is important to\ntrain models using equilibrium propagation at scale, and open new possibilities\nfor their application in complex tasks."
                },
                "authors": [
                    {
                        "name": "Chang Qi"
                    },
                    {
                        "name": "Matteo Forasassi"
                    },
                    {
                        "name": "Thomas Lukasiewicz"
                    },
                    {
                        "name": "Tommaso Salvatori"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Salvatori"
                },
                "author": "Tommaso Salvatori",
                "arxiv_comment": "18 Pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14851v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14851v2",
                "updated": "2025-07-01T10:06:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    10,
                    6,
                    22,
                    1,
                    182,
                    0
                ],
                "published": "2025-02-20T18:57:06Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    18,
                    57,
                    6,
                    3,
                    51,
                    0
                ],
                "title": "Fast Generation of Weak Lensing Maps in Modified Gravity with COLA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Generation of Weak Lensing Maps in Modified Gravity with COLA"
                },
                "summary": "Accurate predictions of weak lensing observables are essential for\nunderstanding the large-scale structure of the Universe and probing the nature\nof gravity. In this work, we present a lightcone implementation to generate\nmaps of the weak lensing convergence field using the COmoving Lagrangian\nAcceleration (COLA) method. The lightcone is constructed in spherical shells\nfrom the source to the observer following an onion representation of the\nUniverse. We validate the COLA-generated convergence maps in General Relativity\nby comparing five statistics to those of maps obtained with publically\navailable high-resolution $N$-body simulations: the power spectrum, bispectrum,\nprobability distribution function, peak counts and Minkowski functionals. The\nconvergence power spectrum is accurate to within $5\\%$ up to $\\ell\\sim500$ and\nto within $10\\%$ up to $\\ell\\sim750$, confirming the accuracy of this method on\nboth linear and non-linear scales. For the probability distribution function,\npeak counts and Minkowski functionals, we determine the map pixel resolution\nrequired for COLA to capture the statistical features of the $N$-body\nconvergence maps. Our validation tests provide a baseline for the convergence\nmap specifications at which we can trust COLA for each statistic considered.\nUsing these map specifications, we extend our analyses to two representative\ntheories of Modified Gravity, and demonstrate their imprints on the five\nconvergence statistics considered. This work represents a step towards precise\nweak lensing predictions under both General Relativity and Modified Gravity\nwith reduced computational cost, providing a robust framework to explore the\nnature of gravity using field-level inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate predictions of weak lensing observables are essential for\nunderstanding the large-scale structure of the Universe and probing the nature\nof gravity. In this work, we present a lightcone implementation to generate\nmaps of the weak lensing convergence field using the COmoving Lagrangian\nAcceleration (COLA) method. The lightcone is constructed in spherical shells\nfrom the source to the observer following an onion representation of the\nUniverse. We validate the COLA-generated convergence maps in General Relativity\nby comparing five statistics to those of maps obtained with publically\navailable high-resolution $N$-body simulations: the power spectrum, bispectrum,\nprobability distribution function, peak counts and Minkowski functionals. The\nconvergence power spectrum is accurate to within $5\\%$ up to $\\ell\\sim500$ and\nto within $10\\%$ up to $\\ell\\sim750$, confirming the accuracy of this method on\nboth linear and non-linear scales. For the probability distribution function,\npeak counts and Minkowski functionals, we determine the map pixel resolution\nrequired for COLA to capture the statistical features of the $N$-body\nconvergence maps. Our validation tests provide a baseline for the convergence\nmap specifications at which we can trust COLA for each statistic considered.\nUsing these map specifications, we extend our analyses to two representative\ntheories of Modified Gravity, and demonstrate their imprints on the five\nconvergence statistics considered. This work represents a step towards precise\nweak lensing predictions under both General Relativity and Modified Gravity\nwith reduced computational cost, providing a robust framework to explore the\nnature of gravity using field-level inference."
                },
                "authors": [
                    {
                        "name": "Sophie Hoyland"
                    },
                    {
                        "name": "Hans A. Winther"
                    },
                    {
                        "name": "Daniela Saadeh"
                    },
                    {
                        "name": "Kazuya Koyama"
                    },
                    {
                        "name": "Albert Izard"
                    }
                ],
                "author_detail": {
                    "name": "Albert Izard"
                },
                "author": "Albert Izard",
                "arxiv_doi": "10.1093/mnras/staf1071",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/staf1071",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.14851v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14851v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 12 figures; Accepted for publication in MNRAS",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13810v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13810v2",
                "updated": "2025-07-01T09:48:50Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    48,
                    50,
                    1,
                    182,
                    0
                ],
                "published": "2024-11-21T03:20:17Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    20,
                    17,
                    3,
                    326,
                    0
                ],
                "title": "Dynamic Spatial Interaction Models for a Resource Allocator's Decisions\n  and Local Agents' Multiple Activities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Spatial Interaction Models for a Resource Allocator's Decisions\n  and Local Agents' Multiple Activities"
                },
                "summary": "This paper introduces a novel spatial interaction model to explore the\ndecision-making processes of a resource allocator and local agents, with\ncentral and local governments serving as empirical representations. The model\ncaptures two key features: (i) resource allocations from the allocator to local\nagents and the resulting strategic interactions, and (ii) local agents'\nmultiple activities and their interactions. We develop a network game for the\nmicro-foundations of these processes. In this game, local agents engage in\nmultiple activities, while the allocator distributes resources by monitoring\nthe externalities arising from their interactions. The game's unique Nash\nequilibrium establishes our econometric framework. To estimate the agent payoff\nparameters, we employ the quasi-maximum likelihood (QML) estimation method and\nexamine the asymptotic properties of the QML estimator to ensure robust\nstatistical inference. Empirically, we study interactions among U.S. states in\npublic welfare and housing and community development expenditures, focusing on\nhow federal grants influence these expenditures and the interdependencies among\nstate governments. Our findings reveal significant spillovers across the\nstates' two expenditures. Additionally, we detect positive effects of federal\ngrants on both types of expenditures, inducing a responsive grant scheme based\non states' decisions. Last, we compare state expenditures and social welfare\nthrough counterfactual simulations under two scenarios: (i) responsive\nintervention by monitoring states' decisions and (ii) autonomous transfers. We\nfind that responsive intervention enhances social welfare by leading to an\nincrease in the states' two expenditures. However, due to the heavy reliance on\nautonomous transfers, the magnitude of these improvements remains relatively\nsmall compared to the share of federal grants in total state revenues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel spatial interaction model to explore the\ndecision-making processes of a resource allocator and local agents, with\ncentral and local governments serving as empirical representations. The model\ncaptures two key features: (i) resource allocations from the allocator to local\nagents and the resulting strategic interactions, and (ii) local agents'\nmultiple activities and their interactions. We develop a network game for the\nmicro-foundations of these processes. In this game, local agents engage in\nmultiple activities, while the allocator distributes resources by monitoring\nthe externalities arising from their interactions. The game's unique Nash\nequilibrium establishes our econometric framework. To estimate the agent payoff\nparameters, we employ the quasi-maximum likelihood (QML) estimation method and\nexamine the asymptotic properties of the QML estimator to ensure robust\nstatistical inference. Empirically, we study interactions among U.S. states in\npublic welfare and housing and community development expenditures, focusing on\nhow federal grants influence these expenditures and the interdependencies among\nstate governments. Our findings reveal significant spillovers across the\nstates' two expenditures. Additionally, we detect positive effects of federal\ngrants on both types of expenditures, inducing a responsive grant scheme based\non states' decisions. Last, we compare state expenditures and social welfare\nthrough counterfactual simulations under two scenarios: (i) responsive\nintervention by monitoring states' decisions and (ii) autonomous transfers. We\nfind that responsive intervention enhances social welfare by leading to an\nincrease in the states' two expenditures. However, due to the heavy reliance on\nautonomous transfers, the magnitude of these improvements remains relatively\nsmall compared to the share of federal grants in total state revenues."
                },
                "authors": [
                    {
                        "name": "Hanbat Jeong"
                    }
                ],
                "author_detail": {
                    "name": "Hanbat Jeong"
                },
                "author": "Hanbat Jeong",
                "arxiv_comment": "57 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13810v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13810v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19676v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19676v3",
                "updated": "2025-07-02T08:50:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    50,
                    11,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-24T14:44:28Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    44,
                    28,
                    1,
                    175,
                    0
                ],
                "title": "A Survey of LLM-Driven AI Agent Communication: Protocols, Security\n  Risks, and Defense Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM-Driven AI Agent Communication: Protocols, Security\n  Risks, and Defense Countermeasures"
                },
                "summary": "In recent years, Large-Language-Model-driven AI agents have exhibited\nunprecedented intelligence and adaptability, and are rapidly changing human\nproduction and life. Nowadays, agents are undergoing a new round of evolution.\nThey no longer act as an isolated island like LLMs. Instead, they start to\ncommunicate with diverse external entities, such as other agents and tools, to\nperform more complex tasks collectively. Under this trend, agent communication\nis regarded as a foundational pillar of the future AI ecosystem, and many\norganizations have intensively begun to design related communication protocols\n(e.g., Anthropic's MCP and Google's A2A) within the recent few months. However,\nthis new field exposes significant security hazards, which can cause severe\ndamage to real-world scenarios. To help researchers quickly figure out this\npromising topic and benefit the future agent communication development, this\npaper presents a comprehensive survey of agent communication security. More\nprecisely, we first present a clear definition of agent communication and\ncategorize the entire lifecycle of agent communication into three stages:\nuser-agent interaction, agent-agent communication, and agent-environment\ncommunication. Next, for each communication phase, we dissect related protocols\nand analyze the security risks according to the communication characteristics.\nThen, we summarize and outlook on the possible defense countermeasures for each\nrisk. In addition, we conduct experiments using MCP and A2A to help readers\nbetter understand the novel vulnerabilities brought by agent communication.\nFinally, we discuss open issues and future directions in this promising\nresearch field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large-Language-Model-driven AI agents have exhibited\nunprecedented intelligence and adaptability, and are rapidly changing human\nproduction and life. Nowadays, agents are undergoing a new round of evolution.\nThey no longer act as an isolated island like LLMs. Instead, they start to\ncommunicate with diverse external entities, such as other agents and tools, to\nperform more complex tasks collectively. Under this trend, agent communication\nis regarded as a foundational pillar of the future AI ecosystem, and many\norganizations have intensively begun to design related communication protocols\n(e.g., Anthropic's MCP and Google's A2A) within the recent few months. However,\nthis new field exposes significant security hazards, which can cause severe\ndamage to real-world scenarios. To help researchers quickly figure out this\npromising topic and benefit the future agent communication development, this\npaper presents a comprehensive survey of agent communication security. More\nprecisely, we first present a clear definition of agent communication and\ncategorize the entire lifecycle of agent communication into three stages:\nuser-agent interaction, agent-agent communication, and agent-environment\ncommunication. Next, for each communication phase, we dissect related protocols\nand analyze the security risks according to the communication characteristics.\nThen, we summarize and outlook on the possible defense countermeasures for each\nrisk. In addition, we conduct experiments using MCP and A2A to help readers\nbetter understand the novel vulnerabilities brought by agent communication.\nFinally, we discuss open issues and future directions in this promising\nresearch field."
                },
                "authors": [
                    {
                        "name": "Dezhang Kong"
                    },
                    {
                        "name": "Shi Lin"
                    },
                    {
                        "name": "Zhenhua Xu"
                    },
                    {
                        "name": "Zhebo Wang"
                    },
                    {
                        "name": "Minghao Li"
                    },
                    {
                        "name": "Yufeng Li"
                    },
                    {
                        "name": "Yilun Zhang"
                    },
                    {
                        "name": "Hujin Peng"
                    },
                    {
                        "name": "Zeyang Sha"
                    },
                    {
                        "name": "Yuyuan Li"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Chaochao Chen"
                    },
                    {
                        "name": "Muhammad Khurram Khan"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "arxiv_comment": "41 pages, 13 figures, submitted to IEEE COMST",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19676v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19676v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06432v2",
                "updated": "2025-07-01T09:36:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    36,
                    47,
                    1,
                    182,
                    0
                ],
                "published": "2024-12-09T12:20:33Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    20,
                    33,
                    0,
                    344,
                    0
                ],
                "title": "Integrating Expert Labels into LLM-based Emission Goal Detection:\n  Example Selection vs Automatic Prompt Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Expert Labels into LLM-based Emission Goal Detection:\n  Example Selection vs Automatic Prompt Design"
                },
                "summary": "We address the detection of emission reduction goals in corporate reports, an\nimportant task for monitoring companies' progress in addressing climate change.\nSpecifically, we focus on the issue of integrating expert feedback in the form\nof labeled example passages into LLM-based pipelines, and compare the two\nstrategies of (1) a dynamic selection of few-shot examples and (2) the\nautomatic optimization of the prompt by the LLM itself. Our findings on a\npublic dataset of 769 climate-related passages from real-world business reports\nindicate that automatic prompt optimization is the superior approach, while\ncombining both methods provides only limited benefit. Qualitative results\nindicate that optimized prompts do indeed capture many intricacies of the\ntargeted emission goal extraction task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the detection of emission reduction goals in corporate reports, an\nimportant task for monitoring companies' progress in addressing climate change.\nSpecifically, we focus on the issue of integrating expert feedback in the form\nof labeled example passages into LLM-based pipelines, and compare the two\nstrategies of (1) a dynamic selection of few-shot examples and (2) the\nautomatic optimization of the prompt by the LLM itself. Our findings on a\npublic dataset of 769 climate-related passages from real-world business reports\nindicate that automatic prompt optimization is the superior approach, while\ncombining both methods provides only limited benefit. Qualitative results\nindicate that optimized prompts do indeed capture many intricacies of the\ntargeted emission goal extraction task."
                },
                "authors": [
                    {
                        "name": "Marco Wrzalik"
                    },
                    {
                        "name": "Adrian Ulges"
                    },
                    {
                        "name": "Anne Uersfeld"
                    },
                    {
                        "name": "Florian Faust"
                    },
                    {
                        "name": "Viola Campos"
                    }
                ],
                "author_detail": {
                    "name": "Viola Campos"
                },
                "author": "Viola Campos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20580v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20580v3",
                "updated": "2025-07-01T09:29:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    29,
                    7,
                    1,
                    182,
                    0
                ],
                "published": "2025-03-26T14:26:23Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    26,
                    23,
                    2,
                    85,
                    0
                ],
                "title": "Experiments and modeling of mechanically-soft, hard magnetorheological\n  foams with potential applications in haptic sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experiments and modeling of mechanically-soft, hard magnetorheological\n  foams with potential applications in haptic sensing"
                },
                "summary": "This study proposes a family of novel mechanically-soft and magnetically-hard\nmagnetorheological foams that, upon deformation, lead to robust and measurable\nmagnetic flux changes in their surroundings. This allows to infer qualitatively\nand even quantitatively the imposed deformation and, eventually from that, an\nestimation of the stiffness and average stress on the sample even in complex\nloading scenarios involving combinations of uniform or nonuniform\ncompression/tension with superposed shearing in different directions. The work\nprovides a complete experimental, theoretical and numerical framework on finite\nstrain, compressible magneto-elasticity, thereby allowing to measure and\npredict coupled magneto-mechanical properties of such materials with different\nparticle volume fractions and then use it to estimate and design potential\nhaptic sensing devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes a family of novel mechanically-soft and magnetically-hard\nmagnetorheological foams that, upon deformation, lead to robust and measurable\nmagnetic flux changes in their surroundings. This allows to infer qualitatively\nand even quantitatively the imposed deformation and, eventually from that, an\nestimation of the stiffness and average stress on the sample even in complex\nloading scenarios involving combinations of uniform or nonuniform\ncompression/tension with superposed shearing in different directions. The work\nprovides a complete experimental, theoretical and numerical framework on finite\nstrain, compressible magneto-elasticity, thereby allowing to measure and\npredict coupled magneto-mechanical properties of such materials with different\nparticle volume fractions and then use it to estimate and design potential\nhaptic sensing devices."
                },
                "authors": [
                    {
                        "name": "Zehui Lin"
                    },
                    {
                        "name": "Zahra Hooshmand-Ahoor"
                    },
                    {
                        "name": "Laurence Bodelot"
                    },
                    {
                        "name": "Kostas Danas"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Danas"
                },
                "author": "Kostas Danas",
                "arxiv_doi": "10.1016/j.jmps.2025.106218",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jmps.2025.106218",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.20580v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20580v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "J. Mech. Phys. Solids 203 (2025) 106218",
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01933v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01933v6",
                "updated": "2025-07-01T08:40:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    8,
                    40,
                    39,
                    1,
                    182,
                    0
                ],
                "published": "2024-08-04T05:15:02Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    5,
                    15,
                    2,
                    6,
                    217,
                    0
                ],
                "title": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios."
                },
                "authors": [
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Jiuyang Chang"
                    },
                    {
                        "name": "Yiming Qian"
                    },
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Zhouqiang Jiang"
                    },
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Yuta Nakashima"
                    },
                    {
                        "name": "Hajime Nagahara"
                    }
                ],
                "author_detail": {
                    "name": "Hajime Nagahara"
                },
                "author": "Hajime Nagahara",
                "arxiv_comment": "Accepted by NeurIPS 2024 D&B Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01933v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01933v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16845v2",
                "updated": "2025-07-01T08:22:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    8,
                    22,
                    44,
                    1,
                    182,
                    0
                ],
                "published": "2025-04-23T16:09:33Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    16,
                    9,
                    33,
                    2,
                    113,
                    0
                ],
                "title": "An accreting dwarf star orbiting the S-type giant star pi1 Gru",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An accreting dwarf star orbiting the S-type giant star pi1 Gru"
                },
                "summary": "Aims. We aim to characterize the properties of the inner companion of the\nS-type AGB star pi1 Gru and to identify plausible future evolutionary scenarios\nfor this triple system. Methods. We observed pi1 Gru with ALMA and VLT/SPHERE.\nIn addition, we collected archival photometry data and used the Hipparcos-Gaia\nproper motion anomaly. We derive the best orbital parameters from Bayesian\ninference. Results. In June-July 2019, the inner companion, pi1 Gru C, was\nlocated at 37.4 +/- 2.0 mas from the primary (a projected separation of 6.05\n+/- 0.55 au at 161.7 +/- 11.7 pc). The best orbital solution yields a companion\nmass of 0.86 (+0.22/-0.20) Msun (using the derived mass of the primary) and a\nsemi-major axis of 7.05(+0.54/-0.57) au, corresponding to an orbital period of\n11.0 (+1.7/-1.5) yr. The preferred solution is an elliptical orbit with\neccentricity e = 0.35(+0.18/-0.17), although a circular orbit cannot be fully\nexcluded. The close companion could be either a K1V (F9.5V to K7V) star or a\nwhite dwarf (WD). Ultraviolet and millimeter continuum photometry are\nconsistent with the presence of an accretion disk around the close companion.\nThe ultraviolet emission may originate from hot spots in an overall cooler\ndisk, or from a hot disk if the companion is a WD. Conclusions. Although the\nclose companion and the AGB star are interacting and an accretion disk is\nobserved around the companion, the mass-accretion rate is too low to trigger a\nType Ia supernova, but it could produce novae every ~900 yr. Short-wavelength,\nspatially resolved observations are required to further constrain the nature of\nthe C companion. Searches for close-in companions similar to this system will\nimprove our understanding of the physics of mass and angular momentum transfer,\nas well as orbital evolution during late evolutionary stages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aims. We aim to characterize the properties of the inner companion of the\nS-type AGB star pi1 Gru and to identify plausible future evolutionary scenarios\nfor this triple system. Methods. We observed pi1 Gru with ALMA and VLT/SPHERE.\nIn addition, we collected archival photometry data and used the Hipparcos-Gaia\nproper motion anomaly. We derive the best orbital parameters from Bayesian\ninference. Results. In June-July 2019, the inner companion, pi1 Gru C, was\nlocated at 37.4 +/- 2.0 mas from the primary (a projected separation of 6.05\n+/- 0.55 au at 161.7 +/- 11.7 pc). The best orbital solution yields a companion\nmass of 0.86 (+0.22/-0.20) Msun (using the derived mass of the primary) and a\nsemi-major axis of 7.05(+0.54/-0.57) au, corresponding to an orbital period of\n11.0 (+1.7/-1.5) yr. The preferred solution is an elliptical orbit with\neccentricity e = 0.35(+0.18/-0.17), although a circular orbit cannot be fully\nexcluded. The close companion could be either a K1V (F9.5V to K7V) star or a\nwhite dwarf (WD). Ultraviolet and millimeter continuum photometry are\nconsistent with the presence of an accretion disk around the close companion.\nThe ultraviolet emission may originate from hot spots in an overall cooler\ndisk, or from a hot disk if the companion is a WD. Conclusions. Although the\nclose companion and the AGB star are interacting and an accretion disk is\nobserved around the companion, the mass-accretion rate is too low to trigger a\nType Ia supernova, but it could produce novae every ~900 yr. Short-wavelength,\nspatially resolved observations are required to further constrain the nature of\nthe C companion. Searches for close-in companions similar to this system will\nimprove our understanding of the physics of mass and angular momentum transfer,\nas well as orbital evolution during late evolutionary stages."
                },
                "authors": [
                    {
                        "name": "M. Montargs"
                    },
                    {
                        "name": "J. Malfait"
                    },
                    {
                        "name": "M. Esseldeurs"
                    },
                    {
                        "name": "A. de Koter"
                    },
                    {
                        "name": "F. Baron"
                    },
                    {
                        "name": "P. Kervella"
                    },
                    {
                        "name": "T. Danilovich"
                    },
                    {
                        "name": "A. M. S. Richards"
                    },
                    {
                        "name": "R. Sahai"
                    },
                    {
                        "name": "I. McDonald"
                    },
                    {
                        "name": "T. Khouri"
                    },
                    {
                        "name": "S. Shetye"
                    },
                    {
                        "name": "A. Zijlstra"
                    },
                    {
                        "name": "M. Van de Sande"
                    },
                    {
                        "name": "I. El Mellah"
                    },
                    {
                        "name": "F. Herpin"
                    },
                    {
                        "name": "L. Siess"
                    },
                    {
                        "name": "S. Etoka"
                    },
                    {
                        "name": "D. Gobrecht"
                    },
                    {
                        "name": "L. Marinho"
                    },
                    {
                        "name": "S. H. J. Wallstrm"
                    },
                    {
                        "name": "K. T. Wong"
                    },
                    {
                        "name": "J. Yates"
                    }
                ],
                "author_detail": {
                    "name": "J. Yates"
                },
                "author": "J. Yates",
                "arxiv_doi": "10.1051/0004-6361/202452587",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202452587",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publications in Astronomy & Astrophysics. 21 pages, 10+2\n  figures, 3+4 tables. v2: Language and formatting editing. Correction in\n  inclination convention",
                "arxiv_journal_ref": "A&A 699, A22 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10967v2",
                "updated": "2025-07-01T08:19:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    8,
                    19,
                    8,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-12T17:59:09Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    17,
                    59,
                    9,
                    3,
                    163,
                    0
                ],
                "title": "Beyond Attention or Similarity: Maximizing Conditional Diversity for\n  Token Pruning in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Attention or Similarity: Maximizing Conditional Diversity for\n  Token Pruning in MLLMs"
                },
                "summary": "In multimodal large language models (MLLMs), the length of input visual\ntokens is often significantly greater than that of their textual counterparts,\nleading to a high inference cost. Many works aim to address this issue by\nremoving redundant visual tokens. However, current approaches either rely on\nattention-based pruning, which retains numerous duplicate tokens, or use\nsimilarity-based pruning, overlooking the instruction relevance, consequently\ncausing suboptimal performance. In this paper, we go beyond attention or\nsimilarity by proposing a novel visual token pruning method named CDPruner,\nwhich maximizes the conditional diversity of retained tokens. We first define\nthe conditional similarity between visual tokens conditioned on the\ninstruction, and then reformulate the token pruning problem with determinantal\npoint process (DPP) to maximize the conditional diversity of the selected\nsubset. The proposed CDPruner is training-free and model-agnostic, allowing\neasy application to various MLLMs. Extensive experiments across diverse MLLMs\nshow that CDPruner establishes new state-of-the-art on various vision-language\nbenchmarks. By maximizing conditional diversity through DPP, the selected\nsubset better represents the input images while closely adhering to user\ninstructions, thereby preserving strong performance even with high reduction\nratios. When applied to LLaVA, CDPruner reduces FLOPs by 95\\% and CUDA latency\nby 78\\%, while maintaining 94\\% of the original accuracy. Our code is available\nat https://github.com/Theia-4869/CDPruner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multimodal large language models (MLLMs), the length of input visual\ntokens is often significantly greater than that of their textual counterparts,\nleading to a high inference cost. Many works aim to address this issue by\nremoving redundant visual tokens. However, current approaches either rely on\nattention-based pruning, which retains numerous duplicate tokens, or use\nsimilarity-based pruning, overlooking the instruction relevance, consequently\ncausing suboptimal performance. In this paper, we go beyond attention or\nsimilarity by proposing a novel visual token pruning method named CDPruner,\nwhich maximizes the conditional diversity of retained tokens. We first define\nthe conditional similarity between visual tokens conditioned on the\ninstruction, and then reformulate the token pruning problem with determinantal\npoint process (DPP) to maximize the conditional diversity of the selected\nsubset. The proposed CDPruner is training-free and model-agnostic, allowing\neasy application to various MLLMs. Extensive experiments across diverse MLLMs\nshow that CDPruner establishes new state-of-the-art on various vision-language\nbenchmarks. By maximizing conditional diversity through DPP, the selected\nsubset better represents the input images while closely adhering to user\ninstructions, thereby preserving strong performance even with high reduction\nratios. When applied to LLaVA, CDPruner reduces FLOPs by 95\\% and CUDA latency\nby 78\\%, while maintaining 94\\% of the original accuracy. Our code is available\nat https://github.com/Theia-4869/CDPruner."
                },
                "authors": [
                    {
                        "name": "Qizhe Zhang"
                    },
                    {
                        "name": "Mengzhen Liu"
                    },
                    {
                        "name": "Lichen Li"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Junwen Pan"
                    },
                    {
                        "name": "Qi She"
                    },
                    {
                        "name": "Shanghang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shanghang Zhang"
                },
                "author": "Shanghang Zhang",
                "arxiv_comment": "22 pages, 5 figures, code: https://github.com/Theia-4869/CDPruner,\n  project page: https://theia-4869.github.io/CDPruner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05795v2",
                "updated": "2025-07-01T08:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    8,
                    18,
                    41,
                    1,
                    182,
                    0
                ],
                "published": "2025-02-09T07:03:36Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    7,
                    3,
                    36,
                    6,
                    40,
                    0
                ],
                "title": "The Curse of Depth in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Curse of Depth in Large Language Models"
                },
                "summary": "In this paper, we introduce the Curse of Depth, a concept that highlights,\nexplains, and addresses the recent observation in modern Large Language Models\n(LLMs) where nearly half of the layers are less effective than expected. We\nfirst confirm the wide existence of this phenomenon across the most popular\nfamilies of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis,\ntheoretically and empirically, identifies that the underlying reason for the\nineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer\nNormalization (Pre-LN). While Pre-LN stabilizes the training of Transformer\nLLMs, its output variance exponentially grows with the model depth, which\nundesirably causes the derivative of the deep Transformer blocks to be an\nidentity matrix, and therefore barely contributes to the training. To resolve\nthis training pitfall, we propose LayerNorm Scaling (LNS), which scales the\nvariance of output of the layer normalization inversely by the square root of\nits depth. This simple modification mitigates the output variance explosion of\ndeeper Transformer layers, improving their contribution. Across a wide range of\nmodel sizes (130M to 7B), our experiments show that LNS consistently\noutperforms previous normalization and scaling techniques in enhancing LLM\npre-training performance. Moreover, this improvement seamlessly carries over to\nsupervised fine-tuning. All these gains can be attributed to the fact that\nLayerNorm Scaling enables deeper layers to contribute more effectively during\ntraining. Our code is available at\n\\href{https://github.com/lmsdss/LayerNorm-Scaling}{LayerNorm-Scaling}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce the Curse of Depth, a concept that highlights,\nexplains, and addresses the recent observation in modern Large Language Models\n(LLMs) where nearly half of the layers are less effective than expected. We\nfirst confirm the wide existence of this phenomenon across the most popular\nfamilies of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis,\ntheoretically and empirically, identifies that the underlying reason for the\nineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer\nNormalization (Pre-LN). While Pre-LN stabilizes the training of Transformer\nLLMs, its output variance exponentially grows with the model depth, which\nundesirably causes the derivative of the deep Transformer blocks to be an\nidentity matrix, and therefore barely contributes to the training. To resolve\nthis training pitfall, we propose LayerNorm Scaling (LNS), which scales the\nvariance of output of the layer normalization inversely by the square root of\nits depth. This simple modification mitigates the output variance explosion of\ndeeper Transformer layers, improving their contribution. Across a wide range of\nmodel sizes (130M to 7B), our experiments show that LNS consistently\noutperforms previous normalization and scaling techniques in enhancing LLM\npre-training performance. Moreover, this improvement seamlessly carries over to\nsupervised fine-tuning. All these gains can be attributed to the fact that\nLayerNorm Scaling enables deeper layers to contribute more effectively during\ntraining. Our code is available at\n\\href{https://github.com/lmsdss/LayerNorm-Scaling}{LayerNorm-Scaling}."
                },
                "authors": [
                    {
                        "name": "Wenfang Sun"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Yefeng Zheng"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23520v2",
                "updated": "2025-07-01T08:11:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    8,
                    11,
                    18,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T05:11:19Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    11,
                    19,
                    0,
                    181,
                    0
                ],
                "title": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions\n  with LLM-Generated Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions\n  with LLM-Generated Data"
                },
                "summary": "With the increasing interest in robotic synthesis in the context of organic\nchemistry, the automated extraction of chemical procedures from literature is\ncritical. However, this task remains challenging due to the inherent ambiguity\nof chemical language and the high cost of human annotation required for\ndeveloping reliable computer-aided extraction protocols. Here, we present\nChemActor, a fully fine-tuned large language model (LLM), as a chemical\nexecutor to convert between unstructured experimental procedures and structured\naction sequences. We propose a sequential LLM-generated data framework to\naddress the challenges of insufficient and low-quality annotated data. This\nframework integrates a data selection module that selects data based on\ndistribution divergence, with a general-purpose LLM, to generate\nmachine-executable actions from a single molecule input. Additionally, we\nintroduce a novel multi-round LLMs circle review metric, which reflects the\nmodel's advanced understanding of chemical experimental procedures. Extensive\nexperiments on reaction-to-description (R2D) and description-to-action (D2A)\ntasks demonstrate that ChemActor, augmented by LLM-generated data, achieves\nstate-of-the-art performance, outperforming the baseline model by 10%. The code\nis available at: https://github.com/Zhanghahah/ChemActor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing interest in robotic synthesis in the context of organic\nchemistry, the automated extraction of chemical procedures from literature is\ncritical. However, this task remains challenging due to the inherent ambiguity\nof chemical language and the high cost of human annotation required for\ndeveloping reliable computer-aided extraction protocols. Here, we present\nChemActor, a fully fine-tuned large language model (LLM), as a chemical\nexecutor to convert between unstructured experimental procedures and structured\naction sequences. We propose a sequential LLM-generated data framework to\naddress the challenges of insufficient and low-quality annotated data. This\nframework integrates a data selection module that selects data based on\ndistribution divergence, with a general-purpose LLM, to generate\nmachine-executable actions from a single molecule input. Additionally, we\nintroduce a novel multi-round LLMs circle review metric, which reflects the\nmodel's advanced understanding of chemical experimental procedures. Extensive\nexperiments on reaction-to-description (R2D) and description-to-action (D2A)\ntasks demonstrate that ChemActor, augmented by LLM-generated data, achieves\nstate-of-the-art performance, outperforming the baseline model by 10%. The code\nis available at: https://github.com/Zhanghahah/ChemActor."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ruijie Yu"
                    },
                    {
                        "name": "Jidong Tian"
                    },
                    {
                        "name": "Feng Zhu"
                    },
                    {
                        "name": "Jiapeng Liu"
                    },
                    {
                        "name": "Xiaokang Yang"
                    },
                    {
                        "name": "Yaohui Jin"
                    },
                    {
                        "name": "Yanyan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yanyan Xu"
                },
                "author": "Yanyan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21393v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21393v3",
                "updated": "2025-07-01T08:05:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    8,
                    5,
                    1,
                    1,
                    182,
                    0
                ],
                "published": "2025-03-27T11:35:40Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    35,
                    40,
                    3,
                    86,
                    0
                ],
                "title": "An evaluation of LLMs and Google Translate for translation of selected\n  Indian languages via sentiment and semantic analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evaluation of LLMs and Google Translate for translation of selected\n  Indian languages via sentiment and semantic analyses"
                },
                "summary": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study on the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT, and Google Translate. This study addresses this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts (Bhagavad Gita,\nTamas and Maha Prasthanam ) that have been well translated by experts and use\nLLMs to generate their translations into English, and provide a comparison with\nselected expert (human) translations. Our investigation revealed that while\nLLMs have made significant progress in translation accuracy, challenges remain\nin preserving sentiment and semantic integrity, especially in metaphorical and\nphilosophical contexts for texts such as the Bhagavad Gita. The sentiment\nanalysis revealed that GPT models are better at preserving the sentiment\npolarity for the given texts when compared to human (expert) translation. The\nresults revealed that GPT models are generally better at maintaining the\nsentiment and semantics when compared to Google Translate. This study could\nhelp in the development of accurate and culturally sensitive translation\nsystems for large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study on the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT, and Google Translate. This study addresses this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts (Bhagavad Gita,\nTamas and Maha Prasthanam ) that have been well translated by experts and use\nLLMs to generate their translations into English, and provide a comparison with\nselected expert (human) translations. Our investigation revealed that while\nLLMs have made significant progress in translation accuracy, challenges remain\nin preserving sentiment and semantic integrity, especially in metaphorical and\nphilosophical contexts for texts such as the Bhagavad Gita. The sentiment\nanalysis revealed that GPT models are better at preserving the sentiment\npolarity for the given texts when compared to human (expert) translation. The\nresults revealed that GPT models are generally better at maintaining the\nsentiment and semantics when compared to Google Translate. This study could\nhelp in the development of accurate and culturally sensitive translation\nsystems for large language models."
                },
                "authors": [
                    {
                        "name": "Rohitash Chandra"
                    },
                    {
                        "name": "Aryan Chaudhari"
                    },
                    {
                        "name": "Yeshwanth Rayavarapu"
                    }
                ],
                "author_detail": {
                    "name": "Yeshwanth Rayavarapu"
                },
                "author": "Yeshwanth Rayavarapu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21393v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21393v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23815v2",
                "updated": "2025-07-01T07:51:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    7,
                    51,
                    20,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T13:02:01Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    2,
                    1,
                    0,
                    181,
                    0
                ],
                "title": "The Impact of AI on Educational Assessment: A Framework for Constructive\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of AI on Educational Assessment: A Framework for Constructive\n  Alignment"
                },
                "summary": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods."
                },
                "authors": [
                    {
                        "name": "Patrick Stokkink"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Stokkink"
                },
                "author": "Patrick Stokkink",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03040v2",
                "updated": "2025-07-01T07:35:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    7,
                    35,
                    25,
                    1,
                    182,
                    0
                ],
                "published": "2025-03-04T22:45:24Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    22,
                    45,
                    24,
                    1,
                    63,
                    0
                ],
                "title": "SAGE: Steering Dialog Generation with Future-Aware State-Action\n  Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGE: Steering Dialog Generation with Future-Aware State-Action\n  Augmentation"
                },
                "summary": "Recent advances in large language models have demonstrated impressive\ncapabilities in task-oriented applications, yet building emotionally\nintelligent chatbots that can engage in natural, strategic conversations\nremains a challenge. We present a novel approach called SAGE that uses latent\nvariables to control long-horizon behavior in dialogue generation. At the core\nof our method is the State-Action Chain (SAC), which augments standard language\nmodel fine-tuning by introducing latent variables that encapsulate emotional\nstates and conversational strategies between dialogue turns. During inference,\nthese variables are generated before each response, enabling coarse-grained\ncontrol over dialogue progression while maintaining natural interaction\npatterns. We also introduce a self-improvement pipeline that leverages dialogue\ntree search, LLM-based reward modeling, and targeted fine-tuning to optimize\nconversational trajectories. Our experimental results show that models trained\nwith this approach demonstrate improved performance in emotional intelligence\nmetrics while maintaining strong capabilities on LLM benchmarks. The discrete\nnature of our latent variables facilitates search-based strategies and provides\na foundation for future applications of reinforcement learning to dialogue\nsystems, where learning can occur at the state level rather than the token\nlevel. https://github.com/apple/ml-sage-dialog-gen",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated impressive\ncapabilities in task-oriented applications, yet building emotionally\nintelligent chatbots that can engage in natural, strategic conversations\nremains a challenge. We present a novel approach called SAGE that uses latent\nvariables to control long-horizon behavior in dialogue generation. At the core\nof our method is the State-Action Chain (SAC), which augments standard language\nmodel fine-tuning by introducing latent variables that encapsulate emotional\nstates and conversational strategies between dialogue turns. During inference,\nthese variables are generated before each response, enabling coarse-grained\ncontrol over dialogue progression while maintaining natural interaction\npatterns. We also introduce a self-improvement pipeline that leverages dialogue\ntree search, LLM-based reward modeling, and targeted fine-tuning to optimize\nconversational trajectories. Our experimental results show that models trained\nwith this approach demonstrate improved performance in emotional intelligence\nmetrics while maintaining strong capabilities on LLM benchmarks. The discrete\nnature of our latent variables facilitates search-based strategies and provides\na foundation for future applications of reinforcement learning to dialogue\nsystems, where learning can occur at the state level rather than the token\nlevel. https://github.com/apple/ml-sage-dialog-gen"
                },
                "authors": [
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    }
                ],
                "author_detail": {
                    "name": "Navdeep Jaitly"
                },
                "author": "Navdeep Jaitly",
                "arxiv_comment": "9 pages main text",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06722v2",
                "updated": "2025-07-01T07:26:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    7,
                    26,
                    17,
                    1,
                    182,
                    0
                ],
                "published": "2025-04-09T09:23:11Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    23,
                    11,
                    2,
                    99,
                    0
                ],
                "title": "Plastic tensor networks for interpretable generative modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plastic tensor networks for interpretable generative modeling"
                },
                "summary": "A structural optimization scheme for a single-layer nonnegative adaptive\ntensor tree (NATT) that models a target probability distribution is proposed as\nan alternative paradigm for generative modeling. The NATT scheme, by\nconstruction, automatically searches for a tree structure that best fits a\ngiven discrete dataset whose features serve as inputs, and has the advantage\nthat it is interpretable as a probabilistic graphical model. We consider the\nNATT scheme and a recently proposed Born machine adaptive tensor tree (BMATT)\noptimization scheme and demonstrate their effectiveness on a variety of\ngenerative modeling tasks where the objective is to infer the hidden structure\nof a provided dataset. Our results show that in terms of minimizing the\nnegative log-likelihood, the single-layer scheme has model performance\ncomparable to the Born machine scheme, though not better. The tasks include\ndeducing the structure of binary bitwise operations, learning the internal\nstructure of random Bayesian networks given only visible sites, and a\nreal-world example related to hierarchical clustering where a cladogram is\nconstructed from mitochondrial DNA sequences. In doing so, we also show the\nimportance of the choice of network topology and the versatility of a\nleast-mutual information criterion in selecting a candidate structure for a\ntensor tree, as well as discuss aspects of these tensor tree generative models\nincluding their information content and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A structural optimization scheme for a single-layer nonnegative adaptive\ntensor tree (NATT) that models a target probability distribution is proposed as\nan alternative paradigm for generative modeling. The NATT scheme, by\nconstruction, automatically searches for a tree structure that best fits a\ngiven discrete dataset whose features serve as inputs, and has the advantage\nthat it is interpretable as a probabilistic graphical model. We consider the\nNATT scheme and a recently proposed Born machine adaptive tensor tree (BMATT)\noptimization scheme and demonstrate their effectiveness on a variety of\ngenerative modeling tasks where the objective is to infer the hidden structure\nof a provided dataset. Our results show that in terms of minimizing the\nnegative log-likelihood, the single-layer scheme has model performance\ncomparable to the Born machine scheme, though not better. The tasks include\ndeducing the structure of binary bitwise operations, learning the internal\nstructure of random Bayesian networks given only visible sites, and a\nreal-world example related to hierarchical clustering where a cladogram is\nconstructed from mitochondrial DNA sequences. In doing so, we also show the\nimportance of the choice of network topology and the versatility of a\nleast-mutual information criterion in selecting a candidate structure for a\ntensor tree, as well as discuss aspects of these tensor tree generative models\nincluding their information content and interpretability."
                },
                "authors": [
                    {
                        "name": "Katsuya O. Akamatsu"
                    },
                    {
                        "name": "Kenji Harada"
                    },
                    {
                        "name": "Tsuyoshi Okubo"
                    },
                    {
                        "name": "Naoki Kawashima"
                    }
                ],
                "author_detail": {
                    "name": "Naoki Kawashima"
                },
                "author": "Naoki Kawashima",
                "arxiv_comment": "18 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23146v2",
                "updated": "2025-07-01T07:03:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    7,
                    3,
                    17,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-29T08:55:37Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    8,
                    55,
                    37,
                    6,
                    180,
                    0
                ],
                "title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness\n  Beyond Performance Illusions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness\n  Beyond Performance Illusions"
                },
                "summary": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success."
                },
                "authors": [
                    {
                        "name": "Dingzriui Wang"
                    },
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Keyan Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Yang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Deng"
                },
                "author": "Yang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21248v2",
                "updated": "2025-07-01T07:00:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    7,
                    0,
                    59,
                    1,
                    182,
                    0
                ],
                "published": "2025-03-27T08:09:15Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    8,
                    9,
                    15,
                    3,
                    86,
                    0
                ],
                "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition"
                },
                "summary": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention."
                },
                "authors": [
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Zonglin Yang"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Ben Gao"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Shixiang Tang"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18241v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18241v2",
                "updated": "2025-07-01T06:19:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    19,
                    39,
                    1,
                    182,
                    0
                ],
                "published": "2024-12-24T07:51:29Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    51,
                    29,
                    1,
                    359,
                    0
                ],
                "title": "An Automatic Graph Construction Framework based on Large Language Models\n  for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Automatic Graph Construction Framework based on Large Language Models\n  for Recommendation"
                },
                "summary": "Graph neural networks (GNNs) have emerged as state-of-the-art methods to\nlearn from graph-structured data for recommendation. However, most existing\nGNN-based recommendation methods focus on the optimization of model structures\nand learning strategies based on pre-defined graphs, neglecting the importance\nof the graph construction stage. Earlier works for graph construction usually\nrely on speciffic rules or crowdsourcing, which are either too simplistic or\ntoo labor-intensive. Recent works start to utilize large language models (LLMs)\nto automate the graph construction, in view of their abundant open-world\nknowledge and remarkable reasoning capabilities. Nevertheless, they generally\nsuffer from two limitations: (1) invisibility of global view (e.g., overlooking\ncontextual information) and (2) construction inefficiency. To this end, we\nintroduce AutoGraph, an automatic graph construction framework based on LLMs\nfor recommendation. Specifically, we first use LLMs to infer the user\npreference and item knowledge, which is encoded as semantic vectors. Next, we\nemploy vector quantization to extract the latent factors from the semantic\nvectors. The latent factors are then incorporated as extra nodes to link the\nuser/item nodes, resulting in a graph with in-depth global-view semantics. We\nfurther design metapath-based message aggregation to effectively aggregate the\nsemantic and collaborative information. The framework is model-agnostic and\ncompatible with different backbone models. Extensive experiments on three\nreal-world datasets demonstrate the efficacy and efffciency of AutoGraph\ncompared to existing baseline methods. We have deployed AutoGraph in Huawei\nadvertising platform, and gain a 2.69% improvement on RPM and a 7.31%\nimprovement on eCPM in the online A/B test. Currently AutoGraph has been used\nas the main trafffc model, serving hundreds of millions of people.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) have emerged as state-of-the-art methods to\nlearn from graph-structured data for recommendation. However, most existing\nGNN-based recommendation methods focus on the optimization of model structures\nand learning strategies based on pre-defined graphs, neglecting the importance\nof the graph construction stage. Earlier works for graph construction usually\nrely on speciffic rules or crowdsourcing, which are either too simplistic or\ntoo labor-intensive. Recent works start to utilize large language models (LLMs)\nto automate the graph construction, in view of their abundant open-world\nknowledge and remarkable reasoning capabilities. Nevertheless, they generally\nsuffer from two limitations: (1) invisibility of global view (e.g., overlooking\ncontextual information) and (2) construction inefficiency. To this end, we\nintroduce AutoGraph, an automatic graph construction framework based on LLMs\nfor recommendation. Specifically, we first use LLMs to infer the user\npreference and item knowledge, which is encoded as semantic vectors. Next, we\nemploy vector quantization to extract the latent factors from the semantic\nvectors. The latent factors are then incorporated as extra nodes to link the\nuser/item nodes, resulting in a graph with in-depth global-view semantics. We\nfurther design metapath-based message aggregation to effectively aggregate the\nsemantic and collaborative information. The framework is model-agnostic and\ncompatible with different backbone models. Extensive experiments on three\nreal-world datasets demonstrate the efficacy and efffciency of AutoGraph\ncompared to existing baseline methods. We have deployed AutoGraph in Huawei\nadvertising platform, and gain a 2.69% improvement on RPM and a 7.31%\nimprovement on eCPM in the online A/B test. Currently AutoGraph has been used\nas the main trafffc model, serving hundreds of millions of people."
                },
                "authors": [
                    {
                        "name": "Rong Shan"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Chenxu Zhu"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Menghui Zhu"
                    },
                    {
                        "name": "Kangning Zhang"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "Accepted by KDD'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18241v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18241v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24778v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24778v2",
                "updated": "2025-07-01T05:53:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    53,
                    22,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-30T16:41:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    41,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers\n  Accurately Reflect Large Language Models' Uncertainty?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers\n  Accurately Reflect Large Language Models' Uncertainty?"
                },
                "summary": "As large language models (LLMs) are increasingly used in high-stakes domains,\naccurately assessing their confidence is crucial. Humans typically express\nconfidence through epistemic markers (e.g., \"fairly confident\") instead of\nnumerical values. However, it remains unclear whether LLMs consistently use\nthese markers to reflect their intrinsic confidence due to the difficulty of\nquantifying uncertainty associated with various markers. To address this gap,\nwe first define marker confidence as the observed accuracy when a model employs\nan epistemic marker. We evaluate its stability across multiple\nquestion-answering datasets in both in-distribution and out-of-distribution\nsettings for open-source and proprietary LLMs. Our results show that while\nmarkers generalize well within the same distribution, their confidence is\ninconsistent in out-of-distribution scenarios. These findings raise significant\nconcerns about the reliability of epistemic markers for confidence estimation,\nunderscoring the need for improved alignment between marker based confidence\nand actual model uncertainty. Our code is available at\nhttps://github.com/HKUST-KnowComp/MarCon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used in high-stakes domains,\naccurately assessing their confidence is crucial. Humans typically express\nconfidence through epistemic markers (e.g., \"fairly confident\") instead of\nnumerical values. However, it remains unclear whether LLMs consistently use\nthese markers to reflect their intrinsic confidence due to the difficulty of\nquantifying uncertainty associated with various markers. To address this gap,\nwe first define marker confidence as the observed accuracy when a model employs\nan epistemic marker. We evaluate its stability across multiple\nquestion-answering datasets in both in-distribution and out-of-distribution\nsettings for open-source and proprietary LLMs. Our results show that while\nmarkers generalize well within the same distribution, their confidence is\ninconsistent in out-of-distribution scenarios. These findings raise significant\nconcerns about the reliability of epistemic markers for confidence estimation,\nunderscoring the need for improved alignment between marker based confidence\nand actual model uncertainty. Our code is available at\nhttps://github.com/HKUST-KnowComp/MarCon."
                },
                "authors": [
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "ACL2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24778v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24778v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17765v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17765v2",
                "updated": "2025-07-01T05:47:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    47,
                    5,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-21T17:18:35Z",
                "published_parsed": [
                    2025,
                    6,
                    21,
                    17,
                    18,
                    35,
                    5,
                    172,
                    0
                ],
                "title": "CARTS: Collaborative Agents for Recommendation Textual Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARTS: Collaborative Agents for Recommendation Textual Summarization"
                },
                "summary": "Current recommendation systems often require some form of textual data\nsummarization, such as generating concise and coherent titles for product\ncarousels or other grouped item displays. While large language models have\nshown promise in NLP domains for textual summarization, these approaches do not\ndirectly apply to recommendation systems, where explanations must be highly\nrelevant to the core features of item sets, adhere to strict word limit\nconstraints. In this paper, we propose CARTS (Collaborative Agents for\nRecommendation Textual Summarization), a multi-agent LLM framework designed for\nstructured summarization in recommendation systems. CARTS decomposes the task\ninto three stages-Generation Augmented Generation (GAG), refinement circle, and\narbitration, where successive agent roles are responsible for extracting\nsalient item features, iteratively refining candidate titles based on relevance\nand length feedback, and selecting the final title through a collaborative\narbitration process. Experiments on large-scale e-commerce data and live A/B\ntesting show that CARTS significantly outperforms single-pass and\nchain-of-thought LLM baselines, delivering higher title relevance and improved\nuser engagement metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current recommendation systems often require some form of textual data\nsummarization, such as generating concise and coherent titles for product\ncarousels or other grouped item displays. While large language models have\nshown promise in NLP domains for textual summarization, these approaches do not\ndirectly apply to recommendation systems, where explanations must be highly\nrelevant to the core features of item sets, adhere to strict word limit\nconstraints. In this paper, we propose CARTS (Collaborative Agents for\nRecommendation Textual Summarization), a multi-agent LLM framework designed for\nstructured summarization in recommendation systems. CARTS decomposes the task\ninto three stages-Generation Augmented Generation (GAG), refinement circle, and\narbitration, where successive agent roles are responsible for extracting\nsalient item features, iteratively refining candidate titles based on relevance\nand length feedback, and selecting the final title through a collaborative\narbitration process. Experiments on large-scale e-commerce data and live A/B\ntesting show that CARTS significantly outperforms single-pass and\nchain-of-thought LLM baselines, delivering higher title relevance and improved\nuser engagement metrics."
                },
                "authors": [
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Kehui Yao"
                    },
                    {
                        "name": "Reza Yousefi Maragheh"
                    },
                    {
                        "name": "Kai Zhao"
                    },
                    {
                        "name": "Jianpeng Xu"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Evren Korpeoglu"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17765v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12036v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12036v3",
                "updated": "2025-07-01T05:46:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    46,
                    31,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-23T00:01:52Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    0,
                    1,
                    52,
                    4,
                    143,
                    0
                ],
                "title": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models"
                },
                "summary": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work uses reinforcement learning (RL) to fine-tune text-to-image\ndiffusion models, improving text-image alignment and sample quality. However,\nexisting approaches introduce unnecessary complexity: they cache the full\nsampling trajectory, depend on differentiable reward models or large preference\ndatasets, or require specialized guidance techniques. Motivated by the \"golden\nnoise\" hypothesis -- that certain initial noise samples can consistently yield\nsuperior alignment -- we introduce Noise PPO, a minimalist RL algorithm that\nleaves the pre-trained diffusion model entirely frozen and learns a\nprompt-conditioned initial noise generator. Our approach requires no trajectory\nstorage, reward backpropagation, or complex guidance tricks. Extensive\nexperiments show that optimizing the initial noise distribution consistently\nimproves alignment and sample quality over the original model, with the most\nsignificant gains at low inference steps. As the number of inference steps\nincreases, the benefit of noise optimization diminishes but remains present.\nThese findings clarify the scope and limitations of the golden noise hypothesis\nand reinforce the practical value of minimalist RL fine-tuning for diffusion\nmodels."
                },
                "authors": [
                    {
                        "name": "Yanting Miao"
                    },
                    {
                        "name": "William Loh"
                    },
                    {
                        "name": "Pacal Poupart"
                    },
                    {
                        "name": "Suraj Kothawade"
                    }
                ],
                "author_detail": {
                    "name": "Suraj Kothawade"
                },
                "author": "Suraj Kothawade",
                "arxiv_comment": "17 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12036v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12036v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.16579v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.16579v2",
                "updated": "2025-07-01T05:44:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    44,
                    57,
                    1,
                    182,
                    0
                ],
                "published": "2023-07-31T11:29:50Z",
                "published_parsed": [
                    2023,
                    7,
                    31,
                    11,
                    29,
                    50,
                    0,
                    212,
                    0
                ],
                "title": "Contrastive Conditional Latent Diffusion for Audio-visual Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Conditional Latent Diffusion for Audio-visual Segmentation"
                },
                "summary": "We propose a contrastive conditional latent diffusion model for audio-visual\nsegmentation (AVS) to thoroughly investigate the impact of audio, where the\ncorrelation between audio and the final segmentation map is modeled to\nguarantee the strong correlation between them. To achieve semantic-correlated\nrepresentation learning, our framework incorporates a latent diffusion model.\nThe diffusion model learns the conditional generation process of the\nground-truth segmentation map, resulting in ground-truth aware inference during\nthe denoising process at the test stage. As our model is conditional, it is\nvital to ensure that the conditional variable contributes to the model output.\nWe thus extensively model the contribution of the audio signal by minimizing\nthe density ratio between the conditional probability of the multimodal data,\ne.g. conditioned on the audio-visual data, and that of the unimodal data, e.g.\nconditioned on the audio data only. In this way, our latent diffusion model via\ndensity ratio optimization explicitly maximizes the contribution of audio for\nAVS, which can then be achieved with contrastive learning as a constraint,\nwhere the diffusion part serves as the main objective to achieve maximum\nlikelihood estimation, and the density ratio optimization part imposes the\nconstraint. By adopting this latent diffusion model via contrastive learning,\nwe effectively enhance the contribution of audio for AVS. The effectiveness of\nour solution is validated through experimental results on the benchmark\ndataset. Code and results are online via our project page:\nhttps://github.com/OpenNLPLab/DiffusionAVS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a contrastive conditional latent diffusion model for audio-visual\nsegmentation (AVS) to thoroughly investigate the impact of audio, where the\ncorrelation between audio and the final segmentation map is modeled to\nguarantee the strong correlation between them. To achieve semantic-correlated\nrepresentation learning, our framework incorporates a latent diffusion model.\nThe diffusion model learns the conditional generation process of the\nground-truth segmentation map, resulting in ground-truth aware inference during\nthe denoising process at the test stage. As our model is conditional, it is\nvital to ensure that the conditional variable contributes to the model output.\nWe thus extensively model the contribution of the audio signal by minimizing\nthe density ratio between the conditional probability of the multimodal data,\ne.g. conditioned on the audio-visual data, and that of the unimodal data, e.g.\nconditioned on the audio data only. In this way, our latent diffusion model via\ndensity ratio optimization explicitly maximizes the contribution of audio for\nAVS, which can then be achieved with contrastive learning as a constraint,\nwhere the diffusion part serves as the main objective to achieve maximum\nlikelihood estimation, and the density ratio optimization part imposes the\nconstraint. By adopting this latent diffusion model via contrastive learning,\nwe effectively enhance the contribution of audio for AVS. The effectiveness of\nour solution is validated through experimental results on the benchmark\ndataset. Code and results are online via our project page:\nhttps://github.com/OpenNLPLab/DiffusionAVS."
                },
                "authors": [
                    {
                        "name": "Yuxin Mao"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Mochu Xiang"
                    },
                    {
                        "name": "Yunqiu Lv"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Yiran Zhong"
                    },
                    {
                        "name": "Yuchao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Yuchao Dai"
                },
                "author": "Yuchao Dai",
                "arxiv_doi": "10.1109/TIP.2025.3580269",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TIP.2025.3580269",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.16579v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.16579v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Image Processing 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07416v2",
                "updated": "2025-07-01T05:13:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    13,
                    39,
                    1,
                    182,
                    0
                ],
                "published": "2025-04-10T03:14:17Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    3,
                    14,
                    17,
                    3,
                    100,
                    0
                ],
                "title": "RadZero: Similarity-Based Cross-Attention for Explainable\n  Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RadZero: Similarity-Based Cross-Attention for Explainable\n  Vision-Language Alignment in Radiology with Zero-Shot Multi-Task Capability"
                },
                "summary": "Recent advancements in multi-modal models have significantly improved\nvision-language (VL) alignment in radiology. However, existing approaches\nstruggle to effectively utilize complex radiology reports for learning and\noffer limited interpretability through attention probability visualizations. To\naddress these challenges, we introduce RadZero, a novel framework for VL\nalignment in radiology with zero-shot multi-task capability. A key component of\nour approach is VL-CABS (Vision-Language Cross-Attention Based on Similarity),\nwhich aligns text embeddings with local image features for interpretable,\nfine-grained VL reasoning. RadZero leverages large language models to extract\nconcise semantic sentences from radiology reports and employs multi-positive\ncontrastive training to effectively capture relationships between images and\nmultiple relevant textual descriptions. It uses a pre-trained vision encoder\nwith additional trainable Transformer layers, allowing efficient\nhigh-resolution image processing. By computing similarity between text\nembeddings and local image patch features, VL-CABS enables zero-shot inference\nwith similarity probability for classification, and pixel-level VL similarity\nmaps for grounding and segmentation. Experimental results on public chest\nradiograph benchmarks show that RadZero outperforms state-of-the-art methods in\nzero-shot classification, grounding, and segmentation. Furthermore, VL\nsimilarity map analysis highlights the potential of VL-CABS for improving\nexplainability in VL alignment. Additionally, qualitative evaluation\ndemonstrates RadZero's capability for open-vocabulary semantic segmentation,\nfurther validating its effectiveness in medical imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in multi-modal models have significantly improved\nvision-language (VL) alignment in radiology. However, existing approaches\nstruggle to effectively utilize complex radiology reports for learning and\noffer limited interpretability through attention probability visualizations. To\naddress these challenges, we introduce RadZero, a novel framework for VL\nalignment in radiology with zero-shot multi-task capability. A key component of\nour approach is VL-CABS (Vision-Language Cross-Attention Based on Similarity),\nwhich aligns text embeddings with local image features for interpretable,\nfine-grained VL reasoning. RadZero leverages large language models to extract\nconcise semantic sentences from radiology reports and employs multi-positive\ncontrastive training to effectively capture relationships between images and\nmultiple relevant textual descriptions. It uses a pre-trained vision encoder\nwith additional trainable Transformer layers, allowing efficient\nhigh-resolution image processing. By computing similarity between text\nembeddings and local image patch features, VL-CABS enables zero-shot inference\nwith similarity probability for classification, and pixel-level VL similarity\nmaps for grounding and segmentation. Experimental results on public chest\nradiograph benchmarks show that RadZero outperforms state-of-the-art methods in\nzero-shot classification, grounding, and segmentation. Furthermore, VL\nsimilarity map analysis highlights the potential of VL-CABS for improving\nexplainability in VL alignment. Additionally, qualitative evaluation\ndemonstrates RadZero's capability for open-vocabulary semantic segmentation,\nfurther validating its effectiveness in medical imaging."
                },
                "authors": [
                    {
                        "name": "Jonggwon Park"
                    },
                    {
                        "name": "Soobum Kim"
                    },
                    {
                        "name": "Byungmu Yoon"
                    },
                    {
                        "name": "Kyoyun Choi"
                    }
                ],
                "author_detail": {
                    "name": "Kyoyun Choi"
                },
                "author": "Kyoyun Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24124v2",
                "updated": "2025-07-01T03:40:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    3,
                    40,
                    22,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T17:59:14Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    59,
                    14,
                    0,
                    181,
                    0
                ],
                "title": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual\n  and Textual Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual\n  and Textual Perspectives"
                },
                "summary": "Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP."
                },
                "authors": [
                    {
                        "name": "Sixun Dong"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Teresa Wu"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu",
                "arxiv_comment": "Code: https://github.com/Ironieser/TimesCLIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18232v2",
                "updated": "2025-07-01T03:31:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    3,
                    31,
                    12,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-23T12:40:59Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    40,
                    59,
                    4,
                    143,
                    0
                ],
                "title": "Two-Stage Regularization-Based Structured Pruning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Stage Regularization-Based Structured Pruning for LLMs"
                },
                "summary": "The deployment of large language models (LLMs) is largely hindered by their\nlarge number of parameters. Structural pruning has emerged as a promising\nsolution. Prior structured pruning methods directly remove unimportant\nparameters based on certain metrics, which often causes knowledge loss and\nnecessitates extensive retraining. To overcome this, we introduce a novel\npruning method TRSP: Two-Stage Regularization-Based Structured Pruning for\nLLMs. Specifically, we multiply the output of each transformer layer by an\ninitial learnable weight and iteratively learn these weights by adding their\n$\\ell_1$-norm as a regularization term to the loss function, serving as the\nfirst-stage regularization. Subsequently, we apply additional regularization to\nthe difference between the output and input of layers with smaller weights,\nencouraging the shift of knowledge to the preserved layers. This serves as the\nsecond-stage regularization. TRSP retains more knowledge and better preserves\nmodel performance than direct parameter elimination. Through extensive\nexperimentation we show that TRSP outperforms strong layer-wise structured\npruning methods without requiring retraining. As a layer-wise pruning method,\nit delivers notable end-to-end acceleration, making it a promising solution for\nefficient LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is largely hindered by their\nlarge number of parameters. Structural pruning has emerged as a promising\nsolution. Prior structured pruning methods directly remove unimportant\nparameters based on certain metrics, which often causes knowledge loss and\nnecessitates extensive retraining. To overcome this, we introduce a novel\npruning method TRSP: Two-Stage Regularization-Based Structured Pruning for\nLLMs. Specifically, we multiply the output of each transformer layer by an\ninitial learnable weight and iteratively learn these weights by adding their\n$\\ell_1$-norm as a regularization term to the loss function, serving as the\nfirst-stage regularization. Subsequently, we apply additional regularization to\nthe difference between the output and input of layers with smaller weights,\nencouraging the shift of knowledge to the preserved layers. This serves as the\nsecond-stage regularization. TRSP retains more knowledge and better preserves\nmodel performance than direct parameter elimination. Through extensive\nexperimentation we show that TRSP outperforms strong layer-wise structured\npruning methods without requiring retraining. As a layer-wise pruning method,\nit delivers notable end-to-end acceleration, making it a promising solution for\nefficient LLM deployment."
                },
                "authors": [
                    {
                        "name": "Mingkuan Feng"
                    },
                    {
                        "name": "Jinyang Wu"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Ruihan Jin"
                    },
                    {
                        "name": "Feihu Che"
                    },
                    {
                        "name": "Pengpeng Shao"
                    },
                    {
                        "name": "Zhengqi Wen"
                    },
                    {
                        "name": "Jianhua Tao"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Tao"
                },
                "author": "Jianhua Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23940v2",
                "updated": "2025-07-01T03:26:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    3,
                    26,
                    52,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T15:07:41Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    7,
                    41,
                    0,
                    181,
                    0
                ],
                "title": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy\n  for MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy\n  for MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved success across various\ndomains. However, their applicability tends to degrade when confronted with\ndifferent types of data inputs, especially for MLLMs that have been fine-tuned\nfor specific tasks. Despite its importance, the study of knowledge sharing\namong domain-specific MLLMs--such as those trained for mathematics or\ncode--remains largely underexplored. To address the fragmentation of knowledge\nacross domain-specialized MLLMs, we propose a unified parameter integration\nframework that enables modular composition of expert capabilities. Our method\nis grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,\nwhich leverages both local functional attribution and global\ninformation-theoretic signals to guide selective parameter fusion. By extending\nthis mechanism to the low-rank adaptation layer granularity, we ensure\nefficient integration with minimal inference overhead. Furthermore, we\nintroduce a domain compatibility scoring mechanism that quantifies inter-expert\nalignment at the activation level and correlates with downstream task utility.\nThis principled fusion protocol allows the final model to synergize\nheterogeneous expertise while preserving structural modularity. Extensive\nevaluations across diverse multimodal benchmarks validate the effectiveness of\nour framework, offering a scalable path toward compositional, domain-adaptive\nMLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved success across various\ndomains. However, their applicability tends to degrade when confronted with\ndifferent types of data inputs, especially for MLLMs that have been fine-tuned\nfor specific tasks. Despite its importance, the study of knowledge sharing\namong domain-specific MLLMs--such as those trained for mathematics or\ncode--remains largely underexplored. To address the fragmentation of knowledge\nacross domain-specialized MLLMs, we propose a unified parameter integration\nframework that enables modular composition of expert capabilities. Our method\nis grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,\nwhich leverages both local functional attribution and global\ninformation-theoretic signals to guide selective parameter fusion. By extending\nthis mechanism to the low-rank adaptation layer granularity, we ensure\nefficient integration with minimal inference overhead. Furthermore, we\nintroduce a domain compatibility scoring mechanism that quantifies inter-expert\nalignment at the activation level and correlates with downstream task utility.\nThis principled fusion protocol allows the final model to synergize\nheterogeneous expertise while preserving structural modularity. Extensive\nevaluations across diverse multimodal benchmarks validate the effectiveness of\nour framework, offering a scalable path toward compositional, domain-adaptive\nMLLMs."
                },
                "authors": [
                    {
                        "name": "Yang Dai"
                    },
                    {
                        "name": "Jianxiang An"
                    },
                    {
                        "name": "Tianwei Lin"
                    },
                    {
                        "name": "Hongyang He"
                    },
                    {
                        "name": "Hongzhe Huang"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Zheqi Lv"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22523v2",
                "updated": "2025-07-01T03:17:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    3,
                    17,
                    10,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-26T23:11:49Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    23,
                    11,
                    49,
                    3,
                    177,
                    0
                ],
                "title": "Red Teaming for Generative AI, Report on a Copyright-Focused Exercise\n  Completed in an Academic Medical Center",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red Teaming for Generative AI, Report on a Copyright-Focused Exercise\n  Completed in an Academic Medical Center"
                },
                "summary": "Background: Generative artificial intelligence (AI) deployment in healthcare\nsettings raises copyright compliance concerns. Dana-Farber Cancer Institute\nimplemented GPT4DFCI, an internal generative AI tool utilizing OpenAI models,\nthat is approved for enterprise use in research and operations. Given (i) the\nexceptionally broad adoption of the tool in our organization, (ii) our research\nmission, and (iii) the shared responsibility model required by Microsoft OpenAI\nproducts, we deemed rigorous copyright compliance testing necessary.\n  Case Description: We conducted a structured red teaming exercise in Nov.\n2024, with 42 participants from academic, industry, and government\ninstitutions. Four teams attempted to extract copyrighted content from GPT4DFCI\nacross four domains: literary works, news articles, scientific publications,\nand access-restricted clinical notes. Teams successfully extracted verbatim\nbook dedications and near-exact passages through indirect prompting strategies.\nNews article extraction failed despite jailbreak attempts. Scientific article\nreproduction yielded only high-level summaries. Clinical note testing revealed\nappropriate privacy safeguards with data reformatting rather than reproduction.\n  Discussion: The successful extraction of literary content indicates potential\ncopyright material presence in training data, necessitating enhanced\ninference-time filtering. Differential success rates across content types\nsuggest varying protective mechanisms. The event led to implementation of a\ncopyright-specific meta-prompt in GPT4DFCI; this mitigation is in production\nsince Jan. 2025.\n  Conclusion: Systematic red teaming revealed specific vulnerabilities in\ngenerative AI copyright compliance, leading to concrete mitigation strategies.\nAcademic medical institutions deploying generative AI must implement continuous\ntesting protocols to ensure legal and ethical compliance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Generative artificial intelligence (AI) deployment in healthcare\nsettings raises copyright compliance concerns. Dana-Farber Cancer Institute\nimplemented GPT4DFCI, an internal generative AI tool utilizing OpenAI models,\nthat is approved for enterprise use in research and operations. Given (i) the\nexceptionally broad adoption of the tool in our organization, (ii) our research\nmission, and (iii) the shared responsibility model required by Microsoft OpenAI\nproducts, we deemed rigorous copyright compliance testing necessary.\n  Case Description: We conducted a structured red teaming exercise in Nov.\n2024, with 42 participants from academic, industry, and government\ninstitutions. Four teams attempted to extract copyrighted content from GPT4DFCI\nacross four domains: literary works, news articles, scientific publications,\nand access-restricted clinical notes. Teams successfully extracted verbatim\nbook dedications and near-exact passages through indirect prompting strategies.\nNews article extraction failed despite jailbreak attempts. Scientific article\nreproduction yielded only high-level summaries. Clinical note testing revealed\nappropriate privacy safeguards with data reformatting rather than reproduction.\n  Discussion: The successful extraction of literary content indicates potential\ncopyright material presence in training data, necessitating enhanced\ninference-time filtering. Differential success rates across content types\nsuggest varying protective mechanisms. The event led to implementation of a\ncopyright-specific meta-prompt in GPT4DFCI; this mitigation is in production\nsince Jan. 2025.\n  Conclusion: Systematic red teaming revealed specific vulnerabilities in\ngenerative AI copyright compliance, leading to concrete mitigation strategies.\nAcademic medical institutions deploying generative AI must implement continuous\ntesting protocols to ensure legal and ethical compliance."
                },
                "authors": [
                    {
                        "name": "James Wen"
                    },
                    {
                        "name": "Sahil Nalawade"
                    },
                    {
                        "name": "Zhiwei Liang"
                    },
                    {
                        "name": "Catherine Bielick"
                    },
                    {
                        "name": "Marisa Ferrara Boston"
                    },
                    {
                        "name": "Alexander Chowdhury"
                    },
                    {
                        "name": "Adele Collin"
                    },
                    {
                        "name": "Luigi De Angelis"
                    },
                    {
                        "name": "Jacob Ellen"
                    },
                    {
                        "name": "Heather Frase"
                    },
                    {
                        "name": "Rodrigo R. Gameiro"
                    },
                    {
                        "name": "Juan Manuel Gutierrez"
                    },
                    {
                        "name": "Pooja Kadam"
                    },
                    {
                        "name": "Murat Keceli"
                    },
                    {
                        "name": "Srikanth Krishnamurthy"
                    },
                    {
                        "name": "Anne Kwok"
                    },
                    {
                        "name": "Yanan Lance Lu"
                    },
                    {
                        "name": "Heather Mattie"
                    },
                    {
                        "name": "Liam G. McCoy"
                    },
                    {
                        "name": "Katherine Miller"
                    },
                    {
                        "name": "Allison C. Morgan"
                    },
                    {
                        "name": "Marlene Louisa Moerig"
                    },
                    {
                        "name": "Trang Nguyen"
                    },
                    {
                        "name": "Alexander Owen-Post"
                    },
                    {
                        "name": "Alex D. Ruiz"
                    },
                    {
                        "name": "Sreekar Reddy Puchala"
                    },
                    {
                        "name": "Soujanya Samineni"
                    },
                    {
                        "name": "Takeshi Tohyama"
                    },
                    {
                        "name": "Varun Ullanat"
                    },
                    {
                        "name": "Carmine Valenza"
                    },
                    {
                        "name": "Camilo Velez"
                    },
                    {
                        "name": "Pengcheng Wang"
                    },
                    {
                        "name": "Anna Wuest"
                    },
                    {
                        "name": "Yuxiang Zhou"
                    },
                    {
                        "name": "Yingde Zhu"
                    },
                    {
                        "name": "Jason M. Johnson"
                    },
                    {
                        "name": "Naomi Lenane"
                    },
                    {
                        "name": "Jennifer Willcox"
                    },
                    {
                        "name": "Francis J. Vitiello"
                    },
                    {
                        "name": "Leo Anthony G. Celi"
                    },
                    {
                        "name": "Renato Umeton"
                    }
                ],
                "author_detail": {
                    "name": "Renato Umeton"
                },
                "author": "Renato Umeton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04403v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04403v2",
                "updated": "2025-07-01T03:06:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    3,
                    6,
                    14,
                    1,
                    182,
                    0
                ],
                "published": "2024-11-07T03:46:43Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    3,
                    46,
                    43,
                    3,
                    312,
                    0
                ],
                "title": "Towards Competitive Search Relevance For Inference-Free Learned Sparse\n  Retrievers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Competitive Search Relevance For Inference-Free Learned Sparse\n  Retrievers"
                },
                "summary": "Learned sparse retrieval, which can efficiently perform retrieval through\nmature inverted-index engines, has garnered growing attention in recent years.\nParticularly, the inference-free sparse retrievers are attractive as they\neliminate online model inference in the retrieval phase thereby avoids huge\ncomputational cost, offering reasonable throughput and latency. However, even\nthe state-of-the-art (SOTA) inference-free sparse models lag far behind in\nterms of search relevance when compared to both sparse and dense siamese\nmodels. Towards competitive search relevance for inference-free sparse\nretrievers, we argue that they deserve dedicated training methods other than\nusing same ones with siamese encoders. In this paper, we propose two different\napproaches for performance improvement. First, we propose an IDF-aware penalty\nfor the matching function that suppresses the contribution of low-IDF tokens\nand increases the model's focus on informative terms. Moreover, we propose a\nheterogeneous ensemble knowledge distillation framework that combines siamese\ndense and sparse retrievers to generate supervisory signals during the\npre-training phase. The ensemble framework of dense and sparse retriever\ncapitalizes on their strengths respectively, providing a strong upper bound for\nknowledge distillation. To concur the diverse feedback from heterogeneous\nsupervisors, we normalize and then aggregate the outputs of the teacher models\nto eliminate score scale differences. On the BEIR benchmark, our model\noutperforms existing SOTA inference-free sparse model by \\textbf{3.3 NDCG@10\nscore}. It exhibits search relevance comparable to siamese sparse retrievers\nand client-side latency only \\textbf{1.1x that of BM25}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned sparse retrieval, which can efficiently perform retrieval through\nmature inverted-index engines, has garnered growing attention in recent years.\nParticularly, the inference-free sparse retrievers are attractive as they\neliminate online model inference in the retrieval phase thereby avoids huge\ncomputational cost, offering reasonable throughput and latency. However, even\nthe state-of-the-art (SOTA) inference-free sparse models lag far behind in\nterms of search relevance when compared to both sparse and dense siamese\nmodels. Towards competitive search relevance for inference-free sparse\nretrievers, we argue that they deserve dedicated training methods other than\nusing same ones with siamese encoders. In this paper, we propose two different\napproaches for performance improvement. First, we propose an IDF-aware penalty\nfor the matching function that suppresses the contribution of low-IDF tokens\nand increases the model's focus on informative terms. Moreover, we propose a\nheterogeneous ensemble knowledge distillation framework that combines siamese\ndense and sparse retrievers to generate supervisory signals during the\npre-training phase. The ensemble framework of dense and sparse retriever\ncapitalizes on their strengths respectively, providing a strong upper bound for\nknowledge distillation. To concur the diverse feedback from heterogeneous\nsupervisors, we normalize and then aggregate the outputs of the teacher models\nto eliminate score scale differences. On the BEIR benchmark, our model\noutperforms existing SOTA inference-free sparse model by \\textbf{3.3 NDCG@10\nscore}. It exhibits search relevance comparable to siamese sparse retrievers\nand client-side latency only \\textbf{1.1x that of BM25}."
                },
                "authors": [
                    {
                        "name": "Zhichao Geng"
                    },
                    {
                        "name": "Yiwen Wang"
                    },
                    {
                        "name": "Dongyu Ru"
                    },
                    {
                        "name": "Yang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Yang"
                },
                "author": "Yang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04403v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04403v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01144v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01144v4",
                "updated": "2025-07-01T03:02:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    3,
                    2,
                    26,
                    1,
                    182,
                    0
                ],
                "published": "2025-01-02T08:57:00Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    57,
                    0,
                    3,
                    2,
                    0
                ],
                "title": "BlockDialect: Block-wise Fine-grained Mixed Format Quantization for\n  Energy-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDialect: Block-wise Fine-grained Mixed Format Quantization for\n  Energy-Efficient LLM Inference"
                },
                "summary": "The rapidly increasing size of large language models (LLMs) presents\nsignificant challenges in memory usage and computational costs. Quantizing both\nweights and activations can address these issues, with hardware-supported\nfine-grained scaling emerging as a promising solution to mitigate outliers.\nHowever, existing methods struggle to capture nuanced block data distributions.\nWe propose BlockDialect, a block-wise fine-grained mixed format technique that\nassigns a per-block optimal number format from a formatbook for better data\nrepresentation. Additionally, we introduce DialectFP4, a formatbook of FP4\nvariants (akin to dialects) that adapt to diverse data distributions. To\nleverage this efficiently, we propose a two-stage approach for online\nDialectFP4 activation quantization. Importantly, DialectFP4 ensures energy\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy\ngain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit\nusage per data, while being only 5.45% (2.69%) below full precision even when\nquantizing full-path matrix multiplication. Focusing on how to represent over\nhow to scale, our work presents a promising path for energy-efficient LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapidly increasing size of large language models (LLMs) presents\nsignificant challenges in memory usage and computational costs. Quantizing both\nweights and activations can address these issues, with hardware-supported\nfine-grained scaling emerging as a promising solution to mitigate outliers.\nHowever, existing methods struggle to capture nuanced block data distributions.\nWe propose BlockDialect, a block-wise fine-grained mixed format technique that\nassigns a per-block optimal number format from a formatbook for better data\nrepresentation. Additionally, we introduce DialectFP4, a formatbook of FP4\nvariants (akin to dialects) that adapt to diverse data distributions. To\nleverage this efficiently, we propose a two-stage approach for online\nDialectFP4 activation quantization. Importantly, DialectFP4 ensures energy\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy\ngain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit\nusage per data, while being only 5.45% (2.69%) below full precision even when\nquantizing full-path matrix multiplication. Focusing on how to represent over\nhow to scale, our work presents a promising path for energy-efficient LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Wonsuk Jang"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01144v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01144v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10774v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10774v4",
                "updated": "2025-07-01T02:38:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    2,
                    38,
                    26,
                    1,
                    182,
                    0
                ],
                "published": "2024-08-20T12:13:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    13,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Flexora: Flexible Low Rank Adaptation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexora: Flexible Low Rank Adaptation for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora."
                },
                "authors": [
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Ying Tiffany He"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "arxiv_comment": "40 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10774v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10774v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04065v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04065v2",
                "updated": "2025-07-01T02:37:16Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    2,
                    37,
                    16,
                    1,
                    182,
                    0
                ],
                "published": "2025-04-05T05:42:12Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    5,
                    42,
                    12,
                    5,
                    95,
                    0
                ],
                "title": "Enabling Collaborative Parametric Knowledge Calibration for\n  Retrieval-Augmented Vision Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Collaborative Parametric Knowledge Calibration for\n  Retrieval-Augmented Vision Question Answering"
                },
                "summary": "Knowledge-based Vision Question Answering (KB-VQA) systems address complex\nvisual-grounded questions with knowledge retrieved from external knowledge\nbases. The tasks of knowledge retrieval and answer generation tasks both\nnecessitate precise multimodal understanding of question context and external\nknowledge. However, existing methods treat these two stages as separate modules\nwith limited interaction during training, which hinders bi-directional\nparametric knowledge sharing, ultimately leading to suboptimal performance. To\nfully exploit the cross-task synergy in KB-VQA, we propose a unified\nretrieval-augmented VQA framework with collaborative parametric knowledge\ncalibration. The proposed framework can effectively adapt general multimodal\npre-trained models for fine-grained, knowledge-intensive tasks while enabling\nthe retriever and generator to collaboratively enhance and share their\nparametric knowledge during both training and inference. To enhance\nfine-grained understanding of questions and external documents, we also\nintegrate late interaction mechanism into the proposed training framework.\nAdditionally, we introduce a reflective-answering mechanism that allows the\nmodel to explicitly evaluate and refine its knowledge boundary. Our approach\nachieves competitive performance against state-of-the-art models, delivering a\nsignificant 4.7\\% improvement in answering accuracy, and brings an average\n7.5\\% boost in base MLLMs' VQA performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-based Vision Question Answering (KB-VQA) systems address complex\nvisual-grounded questions with knowledge retrieved from external knowledge\nbases. The tasks of knowledge retrieval and answer generation tasks both\nnecessitate precise multimodal understanding of question context and external\nknowledge. However, existing methods treat these two stages as separate modules\nwith limited interaction during training, which hinders bi-directional\nparametric knowledge sharing, ultimately leading to suboptimal performance. To\nfully exploit the cross-task synergy in KB-VQA, we propose a unified\nretrieval-augmented VQA framework with collaborative parametric knowledge\ncalibration. The proposed framework can effectively adapt general multimodal\npre-trained models for fine-grained, knowledge-intensive tasks while enabling\nthe retriever and generator to collaboratively enhance and share their\nparametric knowledge during both training and inference. To enhance\nfine-grained understanding of questions and external documents, we also\nintegrate late interaction mechanism into the proposed training framework.\nAdditionally, we introduce a reflective-answering mechanism that allows the\nmodel to explicitly evaluate and refine its knowledge boundary. Our approach\nachieves competitive performance against state-of-the-art models, delivering a\nsignificant 4.7\\% improvement in answering accuracy, and brings an average\n7.5\\% boost in base MLLMs' VQA performance."
                },
                "authors": [
                    {
                        "name": "Jiaqi Deng"
                    },
                    {
                        "name": "Kaize Shi"
                    },
                    {
                        "name": "Zonghan Wu"
                    },
                    {
                        "name": "Huan Huo"
                    },
                    {
                        "name": "Dingxian Wang"
                    },
                    {
                        "name": "Guandong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Xu"
                },
                "author": "Guandong Xu",
                "arxiv_comment": "10 pages, 5 figures, Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04065v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04065v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22704v2",
                "updated": "2025-07-01T02:35:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    2,
                    35,
                    48,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-28T01:10:24Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    1,
                    10,
                    24,
                    5,
                    179,
                    0
                ],
                "title": "Beyond Code: The Multidimensional Impacts of Large Language Models in\n  Software Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Code: The Multidimensional Impacts of Large Language Models in\n  Software Development"
                },
                "summary": "Large language models (LLMs) are poised to significantly impact software\ndevelopment, especially in the Open-Source Software (OSS) sector. To understand\nthis impact, we first outline the mechanisms through which LLMs may influence\nOSS through code development, collaborative knowledge transfer, and skill\ndevelopment. We then empirically examine how LLMs affect OSS developers' work\nin these three key areas. Leveraging a natural experiment from a temporary\nChatGPT ban in Italy, we employ a Difference-in-Differences framework with\ntwo-way fixed effects to analyze data from all OSS developers on GitHub in\nthree similar countries, Italy, France, and Portugal, totaling 88,022 users. We\nfind that access to ChatGPT increases developer productivity by 6.4%, knowledge\nsharing by 9.6%, and skill acquisition by 8.4%. These benefits vary\nsignificantly by user experience level: novice developers primarily experience\nproductivity gains, whereas more experienced developers benefit more from\nimproved knowledge sharing and accelerated skill acquisition. In addition, we\nfind that LLM-assisted learning is highly context-dependent, with the greatest\nbenefits observed in technically complex, fragmented, or rapidly evolving\ncontexts. We show that the productivity effects of LLMs extend beyond direct\ncode generation to include enhanced collaborative learning and knowledge\nexchange among developers, dynamics that are essential for gaining a holistic\nunderstanding of LLMs' impact in OSS. Our findings offer critical managerial\nimplications: strategically deploying LLMs can accelerate novice developers'\nonboarding and productivity, empower intermediate developers to foster\nknowledge sharing and collaboration, and support rapid skill acquisition,\ntogether enhancing long-term organizational productivity and agility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are poised to significantly impact software\ndevelopment, especially in the Open-Source Software (OSS) sector. To understand\nthis impact, we first outline the mechanisms through which LLMs may influence\nOSS through code development, collaborative knowledge transfer, and skill\ndevelopment. We then empirically examine how LLMs affect OSS developers' work\nin these three key areas. Leveraging a natural experiment from a temporary\nChatGPT ban in Italy, we employ a Difference-in-Differences framework with\ntwo-way fixed effects to analyze data from all OSS developers on GitHub in\nthree similar countries, Italy, France, and Portugal, totaling 88,022 users. We\nfind that access to ChatGPT increases developer productivity by 6.4%, knowledge\nsharing by 9.6%, and skill acquisition by 8.4%. These benefits vary\nsignificantly by user experience level: novice developers primarily experience\nproductivity gains, whereas more experienced developers benefit more from\nimproved knowledge sharing and accelerated skill acquisition. In addition, we\nfind that LLM-assisted learning is highly context-dependent, with the greatest\nbenefits observed in technically complex, fragmented, or rapidly evolving\ncontexts. We show that the productivity effects of LLMs extend beyond direct\ncode generation to include enhanced collaborative learning and knowledge\nexchange among developers, dynamics that are essential for gaining a holistic\nunderstanding of LLMs' impact in OSS. Our findings offer critical managerial\nimplications: strategically deploying LLMs can accelerate novice developers'\nonboarding and productivity, empower intermediate developers to foster\nknowledge sharing and collaboration, and support rapid skill acquisition,\ntogether enhancing long-term organizational productivity and agility."
                },
                "authors": [
                    {
                        "name": "Sardar Bonabi"
                    },
                    {
                        "name": "Sarah Bana"
                    },
                    {
                        "name": "Vijay Gurbaxani"
                    },
                    {
                        "name": "Tingting Nian"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Nian"
                },
                "author": "Tingting Nian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24119v2",
                "updated": "2025-07-01T02:29:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    2,
                    29,
                    52,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T17:58:13Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    58,
                    13,
                    0,
                    181,
                    0
                ],
                "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning"
                },
                "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development."
                },
                "authors": [
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Leon Guertler"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Penghui Qi"
                    },
                    {
                        "name": "Daniel Balcells"
                    },
                    {
                        "name": "Mickel Liu"
                    },
                    {
                        "name": "Cheston Tan"
                    },
                    {
                        "name": "Weiyan Shi"
                    },
                    {
                        "name": "Min Lin"
                    },
                    {
                        "name": "Wee Sun Lee"
                    },
                    {
                        "name": "Natasha Jaques"
                    }
                ],
                "author_detail": {
                    "name": "Natasha Jaques"
                },
                "author": "Natasha Jaques",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14518v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14518v2",
                "updated": "2025-07-01T02:25:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    2,
                    25,
                    58,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-20T15:44:01Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    44,
                    1,
                    1,
                    140,
                    0
                ],
                "title": "Teaching Audio-Aware Large Language Models What Does Not Hear:\n  Mitigating Hallucinations through Synthesized Negative Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Audio-Aware Large Language Models What Does Not Hear:\n  Mitigating Hallucinations through Synthesized Negative Samples"
                },
                "summary": "Recent advancements in audio-aware large language models (ALLMs) enable them\nto process and understand audio inputs. However, these models often hallucinate\nnon-existent sound events, reducing their reliability in real-world\napplications. To address this, we propose LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method that\nenhances ALLMs' ability to distinguish between present and absent sounds using\nsynthesized data from the backbone LLM. Unlike prior approaches, our method\nrequires no modification to LLM parameters and efficiently integrates audio\nrepresentations via a lightweight adapter. Experiments show that LISTEN\neffectively mitigates hallucinations while maintaining impressive performance\non existing audio question and reasoning benchmarks. At the same time, it is\nmore efficient in both data and computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in audio-aware large language models (ALLMs) enable them\nto process and understand audio inputs. However, these models often hallucinate\nnon-existent sound events, reducing their reliability in real-world\napplications. To address this, we propose LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method that\nenhances ALLMs' ability to distinguish between present and absent sounds using\nsynthesized data from the backbone LLM. Unlike prior approaches, our method\nrequires no modification to LLM parameters and efficiently integrates audio\nrepresentations via a lightweight adapter. Experiments show that LISTEN\neffectively mitigates hallucinations while maintaining impressive performance\non existing audio question and reasoning benchmarks. At the same time, it is\nmore efficient in both data and computation."
                },
                "authors": [
                    {
                        "name": "Chun-Yi Kuan"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Accepted to Interspeech 2025. Project Website:\n  https://kuan2jiu99.github.io/Balsa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14518v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14518v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23298v2",
                "updated": "2025-07-01T01:57:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    1,
                    57,
                    28,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-29T15:37:17Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    15,
                    37,
                    17,
                    6,
                    180,
                    0
                ],
                "title": "Exposing and Mitigating Calibration Biases and Demographic Unfairness in\n  MLLM Few-Shot In-Context Learning for Medical Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing and Mitigating Calibration Biases and Demographic Unfairness in\n  MLLM Few-Shot In-Context Learning for Medical Image Classification"
                },
                "summary": "Multimodal large language models (MLLMs) have enormous potential to perform\nfew-shot in-context learning in the context of medical image analysis. However,\nsafe deployment of these models into real-world clinical practice requires an\nin-depth analysis of the accuracies of their predictions, and their associated\ncalibration errors, particularly across different demographic subgroups. In\nthis work, we present the first investigation into the calibration biases and\ndemographic unfairness of MLLMs' predictions and confidence scores in few-shot\nin-context learning for medical image classification. We introduce CALIN, an\ninference-time calibration method designed to mitigate the associated biases.\nSpecifically, CALIN estimates the amount of calibration needed, represented by\ncalibration matrices, using a bi-level procedure: progressing from the\npopulation level to the subgroup level prior to inference. It then applies this\nestimation to calibrate the predicted confidence scores during inference.\nExperimental results on three medical imaging datasets: PAPILA for fundus image\nclassification, HAM10000 for skin cancer classification, and MIMIC-CXR for\nchest X-ray classification demonstrate CALIN's effectiveness at ensuring fair\nconfidence calibration in its prediction, while improving its overall\nprediction accuracies and exhibiting minimum fairness-utility trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have enormous potential to perform\nfew-shot in-context learning in the context of medical image analysis. However,\nsafe deployment of these models into real-world clinical practice requires an\nin-depth analysis of the accuracies of their predictions, and their associated\ncalibration errors, particularly across different demographic subgroups. In\nthis work, we present the first investigation into the calibration biases and\ndemographic unfairness of MLLMs' predictions and confidence scores in few-shot\nin-context learning for medical image classification. We introduce CALIN, an\ninference-time calibration method designed to mitigate the associated biases.\nSpecifically, CALIN estimates the amount of calibration needed, represented by\ncalibration matrices, using a bi-level procedure: progressing from the\npopulation level to the subgroup level prior to inference. It then applies this\nestimation to calibrate the predicted confidence scores during inference.\nExperimental results on three medical imaging datasets: PAPILA for fundus image\nclassification, HAM10000 for skin cancer classification, and MIMIC-CXR for\nchest X-ray classification demonstrate CALIN's effectiveness at ensuring fair\nconfidence calibration in its prediction, while improving its overall\nprediction accuracies and exhibiting minimum fairness-utility trade-off."
                },
                "authors": [
                    {
                        "name": "Xing Shen"
                    },
                    {
                        "name": "Justin Szeto"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Hengguan Huang"
                    },
                    {
                        "name": "Tal Arbel"
                    }
                ],
                "author_detail": {
                    "name": "Tal Arbel"
                },
                "author": "Tal Arbel",
                "arxiv_comment": "Preprint version. The peer-reviewed version of this paper has been\n  accepted to MICCAI 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11281v2",
                "updated": "2025-07-01T01:20:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    1,
                    20,
                    3,
                    1,
                    182,
                    0
                ],
                "published": "2024-10-15T05:01:25Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    25,
                    1,
                    289,
                    0
                ],
                "title": "DynaCLR: Contrastive Learning of Cellular Dynamics with Temporal\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynaCLR: Contrastive Learning of Cellular Dynamics with Temporal\n  Regularization"
                },
                "summary": "We report DynaCLR, a self-supervised method for embedding cell and organelle\nDynamics via Contrastive Learning of Representations of time-lapse images.\nDynaCLR integrates single-cell tracking and time-aware contrastive sampling to\nlearn robust, temporally regularized representations of cell dynamics. DynaCLR\nembeddings generalize effectively to in-distribution and out-of-distribution\ndatasets, and can be used for several downstream tasks with sparse human\nannotations. We demonstrate efficient annotations of cell states with a\nhuman-in-the-loop using fluorescence and label-free imaging channels. DynaCLR\nmethod enables diverse downstream biological analyses: classification of cell\ndivision and infection, clustering heterogeneous cell migration patterns,\ncross-modal distillation of cell states from fluorescence to label-free\nchannel, alignment of asynchronous cellular responses and broken cell tracks,\nand discovering organelle response due to infection. DynaCLR is a flexible\nmethod for comparative analyses of dynamic cellular responses to\npharmacological, microbial, and genetic perturbations. We provide PyTorch-based\nimplementations of the model training and inference pipeline\n(https://github.com/mehta-lab/viscy) and a GUI\n(https://github.com/czbiohub-sf/napari-iohub) for the visualization and\nannotation of trajectories of cells in the real space and the embedding space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report DynaCLR, a self-supervised method for embedding cell and organelle\nDynamics via Contrastive Learning of Representations of time-lapse images.\nDynaCLR integrates single-cell tracking and time-aware contrastive sampling to\nlearn robust, temporally regularized representations of cell dynamics. DynaCLR\nembeddings generalize effectively to in-distribution and out-of-distribution\ndatasets, and can be used for several downstream tasks with sparse human\nannotations. We demonstrate efficient annotations of cell states with a\nhuman-in-the-loop using fluorescence and label-free imaging channels. DynaCLR\nmethod enables diverse downstream biological analyses: classification of cell\ndivision and infection, clustering heterogeneous cell migration patterns,\ncross-modal distillation of cell states from fluorescence to label-free\nchannel, alignment of asynchronous cellular responses and broken cell tracks,\nand discovering organelle response due to infection. DynaCLR is a flexible\nmethod for comparative analyses of dynamic cellular responses to\npharmacological, microbial, and genetic perturbations. We provide PyTorch-based\nimplementations of the model training and inference pipeline\n(https://github.com/mehta-lab/viscy) and a GUI\n(https://github.com/czbiohub-sf/napari-iohub) for the visualization and\nannotation of trajectories of cells in the real space and the embedding space."
                },
                "authors": [
                    {
                        "name": "Eduardo Hirata-Miyasaki"
                    },
                    {
                        "name": "Soorya Pradeep"
                    },
                    {
                        "name": "Ziwen Liu"
                    },
                    {
                        "name": "Alishba Imran"
                    },
                    {
                        "name": "Taylla Milena Theodoro"
                    },
                    {
                        "name": "Ivan E. Ivanov"
                    },
                    {
                        "name": "Sudip Khadka"
                    },
                    {
                        "name": "See-Chi Lee"
                    },
                    {
                        "name": "Michelle Grunberg"
                    },
                    {
                        "name": "Hunter Woosley"
                    },
                    {
                        "name": "Madhura Bhave"
                    },
                    {
                        "name": "Carolina Arias"
                    },
                    {
                        "name": "Shalin B. Mehta"
                    }
                ],
                "author_detail": {
                    "name": "Shalin B. Mehta"
                },
                "author": "Shalin B. Mehta",
                "arxiv_comment": "30 pages, 6 figures, 13 appendix figures, 5 videos (ancillary files)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15044v2",
                "updated": "2025-07-01T01:18:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    1,
                    18,
                    26,
                    1,
                    182,
                    0
                ],
                "published": "2025-03-19T09:32:52Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    32,
                    52,
                    2,
                    78,
                    0
                ],
                "title": "SPADE: Structured Prompting Augmentation for Dialogue Enhancement in\n  Machine-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPADE: Structured Prompting Augmentation for Dialogue Enhancement in\n  Machine-Generated Text Detection"
                },
                "summary": "The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of high-quality synthetic\ndatasets for training. To address this issue, we propose SPADE, a structured\nframework for detecting synthetic dialogues using prompt-based positive and\nnegative samples. Our proposed methods yield 14 new dialogue datasets, which we\nbenchmark against eight MGT detection models. The results demonstrate improved\ngeneralization performance when utilizing a mixed dataset produced by proposed\naugmentation frameworks, offering a practical approach to enhancing LLM\napplication security. Considering that real-world agents lack knowledge of\nfuture opponent utterances, we simulate online dialogue detection and examine\nthe relationship between chat history length and detection accuracy. Our\nopen-source datasets, code and prompts can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of high-quality synthetic\ndatasets for training. To address this issue, we propose SPADE, a structured\nframework for detecting synthetic dialogues using prompt-based positive and\nnegative samples. Our proposed methods yield 14 new dialogue datasets, which we\nbenchmark against eight MGT detection models. The results demonstrate improved\ngeneralization performance when utilizing a mixed dataset produced by proposed\naugmentation frameworks, offering a practical approach to enhancing LLM\napplication security. Considering that real-world agents lack knowledge of\nfuture opponent utterances, we simulate online dialogue detection and examine\nthe relationship between chat history length and detection accuracy. Our\nopen-source datasets, code and prompts can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue."
                },
                "authors": [
                    {
                        "name": "Haoyi Li"
                    },
                    {
                        "name": "Angela Yifei Yuan"
                    },
                    {
                        "name": "Soyeon Caren Han"
                    },
                    {
                        "name": "Christopher Leckie"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Leckie"
                },
                "author": "Christopher Leckie",
                "arxiv_comment": "ACL LLMSEC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22554v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22554v2",
                "updated": "2025-07-01T01:02:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    1,
                    2,
                    44,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-27T18:09:49Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    18,
                    9,
                    49,
                    4,
                    178,
                    0
                ],
                "title": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale\n  Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale\n  Dataset"
                },
                "summary": "Human communication involves a complex interplay of verbal and nonverbal\nsignals, essential for conveying meaning and achieving interpersonal goals. To\ndevelop socially intelligent AI technologies, it is crucial to develop models\nthat can both comprehend and generate dyadic behavioral dynamics. To this end,\nwe introduce the Seamless Interaction Dataset, a large-scale collection of over\n4,000 hours of face-to-face interaction footage from over 4,000 participants in\ndiverse contexts. This dataset enables the development of AI technologies that\nunderstand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,\ntelepresence experiences, and multimodal content analysis tools. We also\ndevelop a suite of models that utilize the dataset to generate dyadic motion\ngestures and facial expressions aligned with human speech. These models can\ntake as input both the speech and visual behavior of their interlocutors. We\npresent a variant with speech from an LLM model and integrations with 2D and 3D\nrendering methods, bringing us closer to interactive virtual agents.\nAdditionally, we describe controllable variants of our motion models that can\nadapt emotional responses and expressivity levels, as well as generating more\nsemantically-relevant gestures. Finally, we discuss methods for assessing the\nquality of these dyadic motion models, which are demonstrating the potential\nfor more intuitive and responsive human-AI interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human communication involves a complex interplay of verbal and nonverbal\nsignals, essential for conveying meaning and achieving interpersonal goals. To\ndevelop socially intelligent AI technologies, it is crucial to develop models\nthat can both comprehend and generate dyadic behavioral dynamics. To this end,\nwe introduce the Seamless Interaction Dataset, a large-scale collection of over\n4,000 hours of face-to-face interaction footage from over 4,000 participants in\ndiverse contexts. This dataset enables the development of AI technologies that\nunderstand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,\ntelepresence experiences, and multimodal content analysis tools. We also\ndevelop a suite of models that utilize the dataset to generate dyadic motion\ngestures and facial expressions aligned with human speech. These models can\ntake as input both the speech and visual behavior of their interlocutors. We\npresent a variant with speech from an LLM model and integrations with 2D and 3D\nrendering methods, bringing us closer to interactive virtual agents.\nAdditionally, we describe controllable variants of our motion models that can\nadapt emotional responses and expressivity levels, as well as generating more\nsemantically-relevant gestures. Finally, we discuss methods for assessing the\nquality of these dyadic motion models, which are demonstrating the potential\nfor more intuitive and responsive human-AI interactions."
                },
                "authors": [
                    {
                        "name": "Vasu Agrawal"
                    },
                    {
                        "name": "Akinniyi Akinyemi"
                    },
                    {
                        "name": "Kathryn Alvero"
                    },
                    {
                        "name": "Morteza Behrooz"
                    },
                    {
                        "name": "Julia Buffalini"
                    },
                    {
                        "name": "Fabio Maria Carlucci"
                    },
                    {
                        "name": "Joy Chen"
                    },
                    {
                        "name": "Junming Chen"
                    },
                    {
                        "name": "Zhang Chen"
                    },
                    {
                        "name": "Shiyang Cheng"
                    },
                    {
                        "name": "Praveen Chowdary"
                    },
                    {
                        "name": "Joe Chuang"
                    },
                    {
                        "name": "Antony D'Avirro"
                    },
                    {
                        "name": "Jon Daly"
                    },
                    {
                        "name": "Ning Dong"
                    },
                    {
                        "name": "Mark Duppenthaler"
                    },
                    {
                        "name": "Cynthia Gao"
                    },
                    {
                        "name": "Jeff Girard"
                    },
                    {
                        "name": "Martin Gleize"
                    },
                    {
                        "name": "Sahir Gomez"
                    },
                    {
                        "name": "Hongyu Gong"
                    },
                    {
                        "name": "Srivathsan Govindarajan"
                    },
                    {
                        "name": "Brandon Han"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Denise Hernandez"
                    },
                    {
                        "name": "Yordan Hristov"
                    },
                    {
                        "name": "Rongjie Huang"
                    },
                    {
                        "name": "Hirofumi Inaguma"
                    },
                    {
                        "name": "Somya Jain"
                    },
                    {
                        "name": "Raj Janardhan"
                    },
                    {
                        "name": "Qingyao Jia"
                    },
                    {
                        "name": "Christopher Klaiber"
                    },
                    {
                        "name": "Dejan Kovachev"
                    },
                    {
                        "name": "Moneish Kumar"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Yilei Li"
                    },
                    {
                        "name": "Pavel Litvin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Guangyao Ma"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Martin Ma"
                    },
                    {
                        "name": "Xutai Ma"
                    },
                    {
                        "name": "Lucas Mantovani"
                    },
                    {
                        "name": "Sagar Miglani"
                    },
                    {
                        "name": "Sreyas Mohan"
                    },
                    {
                        "name": "Louis-Philippe Morency"
                    },
                    {
                        "name": "Evonne Ng"
                    },
                    {
                        "name": "Kam-Woh Ng"
                    },
                    {
                        "name": "Tu Anh Nguyen"
                    },
                    {
                        "name": "Amia Oberai"
                    },
                    {
                        "name": "Benjamin Peloquin"
                    },
                    {
                        "name": "Juan Pino"
                    },
                    {
                        "name": "Jovan Popovic"
                    },
                    {
                        "name": "Omid Poursaeed"
                    },
                    {
                        "name": "Fabian Prada"
                    },
                    {
                        "name": "Alice Rakotoarison"
                    },
                    {
                        "name": "Rakesh Ranjan"
                    },
                    {
                        "name": "Alexander Richard"
                    },
                    {
                        "name": "Christophe Ropers"
                    },
                    {
                        "name": "Safiyyah Saleem"
                    },
                    {
                        "name": "Vasu Sharma"
                    },
                    {
                        "name": "Alex Shcherbyna"
                    },
                    {
                        "name": "Jia Shen"
                    },
                    {
                        "name": "Jie Shen"
                    },
                    {
                        "name": "Anastasis Stathopoulos"
                    },
                    {
                        "name": "Anna Sun"
                    },
                    {
                        "name": "Paden Tomasello"
                    },
                    {
                        "name": "Tuan Tran"
                    },
                    {
                        "name": "Arina Turkatenko"
                    },
                    {
                        "name": "Bo Wan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Jeff Wang"
                    },
                    {
                        "name": "Mary Williamson"
                    },
                    {
                        "name": "Carleigh Wood"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Yilin Yang"
                    },
                    {
                        "name": "Julien Yao"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Jiemin Zhang"
                    },
                    {
                        "name": "Xinyue Zhang"
                    },
                    {
                        "name": "Jason Zheng"
                    },
                    {
                        "name": "Pavlo Zhyzheria"
                    },
                    {
                        "name": "Jan Zikes"
                    },
                    {
                        "name": "Michael Zollhoefer"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zollhoefer"
                },
                "author": "Michael Zollhoefer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22554v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22554v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02277v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02277v3",
                "updated": "2025-07-01T00:13:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    0,
                    13,
                    44,
                    1,
                    182,
                    0
                ],
                "published": "2023-09-29T22:55:06Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    22,
                    55,
                    6,
                    4,
                    272,
                    0
                ],
                "title": "Junk DNA Hypothesis: Pruning Small Pre-Trained Weights Irreversibly and\n  Monotonically Impairs \"Difficult\" Downstream Tasks in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Junk DNA Hypothesis: Pruning Small Pre-Trained Weights Irreversibly and\n  Monotonically Impairs \"Difficult\" Downstream Tasks in LLMs"
                },
                "summary": "We present Junk DNA Hypothesis by adopting a novel task-centric angle for the\npre-trained weights of large language models (LLMs). It has been believed that\nweights in LLMs contain significant redundancy, leading to the conception that\na considerable chunk of the parameters can be removed by pruning without\ncompromising performance. Contrary to this belief, this paper presents a\ncounter-argument: small-magnitude weights of pre-trained model weights encode\nvital knowledge essential for tackling difficult downstream tasks - manifested\nas the monotonic relationship between the performance drop of downstream tasks\nacross the difficulty spectrum, as we prune more pre-trained weights by\nmagnitude. Moreover, we reveal that these seemingly inconsequential weights can\nresult in irreparable loss of knowledge and performance degradation in\ndifficult tasks, even when downstream continual training is allowed.\nInterestingly, our evaluations show that the other popular compression, namely\nquantization, fails to exhibit similar monotonic effect and does not as\nconvincingly disentangle this task-difficulty information. To study formally,\nwe introduce several quantifiable metrics to gauge the downstream task\ndifficulty: (1) within the same task category, and (2) across different task\ncategories. Our extensive experiments substantiate the Junk DNA Hypothesis\nacross a diverse range of model sizes, tasks, datasets, and even pruning\nmethods. Codes are available at:\nhttps://github.com/VITA-Group/Junk_DNA_Hypothesis.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Junk DNA Hypothesis by adopting a novel task-centric angle for the\npre-trained weights of large language models (LLMs). It has been believed that\nweights in LLMs contain significant redundancy, leading to the conception that\na considerable chunk of the parameters can be removed by pruning without\ncompromising performance. Contrary to this belief, this paper presents a\ncounter-argument: small-magnitude weights of pre-trained model weights encode\nvital knowledge essential for tackling difficult downstream tasks - manifested\nas the monotonic relationship between the performance drop of downstream tasks\nacross the difficulty spectrum, as we prune more pre-trained weights by\nmagnitude. Moreover, we reveal that these seemingly inconsequential weights can\nresult in irreparable loss of knowledge and performance degradation in\ndifficult tasks, even when downstream continual training is allowed.\nInterestingly, our evaluations show that the other popular compression, namely\nquantization, fails to exhibit similar monotonic effect and does not as\nconvincingly disentangle this task-difficulty information. To study formally,\nwe introduce several quantifiable metrics to gauge the downstream task\ndifficulty: (1) within the same task category, and (2) across different task\ncategories. Our extensive experiments substantiate the Junk DNA Hypothesis\nacross a diverse range of model sizes, tasks, datasets, and even pruning\nmethods. Codes are available at:\nhttps://github.com/VITA-Group/Junk_DNA_Hypothesis.git."
                },
                "authors": [
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Ajay Jaiswal"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "Published at ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02277v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02277v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17924v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17924v3",
                "updated": "2025-06-30T23:31:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    23,
                    31,
                    50,
                    0,
                    181,
                    0
                ],
                "published": "2025-04-24T20:29:45Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    20,
                    29,
                    45,
                    3,
                    114,
                    0
                ],
                "title": "Learning Attentive Neural Processes for Planning with Pushing Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Attentive Neural Processes for Planning with Pushing Actions"
                },
                "summary": "Our goal is to enable robots to plan sequences of tabletop actions to push a\nblock with unknown physical properties to a desired goal pose. We approach this\nproblem by learning the constituent models of a Partially-Observable Markov\nDecision Process (POMDP), where the robot can observe the outcome of a push,\nbut the physical properties of the block that govern the dynamics remain\nunknown. A common solution approach is to train an observation model in a\nsupervised fashion, and do inference with a general inference technique such as\nparticle filters. However, supervised training requires knowledge of the\nrelevant physical properties that determine the problem dynamics, which we do\nnot assume to be known. Planning also requires simulating many belief updates,\nwhich becomes expensive when using particle filters to represent the belief. We\npropose to learn an Attentive Neural Process that computes the belief over a\nlearned latent representation of the relevant physical properties given a\nhistory of actions. To address the pushing planning problem, we integrate a\ntrained Neural Process with a double-progressive widening sampling strategy.\nSimulation results indicate that Neural Process Tree with Double Progressive\nWidening (NPT-DPW) generates better-performing plans faster than traditional\nparticle-filter methods that use a supervised-trained observation model, even\nin complex pushing scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our goal is to enable robots to plan sequences of tabletop actions to push a\nblock with unknown physical properties to a desired goal pose. We approach this\nproblem by learning the constituent models of a Partially-Observable Markov\nDecision Process (POMDP), where the robot can observe the outcome of a push,\nbut the physical properties of the block that govern the dynamics remain\nunknown. A common solution approach is to train an observation model in a\nsupervised fashion, and do inference with a general inference technique such as\nparticle filters. However, supervised training requires knowledge of the\nrelevant physical properties that determine the problem dynamics, which we do\nnot assume to be known. Planning also requires simulating many belief updates,\nwhich becomes expensive when using particle filters to represent the belief. We\npropose to learn an Attentive Neural Process that computes the belief over a\nlearned latent representation of the relevant physical properties given a\nhistory of actions. To address the pushing planning problem, we integrate a\ntrained Neural Process with a double-progressive widening sampling strategy.\nSimulation results indicate that Neural Process Tree with Double Progressive\nWidening (NPT-DPW) generates better-performing plans faster than traditional\nparticle-filter methods that use a supervised-trained observation model, even\nin complex pushing scenarios."
                },
                "authors": [
                    {
                        "name": "Atharv Jain"
                    },
                    {
                        "name": "Seiji Shaw"
                    },
                    {
                        "name": "Nicholas Roy"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Roy"
                },
                "author": "Nicholas Roy",
                "arxiv_comment": "Presented at 2025 RoboReps Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17924v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17924v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19351v2",
                "updated": "2025-06-30T23:24:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    23,
                    24,
                    17,
                    0,
                    181,
                    0
                ],
                "published": "2024-12-26T21:13:12Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    21,
                    13,
                    12,
                    3,
                    361,
                    0
                ],
                "title": "ETTA: Elucidating the Design Space of Text-to-Audio Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETTA: Elucidating the Design Space of Text-to-Audio Models"
                },
                "summary": "Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,\nenabling users to enrich their creative workflows with synthetic audio\ngenerated from natural language prompts. Despite this progress, the effects of\ndata, model architecture, training objective functions, and sampling strategies\non target benchmarks are not well understood. With the purpose of providing a\nholistic understanding of the design space of TTA models, we set up a\nlarge-scale empirical experiment focused on diffusion and flow matching models.\nOur contributions include: 1) AF-Synthetic, a large dataset of high quality\nsynthetic captions obtained from an audio understanding model; 2) a systematic\ncomparison of different architectural, training, and inference design choices\nfor TTA models; 3) an analysis of sampling methods and their Pareto curves with\nrespect to generation quality and inference speed. We leverage the knowledge\nobtained from this extensive analysis to propose our best model dubbed\nElucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,\nETTA provides improvements over the baselines trained on publicly available\ndata, while being competitive with models trained on proprietary data. Finally,\nwe show ETTA's improved ability to generate creative audio following complex\nand imaginative captions -- a task that is more challenging than current\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,\nenabling users to enrich their creative workflows with synthetic audio\ngenerated from natural language prompts. Despite this progress, the effects of\ndata, model architecture, training objective functions, and sampling strategies\non target benchmarks are not well understood. With the purpose of providing a\nholistic understanding of the design space of TTA models, we set up a\nlarge-scale empirical experiment focused on diffusion and flow matching models.\nOur contributions include: 1) AF-Synthetic, a large dataset of high quality\nsynthetic captions obtained from an audio understanding model; 2) a systematic\ncomparison of different architectural, training, and inference design choices\nfor TTA models; 3) an analysis of sampling methods and their Pareto curves with\nrespect to generation quality and inference speed. We leverage the knowledge\nobtained from this extensive analysis to propose our best model dubbed\nElucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,\nETTA provides improvements over the baselines trained on publicly available\ndata, while being competitive with models trained on proprietary data. Finally,\nwe show ETTA's improved ability to generate creative audio following complex\nand imaginative captions -- a task that is more challenging than current\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Sang-gil Lee"
                    },
                    {
                        "name": "Zhifeng Kong"
                    },
                    {
                        "name": "Arushi Goel"
                    },
                    {
                        "name": "Sungwon Kim"
                    },
                    {
                        "name": "Rafael Valle"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Catanzaro"
                },
                "author": "Bryan Catanzaro",
                "arxiv_comment": "ICML 2025. Demo: https://research.nvidia.com/labs/adlr/ETTA/ Code:\n  https://github.com/NVIDIA/elucidated-text-to-audio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16722v2",
                "updated": "2025-06-30T22:55:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    22,
                    55,
                    54,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-22T14:30:14Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    30,
                    14,
                    3,
                    142,
                    0
                ],
                "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification"
                },
                "summary": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 392 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 392 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad."
                },
                "authors": [
                    {
                        "name": "Himanshu Beniwal"
                    },
                    {
                        "name": "Youngwoo Kim"
                    },
                    {
                        "name": "Maarten Sap"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hartvigsen"
                },
                "author": "Thomas Hartvigsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09850v2",
                "updated": "2025-06-30T22:53:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    22,
                    53,
                    19,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-12T21:13:41Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    21,
                    13,
                    41,
                    2,
                    71,
                    0
                ],
                "title": "TabNSA: Native Sparse Attention for Efficient Tabular Data Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabNSA: Native Sparse Attention for Efficient Tabular Data Learning"
                },
                "summary": "Tabular data poses unique challenges for deep learning due to its\nheterogeneous feature types, lack of spatial structure, and often limited\nsample sizes. We propose TabNSA, a novel deep learning framework that\nintegrates Native Sparse Attention (NSA) with a TabMixer backbone to\nefficiently model tabular data. TabNSA tackles computational and\nrepresentational challenges by dynamically focusing on relevant feature subsets\nper instance. The NSA module employs a hierarchical sparse attention mechanism,\nincluding token compression, selective preservation, and localized sliding\nwindows, to significantly reduce the quadratic complexity of standard attention\noperations while addressing feature heterogeneity. Complementing this, the\nTabMixer backbone captures complex, non-linear dependencies through parallel\nmultilayer perceptron (MLP) branches with independent parameters. These modules\nare synergistically combined via element-wise summation and mean pooling,\nenabling TabNSA to model both global context and fine-grained interactions.\nExtensive experiments across supervised and transfer learning settings show\nthat TabNSA consistently outperforms state-of-the-art deep learning models.\nFurthermore, by augmenting TabNSA with a fine-tuned large language model (LLM),\nwe enable it to effectively address Few-Shot Learning challenges through\nlanguage-guided generalization on diverse tabular benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data poses unique challenges for deep learning due to its\nheterogeneous feature types, lack of spatial structure, and often limited\nsample sizes. We propose TabNSA, a novel deep learning framework that\nintegrates Native Sparse Attention (NSA) with a TabMixer backbone to\nefficiently model tabular data. TabNSA tackles computational and\nrepresentational challenges by dynamically focusing on relevant feature subsets\nper instance. The NSA module employs a hierarchical sparse attention mechanism,\nincluding token compression, selective preservation, and localized sliding\nwindows, to significantly reduce the quadratic complexity of standard attention\noperations while addressing feature heterogeneity. Complementing this, the\nTabMixer backbone captures complex, non-linear dependencies through parallel\nmultilayer perceptron (MLP) branches with independent parameters. These modules\nare synergistically combined via element-wise summation and mean pooling,\nenabling TabNSA to model both global context and fine-grained interactions.\nExtensive experiments across supervised and transfer learning settings show\nthat TabNSA consistently outperforms state-of-the-art deep learning models.\nFurthermore, by augmenting TabNSA with a fine-tuned large language model (LLM),\nwe enable it to effectively address Few-Shot Learning challenges through\nlanguage-guided generalization on diverse tabular benchmarks."
                },
                "authors": [
                    {
                        "name": "Ali Eslamian"
                    },
                    {
                        "name": "Qiang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Cheng"
                },
                "author": "Qiang Cheng",
                "arxiv_comment": "26 pages, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05175v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05175v4",
                "updated": "2025-06-30T22:16:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    22,
                    16,
                    39,
                    0,
                    181,
                    0
                ],
                "published": "2023-10-08T14:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    14,
                    22,
                    58,
                    6,
                    281,
                    0
                ],
                "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for\n  Pruning LLMs to High Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for\n  Pruning LLMs to High Sparsity"
                },
                "summary": "Large Language Models (LLMs), renowned for their remarkable performance\nacross diverse domains, present a challenge when it comes to practical\ndeployment due to their colossal model size. In response to this challenge,\nefforts have been directed toward the application of traditional network\npruning techniques to LLMs, uncovering a massive number of parameters that can\nbe pruned in one-shot without hurting performance. Prevailing LLM pruning\nstrategies have consistently adhered to the practice of uniformly pruning all\nlayers at equivalent sparsity, resulting in robust performance. However, this\nobservation stands in contrast to the prevailing trends observed in the field\nof vision models, where non-uniform layerwise sparsity typically yields\nstronger results. To understand the underlying reasons for this disparity, we\nconduct a comprehensive study and discover a strong correlation with the\nemergence of activation outliers in LLMs. Inspired by this finding, we\nintroduce a novel LLM pruning methodology that incorporates a tailored set of\nnon-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise\nsparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio\nobserved within each layer, facilitating a more effective alignment between\nlayerwise weight sparsity and outlier ratios. Our empirical evaluation,\nconducted across the LLaMA-V1 family and OPT, spanning various benchmarks,\ndemonstrates the distinct advantages offered by OWL over previous methods. For\ninstance, OWL exhibits a remarkable performance gain, surpassing the\nstate-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high\nsparsity level of 70%, respectively, while delivering 2.6x end-to-end inference\nspeed-up in the DeepSparse inference engine. Codes are available at\nhttps://github.com/luuyin/OWL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), renowned for their remarkable performance\nacross diverse domains, present a challenge when it comes to practical\ndeployment due to their colossal model size. In response to this challenge,\nefforts have been directed toward the application of traditional network\npruning techniques to LLMs, uncovering a massive number of parameters that can\nbe pruned in one-shot without hurting performance. Prevailing LLM pruning\nstrategies have consistently adhered to the practice of uniformly pruning all\nlayers at equivalent sparsity, resulting in robust performance. However, this\nobservation stands in contrast to the prevailing trends observed in the field\nof vision models, where non-uniform layerwise sparsity typically yields\nstronger results. To understand the underlying reasons for this disparity, we\nconduct a comprehensive study and discover a strong correlation with the\nemergence of activation outliers in LLMs. Inspired by this finding, we\nintroduce a novel LLM pruning methodology that incorporates a tailored set of\nnon-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise\nsparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio\nobserved within each layer, facilitating a more effective alignment between\nlayerwise weight sparsity and outlier ratios. Our empirical evaluation,\nconducted across the LLaMA-V1 family and OPT, spanning various benchmarks,\ndemonstrates the distinct advantages offered by OWL over previous methods. For\ninstance, OWL exhibits a remarkable performance gain, surpassing the\nstate-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high\nsparsity level of 70%, respectively, while delivering 2.6x end-to-end inference\nspeed-up in the DeepSparse inference engine. Codes are available at\nhttps://github.com/luuyin/OWL."
                },
                "authors": [
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Cheng-Yu Hsieh"
                    },
                    {
                        "name": "Yaqing Wang"
                    },
                    {
                        "name": "Yiling Jia"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Ajay Jaiswal"
                    },
                    {
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "name": "Yi Liang"
                    },
                    {
                        "name": "Michael Bendersky"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "arxiv_comment": "Published at ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05175v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05175v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09374v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09374v3",
                "updated": "2025-07-02T01:26:21Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    1,
                    26,
                    21,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-11T03:44:18Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    44,
                    18,
                    2,
                    162,
                    0
                ],
                "title": "Inherited or produced? Inferring protein production kinetics when\n  protein counts are shaped by a cell's division history",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inherited or produced? Inferring protein production kinetics when\n  protein counts are shaped by a cell's division history"
                },
                "summary": "Inferring protein production kinetics for dividing cells is complicated due\nto protein inheritance from the mother cell. For instance, fluorescence\nmeasurements -- commonly used to assess gene activation -- may reflect not only\nnewly produced proteins but also those inherited through successive cell\ndivisions. In such cases, observed protein levels in any given cell are shaped\nby its division history. As a case study, we examine activation of the glc3\ngene in yeast involved in glycogen synthesis and expressed under\nnutrient-limiting conditions. We monitor this activity using snapshot\nfluorescence measurements via flow cytometry, where GFP expression reflects\nglc3 promoter activity. A na\\\"ive analysis of flow cytometry data ignoring cell\ndivision suggests many cells are active with low expression. Explicitly\naccounting for the (non-Markovian) effects of cell division and protein\ninheritance makes it impossible to write down a tractable likelihood -- a key\ningredient in physics-inspired inference, defining the probability of observing\ndata given a model. The dependence on a cell's division history breaks the\nassumptions of standard (Markovian) master equations, rendering traditional\nlikelihood-based approaches inapplicable. Instead, we adapt conditional\nnormalizing flows (a class of neural network models designed to learn\nprobability distributions) to approximate otherwise intractable likelihoods\nfrom simulated data. In doing so, we find that glc3 is mostly inactive under\nstress, showing that while cells occasionally activate the gene, expression is\nbrief and transient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring protein production kinetics for dividing cells is complicated due\nto protein inheritance from the mother cell. For instance, fluorescence\nmeasurements -- commonly used to assess gene activation -- may reflect not only\nnewly produced proteins but also those inherited through successive cell\ndivisions. In such cases, observed protein levels in any given cell are shaped\nby its division history. As a case study, we examine activation of the glc3\ngene in yeast involved in glycogen synthesis and expressed under\nnutrient-limiting conditions. We monitor this activity using snapshot\nfluorescence measurements via flow cytometry, where GFP expression reflects\nglc3 promoter activity. A na\\\"ive analysis of flow cytometry data ignoring cell\ndivision suggests many cells are active with low expression. Explicitly\naccounting for the (non-Markovian) effects of cell division and protein\ninheritance makes it impossible to write down a tractable likelihood -- a key\ningredient in physics-inspired inference, defining the probability of observing\ndata given a model. The dependence on a cell's division history breaks the\nassumptions of standard (Markovian) master equations, rendering traditional\nlikelihood-based approaches inapplicable. Instead, we adapt conditional\nnormalizing flows (a class of neural network models designed to learn\nprobability distributions) to approximate otherwise intractable likelihoods\nfrom simulated data. In doing so, we find that glc3 is mostly inactive under\nstress, showing that while cells occasionally activate the gene, expression is\nbrief and transient."
                },
                "authors": [
                    {
                        "name": "Pedro Pessoa"
                    },
                    {
                        "name": "Juan Andres Martinez"
                    },
                    {
                        "name": "Vincent Vandenbroucke"
                    },
                    {
                        "name": "Frank Delvigne"
                    },
                    {
                        "name": "Steve Press"
                    }
                ],
                "author_detail": {
                    "name": "Steve Press"
                },
                "author": "Steve Press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09374v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09374v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22419v2",
                "updated": "2025-06-30T21:56:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    21,
                    56,
                    29,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-27T17:44:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    44,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements"
                },
                "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent."
                },
                "authors": [
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Despoina Magka"
                    },
                    {
                        "name": "Minqi Jiang"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Roberta Raileanu"
                    },
                    {
                        "name": "Tatiana Shavrina"
                    },
                    {
                        "name": "Jean-Christophe Gagnon-Audet"
                    },
                    {
                        "name": "Kelvin Niu"
                    },
                    {
                        "name": "Shagun Sodhani"
                    },
                    {
                        "name": "Michael Shvartsman"
                    },
                    {
                        "name": "Andrei Lupu"
                    },
                    {
                        "name": "Alisia Lupidi"
                    },
                    {
                        "name": "Edan Toledo"
                    },
                    {
                        "name": "Karen Hambardzumyan"
                    },
                    {
                        "name": "Martin Josifoski"
                    },
                    {
                        "name": "Thomas Foster"
                    },
                    {
                        "name": "Lucia Cipolina-Kun"
                    },
                    {
                        "name": "Abhishek Charnalia"
                    },
                    {
                        "name": "Derek Dunfield"
                    },
                    {
                        "name": "Alexander H. Miller"
                    },
                    {
                        "name": "Oisin Mac Aodha"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Yoram Bachrach"
                    }
                ],
                "author_detail": {
                    "name": "Yoram Bachrach"
                },
                "author": "Yoram Bachrach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14640v2",
                "updated": "2025-06-30T21:28:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    21,
                    28,
                    41,
                    0,
                    181,
                    0
                ],
                "published": "2024-01-26T04:11:07Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    4,
                    11,
                    7,
                    4,
                    26,
                    0
                ],
                "title": "Can LLMs Evaluate Complex Attribution in QA? Automatic Benchmarking\n  using Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Evaluate Complex Attribution in QA? Automatic Benchmarking\n  using Knowledge Graphs"
                },
                "summary": "Attributed Question Answering (AQA) has attracted wide attention, but there\nare still several limitations in evaluating the attributions, including lacking\nfine-grained attribution categories, relying on manual annotations, and failing\nto compare attributions with only subtle differences. To bridge these gaps, we\nintroduce Complex Attributed Question Answering (CAQA), a large-scale benchmark\ncontaining comprehensive attribution categories, automatically generated using\nKnowledge Graphs (KGs), and complex attribution scenarios. We have conducted\nextensive experiments to verify the effectiveness of CAQA, including the\nbenchmarking of 25 automatic evaluators, their comparison with human\nevaluators, the testing of LLM evaluators fine-tuned by CAQA and so on. These\nexperiments also lead to a series of important findings that can benefit the\nfuture research of AQA. All the codes and data are publicly accessible at\nhttps://github.com/HuuuNan/CAQA-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attributed Question Answering (AQA) has attracted wide attention, but there\nare still several limitations in evaluating the attributions, including lacking\nfine-grained attribution categories, relying on manual annotations, and failing\nto compare attributions with only subtle differences. To bridge these gaps, we\nintroduce Complex Attributed Question Answering (CAQA), a large-scale benchmark\ncontaining comprehensive attribution categories, automatically generated using\nKnowledge Graphs (KGs), and complex attribution scenarios. We have conducted\nextensive experiments to verify the effectiveness of CAQA, including the\nbenchmarking of 25 automatic evaluators, their comparison with human\nevaluators, the testing of LLM evaluators fine-tuned by CAQA and so on. These\nexperiments also lead to a series of important findings that can benefit the\nfuture research of AQA. All the codes and data are publicly accessible at\nhttps://github.com/HuuuNan/CAQA-Benchmark."
                },
                "authors": [
                    {
                        "name": "Nan Hu"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Yike Wu"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Sheng Bi"
                    },
                    {
                        "name": "Yongrui Chen"
                    },
                    {
                        "name": "Tongtong Wu"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "arxiv_comment": "Accepted to ACL 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17117v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17117v3",
                "updated": "2025-06-30T21:22:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    21,
                    22,
                    39,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-21T16:29:00Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    29,
                    0,
                    2,
                    141,
                    0
                ],
                "title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for\n  Meaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for\n  Meaning"
                },
                "summary": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations."
                },
                "authors": [
                    {
                        "name": "Chen Shani"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Ravid Shwartz-Ziv"
                    }
                ],
                "author_detail": {
                    "name": "Ravid Shwartz-Ziv"
                },
                "author": "Ravid Shwartz-Ziv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17117v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17117v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14373v2",
                "updated": "2025-06-30T21:12:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    21,
                    12,
                    19,
                    0,
                    181,
                    0
                ],
                "published": "2024-12-18T22:13:21Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    13,
                    21,
                    2,
                    353,
                    0
                ],
                "title": "ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram\n  Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram\n  Language Modeling"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndomains, including applications to electrocardiograms (ECGs). A growing body of\nwork focuses on generating text from multi-channeled ECG signals and\ncorresponding textual prompts. Existing approaches often involve a two-stage\nprocess: pretraining an ECG-specific encoder with a self-supervised learning\n(SSL) objective, followed by finetuning an LLM for natural language generation\n(NLG) using encoder-derived features. However, these methods face two key\nlimitations: inefficiency due to multi-stage training and challenges in\ninterpreting encoder-generated features. To overcome these issues, we propose\nECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for\nautoregressive language modeling of ECGs. ECG-Byte compresses and encodes ECG\nsignals into tokens, enabling direct end-to-end LLM training by combining ECG\nand text tokens. This approach enhances interpretability, as ECG tokens can be\ndirectly mapped back to the original signals. Leveraging ECG-Byte, we achieve\ncompetitive NLG performance while training 3 times faster and using just 48\\%\nof the data required by traditional two-stage methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndomains, including applications to electrocardiograms (ECGs). A growing body of\nwork focuses on generating text from multi-channeled ECG signals and\ncorresponding textual prompts. Existing approaches often involve a two-stage\nprocess: pretraining an ECG-specific encoder with a self-supervised learning\n(SSL) objective, followed by finetuning an LLM for natural language generation\n(NLG) using encoder-derived features. However, these methods face two key\nlimitations: inefficiency due to multi-stage training and challenges in\ninterpreting encoder-generated features. To overcome these issues, we propose\nECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for\nautoregressive language modeling of ECGs. ECG-Byte compresses and encodes ECG\nsignals into tokens, enabling direct end-to-end LLM training by combining ECG\nand text tokens. This approach enhances interpretability, as ECG tokens can be\ndirectly mapped back to the original signals. Leveraging ECG-Byte, we achieve\ncompetitive NLG performance while training 3 times faster and using just 48\\%\nof the data required by traditional two-stage methods."
                },
                "authors": [
                    {
                        "name": "William Han"
                    },
                    {
                        "name": "Chaojing Duan"
                    },
                    {
                        "name": "Michael A. Rosenberg"
                    },
                    {
                        "name": "Emerson Liu"
                    },
                    {
                        "name": "Ding Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Ding Zhao"
                },
                "author": "Ding Zhao",
                "arxiv_comment": "38 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11585v3",
                "updated": "2025-06-30T20:54:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    20,
                    54,
                    58,
                    0,
                    181,
                    0
                ],
                "published": "2024-06-17T14:29:42Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    14,
                    29,
                    42,
                    0,
                    169,
                    0
                ],
                "title": "Bayesian regression discontinuity design with unknown cutoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian regression discontinuity design with unknown cutoff"
                },
                "summary": "The regression discontinuity design (RDD) is a quasi-experimental approach\nused to estimate the causal effects of an intervention assigned based on a\ncutoff criterion. RDD exploits the idea that close to the cutoff units below\nand above are similar; hence, they can be meaningfully compared. Consequently,\nthe causal effect can be estimated only locally at the cutoff point. This makes\nthe cutoff point an essential element of RDD. However, the exact cutoff\nlocation may not always be disclosed to the researchers, and even when it is,\nthe actual location may deviate from the official one. As we illustrate on the\napplication of RDD to the HIV treatment eligibility data, estimating the causal\neffect at an incorrect cutoff point leads to meaningless results. The method we\npresent, LoTTA (Local Trimmed Taylor Approximation), can be applied both as an\nestimation and validation tool in RDD. We use a Bayesian approach to\nincorporate prior knowledge and uncertainty about the cutoff location in the\ncausal effect estimation. At the same time, LoTTA is fitted globally to the\nwhole data, whereas RDD is a local, boundary point estimation problem. In this\nwork we address a natural question that arises: how to make Bayesian inference\nmore local to render a meaningful and powerful estimate of the treatment\neffect?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The regression discontinuity design (RDD) is a quasi-experimental approach\nused to estimate the causal effects of an intervention assigned based on a\ncutoff criterion. RDD exploits the idea that close to the cutoff units below\nand above are similar; hence, they can be meaningfully compared. Consequently,\nthe causal effect can be estimated only locally at the cutoff point. This makes\nthe cutoff point an essential element of RDD. However, the exact cutoff\nlocation may not always be disclosed to the researchers, and even when it is,\nthe actual location may deviate from the official one. As we illustrate on the\napplication of RDD to the HIV treatment eligibility data, estimating the causal\neffect at an incorrect cutoff point leads to meaningless results. The method we\npresent, LoTTA (Local Trimmed Taylor Approximation), can be applied both as an\nestimation and validation tool in RDD. We use a Bayesian approach to\nincorporate prior knowledge and uncertainty about the cutoff location in the\ncausal effect estimation. At the same time, LoTTA is fitted globally to the\nwhole data, whereas RDD is a local, boundary point estimation problem. In this\nwork we address a natural question that arises: how to make Bayesian inference\nmore local to render a meaningful and powerful estimate of the treatment\neffect?"
                },
                "authors": [
                    {
                        "name": "Julia Kowalska"
                    },
                    {
                        "name": "Mark van de Wiel"
                    },
                    {
                        "name": "Stphanie van der Pas"
                    }
                ],
                "author_detail": {
                    "name": "Stphanie van der Pas"
                },
                "author": "Stphanie van der Pas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00949v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00949v4",
                "updated": "2025-06-30T20:37:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    20,
                    37,
                    51,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-02T01:35:35Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    1,
                    35,
                    35,
                    4,
                    122,
                    0
                ],
                "title": "Llama-Nemotron: Efficient Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama-Nemotron: Efficient Reasoning Models"
                },
                "summary": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Llama-Nemotron series of models, an open family of\nheterogeneous reasoning models that deliver exceptional reasoning capabilities,\ninference efficiency, and an open license for enterprise use. The family comes\nin three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs\ncompetitively with state-of-the-art reasoning models such as DeepSeek-R1 while\noffering superior inference throughput and memory efficiency. In this report,\nwe discuss the training procedure for these models, which entails using neural\narchitecture search from Llama 3 models for accelerated inference, knowledge\ndistillation, and continued pretraining, followed by a reasoning-focused\npost-training stage consisting of two main parts: supervised fine-tuning and\nlarge scale reinforcement learning. Llama-Nemotron models are the first\nopen-source models to support a dynamic reasoning toggle, allowing users to\nswitch between standard chat and reasoning modes during inference. To further\nsupport open research and facilitate model development, we provide the\nfollowing resources: 1. We release the Llama-Nemotron reasoning models --\nLN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA\nOpen Model License Agreement. 2. We release the complete post-training dataset:\nLlama-Nemotron-Post-Training-Dataset. 3. We also release our training\ncodebases: NeMo, NeMo-Aligner, and Megatron-LM."
                },
                "authors": [
                    {
                        "name": "Akhiad Bercovich"
                    },
                    {
                        "name": "Itay Levy"
                    },
                    {
                        "name": "Izik Golan"
                    },
                    {
                        "name": "Mohammad Dabbah"
                    },
                    {
                        "name": "Ran El-Yaniv"
                    },
                    {
                        "name": "Omri Puny"
                    },
                    {
                        "name": "Ido Galil"
                    },
                    {
                        "name": "Zach Moshe"
                    },
                    {
                        "name": "Tomer Ronen"
                    },
                    {
                        "name": "Najeeb Nabwani"
                    },
                    {
                        "name": "Ido Shahaf"
                    },
                    {
                        "name": "Oren Tropp"
                    },
                    {
                        "name": "Ehud Karpas"
                    },
                    {
                        "name": "Ran Zilberstein"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Soumye Singhal"
                    },
                    {
                        "name": "Alexander Bukharin"
                    },
                    {
                        "name": "Yian Zhang"
                    },
                    {
                        "name": "Tugrul Konuk"
                    },
                    {
                        "name": "Gerald Shen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Bilal Kartal"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "David Mosallanezhad"
                    },
                    {
                        "name": "Adi Renduchintala"
                    },
                    {
                        "name": "Haifeng Qian"
                    },
                    {
                        "name": "Dima Rekesh"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Somshubra Majumdar"
                    },
                    {
                        "name": "Vahid Noroozi"
                    },
                    {
                        "name": "Wasi Uddin Ahmad"
                    },
                    {
                        "name": "Sean Narenthiran"
                    },
                    {
                        "name": "Aleksander Ficek"
                    },
                    {
                        "name": "Mehrzad Samadi"
                    },
                    {
                        "name": "Jocelyn Huang"
                    },
                    {
                        "name": "Siddhartha Jain"
                    },
                    {
                        "name": "Igor Gitman"
                    },
                    {
                        "name": "Ivan Moshkov"
                    },
                    {
                        "name": "Wei Du"
                    },
                    {
                        "name": "Shubham Toshniwal"
                    },
                    {
                        "name": "George Armstrong"
                    },
                    {
                        "name": "Branislav Kisacanin"
                    },
                    {
                        "name": "Matvei Novikov"
                    },
                    {
                        "name": "Daria Gitman"
                    },
                    {
                        "name": "Evelina Bakhturina"
                    },
                    {
                        "name": "Prasoon Varshney"
                    },
                    {
                        "name": "Makesh Narsimhan"
                    },
                    {
                        "name": "Jane Polak Scowcroft"
                    },
                    {
                        "name": "John Kamalu"
                    },
                    {
                        "name": "Dan Su"
                    },
                    {
                        "name": "Kezhi Kong"
                    },
                    {
                        "name": "Markus Kliegl"
                    },
                    {
                        "name": "Rabeeh Karimi"
                    },
                    {
                        "name": "Ying Lin"
                    },
                    {
                        "name": "Sanjeev Satheesh"
                    },
                    {
                        "name": "Jupinder Parmar"
                    },
                    {
                        "name": "Pritam Gundecha"
                    },
                    {
                        "name": "Brandon Norick"
                    },
                    {
                        "name": "Joseph Jennings"
                    },
                    {
                        "name": "Shrimai Prabhumoye"
                    },
                    {
                        "name": "Syeda Nahida Akter"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Abhinav Khattar"
                    },
                    {
                        "name": "Deepak Narayanan"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Jimmy Zhang"
                    },
                    {
                        "name": "Bor-Yiing Su"
                    },
                    {
                        "name": "Guyue Huang"
                    },
                    {
                        "name": "Terry Kong"
                    },
                    {
                        "name": "Parth Chadha"
                    },
                    {
                        "name": "Sahil Jain"
                    },
                    {
                        "name": "Christine Harvey"
                    },
                    {
                        "name": "Elad Segal"
                    },
                    {
                        "name": "Jining Huang"
                    },
                    {
                        "name": "Sergey Kashirsky"
                    },
                    {
                        "name": "Robert McQueen"
                    },
                    {
                        "name": "Izzy Putterman"
                    },
                    {
                        "name": "George Lam"
                    },
                    {
                        "name": "Arun Venkatesan"
                    },
                    {
                        "name": "Sherry Wu"
                    },
                    {
                        "name": "Vinh Nguyen"
                    },
                    {
                        "name": "Manoj Kilaru"
                    },
                    {
                        "name": "Andrew Wang"
                    },
                    {
                        "name": "Anna Warno"
                    },
                    {
                        "name": "Abhilash Somasamudramath"
                    },
                    {
                        "name": "Sandip Bhaskar"
                    },
                    {
                        "name": "Maka Dong"
                    },
                    {
                        "name": "Nave Assaf"
                    },
                    {
                        "name": "Shahar Mor"
                    },
                    {
                        "name": "Omer Ullman Argov"
                    },
                    {
                        "name": "Scot Junkin"
                    },
                    {
                        "name": "Oleksandr Romanenko"
                    },
                    {
                        "name": "Pedro Larroy"
                    },
                    {
                        "name": "Monika Katariya"
                    },
                    {
                        "name": "Marco Rovinelli"
                    },
                    {
                        "name": "Viji Balas"
                    },
                    {
                        "name": "Nicholas Edelman"
                    },
                    {
                        "name": "Anahita Bhiwandiwalla"
                    },
                    {
                        "name": "Muthu Subramaniam"
                    },
                    {
                        "name": "Smita Ithape"
                    },
                    {
                        "name": "Karthik Ramamoorthy"
                    },
                    {
                        "name": "Yuting Wu"
                    },
                    {
                        "name": "Suguna Varshini Velury"
                    },
                    {
                        "name": "Omri Almog"
                    },
                    {
                        "name": "Joyjit Daw"
                    },
                    {
                        "name": "Denys Fridman"
                    },
                    {
                        "name": "Erick Galinkin"
                    },
                    {
                        "name": "Michael Evans"
                    },
                    {
                        "name": "Shaona Ghosh"
                    },
                    {
                        "name": "Katherine Luna"
                    },
                    {
                        "name": "Leon Derczynski"
                    },
                    {
                        "name": "Nikki Pope"
                    },
                    {
                        "name": "Eileen Long"
                    },
                    {
                        "name": "Seth Schneider"
                    },
                    {
                        "name": "Guillermo Siman"
                    },
                    {
                        "name": "Tomasz Grzegorzek"
                    },
                    {
                        "name": "Pablo Ribalta"
                    },
                    {
                        "name": "Monika Katariya"
                    },
                    {
                        "name": "Chris Alexiuk"
                    },
                    {
                        "name": "Joey Conway"
                    },
                    {
                        "name": "Trisha Saar"
                    },
                    {
                        "name": "Ann Guan"
                    },
                    {
                        "name": "Krzysztof Pawelec"
                    },
                    {
                        "name": "Shyamala Prayaga"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Oluwatobi Olabiyi"
                    },
                    {
                        "name": "Kari Briski"
                    },
                    {
                        "name": "Jonathan Cohen"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Jonah Alben"
                    },
                    {
                        "name": "Yonatan Geifman"
                    },
                    {
                        "name": "Eric Chung"
                    }
                ],
                "author_detail": {
                    "name": "Eric Chung"
                },
                "author": "Eric Chung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00949v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00949v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21329v2",
                "updated": "2025-06-30T20:37:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    20,
                    37,
                    51,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-26T14:43:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    14,
                    43,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "Active Inference AI Systems for Scientific Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Inference AI Systems for Scientific Discovery"
                },
                "summary": "The rapid evolution of artificial intelligence has led to expectations of\ntransformative scientific discovery, yet current systems remain fundamentally\nlimited by their operational architectures, brittle reasoning mechanisms, and\ntheir separation from experimental reality. Building on earlier work, we\ncontend that progress in AI-driven science now depends on closing three\nfundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap\n-- rather than on model size/data/test time compute. Scientific reasoning\ndemands internal representations that support simulation of actions and\nresponse, causal structures that distinguish correlation from mechanism, and\ncontinuous calibration. We define active inference AI systems for scientific\ndiscovery as those that (i) maintain long-lived research memories grounded in\ncausal self-supervised foundation models, (ii) symbolic or neuro-symbolic\nplanners equipped with Bayesian guardrails, (iii) grow persistent knowledge\ngraphs where thinking generates novel conceptual nodes, reasoning establishes\ncausal edges, and real-world interaction prunes false connections while\nstrengthening verified pathways, and (iv) refine their internal representations\nthrough closed-loop interaction with both high-fidelity simulators and\nautomated laboratories - an operational loop where mental simulation guides\naction and empirical surprise reshapes understanding. In essence, we outline an\narchitecture where discovery arises from the interplay between internal models\nthat enable counterfactual reasoning and external validation that grounds\nhypotheses in reality. It is also argued that the inherent ambiguity in\nfeedback from simulations and experiments, and underlying uncertainties makes\nhuman judgment indispensable, not as a temporary scaffold but as a permanent\narchitectural component.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of artificial intelligence has led to expectations of\ntransformative scientific discovery, yet current systems remain fundamentally\nlimited by their operational architectures, brittle reasoning mechanisms, and\ntheir separation from experimental reality. Building on earlier work, we\ncontend that progress in AI-driven science now depends on closing three\nfundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap\n-- rather than on model size/data/test time compute. Scientific reasoning\ndemands internal representations that support simulation of actions and\nresponse, causal structures that distinguish correlation from mechanism, and\ncontinuous calibration. We define active inference AI systems for scientific\ndiscovery as those that (i) maintain long-lived research memories grounded in\ncausal self-supervised foundation models, (ii) symbolic or neuro-symbolic\nplanners equipped with Bayesian guardrails, (iii) grow persistent knowledge\ngraphs where thinking generates novel conceptual nodes, reasoning establishes\ncausal edges, and real-world interaction prunes false connections while\nstrengthening verified pathways, and (iv) refine their internal representations\nthrough closed-loop interaction with both high-fidelity simulators and\nautomated laboratories - an operational loop where mental simulation guides\naction and empirical surprise reshapes understanding. In essence, we outline an\narchitecture where discovery arises from the interplay between internal models\nthat enable counterfactual reasoning and external validation that grounds\nhypotheses in reality. It is also argued that the inherent ambiguity in\nfeedback from simulations and experiments, and underlying uncertainties makes\nhuman judgment indispensable, not as a temporary scaffold but as a permanent\narchitectural component."
                },
                "authors": [
                    {
                        "name": "Karthik Duraisamy"
                    }
                ],
                "author_detail": {
                    "name": "Karthik Duraisamy"
                },
                "author": "Karthik Duraisamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03704v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03704v3",
                "updated": "2025-06-30T20:29:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    20,
                    29,
                    34,
                    0,
                    181,
                    0
                ],
                "published": "2024-12-04T20:35:07Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    20,
                    35,
                    7,
                    2,
                    339,
                    0
                ],
                "title": "Scaling Inference-Time Search with Vision Value Model for Improved\n  Visual Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Inference-Time Search with Vision Value Model for Improved\n  Visual Comprehension"
                },
                "summary": "Despite significant advancements in vision-language models (VLMs), there\nlacks effective approaches to enhance response quality by scaling\ninference-time computation. This capability is known to be a core step towards\nthe self-improving models in recent large language model studies. In this\npaper, we present Vision Value Model (VisVM) that can guide VLM inference-time\nsearch to generate responses with better visual comprehension. Specifically,\nVisVM not only evaluates the generated sentence quality in the current search\nstep, but also anticipates the quality of subsequent sentences that may result\nfrom the current step, thus providing a long-term value. In this way, VisVM\nsteers VLMs away from generating sentences prone to hallucinations or\ninsufficient detail, thereby producing higher quality responses. Experimental\nresults demonstrate that VisVM-guided search significantly enhances VLMs'\nability to generate descriptive captions with richer visual details and fewer\nhallucinations, compared with greedy decoding and search methods with other\nvisual reward signals. Furthermore, we find that self-training the model with\nthe VisVM-guided captions improve VLM's performance across a wide range of\nmultimodal benchmarks, indicating the potential for developing self-improving\nVLMs. Our value model and code are available at\nhttps://github.com/si0wang/VisVM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advancements in vision-language models (VLMs), there\nlacks effective approaches to enhance response quality by scaling\ninference-time computation. This capability is known to be a core step towards\nthe self-improving models in recent large language model studies. In this\npaper, we present Vision Value Model (VisVM) that can guide VLM inference-time\nsearch to generate responses with better visual comprehension. Specifically,\nVisVM not only evaluates the generated sentence quality in the current search\nstep, but also anticipates the quality of subsequent sentences that may result\nfrom the current step, thus providing a long-term value. In this way, VisVM\nsteers VLMs away from generating sentences prone to hallucinations or\ninsufficient detail, thereby producing higher quality responses. Experimental\nresults demonstrate that VisVM-guided search significantly enhances VLMs'\nability to generate descriptive captions with richer visual details and fewer\nhallucinations, compared with greedy decoding and search methods with other\nvisual reward signals. Furthermore, we find that self-training the model with\nthe VisVM-guided captions improve VLM's performance across a wide range of\nmultimodal benchmarks, indicating the potential for developing self-improving\nVLMs. Our value model and code are available at\nhttps://github.com/si0wang/VisVM."
                },
                "authors": [
                    {
                        "name": "Xiyao Wang"
                    },
                    {
                        "name": "Zhengyuan Yang"
                    },
                    {
                        "name": "Linjie Li"
                    },
                    {
                        "name": "Hongjin Lu"
                    },
                    {
                        "name": "Yuancheng Xu"
                    },
                    {
                        "name": "Chung-Ching Lin"
                    },
                    {
                        "name": "Kevin Lin"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Lijuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lijuan Wang"
                },
                "author": "Lijuan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03704v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03704v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12147v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12147v3",
                "updated": "2025-06-30T20:10:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    20,
                    10,
                    28,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-17T21:39:51Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    21,
                    39,
                    51,
                    5,
                    137,
                    0
                ],
                "title": "Causal Machine Learning in IoT-based Engineering Problems: A Tool\n  Comparison in the Case of Household Energy Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Machine Learning in IoT-based Engineering Problems: A Tool\n  Comparison in the Case of Household Energy Consumption"
                },
                "summary": "The rapid increase in computing power and the ability to store Big Data in\nthe infrastructure has enabled predictions in a large variety of domains by\nMachine Learning. However, in many cases, existing Machine Learning tools are\nconsidered insufficient or incorrect since they exploit only probabilistic\ndependencies rather than inference logic. Causal Machine Learning methods seem\nto close this gap. In this paper, two prevalent tools based on Causal Machine\nLearning methods are compared, as well as their mathematical underpinning\nbackground. The operation of the tools is demonstrated by examining their\nresponse to 18 queries, based on the IDEAL Household Energy Dataset, published\nby the University of Edinburgh. First, it was important to evaluate the causal\nrelations assumption that allowed the use of this approach; this was based on\nthe preexisting scientific knowledge of the domain and was implemented by use\nof the in-built validation tools. Results were encouraging and may easily be\nextended to other domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid increase in computing power and the ability to store Big Data in\nthe infrastructure has enabled predictions in a large variety of domains by\nMachine Learning. However, in many cases, existing Machine Learning tools are\nconsidered insufficient or incorrect since they exploit only probabilistic\ndependencies rather than inference logic. Causal Machine Learning methods seem\nto close this gap. In this paper, two prevalent tools based on Causal Machine\nLearning methods are compared, as well as their mathematical underpinning\nbackground. The operation of the tools is demonstrated by examining their\nresponse to 18 queries, based on the IDEAL Household Energy Dataset, published\nby the University of Edinburgh. First, it was important to evaluate the causal\nrelations assumption that allowed the use of this approach; this was based on\nthe preexisting scientific knowledge of the domain and was implemented by use\nof the in-built validation tools. Results were encouraging and may easily be\nextended to other domains."
                },
                "authors": [
                    {
                        "name": "Nikolaos-Lysias Kosioris"
                    },
                    {
                        "name": "Sotirios Nikoletseas"
                    },
                    {
                        "name": "Gavrilis Filios"
                    },
                    {
                        "name": "Stefanos Panagiotou"
                    }
                ],
                "author_detail": {
                    "name": "Stefanos Panagiotou"
                },
                "author": "Stefanos Panagiotou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12147v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12147v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.16571v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16571v2",
                "updated": "2025-07-01T17:51:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    51,
                    47,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-19T19:52:53Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    19,
                    52,
                    53,
                    3,
                    170,
                    0
                ],
                "title": "Capturing Visualization Design Rationale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing Visualization Design Rationale"
                },
                "summary": "Prior natural language datasets for data visualization have focused on tasks\nsuch as visualization literacy assessment, insight generation, and\nvisualization generation from natural language instructions. These studies\noften rely on controlled setups with purpose-built visualizations and\nartificially constructed questions. As a result, they tend to prioritize the\ninterpretation of visualizations, focusing on decoding visualizations rather\nthan understanding their encoding. In this paper, we present a new dataset and\nmethodology for probing visualization design rationale through natural\nlanguage. We leverage a unique source of real-world visualizations and natural\nlanguage narratives: literate visualization notebooks created by students as\npart of a data visualization course. These notebooks combine visual artifacts\nwith design exposition, in which students make explicit the rationale behind\ntheir design decisions. We also use large language models (LLMs) to generate\nand categorize question-answer-rationale triples from the narratives and\narticulations in the notebooks. We then carefully validate the triples and\ncurate a dataset that captures and distills the visualization design choices\nand corresponding rationales of the students.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior natural language datasets for data visualization have focused on tasks\nsuch as visualization literacy assessment, insight generation, and\nvisualization generation from natural language instructions. These studies\noften rely on controlled setups with purpose-built visualizations and\nartificially constructed questions. As a result, they tend to prioritize the\ninterpretation of visualizations, focusing on decoding visualizations rather\nthan understanding their encoding. In this paper, we present a new dataset and\nmethodology for probing visualization design rationale through natural\nlanguage. We leverage a unique source of real-world visualizations and natural\nlanguage narratives: literate visualization notebooks created by students as\npart of a data visualization course. These notebooks combine visual artifacts\nwith design exposition, in which students make explicit the rationale behind\ntheir design decisions. We also use large language models (LLMs) to generate\nand categorize question-answer-rationale triples from the narratives and\narticulations in the notebooks. We then carefully validate the triples and\ncurate a dataset that captures and distills the visualization design choices\nand corresponding rationales of the students."
                },
                "authors": [
                    {
                        "name": "Maeve Hutchinson"
                    },
                    {
                        "name": "Radu Jianu"
                    },
                    {
                        "name": "Aidan Slingsby"
                    },
                    {
                        "name": "Jo Wood"
                    },
                    {
                        "name": "Pranava Madhyastha"
                    }
                ],
                "author_detail": {
                    "name": "Pranava Madhyastha"
                },
                "author": "Pranava Madhyastha",
                "arxiv_comment": "To be presented at IEEE VIS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16571v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16571v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10685v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10685v3",
                "updated": "2025-07-01T17:49:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    49,
                    9,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-12T13:30:01Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    13,
                    30,
                    1,
                    3,
                    163,
                    0
                ],
                "title": "Defensive Adversarial CAPTCHA: A Semantics-Driven Framework for Natural\n  Adversarial Example Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defensive Adversarial CAPTCHA: A Semantics-Driven Framework for Natural\n  Adversarial Example Generation"
                },
                "summary": "Traditional CAPTCHA (Completely Automated Public Turing Test to Tell\nComputers and Humans Apart) schemes are increasingly vulnerable to automated\nattacks powered by deep neural networks (DNNs). Existing adversarial attack\nmethods often rely on the original image characteristics, resulting in\ndistortions that hinder human interpretation and limit their applicability in\nscenarios where no initial input images are available. To address these\nchallenges, we propose the Unsourced Adversarial CAPTCHA (DAC), a novel\nframework that generates high-fidelity adversarial examples guided by\nattacker-specified semantics information. Leveraging a Large Language Model\n(LLM), DAC enhances CAPTCHA diversity and enriches the semantic information. To\naddress various application scenarios, we examine the white-box targeted attack\nscenario and the black box untargeted attack scenario. For target attacks, we\nintroduce two latent noise variables that are alternately guided in the\ndiffusion step to achieve robust inversion. The synergy between gradient\nguidance and latent variable optimization achieved in this way ensures that the\ngenerated adversarial examples not only accurately align with the target\nconditions but also achieve optimal performance in terms of distributional\nconsistency and attack effectiveness. In untargeted attacks, especially for\nblack-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA\n(BP-DAC), a two-step optimization strategy employing multimodal gradients and\nbi-path optimization for efficient misclassification. Experiments show that the\ndefensive adversarial CAPTCHA generated by BP-DAC is able to defend against\nmost of the unknown models, and the generated CAPTCHA is indistinguishable to\nboth humans and DNNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional CAPTCHA (Completely Automated Public Turing Test to Tell\nComputers and Humans Apart) schemes are increasingly vulnerable to automated\nattacks powered by deep neural networks (DNNs). Existing adversarial attack\nmethods often rely on the original image characteristics, resulting in\ndistortions that hinder human interpretation and limit their applicability in\nscenarios where no initial input images are available. To address these\nchallenges, we propose the Unsourced Adversarial CAPTCHA (DAC), a novel\nframework that generates high-fidelity adversarial examples guided by\nattacker-specified semantics information. Leveraging a Large Language Model\n(LLM), DAC enhances CAPTCHA diversity and enriches the semantic information. To\naddress various application scenarios, we examine the white-box targeted attack\nscenario and the black box untargeted attack scenario. For target attacks, we\nintroduce two latent noise variables that are alternately guided in the\ndiffusion step to achieve robust inversion. The synergy between gradient\nguidance and latent variable optimization achieved in this way ensures that the\ngenerated adversarial examples not only accurately align with the target\nconditions but also achieve optimal performance in terms of distributional\nconsistency and attack effectiveness. In untargeted attacks, especially for\nblack-box scenarios, we introduce bi-path unsourced adversarial CAPTCHA\n(BP-DAC), a two-step optimization strategy employing multimodal gradients and\nbi-path optimization for efficient misclassification. Experiments show that the\ndefensive adversarial CAPTCHA generated by BP-DAC is able to defend against\nmost of the unknown models, and the generated CAPTCHA is indistinguishable to\nboth humans and DNNs."
                },
                "authors": [
                    {
                        "name": "Xia Du"
                    },
                    {
                        "name": "Xiaoyuan Liu"
                    },
                    {
                        "name": "Jizhe Zhou"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Chi-man Pun"
                    },
                    {
                        "name": "Cong Wu"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wei Ni"
                    },
                    {
                        "name": "Jun Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jun Luo"
                },
                "author": "Jun Luo",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10685v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10685v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22773v2",
                "updated": "2025-07-01T17:12:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    12,
                    12,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-28T06:26:06Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    6,
                    26,
                    6,
                    5,
                    179,
                    0
                ],
                "title": "Not All Water Consumption Is Equal: A Water Stress Weighted Metric for\n  Sustainable Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Water Consumption Is Equal: A Water Stress Weighted Metric for\n  Sustainable Computing"
                },
                "summary": "Water consumption is an increasingly critical dimension of computing\nsustainability, especially as AI workloads rapidly scale. However, current\nwater impact assessment often overlooks where and when water stress is more\nsevere. To fill in this gap, we present SCARF, the first general framework that\nevaluates water impact of computing by factoring in both spatial and temporal\nvariations in water stress. SCARF calculates an Adjusted Water Impact (AWI)\nmetric that considers both consumption volume and local water stress over time.\nThrough three case studies on LLM serving, datacenters, and semiconductor\nfabrication plants, we show the hidden opportunities for reducing water impact\nby optimizing location and time choices, paving the way for water-sustainable\ncomputing. The code is available at https://github.com/jojacola/SCARF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water consumption is an increasingly critical dimension of computing\nsustainability, especially as AI workloads rapidly scale. However, current\nwater impact assessment often overlooks where and when water stress is more\nsevere. To fill in this gap, we present SCARF, the first general framework that\nevaluates water impact of computing by factoring in both spatial and temporal\nvariations in water stress. SCARF calculates an Adjusted Water Impact (AWI)\nmetric that considers both consumption volume and local water stress over time.\nThrough three case studies on LLM serving, datacenters, and semiconductor\nfabrication plants, we show the hidden opportunities for reducing water impact\nby optimizing location and time choices, paving the way for water-sustainable\ncomputing. The code is available at https://github.com/jojacola/SCARF."
                },
                "authors": [
                    {
                        "name": "Yanran Wu"
                    },
                    {
                        "name": "Inez Hua"
                    },
                    {
                        "name": "Yi Ding"
                    }
                ],
                "author_detail": {
                    "name": "Yi Ding"
                },
                "author": "Yi Ding",
                "arxiv_comment": "7 pages, 9 figures, The 4th Workshop on Sustainable Computer Systems\n  (HotCarbon'25), Cambridge, MA, July 10-11th, 2025",
                "arxiv_journal_ref": "ACM SIGEnergy Energy Informatics Review (EIR), Volume 5 Issue 2,\n  July 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04370v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04370v4",
                "updated": "2025-07-01T17:12:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    12,
                    1,
                    1,
                    182,
                    0
                ],
                "published": "2024-06-01T02:08:44Z",
                "published_parsed": [
                    2024,
                    6,
                    1,
                    2,
                    8,
                    44,
                    5,
                    153,
                    0
                ],
                "title": "Large Language Model Confidence Estimation via Black-Box Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Confidence Estimation via Black-Box Access"
                },
                "summary": "Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\\&A tasks as well\nas of Pegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating uncertainty or confidence in the responses of a model can be\nsignificant in evaluating trust not only in the responses, but also in the\nmodel as a whole. In this paper, we explore the problem of estimating\nconfidence for responses of large language models (LLMs) with simply black-box\nor query access to them. We propose a simple and extensible framework where, we\nengineer novel features and train a (interpretable) model (viz. logistic\nregression) on these features to estimate the confidence. We empirically\ndemonstrate that our simple framework is effective in estimating confidence of\nFlan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\\&A tasks as well\nas of Pegasus-large and BART-large on two benchmark summarization tasks with it\nsurpassing baselines by even over $10\\%$ (on AUROC) in some cases.\nAdditionally, our interpretable approach provides insight into features that\nare predictive of confidence, leading to the interesting and useful discovery\nthat our confidence models built for one LLM generalize zero-shot across others\non a given dataset."
                },
                "authors": [
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "Amit Dhurandhar"
                    },
                    {
                        "name": "Soumya Ghosh"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Prasanna Sattigeri"
                    }
                ],
                "author_detail": {
                    "name": "Prasanna Sattigeri"
                },
                "author": "Prasanna Sattigeri",
                "arxiv_comment": "Accepted to TMLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04370v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04370v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19955v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19955v2",
                "updated": "2025-07-01T17:01:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    17,
                    1,
                    12,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-26T13:18:37Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    13,
                    18,
                    37,
                    0,
                    146,
                    0
                ],
                "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research"
                },
                "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery."
                },
                "authors": [
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Miao Xiong"
                    },
                    {
                        "name": "Yujie Lu"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Ailin Deng"
                    },
                    {
                        "name": "Yufei He"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Yibo Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_comment": "42 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19955v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19955v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17336v2",
                "updated": "2025-07-01T16:41:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    41,
                    35,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-19T07:13:30Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    7,
                    13,
                    30,
                    3,
                    170,
                    0
                ],
                "title": "Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought\n  Reasoning and Homomorphically Encrypted Vector Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought\n  Reasoning and Homomorphically Encrypted Vector Databases"
                },
                "summary": "Large language models (LLMs) are increasingly used as personal agents,\naccessing sensitive user data such as calendars, emails, and medical records.\nUsers currently face a trade-off: They can send private records, many of which\nare stored in remote databases, to powerful but untrusted LLM providers,\nincreasing their exposure risk. Alternatively, they can run less powerful\nmodels locally on trusted devices. We bridge this gap. Our Socratic\nChain-of-Thought Reasoning first sends a generic, non-private user query to a\npowerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and\ndetailed sub-queries without accessing user data. Next, we embed these\nsub-queries and perform encrypted sub-second semantic search using our\nHomomorphically Encrypted Vector Database across one million entries of a\nsingle user's private data. This represents a realistic scale of personal\ndocuments, emails, and records accumulated over years of digital activity.\nFinally, we feed the CoT prompt and the decrypted records to a local language\nmodel and generate the final response. On the LoCoMo long-context QA benchmark,\nour hybrid framework, combining GPT-4o with a local Llama-3.2-1B model,\noutperforms using GPT-4o alone by up to 7.1 percentage points. This\ndemonstrates a first step toward systems where tasks are decomposed and split\nbetween untrusted strong LLMs and weak local ones, preserving user privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used as personal agents,\naccessing sensitive user data such as calendars, emails, and medical records.\nUsers currently face a trade-off: They can send private records, many of which\nare stored in remote databases, to powerful but untrusted LLM providers,\nincreasing their exposure risk. Alternatively, they can run less powerful\nmodels locally on trusted devices. We bridge this gap. Our Socratic\nChain-of-Thought Reasoning first sends a generic, non-private user query to a\npowerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and\ndetailed sub-queries without accessing user data. Next, we embed these\nsub-queries and perform encrypted sub-second semantic search using our\nHomomorphically Encrypted Vector Database across one million entries of a\nsingle user's private data. This represents a realistic scale of personal\ndocuments, emails, and records accumulated over years of digital activity.\nFinally, we feed the CoT prompt and the decrypted records to a local language\nmodel and generate the final response. On the LoCoMo long-context QA benchmark,\nour hybrid framework, combining GPT-4o with a local Llama-3.2-1B model,\noutperforms using GPT-4o alone by up to 7.1 percentage points. This\ndemonstrates a first step toward systems where tasks are decomposed and split\nbetween untrusted strong LLMs and weak local ones, preserving user privacy."
                },
                "authors": [
                    {
                        "name": "Yubeen Bae"
                    },
                    {
                        "name": "Minchan Kim"
                    },
                    {
                        "name": "Jaejin Lee"
                    },
                    {
                        "name": "Sangbum Kim"
                    },
                    {
                        "name": "Jaehyung Kim"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Niloofar Mireshghallah"
                    }
                ],
                "author_detail": {
                    "name": "Niloofar Mireshghallah"
                },
                "author": "Niloofar Mireshghallah",
                "arxiv_comment": "29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13030v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13030v5",
                "updated": "2025-07-01T16:36:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    48,
                    1,
                    182,
                    0
                ],
                "published": "2025-02-18T16:46:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    16,
                    46,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Conformal Inference under High-Dimensional Covariate Shifts via\n  Likelihood-Ratio Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Inference under High-Dimensional Covariate Shifts via\n  Likelihood-Ratio Regularization"
                },
                "summary": "We consider the problem of conformal prediction under covariate shift. Given\nlabeled data from a source domain and unlabeled data from a covariate shifted\ntarget domain, we seek to construct prediction sets with valid marginal\ncoverage in the target domain. Most existing methods require estimating the\nunknown likelihood ratio function, which can be prohibitive for\nhigh-dimensional data such as images. To address this challenge, we introduce\nthe likelihood ratio regularized quantile regression (LR-QR) algorithm, which\ncombines the pinball loss with a novel choice of regularization in order to\nconstruct a threshold function without directly estimating the unknown\nlikelihood ratio. We show that the LR-QR method has coverage at the desired\nlevel in the target domain, up to a small error term that we can control. Our\nproofs draw on a novel analysis of coverage via stability bounds from learning\ntheory. Our experiments demonstrate that the LR-QR algorithm outperforms\nexisting methods on high-dimensional prediction tasks, including a regression\ntask for the Communities and Crime dataset, an image classification task from\nthe WILDS repository, and an LLM question-answering task on the MMLU benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of conformal prediction under covariate shift. Given\nlabeled data from a source domain and unlabeled data from a covariate shifted\ntarget domain, we seek to construct prediction sets with valid marginal\ncoverage in the target domain. Most existing methods require estimating the\nunknown likelihood ratio function, which can be prohibitive for\nhigh-dimensional data such as images. To address this challenge, we introduce\nthe likelihood ratio regularized quantile regression (LR-QR) algorithm, which\ncombines the pinball loss with a novel choice of regularization in order to\nconstruct a threshold function without directly estimating the unknown\nlikelihood ratio. We show that the LR-QR method has coverage at the desired\nlevel in the target domain, up to a small error term that we can control. Our\nproofs draw on a novel analysis of coverage via stability bounds from learning\ntheory. Our experiments demonstrate that the LR-QR algorithm outperforms\nexisting methods on high-dimensional prediction tasks, including a regression\ntask for the Communities and Crime dataset, an image classification task from\nthe WILDS repository, and an LLM question-answering task on the MMLU benchmark."
                },
                "authors": [
                    {
                        "name": "Sunay Joshi"
                    },
                    {
                        "name": "Shayan Kiyani"
                    },
                    {
                        "name": "George Pappas"
                    },
                    {
                        "name": "Edgar Dobriban"
                    },
                    {
                        "name": "Hamed Hassani"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Hassani"
                },
                "author": "Hamed Hassani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13030v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13030v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24625v2",
                "updated": "2025-07-01T16:26:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    26,
                    47,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-30T14:16:41Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    14,
                    16,
                    41,
                    4,
                    150,
                    0
                ],
                "title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision\n  Geometry Priors"
                },
                "summary": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method, the Video-3D Geometry Large\nLanguage Model (VG LLM). Our approach employs a 3D visual geometry encoder that\nextracts 3D prior information from video sequences. This information is\nintegrated with visual tokens and fed into the MLLM. Extensive experiments have\nshown that our method has achieved substantial improvements in various tasks\nrelated to 3D scene understanding and spatial reasoning, all directly learned\nfrom video sources. Impressively, our 4B model, which does not rely on explicit\n3D data inputs, achieves competitive results compared to existing\nstate-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous research has investigated the application of Multimodal Large\nLanguage Models (MLLMs) in understanding 3D scenes by interpreting them as\nvideos. These approaches generally depend on comprehensive 3D data inputs, such\nas point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,\nwe advance this field by enhancing the capability of MLLMs to understand and\nreason in 3D spaces directly from video data, without the need for additional\n3D input. We propose a novel and efficient method, the Video-3D Geometry Large\nLanguage Model (VG LLM). Our approach employs a 3D visual geometry encoder that\nextracts 3D prior information from video sequences. This information is\nintegrated with visual tokens and fed into the MLLM. Extensive experiments have\nshown that our method has achieved substantial improvements in various tasks\nrelated to 3D scene understanding and spatial reasoning, all directly learned\nfrom video sources. Impressively, our 4B model, which does not rely on explicit\n3D data inputs, achieves competitive results compared to existing\nstate-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the\nVSI-Bench evaluations."
                },
                "authors": [
                    {
                        "name": "Duo Zheng"
                    },
                    {
                        "name": "Shijia Huang"
                    },
                    {
                        "name": "Yanyang Li"
                    },
                    {
                        "name": "Liwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liwei Wang"
                },
                "author": "Liwei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12652v2",
                "updated": "2025-07-01T16:14:02Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    14,
                    2,
                    1,
                    182,
                    0
                ],
                "published": "2025-04-17T05:23:07Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    5,
                    23,
                    7,
                    3,
                    107,
                    0
                ],
                "title": "AdaptoVision: A Multi-Resolution Image Recognition Model for Robust and\n  Scalable Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaptoVision: A Multi-Resolution Image Recognition Model for Robust and\n  Scalable Classification"
                },
                "summary": "This paper introduces AdaptoVision, a novel convolutional neural network\n(CNN) architecture designed to efficiently balance computational complexity and\nclassification accuracy. By leveraging enhanced residual units, depth-wise\nseparable convolutions, and hierarchical skip connections, AdaptoVision\nsignificantly reduces parameter count and computational requirements while\npreserving competitive performance across various benchmark and medical image\ndatasets. Extensive experimentation demonstrates that AdaptoVision achieves\nstate-of-the-art on BreakHis dataset and comparable accuracy levels, notably\n95.3\\% on CIFAR-10 and 85.77\\% on CIFAR-100, without relying on any pretrained\nweights. The model's streamlined architecture and strategic simplifications\npromote effective feature extraction and robust generalization, making it\nparticularly suitable for deployment in real-time and resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces AdaptoVision, a novel convolutional neural network\n(CNN) architecture designed to efficiently balance computational complexity and\nclassification accuracy. By leveraging enhanced residual units, depth-wise\nseparable convolutions, and hierarchical skip connections, AdaptoVision\nsignificantly reduces parameter count and computational requirements while\npreserving competitive performance across various benchmark and medical image\ndatasets. Extensive experimentation demonstrates that AdaptoVision achieves\nstate-of-the-art on BreakHis dataset and comparable accuracy levels, notably\n95.3\\% on CIFAR-10 and 85.77\\% on CIFAR-100, without relying on any pretrained\nweights. The model's streamlined architecture and strategic simplifications\npromote effective feature extraction and robust generalization, making it\nparticularly suitable for deployment in real-time and resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Md. Sanaullah Chowdhury Lameya Sabrin"
                    }
                ],
                "author_detail": {
                    "name": "Md. Sanaullah Chowdhury Lameya Sabrin"
                },
                "author": "Md. Sanaullah Chowdhury Lameya Sabrin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18710v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18710v3",
                "updated": "2025-07-01T15:49:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    49,
                    58,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-23T14:49:01Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    14,
                    49,
                    1,
                    0,
                    174,
                    0
                ],
                "title": "Benchmarking the Pedagogical Knowledge of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the Pedagogical Knowledge of Large Language Models"
                },
                "summary": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions."
                },
                "authors": [
                    {
                        "name": "Maxime Lelivre"
                    },
                    {
                        "name": "Amy Waldock"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Natalia Valds Aspillaga"
                    },
                    {
                        "name": "Alasdair Mackintosh"
                    },
                    {
                        "name": "Mara Jos Ogando Portela"
                    },
                    {
                        "name": "Jared Lee"
                    },
                    {
                        "name": "Paul Atherton"
                    },
                    {
                        "name": "Robin A. A. Ince"
                    },
                    {
                        "name": "Oliver G. B. Garrod"
                    }
                ],
                "author_detail": {
                    "name": "Oliver G. B. Garrod"
                },
                "author": "Oliver G. B. Garrod",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18710v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18710v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12514v2",
                "updated": "2025-07-01T15:33:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    33,
                    56,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-18T18:36:53Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    18,
                    36,
                    53,
                    6,
                    138,
                    0
                ],
                "title": "Reasoning by Superposition: A Theoretical Perspective on Chain of\n  Continuous Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning by Superposition: A Theoretical Perspective on Chain of\n  Continuous Thought"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance in many\napplications, including challenging reasoning problems via chain-of-thoughts\n(CoTs) techniques that generate ``thinking tokens'' before answering the\nquestions. While existing theoretical works demonstrate that CoTs with discrete\ntokens boost the capability of LLMs, recent work on continuous CoTs lacks a\ntheoretical understanding of why it outperforms discrete counterparts in\nvarious reasoning tasks such as directed graph reachability, a fundamental\ngraph reasoning problem that includes many practical domain applications as\nspecial cases. In this paper, we prove that a two-layer transformer with $D$\nsteps of continuous CoTs can solve the directed graph reachability problem,\nwhere $D$ is the diameter of the graph, while the best known result of\nconstant-depth transformers with discrete CoTs requires $O(n^2)$ decoding steps\nwhere $n$ is the number of vertices ($D<n$). In our construction, each\ncontinuous thought vector is a superposition state that encodes multiple search\nfrontiers simultaneously (i.e., parallel breadth-first search (BFS)), while\ndiscrete CoTs must choose a single path sampled from the superposition state,\nwhich leads to sequential search that requires many more steps and may be\ntrapped into local solutions. We also performed extensive experiments to verify\nthat our theoretical construction aligns well with the empirical solution\nobtained via training dynamics. Notably, encoding of multiple search frontiers\nas a superposition state automatically emerges in training continuous CoTs,\nwithout explicit supervision to guide the model to explore multiple paths\nsimultaneously.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance in many\napplications, including challenging reasoning problems via chain-of-thoughts\n(CoTs) techniques that generate ``thinking tokens'' before answering the\nquestions. While existing theoretical works demonstrate that CoTs with discrete\ntokens boost the capability of LLMs, recent work on continuous CoTs lacks a\ntheoretical understanding of why it outperforms discrete counterparts in\nvarious reasoning tasks such as directed graph reachability, a fundamental\ngraph reasoning problem that includes many practical domain applications as\nspecial cases. In this paper, we prove that a two-layer transformer with $D$\nsteps of continuous CoTs can solve the directed graph reachability problem,\nwhere $D$ is the diameter of the graph, while the best known result of\nconstant-depth transformers with discrete CoTs requires $O(n^2)$ decoding steps\nwhere $n$ is the number of vertices ($D<n$). In our construction, each\ncontinuous thought vector is a superposition state that encodes multiple search\nfrontiers simultaneously (i.e., parallel breadth-first search (BFS)), while\ndiscrete CoTs must choose a single path sampled from the superposition state,\nwhich leads to sequential search that requires many more steps and may be\ntrapped into local solutions. We also performed extensive experiments to verify\nthat our theoretical construction aligns well with the empirical solution\nobtained via training dynamics. Notably, encoding of multiple search frontiers\nas a superposition state automatically emerges in training continuous CoTs,\nwithout explicit supervision to guide the model to explore multiple paths\nsimultaneously."
                },
                "authors": [
                    {
                        "name": "Hanlin Zhu"
                    },
                    {
                        "name": "Shibo Hao"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Stuart Russell"
                    },
                    {
                        "name": "Yuandong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yuandong Tian"
                },
                "author": "Yuandong Tian",
                "arxiv_comment": "26 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22698v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22698v2",
                "updated": "2025-07-01T15:26:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    26,
                    29,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-28T00:31:14Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    0,
                    31,
                    14,
                    5,
                    179,
                    0
                ],
                "title": "Text Production and Comprehension by Human and Artificial Intelligence:\n  Interdisciplinary Workshop Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text Production and Comprehension by Human and Artificial Intelligence:\n  Interdisciplinary Workshop Report"
                },
                "summary": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration."
                },
                "authors": [
                    {
                        "name": "Emily Dux Speltz"
                    }
                ],
                "author_detail": {
                    "name": "Emily Dux Speltz"
                },
                "author": "Emily Dux Speltz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22698v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22698v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13759v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13759v2",
                "updated": "2025-07-01T15:08:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    8,
                    58,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-16T17:59:08Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    17,
                    59,
                    8,
                    0,
                    167,
                    0
                ],
                "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete Diffusion in Large Language and Multimodal Models: A Survey"
                },
                "summary": "In this work, we provide a systematic survey of Discrete Diffusion Language\nModels (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).\nUnlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,\nparallel decoding paradigm using full attention and a denoising-based\ngeneration strategy. This paradigm naturally enables parallel generation,\nfine-grained output controllability, and dynamic, response-aware perception.\nThese capabilities are previously difficult to achieve with AR models.\nRecently, a growing number of industrial-scale proprietary d(M)LLMs, as well as\na large number of open-source academic d(M)LLMs, have demonstrated performance\ncomparable to their autoregressive counterparts, while achieving up to 10x\nacceleration in inference speed.\n  The advancement of discrete diffusion LLMs and MLLMs has been largely driven\nby progress in two domains. The first is the development of autoregressive LLMs\nand MLLMs, which has accumulated vast amounts of data, benchmarks, and\nfoundational infrastructure for training and inference. The second contributing\ndomain is the evolution of the mathematical models underlying discrete\ndiffusion. Together, these advancements have catalyzed a surge in dLLMs and\ndMLLMs research in early 2025.\n  In this work, we present a comprehensive overview of the research in the dLLM\nand dMLLM domains. We trace the historical development of dLLMs and dMLLMs,\nformalize the underlying mathematical frameworks, and categorize representative\nmodels. We further analyze key techniques for training and inference, and\nsummarize emerging applications across language, vision-language, and\nbiological domains. We conclude by discussing future directions for research\nand deployment.\n  Paper collection: https://github.com/LiQiiiii/DLLM-Survey",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we provide a systematic survey of Discrete Diffusion Language\nModels (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).\nUnlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,\nparallel decoding paradigm using full attention and a denoising-based\ngeneration strategy. This paradigm naturally enables parallel generation,\nfine-grained output controllability, and dynamic, response-aware perception.\nThese capabilities are previously difficult to achieve with AR models.\nRecently, a growing number of industrial-scale proprietary d(M)LLMs, as well as\na large number of open-source academic d(M)LLMs, have demonstrated performance\ncomparable to their autoregressive counterparts, while achieving up to 10x\nacceleration in inference speed.\n  The advancement of discrete diffusion LLMs and MLLMs has been largely driven\nby progress in two domains. The first is the development of autoregressive LLMs\nand MLLMs, which has accumulated vast amounts of data, benchmarks, and\nfoundational infrastructure for training and inference. The second contributing\ndomain is the evolution of the mathematical models underlying discrete\ndiffusion. Together, these advancements have catalyzed a surge in dLLMs and\ndMLLMs research in early 2025.\n  In this work, we present a comprehensive overview of the research in the dLLM\nand dMLLM domains. We trace the historical development of dLLMs and dMLLMs,\nformalize the underlying mathematical frameworks, and categorize representative\nmodels. We further analyze key techniques for training and inference, and\nsummarize emerging applications across language, vision-language, and\nbiological domains. We conclude by discussing future directions for research\nand deployment.\n  Paper collection: https://github.com/LiQiiiii/DLLM-Survey"
                },
                "authors": [
                    {
                        "name": "Runpeng Yu"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13759v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13759v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16889v2",
                "updated": "2025-07-01T15:08:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    8,
                    41,
                    1,
                    182,
                    0
                ],
                "published": "2025-02-24T06:40:18Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    40,
                    18,
                    0,
                    55,
                    0
                ],
                "title": "Beyond Diagnostic Performance: Revealing and Quantifying Ethical Risks\n  in Pathology Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Diagnostic Performance: Revealing and Quantifying Ethical Risks\n  in Pathology Foundation Models"
                },
                "summary": "Pathology foundation models (PFMs), as large-scale pre-trained models\ntailored for computational pathology, have significantly advanced a wide range\nof applications. Their ability to leverage prior knowledge from massive\ndatasets has streamlined the development of intelligent pathology models.\nHowever, we identify several critical and interrelated ethical risks that\nremain underexplored, yet must be addressed to enable the safe translation of\nPFMs from lab to clinic. These include the potential leakage of\npatient-sensitive attributes, disparities in model performance across\ndemographic and institutional subgroups, and the reliance on\ndiagnosis-irrelevant features that undermine clinical reliability. In this\nstudy, we pioneer the quantitative analysis for ethical risks in PFMs,\nincluding privacy leakage, clinical reliability, and group fairness.\nSpecifically, we propose an evaluation framework that systematically measures\nkey dimensions of ethical concern: the degree to which patient-sensitive\nattributes can be inferred from model representations, the extent of\nperformance disparities across demographic and institutional subgroups, and the\ninfluence of diagnostically irrelevant features on model decisions. We further\ninvestigate the underlying causes of these ethical risks in PFMs and\nempirically validate our findings. Then we offer insights into potential\ndirections for mitigating such risks, aiming to inform the development of more\nethically robust PFMs. This work provides the first quantitative and systematic\nevaluation of ethical risks in PFMs. Our findings highlight the urgent need for\nethical safeguards in PFMs and offer actionable insights for building more\ntrustworthy and clinically robust PFMs. To facilitate future research and\ndeployment, we will release the assessment framework as an online toolkit to\nsupport the development, auditing, and deployment of ethically robust PFMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pathology foundation models (PFMs), as large-scale pre-trained models\ntailored for computational pathology, have significantly advanced a wide range\nof applications. Their ability to leverage prior knowledge from massive\ndatasets has streamlined the development of intelligent pathology models.\nHowever, we identify several critical and interrelated ethical risks that\nremain underexplored, yet must be addressed to enable the safe translation of\nPFMs from lab to clinic. These include the potential leakage of\npatient-sensitive attributes, disparities in model performance across\ndemographic and institutional subgroups, and the reliance on\ndiagnosis-irrelevant features that undermine clinical reliability. In this\nstudy, we pioneer the quantitative analysis for ethical risks in PFMs,\nincluding privacy leakage, clinical reliability, and group fairness.\nSpecifically, we propose an evaluation framework that systematically measures\nkey dimensions of ethical concern: the degree to which patient-sensitive\nattributes can be inferred from model representations, the extent of\nperformance disparities across demographic and institutional subgroups, and the\ninfluence of diagnostically irrelevant features on model decisions. We further\ninvestigate the underlying causes of these ethical risks in PFMs and\nempirically validate our findings. Then we offer insights into potential\ndirections for mitigating such risks, aiming to inform the development of more\nethically robust PFMs. This work provides the first quantitative and systematic\nevaluation of ethical risks in PFMs. Our findings highlight the urgent need for\nethical safeguards in PFMs and offer actionable insights for building more\ntrustworthy and clinically robust PFMs. To facilitate future research and\ndeployment, we will release the assessment framework as an online toolkit to\nsupport the development, auditing, and deployment of ethically robust PFMs."
                },
                "authors": [
                    {
                        "name": "Weiping Lin"
                    },
                    {
                        "name": "Shen Liu"
                    },
                    {
                        "name": "Runchen Zhu"
                    },
                    {
                        "name": "Yixuan Lin"
                    },
                    {
                        "name": "Baoshun Wang"
                    },
                    {
                        "name": "Liansheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liansheng Wang"
                },
                "author": "Liansheng Wang",
                "arxiv_comment": "33 pages,5 figure,23 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09310v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09310v2",
                "updated": "2025-07-01T14:55:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    55,
                    5,
                    1,
                    182,
                    0
                ],
                "published": "2025-01-16T05:54:59Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    5,
                    54,
                    59,
                    3,
                    16,
                    0
                ],
                "title": "A Study of In-Context-Learning-Based Text-to-SQL Errors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Study of In-Context-Learning-Based Text-to-SQL Errors"
                },
                "summary": "Large language models (LLMs) have been adopted to perform text-to-SQL tasks,\nutilizing their in-context learning (ICL) capability to translate natural\nlanguage questions into structured query language (SQL). However, such a\ntechnique faces correctness problems and requires efficient repairing\nsolutions. In this paper, we conduct the first comprehensive study of\ntext-to-SQL errors. Our study covers four representative ICL-based techniques,\nfive basic repairing methods, two benchmarks, and two LLM settings. We find\nthat text-to-SQL errors are widespread and summarize 29 error types of 7\ncategories. We also find that existing repairing attempts have limited\ncorrectness improvement at the cost of high computational overhead with many\nmis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL\nerror detection and repairing framework. The evaluation demonstrates that\nMapleRepair outperforms existing solutions by repairing 13.8% more queries with\nneglectable mis-repairs and 67.4% less overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been adopted to perform text-to-SQL tasks,\nutilizing their in-context learning (ICL) capability to translate natural\nlanguage questions into structured query language (SQL). However, such a\ntechnique faces correctness problems and requires efficient repairing\nsolutions. In this paper, we conduct the first comprehensive study of\ntext-to-SQL errors. Our study covers four representative ICL-based techniques,\nfive basic repairing methods, two benchmarks, and two LLM settings. We find\nthat text-to-SQL errors are widespread and summarize 29 error types of 7\ncategories. We also find that existing repairing attempts have limited\ncorrectness improvement at the cost of high computational overhead with many\nmis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL\nerror detection and repairing framework. The evaluation demonstrates that\nMapleRepair outperforms existing solutions by repairing 13.8% more queries with\nneglectable mis-repairs and 67.4% less overhead."
                },
                "authors": [
                    {
                        "name": "Jiawei Shen"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Ruoyi Qiao"
                    },
                    {
                        "name": "Jiazhen Zou"
                    },
                    {
                        "name": "Hang Xu"
                    },
                    {
                        "name": "Yuchen Shao"
                    },
                    {
                        "name": "Yueling Zhang"
                    },
                    {
                        "name": "Weikai Miao"
                    },
                    {
                        "name": "Geguang Pu"
                    }
                ],
                "author_detail": {
                    "name": "Geguang Pu"
                },
                "author": "Geguang Pu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09310v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09310v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20170v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20170v2",
                "updated": "2025-07-01T14:53:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    53,
                    43,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-26T16:12:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    16,
                    12,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "Program of Equations Thoughts to Solve Algebra Word Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Program of Equations Thoughts to Solve Algebra Word Problems"
                },
                "summary": "Solving algebraic word problems (AWPs) has recently emerged as an important\nnatural language processing task. Recently, large language models (LLMs) have\ndemonstrated powerful mathematical capabilities, and the Chain-of-Thought\ntechnique, which guides LLMs through step-by-step reasoning, has yielded\nimpressive results. However, this reasoning ability is limited by the\ncomputational weaknesses of LLMs themselves, where calculation errors can\naccumulate, leading to incorrect final answers. To address this, we propose\nProgram of Equations Thoughts (POET), which transforms the task of generating\nstep-by-step reasoning answers into a two-stage task of predicting equations\nand generating code, offloading complex computations to a Python interpreter to\navoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which\nutilizes a manually designed template to enable LLMs to directly generate\nPython code for one-step solving. Our method achieves accuracies of 95.3% and\n98.0% on the PEN and ALG514 datasets, respectively, setting a new\nstate-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5%\non the DRAW-1K dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving algebraic word problems (AWPs) has recently emerged as an important\nnatural language processing task. Recently, large language models (LLMs) have\ndemonstrated powerful mathematical capabilities, and the Chain-of-Thought\ntechnique, which guides LLMs through step-by-step reasoning, has yielded\nimpressive results. However, this reasoning ability is limited by the\ncomputational weaknesses of LLMs themselves, where calculation errors can\naccumulate, leading to incorrect final answers. To address this, we propose\nProgram of Equations Thoughts (POET), which transforms the task of generating\nstep-by-step reasoning answers into a two-stage task of predicting equations\nand generating code, offloading complex computations to a Python interpreter to\navoid calculation errors in LLMs. Furthermore, we propose Zero-shot POET, which\nutilizes a manually designed template to enable LLMs to directly generate\nPython code for one-step solving. Our method achieves accuracies of 95.3% and\n98.0% on the PEN and ALG514 datasets, respectively, setting a new\nstate-of-the-art (SOTA). Zero-shot POET also achieves the SOTA result of 95.5%\non the DRAW-1K dataset."
                },
                "authors": [
                    {
                        "name": "Yunze Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yunze Lin"
                },
                "author": "Yunze Lin",
                "arxiv_comment": "Withdrawn pending institutional authorization and core revisions to\n  address methodological inconsistencies in Sections 3-4",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20170v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20170v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.08271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.08271v2",
                "updated": "2025-07-01T14:52:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    52,
                    33,
                    1,
                    182,
                    0
                ],
                "published": "2025-03-11T10:40:39Z",
                "published_parsed": [
                    2025,
                    3,
                    11,
                    10,
                    40,
                    39,
                    1,
                    70,
                    0
                ],
                "title": "LangTime: A Language-Guided Unified Model for Time Series Forecasting\n  with Proximal Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LangTime: A Language-Guided Unified Model for Time Series Forecasting\n  with Proximal Policy Optimization"
                },
                "summary": "Recent research has shown an increasing interest in utilizing pre-trained\nlarge language models (LLMs) for a variety of time series applications.\nHowever, there are three main challenges when using LLMs as foundational models\nfor time series forecasting: (1) Cross-domain generalization. (2)\nCross-modality alignment. (3) Error accumulation in autoregressive frameworks.\nTo address these challenges, we proposed LangTime, a language-guided unified\nmodel for time series forecasting that incorporates cross-domain pre-training\nwith reinforcement learning-based fine-tuning. Specifically, LangTime\nconstructs Temporal Comprehension Prompts (TCPs), which include dataset-wise\nand channel-wise instructions, to facilitate domain adaptation and condense\ntime series into a single token, enabling LLMs to understand better and align\ntemporal data. To improve autoregressive forecasting, we introduce TimePPO, a\nreinforcement learning-based fine-tuning algorithm. TimePPO mitigates error\naccumulation by leveraging a multidimensional rewards function tailored for\ntime series and a repeat-based value estimation strategy. Extensive experiments\ndemonstrate that LangTime achieves state-of-the-art cross-domain forecasting\nperformance, while TimePPO fine-tuning effectively enhances the stability and\naccuracy of autoregressive forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown an increasing interest in utilizing pre-trained\nlarge language models (LLMs) for a variety of time series applications.\nHowever, there are three main challenges when using LLMs as foundational models\nfor time series forecasting: (1) Cross-domain generalization. (2)\nCross-modality alignment. (3) Error accumulation in autoregressive frameworks.\nTo address these challenges, we proposed LangTime, a language-guided unified\nmodel for time series forecasting that incorporates cross-domain pre-training\nwith reinforcement learning-based fine-tuning. Specifically, LangTime\nconstructs Temporal Comprehension Prompts (TCPs), which include dataset-wise\nand channel-wise instructions, to facilitate domain adaptation and condense\ntime series into a single token, enabling LLMs to understand better and align\ntemporal data. To improve autoregressive forecasting, we introduce TimePPO, a\nreinforcement learning-based fine-tuning algorithm. TimePPO mitigates error\naccumulation by leveraging a multidimensional rewards function tailored for\ntime series and a repeat-based value estimation strategy. Extensive experiments\ndemonstrate that LangTime achieves state-of-the-art cross-domain forecasting\nperformance, while TimePPO fine-tuning effectively enhances the stability and\naccuracy of autoregressive forecasting."
                },
                "authors": [
                    {
                        "name": "Wenzhe Niu"
                    },
                    {
                        "name": "Zongxia Xie"
                    },
                    {
                        "name": "Yanru Sun"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Man Xu"
                    },
                    {
                        "name": "Chao Hao"
                    }
                ],
                "author_detail": {
                    "name": "Chao Hao"
                },
                "author": "Chao Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.08271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.08271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14649v2",
                "updated": "2025-07-01T14:41:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    41,
                    8,
                    1,
                    182,
                    0
                ],
                "published": "2024-10-18T17:46:37Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    46,
                    37,
                    4,
                    292,
                    0
                ],
                "title": "EvoPress: Accurate Dynamic Model Compression via Evolutionary Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoPress: Accurate Dynamic Model Compression via Evolutionary Search"
                },
                "summary": "The high computational costs of large language models (LLMs) have led to a\nflurry of research on LLM compression, via methods such as quantization,\nsparsification, or structured pruning. A new frontier in this area is given by\ndynamic, non-uniform compression methods, which adjust the compression levels\n(e.g., sparsity) per-block or even per-layer in order to minimize accuracy\nloss, while guaranteeing a global compression threshold. Yet, current methods\nrely on estimating the importance of a given layer, implicitly assuming that\nlayers contribute independently to the overall compression error. We begin from\nthe motivating observation that this independence assumption does not generally\nhold for LLM compression: pruning a model further may even significantly\nrecover performance. To address this, we propose EvoPress, a novel evolutionary\nframework for dynamic LLM compression. By formulating dynamic compression as a\ngeneral optimization problem, EvoPress identifies optimal compression profiles\nin a highly efficient manner, and generalizes across diverse models and\ncompression techniques. Via EvoPress, we achieve state-of-the-art performance\nfor dynamic compression of Llama, Mistral, and Phi models, setting new\nbenchmarks for structural pruning (block/layer dropping), unstructured\nsparsity, and quantization with dynamic bitwidths. Our code is available at\nhttps://github.com/IST-DASLab/EvoPress}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high computational costs of large language models (LLMs) have led to a\nflurry of research on LLM compression, via methods such as quantization,\nsparsification, or structured pruning. A new frontier in this area is given by\ndynamic, non-uniform compression methods, which adjust the compression levels\n(e.g., sparsity) per-block or even per-layer in order to minimize accuracy\nloss, while guaranteeing a global compression threshold. Yet, current methods\nrely on estimating the importance of a given layer, implicitly assuming that\nlayers contribute independently to the overall compression error. We begin from\nthe motivating observation that this independence assumption does not generally\nhold for LLM compression: pruning a model further may even significantly\nrecover performance. To address this, we propose EvoPress, a novel evolutionary\nframework for dynamic LLM compression. By formulating dynamic compression as a\ngeneral optimization problem, EvoPress identifies optimal compression profiles\nin a highly efficient manner, and generalizes across diverse models and\ncompression techniques. Via EvoPress, we achieve state-of-the-art performance\nfor dynamic compression of Llama, Mistral, and Phi models, setting new\nbenchmarks for structural pruning (block/layer dropping), unstructured\nsparsity, and quantization with dynamic bitwidths. Our code is available at\nhttps://github.com/IST-DASLab/EvoPress}."
                },
                "authors": [
                    {
                        "name": "Oliver Sieberling"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "ICML camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04388v3",
                "updated": "2025-07-01T14:33:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    33,
                    24,
                    1,
                    182,
                    0
                ],
                "published": "2025-02-05T22:20:15Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    22,
                    20,
                    15,
                    2,
                    36,
                    0
                ],
                "title": "Position: Emergent Machina Sapiens Urge Rethinking Multi-Agent Paradigms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Position: Emergent Machina Sapiens Urge Rethinking Multi-Agent Paradigms"
                },
                "summary": "Artificial Intelligence (AI) agents capable of autonomous learning and\nindependent decision-making hold great promise for addressing complex\nchallenges across various critical infrastructure domains, including\ntransportation, energy systems, and manufacturing. However, the surge in the\ndesign and deployment of AI systems, driven by various stakeholders with\ndistinct and unaligned objectives, introduces a crucial challenge: How can\nuncoordinated AI systems coexist and evolve harmoniously in shared environments\nwithout creating chaos or compromising safety? To address this, we advocate for\na fundamental rethinking of existing multi-agent frameworks, such as\nmulti-agent systems and game theory, which are largely limited to predefined\nrules and static objective structures. We posit that AI agents should be\nempowered to adjust their objectives dynamically, make compromises, form\ncoalitions, and safely compete or cooperate through evolving relationships and\nsocial feedback. Through two case studies in critical infrastructure\napplications, we call for a shift toward the emergent, self-organizing, and\ncontext-aware nature of these multi-agentic AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Intelligence (AI) agents capable of autonomous learning and\nindependent decision-making hold great promise for addressing complex\nchallenges across various critical infrastructure domains, including\ntransportation, energy systems, and manufacturing. However, the surge in the\ndesign and deployment of AI systems, driven by various stakeholders with\ndistinct and unaligned objectives, introduces a crucial challenge: How can\nuncoordinated AI systems coexist and evolve harmoniously in shared environments\nwithout creating chaos or compromising safety? To address this, we advocate for\na fundamental rethinking of existing multi-agent frameworks, such as\nmulti-agent systems and game theory, which are largely limited to predefined\nrules and static objective structures. We posit that AI agents should be\nempowered to adjust their objectives dynamically, make compromises, form\ncoalitions, and safely compete or cooperate through evolving relationships and\nsocial feedback. Through two case studies in critical infrastructure\napplications, we call for a shift toward the emergent, self-organizing, and\ncontext-aware nature of these multi-agentic AI systems."
                },
                "authors": [
                    {
                        "name": "Hepeng Li"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Jun Yan"
                    },
                    {
                        "name": "Jie Gao"
                    },
                    {
                        "name": "Xiaoou Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoou Yang"
                },
                "author": "Xiaoou Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09438v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09438v2",
                "updated": "2025-07-01T14:16:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    14,
                    16,
                    43,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-14T14:46:32Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    46,
                    32,
                    2,
                    134,
                    0
                ],
                "title": "Evaluating GPT- and Reasoning-based Large Language Models on Physics\n  Olympiad Problems: Surpassing Human Performance and Implications for\n  Educational Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating GPT- and Reasoning-based Large Language Models on Physics\n  Olympiad Problems: Surpassing Human Performance and Implications for\n  Educational Assessment"
                },
                "summary": "Large language models (LLMs) are now widely accessible, reaching learners at\nall educational levels. This development has raised concerns that their use may\ncircumvent essential learning processes and compromise the integrity of\nestablished assessment formats. In physics education, where problem solving\nplays a central role in instruction and assessment, it is therefore essential\nto understand the physics-specific problem-solving capabilities of LLMs. Such\nunderstanding is key to informing responsible and pedagogically sound\napproaches to integrating LLMs into instruction and assessment. This study\ntherefore compares the problem-solving performance of a general-purpose LLM\n(GPT-4o, using varying prompting techniques) and a reasoning-optimized model\n(o1-preview) with that of participants of the German Physics Olympiad, based on\na set of well-defined Olympiad problems. In addition to evaluating the\ncorrectness of the generated solutions, the study analyzes characteristic\nstrengths and limitations of LLM-generated solutions. The findings of this\nstudy indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate\nadvanced problem-solving capabilities on Olympiad-type physics problems, on\naverage outperforming the human participants. Prompting techniques had little\neffect on GPT-4o's performance, while o1-preview almost consistently\noutperformed both GPT-4o and the human benchmark. Based on these findings, the\nstudy discusses implications for the design of summative and formative\nassessment in physics education, including how to uphold assessment integrity\nand support students in critically engaging with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are now widely accessible, reaching learners at\nall educational levels. This development has raised concerns that their use may\ncircumvent essential learning processes and compromise the integrity of\nestablished assessment formats. In physics education, where problem solving\nplays a central role in instruction and assessment, it is therefore essential\nto understand the physics-specific problem-solving capabilities of LLMs. Such\nunderstanding is key to informing responsible and pedagogically sound\napproaches to integrating LLMs into instruction and assessment. This study\ntherefore compares the problem-solving performance of a general-purpose LLM\n(GPT-4o, using varying prompting techniques) and a reasoning-optimized model\n(o1-preview) with that of participants of the German Physics Olympiad, based on\na set of well-defined Olympiad problems. In addition to evaluating the\ncorrectness of the generated solutions, the study analyzes characteristic\nstrengths and limitations of LLM-generated solutions. The findings of this\nstudy indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate\nadvanced problem-solving capabilities on Olympiad-type physics problems, on\naverage outperforming the human participants. Prompting techniques had little\neffect on GPT-4o's performance, while o1-preview almost consistently\noutperformed both GPT-4o and the human benchmark. Based on these findings, the\nstudy discusses implications for the design of summative and formative\nassessment in physics education, including how to uphold assessment integrity\nand support students in critically engaging with LLMs."
                },
                "authors": [
                    {
                        "name": "Paul Tschisgale"
                    },
                    {
                        "name": "Holger Maus"
                    },
                    {
                        "name": "Fabian Kieser"
                    },
                    {
                        "name": "Ben Kroehs"
                    },
                    {
                        "name": "Stefan Petersen"
                    },
                    {
                        "name": "Peter Wulff"
                    }
                ],
                "author_detail": {
                    "name": "Peter Wulff"
                },
                "author": "Peter Wulff",
                "arxiv_doi": "10.1103/6fmx-bsnl",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/6fmx-bsnl",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.09438v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09438v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16211v2",
                "updated": "2025-07-01T13:22:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    13,
                    22,
                    7,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-22T04:27:46Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    4,
                    27,
                    46,
                    3,
                    142,
                    0
                ],
                "title": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large\n  Language Models"
                },
                "summary": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement and expanding applications of Audio Large Language\nModels (ALLMs) demand a rigorous understanding of their trustworthiness.\nHowever, systematic research on evaluating these models, particularly\nconcerning risks unique to the audio modality, remains largely unexplored.\nExisting evaluation frameworks primarily focus on the text modality or address\nonly a restricted set of safety dimensions, failing to adequately account for\nthe unique characteristics and application scenarios inherent to the audio\nmodality. We introduce AudioTrust-the first multifaceted trustworthiness\nevaluation framework and benchmark specifically designed for ALLMs. AudioTrust\nfacilitates assessments across six key dimensions: fairness, hallucination,\nsafety, privacy, robustness, and authentication. To comprehensively evaluate\nthese dimensions, AudioTrust is structured around 18 distinct experimental\nsetups. Its core is a meticulously constructed dataset of over 4,420 audio/text\nsamples, drawn from real-world scenarios (e.g., daily conversations, emergency\ncalls, voice assistant interactions), specifically designed to probe the\nmultifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully\ndesigns 9 audio-specific evaluation metrics, and we employ a large-scale\nautomated pipeline for objective and scalable scoring of model outputs.\nExperimental results reveal the trustworthiness boundaries and limitations of\ncurrent state-of-the-art open-source and closed-source ALLMs when confronted\nwith various high-risk audio scenarios, offering valuable insights for the\nsecure and trustworthy deployment of future audio models. Our platform and\nbenchmark are available at https://github.com/JusperLee/AudioTrust."
                },
                "authors": [
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Can Shen"
                    },
                    {
                        "name": "Yile Liu"
                    },
                    {
                        "name": "Jirui Han"
                    },
                    {
                        "name": "Kelong Zheng"
                    },
                    {
                        "name": "Xuechao Zou"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Xingjian Du"
                    },
                    {
                        "name": "Shun Zhang"
                    },
                    {
                        "name": "Hanjun Luo"
                    },
                    {
                        "name": "Yingbin Jin"
                    },
                    {
                        "name": "Xinxin Xing"
                    },
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xiaobin Zhuang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haibo Hu"
                    },
                    {
                        "name": "Zhizheng Wu"
                    },
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Eng-Siong Chng"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Wenyuan Xu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Xinfeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Xinfeng Li"
                },
                "author": "Xinfeng Li",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11620v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11620v3",
                "updated": "2025-07-01T12:15:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    12,
                    15,
                    44,
                    1,
                    182,
                    0
                ],
                "published": "2025-02-17T10:03:01Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    10,
                    3,
                    1,
                    0,
                    48,
                    0
                ],
                "title": "Assessing Correctness in LLM-Based Code Generation via Uncertainty\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Correctness in LLM-Based Code Generation via Uncertainty\n  Estimation"
                },
                "summary": "In this work, we explore uncertainty estimation as a proxy for correctness in\nLLM-generated code. To this end, we adapt two state-of-the-art techniques from\nnatural language generation -- one based on entropy and another on mutual\ninformation -- to the domain of code generation. Given the distinct semantic\nproperties of code, we introduce modifications, including a semantic\nequivalence check based on symbolic execution. Our findings indicate a strong\ncorrelation between the uncertainty computed through these techniques and\ncorrectness, highlighting the potential of uncertainty estimation for quality\nassessment. Additionally, we propose a simplified version of the entropy-based\nmethod that assumes a uniform distribution over the LLM's responses,\ndemonstrating comparable effectiveness. Using these techniques, we develop an\nabstention policy that prevents the model from making predictions when\nuncertainty is high, reducing incorrect outputs to near zero. Our evaluation on\nthe LiveCodeBench shows that our approach significantly outperforms a baseline\nrelying solely on LLM-reported log-probabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explore uncertainty estimation as a proxy for correctness in\nLLM-generated code. To this end, we adapt two state-of-the-art techniques from\nnatural language generation -- one based on entropy and another on mutual\ninformation -- to the domain of code generation. Given the distinct semantic\nproperties of code, we introduce modifications, including a semantic\nequivalence check based on symbolic execution. Our findings indicate a strong\ncorrelation between the uncertainty computed through these techniques and\ncorrectness, highlighting the potential of uncertainty estimation for quality\nassessment. Additionally, we propose a simplified version of the entropy-based\nmethod that assumes a uniform distribution over the LLM's responses,\ndemonstrating comparable effectiveness. Using these techniques, we develop an\nabstention policy that prevents the model from making predictions when\nuncertainty is high, reducing incorrect outputs to near zero. Our evaluation on\nthe LiveCodeBench shows that our approach significantly outperforms a baseline\nrelying solely on LLM-reported log-probabilities."
                },
                "authors": [
                    {
                        "name": "Arindam Sharma"
                    },
                    {
                        "name": "Cristina David"
                    }
                ],
                "author_detail": {
                    "name": "Cristina David"
                },
                "author": "Cristina David",
                "arxiv_comment": "18 pages and 3 References Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11620v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11620v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17080v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17080v2",
                "updated": "2025-07-01T11:26:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    11,
                    26,
                    7,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-20T08:49:18Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    8,
                    49,
                    18,
                    1,
                    140,
                    0
                ],
                "title": "Not Minds, but Signs: Reframing LLMs through Semiotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not Minds, but Signs: Reframing LLMs through Semiotics"
                },
                "summary": "This paper challenges the prevailing tendency to frame Large Language Models\n(LLMs) as cognitive systems, arguing instead for a semiotic perspective that\nsituates these models within the broader dynamics of sign manipulation and\nmeaning-making. Rather than assuming that LLMs understand language or simulate\nhuman thought, we propose that their primary function is to recombine,\nrecontextualize, and circulate linguistic forms based on probabilistic\nassociations. By shifting from a cognitivist to a semiotic framework, we avoid\nanthropomorphism and gain a more precise understanding of how LLMs participate\nin cultural processes, not by thinking, but by generating texts that invite\ninterpretation. Through theoretical analysis and practical examples, the paper\ndemonstrates how LLMs function as semiotic agents whose outputs can be treated\nas interpretive acts, open to contextual negotiation and critical reflection.\nWe explore applications in literature, philosophy, education, and cultural\nproduction, emphasizing how LLMs can serve as tools for creativity, dialogue,\nand critical inquiry. The semiotic paradigm foregrounds the situated,\ncontingent, and socially embedded nature of meaning, offering a more rigorous\nand ethically aware framework for studying and using LLMs. Ultimately, this\napproach reframes LLMs as technological participants in an ongoing ecology of\nsigns. They do not possess minds, but they alter how we read, write, and make\nmeaning, compelling us to reconsider the foundations of language,\ninterpretation, and the role of artificial systems in the production of\nknowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper challenges the prevailing tendency to frame Large Language Models\n(LLMs) as cognitive systems, arguing instead for a semiotic perspective that\nsituates these models within the broader dynamics of sign manipulation and\nmeaning-making. Rather than assuming that LLMs understand language or simulate\nhuman thought, we propose that their primary function is to recombine,\nrecontextualize, and circulate linguistic forms based on probabilistic\nassociations. By shifting from a cognitivist to a semiotic framework, we avoid\nanthropomorphism and gain a more precise understanding of how LLMs participate\nin cultural processes, not by thinking, but by generating texts that invite\ninterpretation. Through theoretical analysis and practical examples, the paper\ndemonstrates how LLMs function as semiotic agents whose outputs can be treated\nas interpretive acts, open to contextual negotiation and critical reflection.\nWe explore applications in literature, philosophy, education, and cultural\nproduction, emphasizing how LLMs can serve as tools for creativity, dialogue,\nand critical inquiry. The semiotic paradigm foregrounds the situated,\ncontingent, and socially embedded nature of meaning, offering a more rigorous\nand ethically aware framework for studying and using LLMs. Ultimately, this\napproach reframes LLMs as technological participants in an ongoing ecology of\nsigns. They do not possess minds, but they alter how we read, write, and make\nmeaning, compelling us to reconsider the foundations of language,\ninterpretation, and the role of artificial systems in the production of\nknowledge."
                },
                "authors": [
                    {
                        "name": "Davide Picca"
                    }
                ],
                "author_detail": {
                    "name": "Davide Picca"
                },
                "author": "Davide Picca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17080v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17080v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06657v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06657v3",
                "updated": "2025-07-01T11:12:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    11,
                    12,
                    37,
                    1,
                    182,
                    0
                ],
                "published": "2024-01-12T16:05:13Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    16,
                    5,
                    13,
                    4,
                    12,
                    0
                ],
                "title": "How Resilient is QUIC to Security and Privacy Attacks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Resilient is QUIC to Security and Privacy Attacks?"
                },
                "summary": "QUIC has rapidly evolved into a cornerstone transport protocol for secure,\nlow-latency communications, yet its deployment continues to expose critical\nsecurity and privacy vulnerabilities, particularly during connection\nestablishment phases and via traffic analysis. This paper systematically\nrevisits a comprehensive set of attacks on QUIC and emerging privacy threats.\nBuilding upon these observations, we critically analyze recent IETF mitigation\nefforts, including TLS Encrypted Client Hello (ECH), Oblivious HTTP (OHTTP) and\nMASQUE. We analyze how these mechanisms enhance privacy while introducing new\noperational risks, particularly under adversarial load. Additionally, we\ndiscuss emerging challenges posed by post-quantum cryptographic (PQC)\nhandshakes, including handshake expansion and metadata leakage risks. Our\nanalysis highlights ongoing gaps between theoretical defenses and practical\ndeployments, and proposes new research directions focused on adaptive privacy\nmechanisms. Building on these insights, we propose future directions to ensure\nlong-term security of QUIC and aim to guide its evolution as a robust,\nprivacy-preserving, and resilient transport foundation for the next-generation\nInternet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUIC has rapidly evolved into a cornerstone transport protocol for secure,\nlow-latency communications, yet its deployment continues to expose critical\nsecurity and privacy vulnerabilities, particularly during connection\nestablishment phases and via traffic analysis. This paper systematically\nrevisits a comprehensive set of attacks on QUIC and emerging privacy threats.\nBuilding upon these observations, we critically analyze recent IETF mitigation\nefforts, including TLS Encrypted Client Hello (ECH), Oblivious HTTP (OHTTP) and\nMASQUE. We analyze how these mechanisms enhance privacy while introducing new\noperational risks, particularly under adversarial load. Additionally, we\ndiscuss emerging challenges posed by post-quantum cryptographic (PQC)\nhandshakes, including handshake expansion and metadata leakage risks. Our\nanalysis highlights ongoing gaps between theoretical defenses and practical\ndeployments, and proposes new research directions focused on adaptive privacy\nmechanisms. Building on these insights, we propose future directions to ensure\nlong-term security of QUIC and aim to guide its evolution as a robust,\nprivacy-preserving, and resilient transport foundation for the next-generation\nInternet."
                },
                "authors": [
                    {
                        "name": "Jayasree Sengupta"
                    },
                    {
                        "name": "Debasmita Dey"
                    },
                    {
                        "name": "Simone Ferlin-Reiter"
                    },
                    {
                        "name": "Nirnay Ghosh"
                    },
                    {
                        "name": "Vaibhav Bajpai"
                    }
                ],
                "author_detail": {
                    "name": "Vaibhav Bajpai"
                },
                "author": "Vaibhav Bajpai",
                "arxiv_comment": "7 pages, 1 figure, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06657v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06657v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18568v2",
                "updated": "2025-07-01T10:16:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    10,
                    16,
                    5,
                    1,
                    182,
                    0
                ],
                "published": "2025-03-24T11:24:37Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    24,
                    37,
                    0,
                    83,
                    0
                ],
                "title": "A generalisable data-augmented turbulence model with progressive and\n  interpretable corrections for incompressible wall-bounded flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalisable data-augmented turbulence model with progressive and\n  interpretable corrections for incompressible wall-bounded flows"
                },
                "summary": "The integration of interpretability and generalisability in data-driven\nturbulence modelling remains a fundamental challenge for computational fluid\ndynamics applications. This study yields a generalisable advancement of the\n$k$-$\\omega$ Shear Stress Transport (SST) model through a progressive\ndata-augmented framework, combining Bayesian optimisation with physics-guided\ncorrections to improve the predictions of anisotropy-induced secondary flows\nand flow separation simultaneously. Two interpretable modifications are\nsystematically embedded: 1) a non-linear Reynolds stress anisotropy correction\nto enhance secondary flow predictions, and 2) an activation-based separation\ncorrection in the $\\omega$-equation, regulated by an optimised power-law\nfunction to locally adjust turbulent viscosity under adverse pressure\ngradients. The model is trained using a multi-case computational fluid\ndynamics-driven a posteriori approach, incorporating periodic hills, duct flow,\nand channel flow to balance correction efficacy with baseline consistency.\nValidation across multiple unseen cases -- spanning flat-plate boundary layers,\nhigh-Reynolds-number periodic hills, and flow over diverse obstacle\nconfigurations -- demonstrates enhanced accuracy in velocity profiles,\nrecirculation zones, streamwise vorticity, and skin friction distributions\nwhile retaining the robustness of the original $k$-$\\omega$ SST in attached\nflows. Sparsity-enforced regression ensures reduced parametric complexity,\npreserving computational efficiency and physical transparency. Results\nunderscore the framework's ability to generalise across geometries and Reynolds\nnumbers without destabilising corrections, offering a validated framework\ntoward deployable, data-augmented turbulence models for numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of interpretability and generalisability in data-driven\nturbulence modelling remains a fundamental challenge for computational fluid\ndynamics applications. This study yields a generalisable advancement of the\n$k$-$\\omega$ Shear Stress Transport (SST) model through a progressive\ndata-augmented framework, combining Bayesian optimisation with physics-guided\ncorrections to improve the predictions of anisotropy-induced secondary flows\nand flow separation simultaneously. Two interpretable modifications are\nsystematically embedded: 1) a non-linear Reynolds stress anisotropy correction\nto enhance secondary flow predictions, and 2) an activation-based separation\ncorrection in the $\\omega$-equation, regulated by an optimised power-law\nfunction to locally adjust turbulent viscosity under adverse pressure\ngradients. The model is trained using a multi-case computational fluid\ndynamics-driven a posteriori approach, incorporating periodic hills, duct flow,\nand channel flow to balance correction efficacy with baseline consistency.\nValidation across multiple unseen cases -- spanning flat-plate boundary layers,\nhigh-Reynolds-number periodic hills, and flow over diverse obstacle\nconfigurations -- demonstrates enhanced accuracy in velocity profiles,\nrecirculation zones, streamwise vorticity, and skin friction distributions\nwhile retaining the robustness of the original $k$-$\\omega$ SST in attached\nflows. Sparsity-enforced regression ensures reduced parametric complexity,\npreserving computational efficiency and physical transparency. Results\nunderscore the framework's ability to generalise across geometries and Reynolds\nnumbers without destabilising corrections, offering a validated framework\ntoward deployable, data-augmented turbulence models for numerical simulations."
                },
                "authors": [
                    {
                        "name": "Mario J. Rincn"
                    },
                    {
                        "name": "Martino Reclari"
                    },
                    {
                        "name": "Xiang I. A. Yang"
                    },
                    {
                        "name": "Mahdi Abkar"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Abkar"
                },
                "author": "Mahdi Abkar",
                "arxiv_comment": "Peer-reviewed version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19676v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19676v3",
                "updated": "2025-07-02T08:50:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    2,
                    8,
                    50,
                    11,
                    2,
                    183,
                    0
                ],
                "published": "2025-06-24T14:44:28Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    14,
                    44,
                    28,
                    1,
                    175,
                    0
                ],
                "title": "A Survey of LLM-Driven AI Agent Communication: Protocols, Security\n  Risks, and Defense Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of LLM-Driven AI Agent Communication: Protocols, Security\n  Risks, and Defense Countermeasures"
                },
                "summary": "In recent years, Large-Language-Model-driven AI agents have exhibited\nunprecedented intelligence and adaptability, and are rapidly changing human\nproduction and life. Nowadays, agents are undergoing a new round of evolution.\nThey no longer act as an isolated island like LLMs. Instead, they start to\ncommunicate with diverse external entities, such as other agents and tools, to\nperform more complex tasks collectively. Under this trend, agent communication\nis regarded as a foundational pillar of the future AI ecosystem, and many\norganizations have intensively begun to design related communication protocols\n(e.g., Anthropic's MCP and Google's A2A) within the recent few months. However,\nthis new field exposes significant security hazards, which can cause severe\ndamage to real-world scenarios. To help researchers quickly figure out this\npromising topic and benefit the future agent communication development, this\npaper presents a comprehensive survey of agent communication security. More\nprecisely, we first present a clear definition of agent communication and\ncategorize the entire lifecycle of agent communication into three stages:\nuser-agent interaction, agent-agent communication, and agent-environment\ncommunication. Next, for each communication phase, we dissect related protocols\nand analyze the security risks according to the communication characteristics.\nThen, we summarize and outlook on the possible defense countermeasures for each\nrisk. In addition, we conduct experiments using MCP and A2A to help readers\nbetter understand the novel vulnerabilities brought by agent communication.\nFinally, we discuss open issues and future directions in this promising\nresearch field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large-Language-Model-driven AI agents have exhibited\nunprecedented intelligence and adaptability, and are rapidly changing human\nproduction and life. Nowadays, agents are undergoing a new round of evolution.\nThey no longer act as an isolated island like LLMs. Instead, they start to\ncommunicate with diverse external entities, such as other agents and tools, to\nperform more complex tasks collectively. Under this trend, agent communication\nis regarded as a foundational pillar of the future AI ecosystem, and many\norganizations have intensively begun to design related communication protocols\n(e.g., Anthropic's MCP and Google's A2A) within the recent few months. However,\nthis new field exposes significant security hazards, which can cause severe\ndamage to real-world scenarios. To help researchers quickly figure out this\npromising topic and benefit the future agent communication development, this\npaper presents a comprehensive survey of agent communication security. More\nprecisely, we first present a clear definition of agent communication and\ncategorize the entire lifecycle of agent communication into three stages:\nuser-agent interaction, agent-agent communication, and agent-environment\ncommunication. Next, for each communication phase, we dissect related protocols\nand analyze the security risks according to the communication characteristics.\nThen, we summarize and outlook on the possible defense countermeasures for each\nrisk. In addition, we conduct experiments using MCP and A2A to help readers\nbetter understand the novel vulnerabilities brought by agent communication.\nFinally, we discuss open issues and future directions in this promising\nresearch field."
                },
                "authors": [
                    {
                        "name": "Dezhang Kong"
                    },
                    {
                        "name": "Shi Lin"
                    },
                    {
                        "name": "Zhenhua Xu"
                    },
                    {
                        "name": "Zhebo Wang"
                    },
                    {
                        "name": "Minghao Li"
                    },
                    {
                        "name": "Yufeng Li"
                    },
                    {
                        "name": "Yilun Zhang"
                    },
                    {
                        "name": "Hujin Peng"
                    },
                    {
                        "name": "Zeyang Sha"
                    },
                    {
                        "name": "Yuyuan Li"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Xun Wang"
                    },
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Chaochao Chen"
                    },
                    {
                        "name": "Muhammad Khurram Khan"
                    },
                    {
                        "name": "Meng Han"
                    }
                ],
                "author_detail": {
                    "name": "Meng Han"
                },
                "author": "Meng Han",
                "arxiv_comment": "41 pages, 13 figures, submitted to IEEE COMST",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19676v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19676v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06432v2",
                "updated": "2025-07-01T09:36:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    36,
                    47,
                    1,
                    182,
                    0
                ],
                "published": "2024-12-09T12:20:33Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    12,
                    20,
                    33,
                    0,
                    344,
                    0
                ],
                "title": "Integrating Expert Labels into LLM-based Emission Goal Detection:\n  Example Selection vs Automatic Prompt Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Expert Labels into LLM-based Emission Goal Detection:\n  Example Selection vs Automatic Prompt Design"
                },
                "summary": "We address the detection of emission reduction goals in corporate reports, an\nimportant task for monitoring companies' progress in addressing climate change.\nSpecifically, we focus on the issue of integrating expert feedback in the form\nof labeled example passages into LLM-based pipelines, and compare the two\nstrategies of (1) a dynamic selection of few-shot examples and (2) the\nautomatic optimization of the prompt by the LLM itself. Our findings on a\npublic dataset of 769 climate-related passages from real-world business reports\nindicate that automatic prompt optimization is the superior approach, while\ncombining both methods provides only limited benefit. Qualitative results\nindicate that optimized prompts do indeed capture many intricacies of the\ntargeted emission goal extraction task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the detection of emission reduction goals in corporate reports, an\nimportant task for monitoring companies' progress in addressing climate change.\nSpecifically, we focus on the issue of integrating expert feedback in the form\nof labeled example passages into LLM-based pipelines, and compare the two\nstrategies of (1) a dynamic selection of few-shot examples and (2) the\nautomatic optimization of the prompt by the LLM itself. Our findings on a\npublic dataset of 769 climate-related passages from real-world business reports\nindicate that automatic prompt optimization is the superior approach, while\ncombining both methods provides only limited benefit. Qualitative results\nindicate that optimized prompts do indeed capture many intricacies of the\ntargeted emission goal extraction task."
                },
                "authors": [
                    {
                        "name": "Marco Wrzalik"
                    },
                    {
                        "name": "Adrian Ulges"
                    },
                    {
                        "name": "Anne Uersfeld"
                    },
                    {
                        "name": "Florian Faust"
                    },
                    {
                        "name": "Viola Campos"
                    }
                ],
                "author_detail": {
                    "name": "Viola Campos"
                },
                "author": "Viola Campos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01933v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01933v6",
                "updated": "2025-07-01T08:40:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    8,
                    40,
                    39,
                    1,
                    182,
                    0
                ],
                "published": "2024-08-04T05:15:02Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    5,
                    15,
                    2,
                    6,
                    217,
                    0
                ],
                "title": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios."
                },
                "authors": [
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Jiuyang Chang"
                    },
                    {
                        "name": "Yiming Qian"
                    },
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Zhouqiang Jiang"
                    },
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Yuta Nakashima"
                    },
                    {
                        "name": "Hajime Nagahara"
                    }
                ],
                "author_detail": {
                    "name": "Hajime Nagahara"
                },
                "author": "Hajime Nagahara",
                "arxiv_comment": "Accepted by NeurIPS 2024 D&B Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01933v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01933v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05795v2",
                "updated": "2025-07-01T08:18:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    8,
                    18,
                    41,
                    1,
                    182,
                    0
                ],
                "published": "2025-02-09T07:03:36Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    7,
                    3,
                    36,
                    6,
                    40,
                    0
                ],
                "title": "The Curse of Depth in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Curse of Depth in Large Language Models"
                },
                "summary": "In this paper, we introduce the Curse of Depth, a concept that highlights,\nexplains, and addresses the recent observation in modern Large Language Models\n(LLMs) where nearly half of the layers are less effective than expected. We\nfirst confirm the wide existence of this phenomenon across the most popular\nfamilies of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis,\ntheoretically and empirically, identifies that the underlying reason for the\nineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer\nNormalization (Pre-LN). While Pre-LN stabilizes the training of Transformer\nLLMs, its output variance exponentially grows with the model depth, which\nundesirably causes the derivative of the deep Transformer blocks to be an\nidentity matrix, and therefore barely contributes to the training. To resolve\nthis training pitfall, we propose LayerNorm Scaling (LNS), which scales the\nvariance of output of the layer normalization inversely by the square root of\nits depth. This simple modification mitigates the output variance explosion of\ndeeper Transformer layers, improving their contribution. Across a wide range of\nmodel sizes (130M to 7B), our experiments show that LNS consistently\noutperforms previous normalization and scaling techniques in enhancing LLM\npre-training performance. Moreover, this improvement seamlessly carries over to\nsupervised fine-tuning. All these gains can be attributed to the fact that\nLayerNorm Scaling enables deeper layers to contribute more effectively during\ntraining. Our code is available at\n\\href{https://github.com/lmsdss/LayerNorm-Scaling}{LayerNorm-Scaling}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce the Curse of Depth, a concept that highlights,\nexplains, and addresses the recent observation in modern Large Language Models\n(LLMs) where nearly half of the layers are less effective than expected. We\nfirst confirm the wide existence of this phenomenon across the most popular\nfamilies of LLMs such as Llama, Mistral, DeepSeek, and Qwen. Our analysis,\ntheoretically and empirically, identifies that the underlying reason for the\nineffectiveness of deep layers in LLMs is the widespread usage of Pre-Layer\nNormalization (Pre-LN). While Pre-LN stabilizes the training of Transformer\nLLMs, its output variance exponentially grows with the model depth, which\nundesirably causes the derivative of the deep Transformer blocks to be an\nidentity matrix, and therefore barely contributes to the training. To resolve\nthis training pitfall, we propose LayerNorm Scaling (LNS), which scales the\nvariance of output of the layer normalization inversely by the square root of\nits depth. This simple modification mitigates the output variance explosion of\ndeeper Transformer layers, improving their contribution. Across a wide range of\nmodel sizes (130M to 7B), our experiments show that LNS consistently\noutperforms previous normalization and scaling techniques in enhancing LLM\npre-training performance. Moreover, this improvement seamlessly carries over to\nsupervised fine-tuning. All these gains can be attributed to the fact that\nLayerNorm Scaling enables deeper layers to contribute more effectively during\ntraining. Our code is available at\n\\href{https://github.com/lmsdss/LayerNorm-Scaling}{LayerNorm-Scaling}."
                },
                "authors": [
                    {
                        "name": "Wenfang Sun"
                    },
                    {
                        "name": "Xinyuan Song"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Yefeng Zheng"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23520v2",
                "updated": "2025-07-01T08:11:18Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    8,
                    11,
                    18,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T05:11:19Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    5,
                    11,
                    19,
                    0,
                    181,
                    0
                ],
                "title": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions\n  with LLM-Generated Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions\n  with LLM-Generated Data"
                },
                "summary": "With the increasing interest in robotic synthesis in the context of organic\nchemistry, the automated extraction of chemical procedures from literature is\ncritical. However, this task remains challenging due to the inherent ambiguity\nof chemical language and the high cost of human annotation required for\ndeveloping reliable computer-aided extraction protocols. Here, we present\nChemActor, a fully fine-tuned large language model (LLM), as a chemical\nexecutor to convert between unstructured experimental procedures and structured\naction sequences. We propose a sequential LLM-generated data framework to\naddress the challenges of insufficient and low-quality annotated data. This\nframework integrates a data selection module that selects data based on\ndistribution divergence, with a general-purpose LLM, to generate\nmachine-executable actions from a single molecule input. Additionally, we\nintroduce a novel multi-round LLMs circle review metric, which reflects the\nmodel's advanced understanding of chemical experimental procedures. Extensive\nexperiments on reaction-to-description (R2D) and description-to-action (D2A)\ntasks demonstrate that ChemActor, augmented by LLM-generated data, achieves\nstate-of-the-art performance, outperforming the baseline model by 10%. The code\nis available at: https://github.com/Zhanghahah/ChemActor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing interest in robotic synthesis in the context of organic\nchemistry, the automated extraction of chemical procedures from literature is\ncritical. However, this task remains challenging due to the inherent ambiguity\nof chemical language and the high cost of human annotation required for\ndeveloping reliable computer-aided extraction protocols. Here, we present\nChemActor, a fully fine-tuned large language model (LLM), as a chemical\nexecutor to convert between unstructured experimental procedures and structured\naction sequences. We propose a sequential LLM-generated data framework to\naddress the challenges of insufficient and low-quality annotated data. This\nframework integrates a data selection module that selects data based on\ndistribution divergence, with a general-purpose LLM, to generate\nmachine-executable actions from a single molecule input. Additionally, we\nintroduce a novel multi-round LLMs circle review metric, which reflects the\nmodel's advanced understanding of chemical experimental procedures. Extensive\nexperiments on reaction-to-description (R2D) and description-to-action (D2A)\ntasks demonstrate that ChemActor, augmented by LLM-generated data, achieves\nstate-of-the-art performance, outperforming the baseline model by 10%. The code\nis available at: https://github.com/Zhanghahah/ChemActor."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ruijie Yu"
                    },
                    {
                        "name": "Jidong Tian"
                    },
                    {
                        "name": "Feng Zhu"
                    },
                    {
                        "name": "Jiapeng Liu"
                    },
                    {
                        "name": "Xiaokang Yang"
                    },
                    {
                        "name": "Yaohui Jin"
                    },
                    {
                        "name": "Yanyan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yanyan Xu"
                },
                "author": "Yanyan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21393v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21393v3",
                "updated": "2025-07-01T08:05:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    8,
                    5,
                    1,
                    1,
                    182,
                    0
                ],
                "published": "2025-03-27T11:35:40Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    11,
                    35,
                    40,
                    3,
                    86,
                    0
                ],
                "title": "An evaluation of LLMs and Google Translate for translation of selected\n  Indian languages via sentiment and semantic analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evaluation of LLMs and Google Translate for translation of selected\n  Indian languages via sentiment and semantic analyses"
                },
                "summary": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study on the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT, and Google Translate. This study addresses this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts (Bhagavad Gita,\nTamas and Maha Prasthanam ) that have been well translated by experts and use\nLLMs to generate their translations into English, and provide a comparison with\nselected expert (human) translations. Our investigation revealed that while\nLLMs have made significant progress in translation accuracy, challenges remain\nin preserving sentiment and semantic integrity, especially in metaphorical and\nphilosophical contexts for texts such as the Bhagavad Gita. The sentiment\nanalysis revealed that GPT models are better at preserving the sentiment\npolarity for the given texts when compared to human (expert) translation. The\nresults revealed that GPT models are generally better at maintaining the\nsentiment and semantics when compared to Google Translate. This study could\nhelp in the development of accurate and culturally sensitive translation\nsystems for large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have been prominent for language translation,\nincluding low-resource languages. There has been limited study on the\nassessment of the quality of translations generated by LLMs, including Gemini,\nGPT, and Google Translate. This study addresses this limitation by using\nsemantic and sentiment analysis of selected LLMs for Indian languages,\nincluding Sanskrit, Telugu and Hindi. We select prominent texts (Bhagavad Gita,\nTamas and Maha Prasthanam ) that have been well translated by experts and use\nLLMs to generate their translations into English, and provide a comparison with\nselected expert (human) translations. Our investigation revealed that while\nLLMs have made significant progress in translation accuracy, challenges remain\nin preserving sentiment and semantic integrity, especially in metaphorical and\nphilosophical contexts for texts such as the Bhagavad Gita. The sentiment\nanalysis revealed that GPT models are better at preserving the sentiment\npolarity for the given texts when compared to human (expert) translation. The\nresults revealed that GPT models are generally better at maintaining the\nsentiment and semantics when compared to Google Translate. This study could\nhelp in the development of accurate and culturally sensitive translation\nsystems for large language models."
                },
                "authors": [
                    {
                        "name": "Rohitash Chandra"
                    },
                    {
                        "name": "Aryan Chaudhari"
                    },
                    {
                        "name": "Yeshwanth Rayavarapu"
                    }
                ],
                "author_detail": {
                    "name": "Yeshwanth Rayavarapu"
                },
                "author": "Yeshwanth Rayavarapu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21393v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21393v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23815v2",
                "updated": "2025-07-01T07:51:20Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    7,
                    51,
                    20,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T13:02:01Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    2,
                    1,
                    0,
                    181,
                    0
                ],
                "title": "The Impact of AI on Educational Assessment: A Framework for Constructive\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of AI on Educational Assessment: A Framework for Constructive\n  Alignment"
                },
                "summary": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods."
                },
                "authors": [
                    {
                        "name": "Patrick Stokkink"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Stokkink"
                },
                "author": "Patrick Stokkink",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03040v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03040v2",
                "updated": "2025-07-01T07:35:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    7,
                    35,
                    25,
                    1,
                    182,
                    0
                ],
                "published": "2025-03-04T22:45:24Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    22,
                    45,
                    24,
                    1,
                    63,
                    0
                ],
                "title": "SAGE: Steering Dialog Generation with Future-Aware State-Action\n  Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGE: Steering Dialog Generation with Future-Aware State-Action\n  Augmentation"
                },
                "summary": "Recent advances in large language models have demonstrated impressive\ncapabilities in task-oriented applications, yet building emotionally\nintelligent chatbots that can engage in natural, strategic conversations\nremains a challenge. We present a novel approach called SAGE that uses latent\nvariables to control long-horizon behavior in dialogue generation. At the core\nof our method is the State-Action Chain (SAC), which augments standard language\nmodel fine-tuning by introducing latent variables that encapsulate emotional\nstates and conversational strategies between dialogue turns. During inference,\nthese variables are generated before each response, enabling coarse-grained\ncontrol over dialogue progression while maintaining natural interaction\npatterns. We also introduce a self-improvement pipeline that leverages dialogue\ntree search, LLM-based reward modeling, and targeted fine-tuning to optimize\nconversational trajectories. Our experimental results show that models trained\nwith this approach demonstrate improved performance in emotional intelligence\nmetrics while maintaining strong capabilities on LLM benchmarks. The discrete\nnature of our latent variables facilitates search-based strategies and provides\na foundation for future applications of reinforcement learning to dialogue\nsystems, where learning can occur at the state level rather than the token\nlevel. https://github.com/apple/ml-sage-dialog-gen",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated impressive\ncapabilities in task-oriented applications, yet building emotionally\nintelligent chatbots that can engage in natural, strategic conversations\nremains a challenge. We present a novel approach called SAGE that uses latent\nvariables to control long-horizon behavior in dialogue generation. At the core\nof our method is the State-Action Chain (SAC), which augments standard language\nmodel fine-tuning by introducing latent variables that encapsulate emotional\nstates and conversational strategies between dialogue turns. During inference,\nthese variables are generated before each response, enabling coarse-grained\ncontrol over dialogue progression while maintaining natural interaction\npatterns. We also introduce a self-improvement pipeline that leverages dialogue\ntree search, LLM-based reward modeling, and targeted fine-tuning to optimize\nconversational trajectories. Our experimental results show that models trained\nwith this approach demonstrate improved performance in emotional intelligence\nmetrics while maintaining strong capabilities on LLM benchmarks. The discrete\nnature of our latent variables facilitates search-based strategies and provides\na foundation for future applications of reinforcement learning to dialogue\nsystems, where learning can occur at the state level rather than the token\nlevel. https://github.com/apple/ml-sage-dialog-gen"
                },
                "authors": [
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    }
                ],
                "author_detail": {
                    "name": "Navdeep Jaitly"
                },
                "author": "Navdeep Jaitly",
                "arxiv_comment": "9 pages main text",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03040v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03040v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23146v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23146v2",
                "updated": "2025-07-01T07:03:17Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    7,
                    3,
                    17,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-29T08:55:37Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    8,
                    55,
                    37,
                    6,
                    180,
                    0
                ],
                "title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness\n  Beyond Performance Illusions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness\n  Beyond Performance Illusions"
                },
                "summary": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success."
                },
                "authors": [
                    {
                        "name": "Dingzriui Wang"
                    },
                    {
                        "name": "Xuanliang Zhang"
                    },
                    {
                        "name": "Keyan Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Yang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Deng"
                },
                "author": "Yang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23146v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23146v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21248v2",
                "updated": "2025-07-01T07:00:59Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    7,
                    0,
                    59,
                    1,
                    182,
                    0
                ],
                "published": "2025-03-27T08:09:15Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    8,
                    9,
                    15,
                    3,
                    86,
                    0
                ],
                "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResearchBench: Benchmarking LLMs in Scientific Discovery via\n  Inspiration-Based Task Decomposition"
                },
                "summary": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated potential in assisting\nscientific research, yet their ability to discover high-quality research\nhypotheses remains unexamined due to the lack of a dedicated benchmark. To\naddress this gap, we introduce the first large-scale benchmark for evaluating\nLLMs with a near-sufficient set of sub-tasks of scientific discovery:\ninspiration retrieval, hypothesis composition, and hypothesis ranking. We\ndevelop an automated framework that extracts critical components - research\nquestions, background surveys, inspirations, and hypotheses - from scientific\npapers across 12 disciplines, with expert validation confirming its accuracy.\nTo prevent data contamination, we focus exclusively on papers published in\n2024, ensuring minimal overlap with LLM pretraining data. Our evaluation\nreveals that LLMs perform well in retrieving inspirations, an\nout-of-distribution task, suggesting their ability to surface novel knowledge\nassociations. This positions LLMs as \"research hypothesis mines\", capable of\nfacilitating automated scientific discovery by generating innovative hypotheses\nat scale with minimal human intervention."
                },
                "authors": [
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Zonglin Yang"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Jinjie Ni"
                    },
                    {
                        "name": "Ben Gao"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Shixiang Tang"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Erik Cambria"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dongzhan Zhou"
                },
                "author": "Dongzhan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21098v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21098v2",
                "updated": "2025-07-01T06:31:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    31,
                    6,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-26T08:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    8,
                    48,
                    16,
                    3,
                    177,
                    0
                ],
                "title": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for\n  Real-time Community Question Answering in Industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for\n  Real-time Community Question Answering in Industry"
                },
                "summary": "Community Question Answering (CQA) platforms can be deemed as important\nknowledge bases in community, but effectively leveraging historical\ninteractions and domain knowledge in real-time remains a challenge. Existing\nmethods often underutilize external knowledge, fail to incorporate dynamic\nhistorical QA context, or lack memory mechanisms suited for industrial\ndeployment. We propose ComRAG, a retrieval-augmented generation framework for\nreal-time industrial CQA that integrates static knowledge with dynamic\nhistorical QA pairs via a centroid-based memory mechanism designed for\nretrieval, generation, and efficient storage. Evaluated on three industrial CQA\ndatasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%\nimprovement in vector similarity, reducing latency by 8.7% to 23.3%, and\nlowering chunk growth from 20.23% to 2.06% over iterations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Community Question Answering (CQA) platforms can be deemed as important\nknowledge bases in community, but effectively leveraging historical\ninteractions and domain knowledge in real-time remains a challenge. Existing\nmethods often underutilize external knowledge, fail to incorporate dynamic\nhistorical QA context, or lack memory mechanisms suited for industrial\ndeployment. We propose ComRAG, a retrieval-augmented generation framework for\nreal-time industrial CQA that integrates static knowledge with dynamic\nhistorical QA pairs via a centroid-based memory mechanism designed for\nretrieval, generation, and efficient storage. Evaluated on three industrial CQA\ndatasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%\nimprovement in vector similarity, reducing latency by 8.7% to 23.3%, and\nlowering chunk growth from 20.23% to 2.06% over iterations."
                },
                "authors": [
                    {
                        "name": "Qinwen Chen"
                    },
                    {
                        "name": "Wenbiao Tao"
                    },
                    {
                        "name": "Zhiwei Zhu"
                    },
                    {
                        "name": "Mingfan Xi"
                    },
                    {
                        "name": "Liangzhong Guo"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Yunshi Lan"
                    }
                ],
                "author_detail": {
                    "name": "Yunshi Lan"
                },
                "author": "Yunshi Lan",
                "arxiv_comment": "7 pages, 4 figures. Accepted at ACL 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21098v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21098v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18241v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18241v2",
                "updated": "2025-07-01T06:19:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    6,
                    19,
                    39,
                    1,
                    182,
                    0
                ],
                "published": "2024-12-24T07:51:29Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    7,
                    51,
                    29,
                    1,
                    359,
                    0
                ],
                "title": "An Automatic Graph Construction Framework based on Large Language Models\n  for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Automatic Graph Construction Framework based on Large Language Models\n  for Recommendation"
                },
                "summary": "Graph neural networks (GNNs) have emerged as state-of-the-art methods to\nlearn from graph-structured data for recommendation. However, most existing\nGNN-based recommendation methods focus on the optimization of model structures\nand learning strategies based on pre-defined graphs, neglecting the importance\nof the graph construction stage. Earlier works for graph construction usually\nrely on speciffic rules or crowdsourcing, which are either too simplistic or\ntoo labor-intensive. Recent works start to utilize large language models (LLMs)\nto automate the graph construction, in view of their abundant open-world\nknowledge and remarkable reasoning capabilities. Nevertheless, they generally\nsuffer from two limitations: (1) invisibility of global view (e.g., overlooking\ncontextual information) and (2) construction inefficiency. To this end, we\nintroduce AutoGraph, an automatic graph construction framework based on LLMs\nfor recommendation. Specifically, we first use LLMs to infer the user\npreference and item knowledge, which is encoded as semantic vectors. Next, we\nemploy vector quantization to extract the latent factors from the semantic\nvectors. The latent factors are then incorporated as extra nodes to link the\nuser/item nodes, resulting in a graph with in-depth global-view semantics. We\nfurther design metapath-based message aggregation to effectively aggregate the\nsemantic and collaborative information. The framework is model-agnostic and\ncompatible with different backbone models. Extensive experiments on three\nreal-world datasets demonstrate the efficacy and efffciency of AutoGraph\ncompared to existing baseline methods. We have deployed AutoGraph in Huawei\nadvertising platform, and gain a 2.69% improvement on RPM and a 7.31%\nimprovement on eCPM in the online A/B test. Currently AutoGraph has been used\nas the main trafffc model, serving hundreds of millions of people.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) have emerged as state-of-the-art methods to\nlearn from graph-structured data for recommendation. However, most existing\nGNN-based recommendation methods focus on the optimization of model structures\nand learning strategies based on pre-defined graphs, neglecting the importance\nof the graph construction stage. Earlier works for graph construction usually\nrely on speciffic rules or crowdsourcing, which are either too simplistic or\ntoo labor-intensive. Recent works start to utilize large language models (LLMs)\nto automate the graph construction, in view of their abundant open-world\nknowledge and remarkable reasoning capabilities. Nevertheless, they generally\nsuffer from two limitations: (1) invisibility of global view (e.g., overlooking\ncontextual information) and (2) construction inefficiency. To this end, we\nintroduce AutoGraph, an automatic graph construction framework based on LLMs\nfor recommendation. Specifically, we first use LLMs to infer the user\npreference and item knowledge, which is encoded as semantic vectors. Next, we\nemploy vector quantization to extract the latent factors from the semantic\nvectors. The latent factors are then incorporated as extra nodes to link the\nuser/item nodes, resulting in a graph with in-depth global-view semantics. We\nfurther design metapath-based message aggregation to effectively aggregate the\nsemantic and collaborative information. The framework is model-agnostic and\ncompatible with different backbone models. Extensive experiments on three\nreal-world datasets demonstrate the efficacy and efffciency of AutoGraph\ncompared to existing baseline methods. We have deployed AutoGraph in Huawei\nadvertising platform, and gain a 2.69% improvement on RPM and a 7.31%\nimprovement on eCPM in the online A/B test. Currently AutoGraph has been used\nas the main trafffc model, serving hundreds of millions of people."
                },
                "authors": [
                    {
                        "name": "Rong Shan"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Chenxu Zhu"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Menghui Zhu"
                    },
                    {
                        "name": "Kangning Zhang"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "Accepted by KDD'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18241v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18241v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24778v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24778v2",
                "updated": "2025-07-01T05:53:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    53,
                    22,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-30T16:41:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    16,
                    41,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers\n  Accurately Reflect Large Language Models' Uncertainty?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers\n  Accurately Reflect Large Language Models' Uncertainty?"
                },
                "summary": "As large language models (LLMs) are increasingly used in high-stakes domains,\naccurately assessing their confidence is crucial. Humans typically express\nconfidence through epistemic markers (e.g., \"fairly confident\") instead of\nnumerical values. However, it remains unclear whether LLMs consistently use\nthese markers to reflect their intrinsic confidence due to the difficulty of\nquantifying uncertainty associated with various markers. To address this gap,\nwe first define marker confidence as the observed accuracy when a model employs\nan epistemic marker. We evaluate its stability across multiple\nquestion-answering datasets in both in-distribution and out-of-distribution\nsettings for open-source and proprietary LLMs. Our results show that while\nmarkers generalize well within the same distribution, their confidence is\ninconsistent in out-of-distribution scenarios. These findings raise significant\nconcerns about the reliability of epistemic markers for confidence estimation,\nunderscoring the need for improved alignment between marker based confidence\nand actual model uncertainty. Our code is available at\nhttps://github.com/HKUST-KnowComp/MarCon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly used in high-stakes domains,\naccurately assessing their confidence is crucial. Humans typically express\nconfidence through epistemic markers (e.g., \"fairly confident\") instead of\nnumerical values. However, it remains unclear whether LLMs consistently use\nthese markers to reflect their intrinsic confidence due to the difficulty of\nquantifying uncertainty associated with various markers. To address this gap,\nwe first define marker confidence as the observed accuracy when a model employs\nan epistemic marker. We evaluate its stability across multiple\nquestion-answering datasets in both in-distribution and out-of-distribution\nsettings for open-source and proprietary LLMs. Our results show that while\nmarkers generalize well within the same distribution, their confidence is\ninconsistent in out-of-distribution scenarios. These findings raise significant\nconcerns about the reliability of epistemic markers for confidence estimation,\nunderscoring the need for improved alignment between marker based confidence\nand actual model uncertainty. Our code is available at\nhttps://github.com/HKUST-KnowComp/MarCon."
                },
                "authors": [
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "arxiv_comment": "ACL2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24778v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24778v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17765v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17765v2",
                "updated": "2025-07-01T05:47:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    5,
                    47,
                    5,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-21T17:18:35Z",
                "published_parsed": [
                    2025,
                    6,
                    21,
                    17,
                    18,
                    35,
                    5,
                    172,
                    0
                ],
                "title": "CARTS: Collaborative Agents for Recommendation Textual Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARTS: Collaborative Agents for Recommendation Textual Summarization"
                },
                "summary": "Current recommendation systems often require some form of textual data\nsummarization, such as generating concise and coherent titles for product\ncarousels or other grouped item displays. While large language models have\nshown promise in NLP domains for textual summarization, these approaches do not\ndirectly apply to recommendation systems, where explanations must be highly\nrelevant to the core features of item sets, adhere to strict word limit\nconstraints. In this paper, we propose CARTS (Collaborative Agents for\nRecommendation Textual Summarization), a multi-agent LLM framework designed for\nstructured summarization in recommendation systems. CARTS decomposes the task\ninto three stages-Generation Augmented Generation (GAG), refinement circle, and\narbitration, where successive agent roles are responsible for extracting\nsalient item features, iteratively refining candidate titles based on relevance\nand length feedback, and selecting the final title through a collaborative\narbitration process. Experiments on large-scale e-commerce data and live A/B\ntesting show that CARTS significantly outperforms single-pass and\nchain-of-thought LLM baselines, delivering higher title relevance and improved\nuser engagement metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current recommendation systems often require some form of textual data\nsummarization, such as generating concise and coherent titles for product\ncarousels or other grouped item displays. While large language models have\nshown promise in NLP domains for textual summarization, these approaches do not\ndirectly apply to recommendation systems, where explanations must be highly\nrelevant to the core features of item sets, adhere to strict word limit\nconstraints. In this paper, we propose CARTS (Collaborative Agents for\nRecommendation Textual Summarization), a multi-agent LLM framework designed for\nstructured summarization in recommendation systems. CARTS decomposes the task\ninto three stages-Generation Augmented Generation (GAG), refinement circle, and\narbitration, where successive agent roles are responsible for extracting\nsalient item features, iteratively refining candidate titles based on relevance\nand length feedback, and selecting the final title through a collaborative\narbitration process. Experiments on large-scale e-commerce data and live A/B\ntesting show that CARTS significantly outperforms single-pass and\nchain-of-thought LLM baselines, delivering higher title relevance and improved\nuser engagement metrics."
                },
                "authors": [
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Kehui Yao"
                    },
                    {
                        "name": "Reza Yousefi Maragheh"
                    },
                    {
                        "name": "Kai Zhao"
                    },
                    {
                        "name": "Jianpeng Xu"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Evren Korpeoglu"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17765v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24124v2",
                "updated": "2025-07-01T03:40:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    3,
                    40,
                    22,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T17:59:14Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    59,
                    14,
                    0,
                    181,
                    0
                ],
                "title": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual\n  and Textual Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual\n  and Textual Perspectives"
                },
                "summary": "Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP."
                },
                "authors": [
                    {
                        "name": "Sixun Dong"
                    },
                    {
                        "name": "Wei Fan"
                    },
                    {
                        "name": "Teresa Wu"
                    },
                    {
                        "name": "Yanjie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjie Fu"
                },
                "author": "Yanjie Fu",
                "arxiv_comment": "Code: https://github.com/Ironieser/TimesCLIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18232v2",
                "updated": "2025-07-01T03:31:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    3,
                    31,
                    12,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-23T12:40:59Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    40,
                    59,
                    4,
                    143,
                    0
                ],
                "title": "Two-Stage Regularization-Based Structured Pruning for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Stage Regularization-Based Structured Pruning for LLMs"
                },
                "summary": "The deployment of large language models (LLMs) is largely hindered by their\nlarge number of parameters. Structural pruning has emerged as a promising\nsolution. Prior structured pruning methods directly remove unimportant\nparameters based on certain metrics, which often causes knowledge loss and\nnecessitates extensive retraining. To overcome this, we introduce a novel\npruning method TRSP: Two-Stage Regularization-Based Structured Pruning for\nLLMs. Specifically, we multiply the output of each transformer layer by an\ninitial learnable weight and iteratively learn these weights by adding their\n$\\ell_1$-norm as a regularization term to the loss function, serving as the\nfirst-stage regularization. Subsequently, we apply additional regularization to\nthe difference between the output and input of layers with smaller weights,\nencouraging the shift of knowledge to the preserved layers. This serves as the\nsecond-stage regularization. TRSP retains more knowledge and better preserves\nmodel performance than direct parameter elimination. Through extensive\nexperimentation we show that TRSP outperforms strong layer-wise structured\npruning methods without requiring retraining. As a layer-wise pruning method,\nit delivers notable end-to-end acceleration, making it a promising solution for\nefficient LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is largely hindered by their\nlarge number of parameters. Structural pruning has emerged as a promising\nsolution. Prior structured pruning methods directly remove unimportant\nparameters based on certain metrics, which often causes knowledge loss and\nnecessitates extensive retraining. To overcome this, we introduce a novel\npruning method TRSP: Two-Stage Regularization-Based Structured Pruning for\nLLMs. Specifically, we multiply the output of each transformer layer by an\ninitial learnable weight and iteratively learn these weights by adding their\n$\\ell_1$-norm as a regularization term to the loss function, serving as the\nfirst-stage regularization. Subsequently, we apply additional regularization to\nthe difference between the output and input of layers with smaller weights,\nencouraging the shift of knowledge to the preserved layers. This serves as the\nsecond-stage regularization. TRSP retains more knowledge and better preserves\nmodel performance than direct parameter elimination. Through extensive\nexperimentation we show that TRSP outperforms strong layer-wise structured\npruning methods without requiring retraining. As a layer-wise pruning method,\nit delivers notable end-to-end acceleration, making it a promising solution for\nefficient LLM deployment."
                },
                "authors": [
                    {
                        "name": "Mingkuan Feng"
                    },
                    {
                        "name": "Jinyang Wu"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Ruihan Jin"
                    },
                    {
                        "name": "Feihu Che"
                    },
                    {
                        "name": "Pengpeng Shao"
                    },
                    {
                        "name": "Zhengqi Wen"
                    },
                    {
                        "name": "Jianhua Tao"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Tao"
                },
                "author": "Jianhua Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22523v2",
                "updated": "2025-07-01T03:17:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    3,
                    17,
                    10,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-26T23:11:49Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    23,
                    11,
                    49,
                    3,
                    177,
                    0
                ],
                "title": "Red Teaming for Generative AI, Report on a Copyright-Focused Exercise\n  Completed in an Academic Medical Center",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Red Teaming for Generative AI, Report on a Copyright-Focused Exercise\n  Completed in an Academic Medical Center"
                },
                "summary": "Background: Generative artificial intelligence (AI) deployment in healthcare\nsettings raises copyright compliance concerns. Dana-Farber Cancer Institute\nimplemented GPT4DFCI, an internal generative AI tool utilizing OpenAI models,\nthat is approved for enterprise use in research and operations. Given (i) the\nexceptionally broad adoption of the tool in our organization, (ii) our research\nmission, and (iii) the shared responsibility model required by Microsoft OpenAI\nproducts, we deemed rigorous copyright compliance testing necessary.\n  Case Description: We conducted a structured red teaming exercise in Nov.\n2024, with 42 participants from academic, industry, and government\ninstitutions. Four teams attempted to extract copyrighted content from GPT4DFCI\nacross four domains: literary works, news articles, scientific publications,\nand access-restricted clinical notes. Teams successfully extracted verbatim\nbook dedications and near-exact passages through indirect prompting strategies.\nNews article extraction failed despite jailbreak attempts. Scientific article\nreproduction yielded only high-level summaries. Clinical note testing revealed\nappropriate privacy safeguards with data reformatting rather than reproduction.\n  Discussion: The successful extraction of literary content indicates potential\ncopyright material presence in training data, necessitating enhanced\ninference-time filtering. Differential success rates across content types\nsuggest varying protective mechanisms. The event led to implementation of a\ncopyright-specific meta-prompt in GPT4DFCI; this mitigation is in production\nsince Jan. 2025.\n  Conclusion: Systematic red teaming revealed specific vulnerabilities in\ngenerative AI copyright compliance, leading to concrete mitigation strategies.\nAcademic medical institutions deploying generative AI must implement continuous\ntesting protocols to ensure legal and ethical compliance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Generative artificial intelligence (AI) deployment in healthcare\nsettings raises copyright compliance concerns. Dana-Farber Cancer Institute\nimplemented GPT4DFCI, an internal generative AI tool utilizing OpenAI models,\nthat is approved for enterprise use in research and operations. Given (i) the\nexceptionally broad adoption of the tool in our organization, (ii) our research\nmission, and (iii) the shared responsibility model required by Microsoft OpenAI\nproducts, we deemed rigorous copyright compliance testing necessary.\n  Case Description: We conducted a structured red teaming exercise in Nov.\n2024, with 42 participants from academic, industry, and government\ninstitutions. Four teams attempted to extract copyrighted content from GPT4DFCI\nacross four domains: literary works, news articles, scientific publications,\nand access-restricted clinical notes. Teams successfully extracted verbatim\nbook dedications and near-exact passages through indirect prompting strategies.\nNews article extraction failed despite jailbreak attempts. Scientific article\nreproduction yielded only high-level summaries. Clinical note testing revealed\nappropriate privacy safeguards with data reformatting rather than reproduction.\n  Discussion: The successful extraction of literary content indicates potential\ncopyright material presence in training data, necessitating enhanced\ninference-time filtering. Differential success rates across content types\nsuggest varying protective mechanisms. The event led to implementation of a\ncopyright-specific meta-prompt in GPT4DFCI; this mitigation is in production\nsince Jan. 2025.\n  Conclusion: Systematic red teaming revealed specific vulnerabilities in\ngenerative AI copyright compliance, leading to concrete mitigation strategies.\nAcademic medical institutions deploying generative AI must implement continuous\ntesting protocols to ensure legal and ethical compliance."
                },
                "authors": [
                    {
                        "name": "James Wen"
                    },
                    {
                        "name": "Sahil Nalawade"
                    },
                    {
                        "name": "Zhiwei Liang"
                    },
                    {
                        "name": "Catherine Bielick"
                    },
                    {
                        "name": "Marisa Ferrara Boston"
                    },
                    {
                        "name": "Alexander Chowdhury"
                    },
                    {
                        "name": "Adele Collin"
                    },
                    {
                        "name": "Luigi De Angelis"
                    },
                    {
                        "name": "Jacob Ellen"
                    },
                    {
                        "name": "Heather Frase"
                    },
                    {
                        "name": "Rodrigo R. Gameiro"
                    },
                    {
                        "name": "Juan Manuel Gutierrez"
                    },
                    {
                        "name": "Pooja Kadam"
                    },
                    {
                        "name": "Murat Keceli"
                    },
                    {
                        "name": "Srikanth Krishnamurthy"
                    },
                    {
                        "name": "Anne Kwok"
                    },
                    {
                        "name": "Yanan Lance Lu"
                    },
                    {
                        "name": "Heather Mattie"
                    },
                    {
                        "name": "Liam G. McCoy"
                    },
                    {
                        "name": "Katherine Miller"
                    },
                    {
                        "name": "Allison C. Morgan"
                    },
                    {
                        "name": "Marlene Louisa Moerig"
                    },
                    {
                        "name": "Trang Nguyen"
                    },
                    {
                        "name": "Alexander Owen-Post"
                    },
                    {
                        "name": "Alex D. Ruiz"
                    },
                    {
                        "name": "Sreekar Reddy Puchala"
                    },
                    {
                        "name": "Soujanya Samineni"
                    },
                    {
                        "name": "Takeshi Tohyama"
                    },
                    {
                        "name": "Varun Ullanat"
                    },
                    {
                        "name": "Carmine Valenza"
                    },
                    {
                        "name": "Camilo Velez"
                    },
                    {
                        "name": "Pengcheng Wang"
                    },
                    {
                        "name": "Anna Wuest"
                    },
                    {
                        "name": "Yuxiang Zhou"
                    },
                    {
                        "name": "Yingde Zhu"
                    },
                    {
                        "name": "Jason M. Johnson"
                    },
                    {
                        "name": "Naomi Lenane"
                    },
                    {
                        "name": "Jennifer Willcox"
                    },
                    {
                        "name": "Francis J. Vitiello"
                    },
                    {
                        "name": "Leo Anthony G. Celi"
                    },
                    {
                        "name": "Renato Umeton"
                    }
                ],
                "author_detail": {
                    "name": "Renato Umeton"
                },
                "author": "Renato Umeton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01144v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01144v4",
                "updated": "2025-07-01T03:02:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    3,
                    2,
                    26,
                    1,
                    182,
                    0
                ],
                "published": "2025-01-02T08:57:00Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    57,
                    0,
                    3,
                    2,
                    0
                ],
                "title": "BlockDialect: Block-wise Fine-grained Mixed Format Quantization for\n  Energy-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDialect: Block-wise Fine-grained Mixed Format Quantization for\n  Energy-Efficient LLM Inference"
                },
                "summary": "The rapidly increasing size of large language models (LLMs) presents\nsignificant challenges in memory usage and computational costs. Quantizing both\nweights and activations can address these issues, with hardware-supported\nfine-grained scaling emerging as a promising solution to mitigate outliers.\nHowever, existing methods struggle to capture nuanced block data distributions.\nWe propose BlockDialect, a block-wise fine-grained mixed format technique that\nassigns a per-block optimal number format from a formatbook for better data\nrepresentation. Additionally, we introduce DialectFP4, a formatbook of FP4\nvariants (akin to dialects) that adapt to diverse data distributions. To\nleverage this efficiently, we propose a two-stage approach for online\nDialectFP4 activation quantization. Importantly, DialectFP4 ensures energy\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy\ngain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit\nusage per data, while being only 5.45% (2.69%) below full precision even when\nquantizing full-path matrix multiplication. Focusing on how to represent over\nhow to scale, our work presents a promising path for energy-efficient LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapidly increasing size of large language models (LLMs) presents\nsignificant challenges in memory usage and computational costs. Quantizing both\nweights and activations can address these issues, with hardware-supported\nfine-grained scaling emerging as a promising solution to mitigate outliers.\nHowever, existing methods struggle to capture nuanced block data distributions.\nWe propose BlockDialect, a block-wise fine-grained mixed format technique that\nassigns a per-block optimal number format from a formatbook for better data\nrepresentation. Additionally, we introduce DialectFP4, a formatbook of FP4\nvariants (akin to dialects) that adapt to diverse data distributions. To\nleverage this efficiently, we propose a two-stage approach for online\nDialectFP4 activation quantization. Importantly, DialectFP4 ensures energy\nefficiency by selecting representable values as scaled integers compatible with\nlow-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy\ngain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit\nusage per data, while being only 5.45% (2.69%) below full precision even when\nquantizing full-path matrix multiplication. Focusing on how to represent over\nhow to scale, our work presents a promising path for energy-efficient LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Wonsuk Jang"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01144v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01144v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10774v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10774v4",
                "updated": "2025-07-01T02:38:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    2,
                    38,
                    26,
                    1,
                    182,
                    0
                ],
                "published": "2024-08-20T12:13:04Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    12,
                    13,
                    4,
                    1,
                    233,
                    0
                ],
                "title": "Flexora: Flexible Low Rank Adaptation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexora: Flexible Low Rank Adaptation for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are driving advancements in artificial\nintelligence by increasing the scale of model parameters, which has\nsignificantly enhanced generalization ability and unlocked new capabilities in\npractice. However, their performance in specific downstream tasks is usually\nhindered by their knowledge boundaries on these tasks. Thus, fine-tuning\ntechniques, especially the widely used Low-Rank Adaptation (LoRA) method, have\nbeen introduced to expand the boundaries on these tasks, whereas LoRA would\nunderperform on certain tasks owing to its potential overfitting on these\ntasks. To overcome this overfitting and improve the performance of LoRA, we\npropose the flexible low rank adaptation (Flexora) method to automatically and\nflexibly select the most important layers needing to be fine-tuned to achieve\nthe best performance on different downstream tasks. Specifically, Flexora\nfirstly frames this layer selection problem as a well-defined hyperparameter\noptimization (HPO) problem, then addresses it using the unrolled\ndifferentiation (UD) method, and finally selects the most useful layers based\non the optimized hyperparameters. Our extensive experiments on many pretrained\nmodels and natural language tasks show that Flexora is able to consistently\nimprove over the existing baselines, indicating the effectiveness of our\nFlexora in practice. We additionally provide insightful theoretical results and\nmany ablation studies to deliver a comprehensive understanding of our Flexora."
                },
                "authors": [
                    {
                        "name": "Chenxing Wei"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "Ying Tiffany He"
                    },
                    {
                        "name": "Fei Richard Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Richard Yu"
                },
                "author": "Fei Richard Yu",
                "arxiv_comment": "40 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10774v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10774v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22704v2",
                "updated": "2025-07-01T02:35:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    2,
                    35,
                    48,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-28T01:10:24Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    1,
                    10,
                    24,
                    5,
                    179,
                    0
                ],
                "title": "Beyond Code: The Multidimensional Impacts of Large Language Models in\n  Software Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Code: The Multidimensional Impacts of Large Language Models in\n  Software Development"
                },
                "summary": "Large language models (LLMs) are poised to significantly impact software\ndevelopment, especially in the Open-Source Software (OSS) sector. To understand\nthis impact, we first outline the mechanisms through which LLMs may influence\nOSS through code development, collaborative knowledge transfer, and skill\ndevelopment. We then empirically examine how LLMs affect OSS developers' work\nin these three key areas. Leveraging a natural experiment from a temporary\nChatGPT ban in Italy, we employ a Difference-in-Differences framework with\ntwo-way fixed effects to analyze data from all OSS developers on GitHub in\nthree similar countries, Italy, France, and Portugal, totaling 88,022 users. We\nfind that access to ChatGPT increases developer productivity by 6.4%, knowledge\nsharing by 9.6%, and skill acquisition by 8.4%. These benefits vary\nsignificantly by user experience level: novice developers primarily experience\nproductivity gains, whereas more experienced developers benefit more from\nimproved knowledge sharing and accelerated skill acquisition. In addition, we\nfind that LLM-assisted learning is highly context-dependent, with the greatest\nbenefits observed in technically complex, fragmented, or rapidly evolving\ncontexts. We show that the productivity effects of LLMs extend beyond direct\ncode generation to include enhanced collaborative learning and knowledge\nexchange among developers, dynamics that are essential for gaining a holistic\nunderstanding of LLMs' impact in OSS. Our findings offer critical managerial\nimplications: strategically deploying LLMs can accelerate novice developers'\nonboarding and productivity, empower intermediate developers to foster\nknowledge sharing and collaboration, and support rapid skill acquisition,\ntogether enhancing long-term organizational productivity and agility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are poised to significantly impact software\ndevelopment, especially in the Open-Source Software (OSS) sector. To understand\nthis impact, we first outline the mechanisms through which LLMs may influence\nOSS through code development, collaborative knowledge transfer, and skill\ndevelopment. We then empirically examine how LLMs affect OSS developers' work\nin these three key areas. Leveraging a natural experiment from a temporary\nChatGPT ban in Italy, we employ a Difference-in-Differences framework with\ntwo-way fixed effects to analyze data from all OSS developers on GitHub in\nthree similar countries, Italy, France, and Portugal, totaling 88,022 users. We\nfind that access to ChatGPT increases developer productivity by 6.4%, knowledge\nsharing by 9.6%, and skill acquisition by 8.4%. These benefits vary\nsignificantly by user experience level: novice developers primarily experience\nproductivity gains, whereas more experienced developers benefit more from\nimproved knowledge sharing and accelerated skill acquisition. In addition, we\nfind that LLM-assisted learning is highly context-dependent, with the greatest\nbenefits observed in technically complex, fragmented, or rapidly evolving\ncontexts. We show that the productivity effects of LLMs extend beyond direct\ncode generation to include enhanced collaborative learning and knowledge\nexchange among developers, dynamics that are essential for gaining a holistic\nunderstanding of LLMs' impact in OSS. Our findings offer critical managerial\nimplications: strategically deploying LLMs can accelerate novice developers'\nonboarding and productivity, empower intermediate developers to foster\nknowledge sharing and collaboration, and support rapid skill acquisition,\ntogether enhancing long-term organizational productivity and agility."
                },
                "authors": [
                    {
                        "name": "Sardar Bonabi"
                    },
                    {
                        "name": "Sarah Bana"
                    },
                    {
                        "name": "Vijay Gurbaxani"
                    },
                    {
                        "name": "Tingting Nian"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Nian"
                },
                "author": "Tingting Nian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24119v2",
                "updated": "2025-07-01T02:29:52Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    2,
                    29,
                    52,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T17:58:13Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    58,
                    13,
                    0,
                    181,
                    0
                ],
                "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via\n  Multi-Agent Multi-Turn Reinforcement Learning"
                },
                "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development."
                },
                "authors": [
                    {
                        "name": "Bo Liu"
                    },
                    {
                        "name": "Leon Guertler"
                    },
                    {
                        "name": "Simon Yu"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Penghui Qi"
                    },
                    {
                        "name": "Daniel Balcells"
                    },
                    {
                        "name": "Mickel Liu"
                    },
                    {
                        "name": "Cheston Tan"
                    },
                    {
                        "name": "Weiyan Shi"
                    },
                    {
                        "name": "Min Lin"
                    },
                    {
                        "name": "Wee Sun Lee"
                    },
                    {
                        "name": "Natasha Jaques"
                    }
                ],
                "author_detail": {
                    "name": "Natasha Jaques"
                },
                "author": "Natasha Jaques",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14518v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14518v2",
                "updated": "2025-07-01T02:25:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    2,
                    25,
                    58,
                    1,
                    182,
                    0
                ],
                "published": "2025-05-20T15:44:01Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    15,
                    44,
                    1,
                    1,
                    140,
                    0
                ],
                "title": "Teaching Audio-Aware Large Language Models What Does Not Hear:\n  Mitigating Hallucinations through Synthesized Negative Samples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Audio-Aware Large Language Models What Does Not Hear:\n  Mitigating Hallucinations through Synthesized Negative Samples"
                },
                "summary": "Recent advancements in audio-aware large language models (ALLMs) enable them\nto process and understand audio inputs. However, these models often hallucinate\nnon-existent sound events, reducing their reliability in real-world\napplications. To address this, we propose LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method that\nenhances ALLMs' ability to distinguish between present and absent sounds using\nsynthesized data from the backbone LLM. Unlike prior approaches, our method\nrequires no modification to LLM parameters and efficiently integrates audio\nrepresentations via a lightweight adapter. Experiments show that LISTEN\neffectively mitigates hallucinations while maintaining impressive performance\non existing audio question and reasoning benchmarks. At the same time, it is\nmore efficient in both data and computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in audio-aware large language models (ALLMs) enable them\nto process and understand audio inputs. However, these models often hallucinate\nnon-existent sound events, reducing their reliability in real-world\napplications. To address this, we propose LISTEN (Learning to Identify Sounds\nThrough Extended Negative Samples), a contrastive-like training method that\nenhances ALLMs' ability to distinguish between present and absent sounds using\nsynthesized data from the backbone LLM. Unlike prior approaches, our method\nrequires no modification to LLM parameters and efficiently integrates audio\nrepresentations via a lightweight adapter. Experiments show that LISTEN\neffectively mitigates hallucinations while maintaining impressive performance\non existing audio question and reasoning benchmarks. At the same time, it is\nmore efficient in both data and computation."
                },
                "authors": [
                    {
                        "name": "Chun-Yi Kuan"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Accepted to Interspeech 2025. Project Website:\n  https://kuan2jiu99.github.io/Balsa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14518v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14518v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23298v2",
                "updated": "2025-07-01T01:57:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    1,
                    57,
                    28,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-29T15:37:17Z",
                "published_parsed": [
                    2025,
                    6,
                    29,
                    15,
                    37,
                    17,
                    6,
                    180,
                    0
                ],
                "title": "Exposing and Mitigating Calibration Biases and Demographic Unfairness in\n  MLLM Few-Shot In-Context Learning for Medical Image Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing and Mitigating Calibration Biases and Demographic Unfairness in\n  MLLM Few-Shot In-Context Learning for Medical Image Classification"
                },
                "summary": "Multimodal large language models (MLLMs) have enormous potential to perform\nfew-shot in-context learning in the context of medical image analysis. However,\nsafe deployment of these models into real-world clinical practice requires an\nin-depth analysis of the accuracies of their predictions, and their associated\ncalibration errors, particularly across different demographic subgroups. In\nthis work, we present the first investigation into the calibration biases and\ndemographic unfairness of MLLMs' predictions and confidence scores in few-shot\nin-context learning for medical image classification. We introduce CALIN, an\ninference-time calibration method designed to mitigate the associated biases.\nSpecifically, CALIN estimates the amount of calibration needed, represented by\ncalibration matrices, using a bi-level procedure: progressing from the\npopulation level to the subgroup level prior to inference. It then applies this\nestimation to calibrate the predicted confidence scores during inference.\nExperimental results on three medical imaging datasets: PAPILA for fundus image\nclassification, HAM10000 for skin cancer classification, and MIMIC-CXR for\nchest X-ray classification demonstrate CALIN's effectiveness at ensuring fair\nconfidence calibration in its prediction, while improving its overall\nprediction accuracies and exhibiting minimum fairness-utility trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have enormous potential to perform\nfew-shot in-context learning in the context of medical image analysis. However,\nsafe deployment of these models into real-world clinical practice requires an\nin-depth analysis of the accuracies of their predictions, and their associated\ncalibration errors, particularly across different demographic subgroups. In\nthis work, we present the first investigation into the calibration biases and\ndemographic unfairness of MLLMs' predictions and confidence scores in few-shot\nin-context learning for medical image classification. We introduce CALIN, an\ninference-time calibration method designed to mitigate the associated biases.\nSpecifically, CALIN estimates the amount of calibration needed, represented by\ncalibration matrices, using a bi-level procedure: progressing from the\npopulation level to the subgroup level prior to inference. It then applies this\nestimation to calibrate the predicted confidence scores during inference.\nExperimental results on three medical imaging datasets: PAPILA for fundus image\nclassification, HAM10000 for skin cancer classification, and MIMIC-CXR for\nchest X-ray classification demonstrate CALIN's effectiveness at ensuring fair\nconfidence calibration in its prediction, while improving its overall\nprediction accuracies and exhibiting minimum fairness-utility trade-off."
                },
                "authors": [
                    {
                        "name": "Xing Shen"
                    },
                    {
                        "name": "Justin Szeto"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Hengguan Huang"
                    },
                    {
                        "name": "Tal Arbel"
                    }
                ],
                "author_detail": {
                    "name": "Tal Arbel"
                },
                "author": "Tal Arbel",
                "arxiv_comment": "Preprint version. The peer-reviewed version of this paper has been\n  accepted to MICCAI 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08811v2",
                "updated": "2025-07-01T01:53:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    1,
                    53,
                    9,
                    1,
                    182,
                    0
                ],
                "published": "2025-04-09T03:36:52Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    36,
                    52,
                    2,
                    99,
                    0
                ],
                "title": "Analogical Learning for Cross-Scenario Generalization: Framework and\n  Application to Intelligent Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analogical Learning for Cross-Scenario Generalization: Framework and\n  Application to Intelligent Localization"
                },
                "summary": "Existing learning models often exhibit poor generalization when deployed\nacross diverse scenarios. It is primarily due to that the underlying reference\nframe of the data varies with the deployment environment and settings. However,\ndespite that data of each scenario has a distinct reference frame, its\ngeneration generally follows common underlying physical rules. Based on this\nunderstanding, this article proposes a deep learning framework named analogical\nlearning (AL), which implicitly retrieves the reference frame information\nassociated with a scenario and then to make accurate prediction by relative\nanalogy with other scenarios. Specifically, we design a bipartite neural\nnetwork called Mateformer. Its first part captures the relativity within\nmultiple latent feature spaces between the input data and a small amount of\nembedded data from the studied scenario, while its second part uses this\nrelativity to guide the nonlinear analogy. We apply AL to the typical\nmulti-scenario learning problem of intelligent wireless localization in\ncellular networks. Extensive experiments validate AL's superiority across three\nkey dimensions. First, it achieves state-of-the-art accuracy in single-scenario\nbenchmarks. Second, it demonstrates stable transferability between different\nscenarios, avoiding catastrophic forgetting. Finally, and most importantly, it\nrobustly adapts to new, unseen scenarios--including dynamic weather and traffic\nconditions--without any tuning. All data and code are available at\nhttps://github.com/ziruichen-research/ALLoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing learning models often exhibit poor generalization when deployed\nacross diverse scenarios. It is primarily due to that the underlying reference\nframe of the data varies with the deployment environment and settings. However,\ndespite that data of each scenario has a distinct reference frame, its\ngeneration generally follows common underlying physical rules. Based on this\nunderstanding, this article proposes a deep learning framework named analogical\nlearning (AL), which implicitly retrieves the reference frame information\nassociated with a scenario and then to make accurate prediction by relative\nanalogy with other scenarios. Specifically, we design a bipartite neural\nnetwork called Mateformer. Its first part captures the relativity within\nmultiple latent feature spaces between the input data and a small amount of\nembedded data from the studied scenario, while its second part uses this\nrelativity to guide the nonlinear analogy. We apply AL to the typical\nmulti-scenario learning problem of intelligent wireless localization in\ncellular networks. Extensive experiments validate AL's superiority across three\nkey dimensions. First, it achieves state-of-the-art accuracy in single-scenario\nbenchmarks. Second, it demonstrates stable transferability between different\nscenarios, avoiding catastrophic forgetting. Finally, and most importantly, it\nrobustly adapts to new, unseen scenarios--including dynamic weather and traffic\nconditions--without any tuning. All data and code are available at\nhttps://github.com/ziruichen-research/ALLoc."
                },
                "authors": [
                    {
                        "name": "Zirui Chen"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Ziqing Xing"
                    },
                    {
                        "name": "Ridong Li"
                    },
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Richeng Jin"
                    },
                    {
                        "name": "Chongwen Huang"
                    },
                    {
                        "name": "Yuzhi Yang"
                    },
                    {
                        "name": "Mrouane Debbah"
                    }
                ],
                "author_detail": {
                    "name": "Mrouane Debbah"
                },
                "author": "Mrouane Debbah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23944v2",
                "updated": "2025-07-01T01:36:05Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    1,
                    36,
                    5,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-30T15:09:14Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    9,
                    14,
                    0,
                    181,
                    0
                ],
                "title": "Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning"
                },
                "summary": "Imitation learning models for robotic tasks typically rely on multi-modal\ninputs, such as RGB images, language, and proprioceptive states. While\nproprioception is intuitively important for decision-making and obstacle\navoidance, simply incorporating all proprioceptive states leads to a surprising\ndegradation in imitation learning performance. In this work, we identify the\nunderlying issue as the proprioception shift problem, where the distributions\nof proprioceptive states diverge significantly between training and deployment.\nTo address this challenge, we propose a domain adaptation framework that\nbridges the gap by utilizing rollout data collected during deployment. Using\nWasserstein distance, we quantify the discrepancy between expert and rollout\nproprioceptive states and minimize this gap by adding noise to both sets of\nstates, proportional to the Wasserstein distance. This strategy enhances\nrobustness against proprioception shifts by aligning the training and\ndeployment distributions. Experiments on robotic manipulation tasks demonstrate\nthe efficacy of our method, enabling the imitation policy to leverage\nproprioception while mitigating its adverse effects. Our approach outperforms\nthe naive solution which discards proprioception, and other baselines designed\nto address distributional shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imitation learning models for robotic tasks typically rely on multi-modal\ninputs, such as RGB images, language, and proprioceptive states. While\nproprioception is intuitively important for decision-making and obstacle\navoidance, simply incorporating all proprioceptive states leads to a surprising\ndegradation in imitation learning performance. In this work, we identify the\nunderlying issue as the proprioception shift problem, where the distributions\nof proprioceptive states diverge significantly between training and deployment.\nTo address this challenge, we propose a domain adaptation framework that\nbridges the gap by utilizing rollout data collected during deployment. Using\nWasserstein distance, we quantify the discrepancy between expert and rollout\nproprioceptive states and minimize this gap by adding noise to both sets of\nstates, proportional to the Wasserstein distance. This strategy enhances\nrobustness against proprioception shifts by aligning the training and\ndeployment distributions. Experiments on robotic manipulation tasks demonstrate\nthe efficacy of our method, enabling the imitation policy to leverage\nproprioception while mitigating its adverse effects. Our approach outperforms\nthe naive solution which discards proprioception, and other baselines designed\nto address distributional shifts."
                },
                "authors": [
                    {
                        "name": "Fuhang Kuang"
                    },
                    {
                        "name": "Jiacheng You"
                    },
                    {
                        "name": "Yingdong Hu"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Chuan Wen"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "arxiv_comment": "Need further modification",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19283v2",
                "updated": "2025-07-01T01:20:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    1,
                    20,
                    24,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-24T03:34:39Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    3,
                    34,
                    39,
                    1,
                    175,
                    0
                ],
                "title": "AirV2X: Unified Air-Ground Vehicle-to-Everything Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirV2X: Unified Air-Ground Vehicle-to-Everything Collaboration"
                },
                "summary": "While multi-vehicular collaborative driving demonstrates clear advantages\nover single-vehicle autonomy, traditional infrastructure-based V2X systems\nremain constrained by substantial deployment costs and the creation of\n\"uncovered danger zones\" in rural and suburban areas. We present\nAirV2X-Perception, a large-scale dataset that leverages Unmanned Aerial\nVehicles (UAVs) as a flexible alternative or complement to fixed Road-Side\nUnits (RSUs). Drones offer unique advantages over ground-based perception:\ncomplementary bird's-eye-views that reduce occlusions, dynamic positioning\ncapabilities that enable hovering, patrolling, and escorting navigation rules,\nand significantly lower deployment costs compared to fixed infrastructure. Our\ndataset comprises 6.73 hours of drone-assisted driving scenarios across urban,\nsuburban, and rural environments with varied weather and lighting conditions.\nThe AirV2X-Perception dataset facilitates the development and standardized\nevaluation of Vehicle-to-Drone (V2D) algorithms, addressing a critical gap in\nthe rapidly expanding field of aerial-assisted autonomous driving systems. The\ndataset and development kits are open-sourced at\nhttps://github.com/taco-group/AirV2X-Perception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multi-vehicular collaborative driving demonstrates clear advantages\nover single-vehicle autonomy, traditional infrastructure-based V2X systems\nremain constrained by substantial deployment costs and the creation of\n\"uncovered danger zones\" in rural and suburban areas. We present\nAirV2X-Perception, a large-scale dataset that leverages Unmanned Aerial\nVehicles (UAVs) as a flexible alternative or complement to fixed Road-Side\nUnits (RSUs). Drones offer unique advantages over ground-based perception:\ncomplementary bird's-eye-views that reduce occlusions, dynamic positioning\ncapabilities that enable hovering, patrolling, and escorting navigation rules,\nand significantly lower deployment costs compared to fixed infrastructure. Our\ndataset comprises 6.73 hours of drone-assisted driving scenarios across urban,\nsuburban, and rural environments with varied weather and lighting conditions.\nThe AirV2X-Perception dataset facilitates the development and standardized\nevaluation of Vehicle-to-Drone (V2D) algorithms, addressing a critical gap in\nthe rapidly expanding field of aerial-assisted autonomous driving systems. The\ndataset and development kits are open-sourced at\nhttps://github.com/taco-group/AirV2X-Perception."
                },
                "authors": [
                    {
                        "name": "Xiangbo Gao"
                    },
                    {
                        "name": "Yuheng Wu"
                    },
                    {
                        "name": "Xuewen Luo"
                    },
                    {
                        "name": "Keshu Wu"
                    },
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15044v2",
                "updated": "2025-07-01T01:18:26Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    1,
                    18,
                    26,
                    1,
                    182,
                    0
                ],
                "published": "2025-03-19T09:32:52Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    9,
                    32,
                    52,
                    2,
                    78,
                    0
                ],
                "title": "SPADE: Structured Prompting Augmentation for Dialogue Enhancement in\n  Machine-Generated Text Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPADE: Structured Prompting Augmentation for Dialogue Enhancement in\n  Machine-Generated Text Detection"
                },
                "summary": "The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of high-quality synthetic\ndatasets for training. To address this issue, we propose SPADE, a structured\nframework for detecting synthetic dialogues using prompt-based positive and\nnegative samples. Our proposed methods yield 14 new dialogue datasets, which we\nbenchmark against eight MGT detection models. The results demonstrate improved\ngeneralization performance when utilizing a mixed dataset produced by proposed\naugmentation frameworks, offering a practical approach to enhancing LLM\napplication security. Considering that real-world agents lack knowledge of\nfuture opponent utterances, we simulate online dialogue detection and examine\nthe relationship between chat history length and detection accuracy. Our\nopen-source datasets, code and prompts can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing capability of large language models (LLMs) to generate\nsynthetic content has heightened concerns about their misuse, driving the\ndevelopment of Machine-Generated Text (MGT) detection models. However, these\ndetectors face significant challenges due to the lack of high-quality synthetic\ndatasets for training. To address this issue, we propose SPADE, a structured\nframework for detecting synthetic dialogues using prompt-based positive and\nnegative samples. Our proposed methods yield 14 new dialogue datasets, which we\nbenchmark against eight MGT detection models. The results demonstrate improved\ngeneralization performance when utilizing a mixed dataset produced by proposed\naugmentation frameworks, offering a practical approach to enhancing LLM\napplication security. Considering that real-world agents lack knowledge of\nfuture opponent utterances, we simulate online dialogue detection and examine\nthe relationship between chat history length and detection accuracy. Our\nopen-source datasets, code and prompts can be downloaded from\nhttps://github.com/AngieYYF/SPADE-customer-service-dialogue."
                },
                "authors": [
                    {
                        "name": "Haoyi Li"
                    },
                    {
                        "name": "Angela Yifei Yuan"
                    },
                    {
                        "name": "Soyeon Caren Han"
                    },
                    {
                        "name": "Christopher Leckie"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Leckie"
                },
                "author": "Christopher Leckie",
                "arxiv_comment": "ACL LLMSEC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13504v2",
                "updated": "2025-07-01T01:18:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    1,
                    18,
                    12,
                    1,
                    182,
                    0
                ],
                "published": "2025-03-13T06:41:25Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    6,
                    41,
                    25,
                    3,
                    72,
                    0
                ],
                "title": "CoCMT: Communication-Efficient Cross-Modal Transformer for Collaborative\n  Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCMT: Communication-Efficient Cross-Modal Transformer for Collaborative\n  Perception"
                },
                "summary": "Multi-agent collaborative perception enhances each agent perceptual\ncapabilities by sharing sensing information to cooperatively perform robot\nperception tasks. This approach has proven effective in addressing challenges\nsuch as sensor deficiencies, occlusions, and long-range perception. However,\nexisting representative collaborative perception systems transmit intermediate\nfeature maps, such as bird-eye view (BEV) representations, which contain a\nsignificant amount of non-critical information, leading to high communication\nbandwidth requirements. To enhance communication efficiency while preserving\nperception capability, we introduce CoCMT, an object-query-based collaboration\nframework that optimizes communication bandwidth by selectively extracting and\ntransmitting essential features. Within CoCMT, we introduce the Efficient Query\nTransformer (EQFormer) to effectively fuse multi-agent object queries and\nimplement a synergistic deep supervision to enhance the positive reinforcement\nbetween stages, leading to improved overall performance. Experiments on OPV2V\nand V2V4Real datasets show CoCMT outperforms state-of-the-art methods while\ndrastically reducing communication needs. On V2V4Real, our model (Top-50 object\nqueries) requires only 0.416 Mb bandwidth, 83 times less than SOTA methods,\nwhile improving AP70 by 1.1 percent. This efficiency breakthrough enables\npractical collaborative perception deployment in bandwidth-constrained\nenvironments without sacrificing detection accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent collaborative perception enhances each agent perceptual\ncapabilities by sharing sensing information to cooperatively perform robot\nperception tasks. This approach has proven effective in addressing challenges\nsuch as sensor deficiencies, occlusions, and long-range perception. However,\nexisting representative collaborative perception systems transmit intermediate\nfeature maps, such as bird-eye view (BEV) representations, which contain a\nsignificant amount of non-critical information, leading to high communication\nbandwidth requirements. To enhance communication efficiency while preserving\nperception capability, we introduce CoCMT, an object-query-based collaboration\nframework that optimizes communication bandwidth by selectively extracting and\ntransmitting essential features. Within CoCMT, we introduce the Efficient Query\nTransformer (EQFormer) to effectively fuse multi-agent object queries and\nimplement a synergistic deep supervision to enhance the positive reinforcement\nbetween stages, leading to improved overall performance. Experiments on OPV2V\nand V2V4Real datasets show CoCMT outperforms state-of-the-art methods while\ndrastically reducing communication needs. On V2V4Real, our model (Top-50 object\nqueries) requires only 0.416 Mb bandwidth, 83 times less than SOTA methods,\nwhile improving AP70 by 1.1 percent. This efficiency breakthrough enables\npractical collaborative perception deployment in bandwidth-constrained\nenvironments without sacrificing detection accuracy."
                },
                "authors": [
                    {
                        "name": "Rujia Wang"
                    },
                    {
                        "name": "Xiangbo Gao"
                    },
                    {
                        "name": "Hao Xiang"
                    },
                    {
                        "name": "Runsheng Xu"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19288v2",
                "updated": "2025-07-01T01:07:35Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    1,
                    7,
                    35,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-24T03:48:48Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    3,
                    48,
                    48,
                    1,
                    175,
                    0
                ],
                "title": "Da Yu: Towards USV-Based Image Captioning for Waterway Surveillance and\n  Scene Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Da Yu: Towards USV-Based Image Captioning for Waterway Surveillance and\n  Scene Understanding"
                },
                "summary": "Automated waterway environment perception is crucial for enabling unmanned\nsurface vessels (USVs) to understand their surroundings and make informed\ndecisions. Most existing waterway perception models primarily focus on\ninstance-level object perception paradigms (e.g., detection, segmentation).\nHowever, due to the complexity of waterway environments, current perception\ndatasets and models fail to achieve global semantic understanding of waterways,\nlimiting large-scale monitoring and structured log generation. With the\nadvancement of vision-language models (VLMs), we leverage image captioning to\nintroduce WaterCaption, the first captioning dataset specifically designed for\nwaterway environments. WaterCaption focuses on fine-grained, multi-region\nlong-text descriptions, providing a new research direction for visual\ngeo-understanding and spatial scene cognition. Exactly, it includes 20.2k\nimage-text pair data with 1.8 million vocabulary size. Additionally, we propose\nDa Yu, an edge-deployable multi-modal large language model for USVs, where we\npropose a novel vision-to-language projector called Nano Transformer Adaptor\n(NTA). NTA effectively balances computational efficiency with the capacity for\nboth global and fine-grained local modeling of visual features, thereby\nsignificantly enhancing the model's ability to generate long-form textual\noutputs. Da Yu achieves an optimal balance between performance and efficiency,\nsurpassing state-of-the-art models on WaterCaption and several other captioning\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated waterway environment perception is crucial for enabling unmanned\nsurface vessels (USVs) to understand their surroundings and make informed\ndecisions. Most existing waterway perception models primarily focus on\ninstance-level object perception paradigms (e.g., detection, segmentation).\nHowever, due to the complexity of waterway environments, current perception\ndatasets and models fail to achieve global semantic understanding of waterways,\nlimiting large-scale monitoring and structured log generation. With the\nadvancement of vision-language models (VLMs), we leverage image captioning to\nintroduce WaterCaption, the first captioning dataset specifically designed for\nwaterway environments. WaterCaption focuses on fine-grained, multi-region\nlong-text descriptions, providing a new research direction for visual\ngeo-understanding and spatial scene cognition. Exactly, it includes 20.2k\nimage-text pair data with 1.8 million vocabulary size. Additionally, we propose\nDa Yu, an edge-deployable multi-modal large language model for USVs, where we\npropose a novel vision-to-language projector called Nano Transformer Adaptor\n(NTA). NTA effectively balances computational efficiency with the capacity for\nboth global and fine-grained local modeling of visual features, thereby\nsignificantly enhancing the model's ability to generate long-form textual\noutputs. Da Yu achieves an optimal balance between performance and efficiency,\nsurpassing state-of-the-art models on WaterCaption and several other captioning\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Runwei Guan"
                    },
                    {
                        "name": "Ningwei Ouyang"
                    },
                    {
                        "name": "Tianhao Xu"
                    },
                    {
                        "name": "Shaofeng Liang"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Yafeng Sun"
                    },
                    {
                        "name": "Shang Gao"
                    },
                    {
                        "name": "Songning Lai"
                    },
                    {
                        "name": "Shanliang Yao"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Ryan Wen Liu"
                    },
                    {
                        "name": "Yutao Yue"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "14 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22554v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22554v2",
                "updated": "2025-07-01T01:02:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    1,
                    2,
                    44,
                    1,
                    182,
                    0
                ],
                "published": "2025-06-27T18:09:49Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    18,
                    9,
                    49,
                    4,
                    178,
                    0
                ],
                "title": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale\n  Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale\n  Dataset"
                },
                "summary": "Human communication involves a complex interplay of verbal and nonverbal\nsignals, essential for conveying meaning and achieving interpersonal goals. To\ndevelop socially intelligent AI technologies, it is crucial to develop models\nthat can both comprehend and generate dyadic behavioral dynamics. To this end,\nwe introduce the Seamless Interaction Dataset, a large-scale collection of over\n4,000 hours of face-to-face interaction footage from over 4,000 participants in\ndiverse contexts. This dataset enables the development of AI technologies that\nunderstand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,\ntelepresence experiences, and multimodal content analysis tools. We also\ndevelop a suite of models that utilize the dataset to generate dyadic motion\ngestures and facial expressions aligned with human speech. These models can\ntake as input both the speech and visual behavior of their interlocutors. We\npresent a variant with speech from an LLM model and integrations with 2D and 3D\nrendering methods, bringing us closer to interactive virtual agents.\nAdditionally, we describe controllable variants of our motion models that can\nadapt emotional responses and expressivity levels, as well as generating more\nsemantically-relevant gestures. Finally, we discuss methods for assessing the\nquality of these dyadic motion models, which are demonstrating the potential\nfor more intuitive and responsive human-AI interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human communication involves a complex interplay of verbal and nonverbal\nsignals, essential for conveying meaning and achieving interpersonal goals. To\ndevelop socially intelligent AI technologies, it is crucial to develop models\nthat can both comprehend and generate dyadic behavioral dynamics. To this end,\nwe introduce the Seamless Interaction Dataset, a large-scale collection of over\n4,000 hours of face-to-face interaction footage from over 4,000 participants in\ndiverse contexts. This dataset enables the development of AI technologies that\nunderstand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,\ntelepresence experiences, and multimodal content analysis tools. We also\ndevelop a suite of models that utilize the dataset to generate dyadic motion\ngestures and facial expressions aligned with human speech. These models can\ntake as input both the speech and visual behavior of their interlocutors. We\npresent a variant with speech from an LLM model and integrations with 2D and 3D\nrendering methods, bringing us closer to interactive virtual agents.\nAdditionally, we describe controllable variants of our motion models that can\nadapt emotional responses and expressivity levels, as well as generating more\nsemantically-relevant gestures. Finally, we discuss methods for assessing the\nquality of these dyadic motion models, which are demonstrating the potential\nfor more intuitive and responsive human-AI interactions."
                },
                "authors": [
                    {
                        "name": "Vasu Agrawal"
                    },
                    {
                        "name": "Akinniyi Akinyemi"
                    },
                    {
                        "name": "Kathryn Alvero"
                    },
                    {
                        "name": "Morteza Behrooz"
                    },
                    {
                        "name": "Julia Buffalini"
                    },
                    {
                        "name": "Fabio Maria Carlucci"
                    },
                    {
                        "name": "Joy Chen"
                    },
                    {
                        "name": "Junming Chen"
                    },
                    {
                        "name": "Zhang Chen"
                    },
                    {
                        "name": "Shiyang Cheng"
                    },
                    {
                        "name": "Praveen Chowdary"
                    },
                    {
                        "name": "Joe Chuang"
                    },
                    {
                        "name": "Antony D'Avirro"
                    },
                    {
                        "name": "Jon Daly"
                    },
                    {
                        "name": "Ning Dong"
                    },
                    {
                        "name": "Mark Duppenthaler"
                    },
                    {
                        "name": "Cynthia Gao"
                    },
                    {
                        "name": "Jeff Girard"
                    },
                    {
                        "name": "Martin Gleize"
                    },
                    {
                        "name": "Sahir Gomez"
                    },
                    {
                        "name": "Hongyu Gong"
                    },
                    {
                        "name": "Srivathsan Govindarajan"
                    },
                    {
                        "name": "Brandon Han"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Denise Hernandez"
                    },
                    {
                        "name": "Yordan Hristov"
                    },
                    {
                        "name": "Rongjie Huang"
                    },
                    {
                        "name": "Hirofumi Inaguma"
                    },
                    {
                        "name": "Somya Jain"
                    },
                    {
                        "name": "Raj Janardhan"
                    },
                    {
                        "name": "Qingyao Jia"
                    },
                    {
                        "name": "Christopher Klaiber"
                    },
                    {
                        "name": "Dejan Kovachev"
                    },
                    {
                        "name": "Moneish Kumar"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Yilei Li"
                    },
                    {
                        "name": "Pavel Litvin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Guangyao Ma"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Martin Ma"
                    },
                    {
                        "name": "Xutai Ma"
                    },
                    {
                        "name": "Lucas Mantovani"
                    },
                    {
                        "name": "Sagar Miglani"
                    },
                    {
                        "name": "Sreyas Mohan"
                    },
                    {
                        "name": "Louis-Philippe Morency"
                    },
                    {
                        "name": "Evonne Ng"
                    },
                    {
                        "name": "Kam-Woh Ng"
                    },
                    {
                        "name": "Tu Anh Nguyen"
                    },
                    {
                        "name": "Amia Oberai"
                    },
                    {
                        "name": "Benjamin Peloquin"
                    },
                    {
                        "name": "Juan Pino"
                    },
                    {
                        "name": "Jovan Popovic"
                    },
                    {
                        "name": "Omid Poursaeed"
                    },
                    {
                        "name": "Fabian Prada"
                    },
                    {
                        "name": "Alice Rakotoarison"
                    },
                    {
                        "name": "Rakesh Ranjan"
                    },
                    {
                        "name": "Alexander Richard"
                    },
                    {
                        "name": "Christophe Ropers"
                    },
                    {
                        "name": "Safiyyah Saleem"
                    },
                    {
                        "name": "Vasu Sharma"
                    },
                    {
                        "name": "Alex Shcherbyna"
                    },
                    {
                        "name": "Jia Shen"
                    },
                    {
                        "name": "Jie Shen"
                    },
                    {
                        "name": "Anastasis Stathopoulos"
                    },
                    {
                        "name": "Anna Sun"
                    },
                    {
                        "name": "Paden Tomasello"
                    },
                    {
                        "name": "Tuan Tran"
                    },
                    {
                        "name": "Arina Turkatenko"
                    },
                    {
                        "name": "Bo Wan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Jeff Wang"
                    },
                    {
                        "name": "Mary Williamson"
                    },
                    {
                        "name": "Carleigh Wood"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Yilin Yang"
                    },
                    {
                        "name": "Julien Yao"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Jiemin Zhang"
                    },
                    {
                        "name": "Xinyue Zhang"
                    },
                    {
                        "name": "Jason Zheng"
                    },
                    {
                        "name": "Pavlo Zhyzheria"
                    },
                    {
                        "name": "Jan Zikes"
                    },
                    {
                        "name": "Michael Zollhoefer"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zollhoefer"
                },
                "author": "Michael Zollhoefer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22554v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22554v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.02277v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.02277v3",
                "updated": "2025-07-01T00:13:44Z",
                "updated_parsed": [
                    2025,
                    7,
                    1,
                    0,
                    13,
                    44,
                    1,
                    182,
                    0
                ],
                "published": "2023-09-29T22:55:06Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    22,
                    55,
                    6,
                    4,
                    272,
                    0
                ],
                "title": "Junk DNA Hypothesis: Pruning Small Pre-Trained Weights Irreversibly and\n  Monotonically Impairs \"Difficult\" Downstream Tasks in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Junk DNA Hypothesis: Pruning Small Pre-Trained Weights Irreversibly and\n  Monotonically Impairs \"Difficult\" Downstream Tasks in LLMs"
                },
                "summary": "We present Junk DNA Hypothesis by adopting a novel task-centric angle for the\npre-trained weights of large language models (LLMs). It has been believed that\nweights in LLMs contain significant redundancy, leading to the conception that\na considerable chunk of the parameters can be removed by pruning without\ncompromising performance. Contrary to this belief, this paper presents a\ncounter-argument: small-magnitude weights of pre-trained model weights encode\nvital knowledge essential for tackling difficult downstream tasks - manifested\nas the monotonic relationship between the performance drop of downstream tasks\nacross the difficulty spectrum, as we prune more pre-trained weights by\nmagnitude. Moreover, we reveal that these seemingly inconsequential weights can\nresult in irreparable loss of knowledge and performance degradation in\ndifficult tasks, even when downstream continual training is allowed.\nInterestingly, our evaluations show that the other popular compression, namely\nquantization, fails to exhibit similar monotonic effect and does not as\nconvincingly disentangle this task-difficulty information. To study formally,\nwe introduce several quantifiable metrics to gauge the downstream task\ndifficulty: (1) within the same task category, and (2) across different task\ncategories. Our extensive experiments substantiate the Junk DNA Hypothesis\nacross a diverse range of model sizes, tasks, datasets, and even pruning\nmethods. Codes are available at:\nhttps://github.com/VITA-Group/Junk_DNA_Hypothesis.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Junk DNA Hypothesis by adopting a novel task-centric angle for the\npre-trained weights of large language models (LLMs). It has been believed that\nweights in LLMs contain significant redundancy, leading to the conception that\na considerable chunk of the parameters can be removed by pruning without\ncompromising performance. Contrary to this belief, this paper presents a\ncounter-argument: small-magnitude weights of pre-trained model weights encode\nvital knowledge essential for tackling difficult downstream tasks - manifested\nas the monotonic relationship between the performance drop of downstream tasks\nacross the difficulty spectrum, as we prune more pre-trained weights by\nmagnitude. Moreover, we reveal that these seemingly inconsequential weights can\nresult in irreparable loss of knowledge and performance degradation in\ndifficult tasks, even when downstream continual training is allowed.\nInterestingly, our evaluations show that the other popular compression, namely\nquantization, fails to exhibit similar monotonic effect and does not as\nconvincingly disentangle this task-difficulty information. To study formally,\nwe introduce several quantifiable metrics to gauge the downstream task\ndifficulty: (1) within the same task category, and (2) across different task\ncategories. Our extensive experiments substantiate the Junk DNA Hypothesis\nacross a diverse range of model sizes, tasks, datasets, and even pruning\nmethods. Codes are available at:\nhttps://github.com/VITA-Group/Junk_DNA_Hypothesis.git."
                },
                "authors": [
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Ajay Jaiswal"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "Published at ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.02277v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.02277v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18734v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18734v6",
                "updated": "2025-06-30T23:51:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    23,
                    51,
                    42,
                    0,
                    181,
                    0
                ],
                "published": "2024-05-29T03:28:16Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    3,
                    28,
                    16,
                    2,
                    150,
                    0
                ],
                "title": "Information Entropy Guided Height-aware Histogram for\n  Quantization-friendly Pillar Feature Encoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information Entropy Guided Height-aware Histogram for\n  Quantization-friendly Pillar Feature Encoder"
                },
                "summary": "Real-time and high-performance 3D object detection plays a critical role in\nautonomous driving and robotics. Recent pillar-based 3D object detectors have\ngained significant attention due to their compact representation and low\ncomputational overhead, making them suitable for onboard deployment and\nquantization. However, existing pillar-based detectors still suffer from\ninformation loss along height dimension and large numerical distribution\ndifference during pillar feature encoding (PFE), which severely limits their\nperformance and quantization potential. To address above issue, we first unveil\nthe importance of different input information during PFE and identify the\nheight dimension as a key factor in enhancing 3D detection performance.\nMotivated by this observation, we propose a height-aware pillar feature\nencoder, called PillarHist. Specifically, PillarHist statistics the discrete\ndistribution of points at different heights within one pillar with the\ninformation entropy guidance. This simple yet effective design greatly\npreserves the information along the height dimension while significantly\nreducing the computation overhead of the PFE. Meanwhile, PillarHist also\nconstrains the arithmetic distribution of PFE input to a stable range, making\nit quantization-friendly. Notably, PillarHist operates exclusively within the\nPFE stage to enhance performance, enabling seamless integration into existing\npillar-based methods without introducing complex operations. Extensive\nexperiments show the effectiveness of PillarHist in terms of both efficiency\nand performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time and high-performance 3D object detection plays a critical role in\nautonomous driving and robotics. Recent pillar-based 3D object detectors have\ngained significant attention due to their compact representation and low\ncomputational overhead, making them suitable for onboard deployment and\nquantization. However, existing pillar-based detectors still suffer from\ninformation loss along height dimension and large numerical distribution\ndifference during pillar feature encoding (PFE), which severely limits their\nperformance and quantization potential. To address above issue, we first unveil\nthe importance of different input information during PFE and identify the\nheight dimension as a key factor in enhancing 3D detection performance.\nMotivated by this observation, we propose a height-aware pillar feature\nencoder, called PillarHist. Specifically, PillarHist statistics the discrete\ndistribution of points at different heights within one pillar with the\ninformation entropy guidance. This simple yet effective design greatly\npreserves the information along the height dimension while significantly\nreducing the computation overhead of the PFE. Meanwhile, PillarHist also\nconstrains the arithmetic distribution of PFE input to a stable range, making\nit quantization-friendly. Notably, PillarHist operates exclusively within the\nPFE stage to enhance performance, enabling seamless integration into existing\npillar-based methods without introducing complex operations. Extensive\nexperiments show the effectiveness of PillarHist in terms of both efficiency\nand performance."
                },
                "authors": [
                    {
                        "name": "Sifan Zhou"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Ziyu Zhao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Yuguang Shi"
                    },
                    {
                        "name": "Xiaobo Lu"
                    },
                    {
                        "name": "Qiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Wu"
                },
                "author": "Qiang Wu",
                "arxiv_comment": "Need further revision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18734v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18734v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16722v2",
                "updated": "2025-06-30T22:55:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    22,
                    55,
                    54,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-22T14:30:14Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    14,
                    30,
                    14,
                    3,
                    142,
                    0
                ],
                "title": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification"
                },
                "summary": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 392 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly prevalent in global\napplications, ensuring that they are toxicity-free across diverse linguistic\ncontexts remains a critical challenge. We explore \"Cross-lingual\nDetoxification\", a cross-lingual paradigm that mitigates toxicity, enabling\ndetoxification capabilities to transfer between high and low-resource languages\nacross different script families. We analyze cross-lingual detoxification's\neffectiveness through 392 extensive settings to evaluate toxicity reduction in\ncross-distribution settings with limited data and investigate how mitigation\nimpacts model performance on non-toxic tasks, revealing trade-offs between\nsafety and knowledge preservation. Our code and dataset are publicly available\nat https://github.com/himanshubeniwal/Breaking-mBad."
                },
                "authors": [
                    {
                        "name": "Himanshu Beniwal"
                    },
                    {
                        "name": "Youngwoo Kim"
                    },
                    {
                        "name": "Maarten Sap"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Hartvigsen"
                },
                "author": "Thomas Hartvigsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09850v2",
                "updated": "2025-06-30T22:53:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    22,
                    53,
                    19,
                    0,
                    181,
                    0
                ],
                "published": "2025-03-12T21:13:41Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    21,
                    13,
                    41,
                    2,
                    71,
                    0
                ],
                "title": "TabNSA: Native Sparse Attention for Efficient Tabular Data Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabNSA: Native Sparse Attention for Efficient Tabular Data Learning"
                },
                "summary": "Tabular data poses unique challenges for deep learning due to its\nheterogeneous feature types, lack of spatial structure, and often limited\nsample sizes. We propose TabNSA, a novel deep learning framework that\nintegrates Native Sparse Attention (NSA) with a TabMixer backbone to\nefficiently model tabular data. TabNSA tackles computational and\nrepresentational challenges by dynamically focusing on relevant feature subsets\nper instance. The NSA module employs a hierarchical sparse attention mechanism,\nincluding token compression, selective preservation, and localized sliding\nwindows, to significantly reduce the quadratic complexity of standard attention\noperations while addressing feature heterogeneity. Complementing this, the\nTabMixer backbone captures complex, non-linear dependencies through parallel\nmultilayer perceptron (MLP) branches with independent parameters. These modules\nare synergistically combined via element-wise summation and mean pooling,\nenabling TabNSA to model both global context and fine-grained interactions.\nExtensive experiments across supervised and transfer learning settings show\nthat TabNSA consistently outperforms state-of-the-art deep learning models.\nFurthermore, by augmenting TabNSA with a fine-tuned large language model (LLM),\nwe enable it to effectively address Few-Shot Learning challenges through\nlanguage-guided generalization on diverse tabular benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data poses unique challenges for deep learning due to its\nheterogeneous feature types, lack of spatial structure, and often limited\nsample sizes. We propose TabNSA, a novel deep learning framework that\nintegrates Native Sparse Attention (NSA) with a TabMixer backbone to\nefficiently model tabular data. TabNSA tackles computational and\nrepresentational challenges by dynamically focusing on relevant feature subsets\nper instance. The NSA module employs a hierarchical sparse attention mechanism,\nincluding token compression, selective preservation, and localized sliding\nwindows, to significantly reduce the quadratic complexity of standard attention\noperations while addressing feature heterogeneity. Complementing this, the\nTabMixer backbone captures complex, non-linear dependencies through parallel\nmultilayer perceptron (MLP) branches with independent parameters. These modules\nare synergistically combined via element-wise summation and mean pooling,\nenabling TabNSA to model both global context and fine-grained interactions.\nExtensive experiments across supervised and transfer learning settings show\nthat TabNSA consistently outperforms state-of-the-art deep learning models.\nFurthermore, by augmenting TabNSA with a fine-tuned large language model (LLM),\nwe enable it to effectively address Few-Shot Learning challenges through\nlanguage-guided generalization on diverse tabular benchmarks."
                },
                "authors": [
                    {
                        "name": "Ali Eslamian"
                    },
                    {
                        "name": "Qiang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Cheng"
                },
                "author": "Qiang Cheng",
                "arxiv_comment": "26 pages, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05175v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05175v4",
                "updated": "2025-06-30T22:16:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    22,
                    16,
                    39,
                    0,
                    181,
                    0
                ],
                "published": "2023-10-08T14:22:58Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    14,
                    22,
                    58,
                    6,
                    281,
                    0
                ],
                "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for\n  Pruning LLMs to High Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for\n  Pruning LLMs to High Sparsity"
                },
                "summary": "Large Language Models (LLMs), renowned for their remarkable performance\nacross diverse domains, present a challenge when it comes to practical\ndeployment due to their colossal model size. In response to this challenge,\nefforts have been directed toward the application of traditional network\npruning techniques to LLMs, uncovering a massive number of parameters that can\nbe pruned in one-shot without hurting performance. Prevailing LLM pruning\nstrategies have consistently adhered to the practice of uniformly pruning all\nlayers at equivalent sparsity, resulting in robust performance. However, this\nobservation stands in contrast to the prevailing trends observed in the field\nof vision models, where non-uniform layerwise sparsity typically yields\nstronger results. To understand the underlying reasons for this disparity, we\nconduct a comprehensive study and discover a strong correlation with the\nemergence of activation outliers in LLMs. Inspired by this finding, we\nintroduce a novel LLM pruning methodology that incorporates a tailored set of\nnon-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise\nsparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio\nobserved within each layer, facilitating a more effective alignment between\nlayerwise weight sparsity and outlier ratios. Our empirical evaluation,\nconducted across the LLaMA-V1 family and OPT, spanning various benchmarks,\ndemonstrates the distinct advantages offered by OWL over previous methods. For\ninstance, OWL exhibits a remarkable performance gain, surpassing the\nstate-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high\nsparsity level of 70%, respectively, while delivering 2.6x end-to-end inference\nspeed-up in the DeepSparse inference engine. Codes are available at\nhttps://github.com/luuyin/OWL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), renowned for their remarkable performance\nacross diverse domains, present a challenge when it comes to practical\ndeployment due to their colossal model size. In response to this challenge,\nefforts have been directed toward the application of traditional network\npruning techniques to LLMs, uncovering a massive number of parameters that can\nbe pruned in one-shot without hurting performance. Prevailing LLM pruning\nstrategies have consistently adhered to the practice of uniformly pruning all\nlayers at equivalent sparsity, resulting in robust performance. However, this\nobservation stands in contrast to the prevailing trends observed in the field\nof vision models, where non-uniform layerwise sparsity typically yields\nstronger results. To understand the underlying reasons for this disparity, we\nconduct a comprehensive study and discover a strong correlation with the\nemergence of activation outliers in LLMs. Inspired by this finding, we\nintroduce a novel LLM pruning methodology that incorporates a tailored set of\nnon-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise\nsparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio\nobserved within each layer, facilitating a more effective alignment between\nlayerwise weight sparsity and outlier ratios. Our empirical evaluation,\nconducted across the LLaMA-V1 family and OPT, spanning various benchmarks,\ndemonstrates the distinct advantages offered by OWL over previous methods. For\ninstance, OWL exhibits a remarkable performance gain, surpassing the\nstate-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high\nsparsity level of 70%, respectively, while delivering 2.6x end-to-end inference\nspeed-up in the DeepSparse inference engine. Codes are available at\nhttps://github.com/luuyin/OWL."
                },
                "authors": [
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Cheng-Yu Hsieh"
                    },
                    {
                        "name": "Yaqing Wang"
                    },
                    {
                        "name": "Yiling Jia"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Ajay Jaiswal"
                    },
                    {
                        "name": "Mykola Pechenizkiy"
                    },
                    {
                        "name": "Yi Liang"
                    },
                    {
                        "name": "Michael Bendersky"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "arxiv_comment": "Published at ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05175v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05175v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22419v2",
                "updated": "2025-06-30T21:56:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    21,
                    56,
                    29,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-27T17:44:32Z",
                "published_parsed": [
                    2025,
                    6,
                    27,
                    17,
                    44,
                    32,
                    4,
                    178,
                    0
                ],
                "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements"
                },
                "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent."
                },
                "authors": [
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Despoina Magka"
                    },
                    {
                        "name": "Minqi Jiang"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Roberta Raileanu"
                    },
                    {
                        "name": "Tatiana Shavrina"
                    },
                    {
                        "name": "Jean-Christophe Gagnon-Audet"
                    },
                    {
                        "name": "Kelvin Niu"
                    },
                    {
                        "name": "Shagun Sodhani"
                    },
                    {
                        "name": "Michael Shvartsman"
                    },
                    {
                        "name": "Andrei Lupu"
                    },
                    {
                        "name": "Alisia Lupidi"
                    },
                    {
                        "name": "Edan Toledo"
                    },
                    {
                        "name": "Karen Hambardzumyan"
                    },
                    {
                        "name": "Martin Josifoski"
                    },
                    {
                        "name": "Thomas Foster"
                    },
                    {
                        "name": "Lucia Cipolina-Kun"
                    },
                    {
                        "name": "Abhishek Charnalia"
                    },
                    {
                        "name": "Derek Dunfield"
                    },
                    {
                        "name": "Alexander H. Miller"
                    },
                    {
                        "name": "Oisin Mac Aodha"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Yoram Bachrach"
                    }
                ],
                "author_detail": {
                    "name": "Yoram Bachrach"
                },
                "author": "Yoram Bachrach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14640v2",
                "updated": "2025-06-30T21:28:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    21,
                    28,
                    41,
                    0,
                    181,
                    0
                ],
                "published": "2024-01-26T04:11:07Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    4,
                    11,
                    7,
                    4,
                    26,
                    0
                ],
                "title": "Can LLMs Evaluate Complex Attribution in QA? Automatic Benchmarking\n  using Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Evaluate Complex Attribution in QA? Automatic Benchmarking\n  using Knowledge Graphs"
                },
                "summary": "Attributed Question Answering (AQA) has attracted wide attention, but there\nare still several limitations in evaluating the attributions, including lacking\nfine-grained attribution categories, relying on manual annotations, and failing\nto compare attributions with only subtle differences. To bridge these gaps, we\nintroduce Complex Attributed Question Answering (CAQA), a large-scale benchmark\ncontaining comprehensive attribution categories, automatically generated using\nKnowledge Graphs (KGs), and complex attribution scenarios. We have conducted\nextensive experiments to verify the effectiveness of CAQA, including the\nbenchmarking of 25 automatic evaluators, their comparison with human\nevaluators, the testing of LLM evaluators fine-tuned by CAQA and so on. These\nexperiments also lead to a series of important findings that can benefit the\nfuture research of AQA. All the codes and data are publicly accessible at\nhttps://github.com/HuuuNan/CAQA-Benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attributed Question Answering (AQA) has attracted wide attention, but there\nare still several limitations in evaluating the attributions, including lacking\nfine-grained attribution categories, relying on manual annotations, and failing\nto compare attributions with only subtle differences. To bridge these gaps, we\nintroduce Complex Attributed Question Answering (CAQA), a large-scale benchmark\ncontaining comprehensive attribution categories, automatically generated using\nKnowledge Graphs (KGs), and complex attribution scenarios. We have conducted\nextensive experiments to verify the effectiveness of CAQA, including the\nbenchmarking of 25 automatic evaluators, their comparison with human\nevaluators, the testing of LLM evaluators fine-tuned by CAQA and so on. These\nexperiments also lead to a series of important findings that can benefit the\nfuture research of AQA. All the codes and data are publicly accessible at\nhttps://github.com/HuuuNan/CAQA-Benchmark."
                },
                "authors": [
                    {
                        "name": "Nan Hu"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Yike Wu"
                    },
                    {
                        "name": "Guilin Qi"
                    },
                    {
                        "name": "Hongru Wang"
                    },
                    {
                        "name": "Sheng Bi"
                    },
                    {
                        "name": "Yongrui Chen"
                    },
                    {
                        "name": "Tongtong Wu"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "arxiv_comment": "Accepted to ACL 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17117v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17117v3",
                "updated": "2025-06-30T21:22:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    21,
                    22,
                    39,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-21T16:29:00Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    16,
                    29,
                    0,
                    2,
                    141,
                    0
                ],
                "title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for\n  Meaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for\n  Meaning"
                },
                "summary": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations."
                },
                "authors": [
                    {
                        "name": "Chen Shani"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Ravid Shwartz-Ziv"
                    }
                ],
                "author_detail": {
                    "name": "Ravid Shwartz-Ziv"
                },
                "author": "Ravid Shwartz-Ziv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17117v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17117v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14373v2",
                "updated": "2025-06-30T21:12:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    21,
                    12,
                    19,
                    0,
                    181,
                    0
                ],
                "published": "2024-12-18T22:13:21Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    13,
                    21,
                    2,
                    353,
                    0
                ],
                "title": "ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram\n  Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram\n  Language Modeling"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndomains, including applications to electrocardiograms (ECGs). A growing body of\nwork focuses on generating text from multi-channeled ECG signals and\ncorresponding textual prompts. Existing approaches often involve a two-stage\nprocess: pretraining an ECG-specific encoder with a self-supervised learning\n(SSL) objective, followed by finetuning an LLM for natural language generation\n(NLG) using encoder-derived features. However, these methods face two key\nlimitations: inefficiency due to multi-stage training and challenges in\ninterpreting encoder-generated features. To overcome these issues, we propose\nECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for\nautoregressive language modeling of ECGs. ECG-Byte compresses and encodes ECG\nsignals into tokens, enabling direct end-to-end LLM training by combining ECG\nand text tokens. This approach enhances interpretability, as ECG tokens can be\ndirectly mapped back to the original signals. Leveraging ECG-Byte, we achieve\ncompetitive NLG performance while training 3 times faster and using just 48\\%\nof the data required by traditional two-stage methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional versatility across\ndomains, including applications to electrocardiograms (ECGs). A growing body of\nwork focuses on generating text from multi-channeled ECG signals and\ncorresponding textual prompts. Existing approaches often involve a two-stage\nprocess: pretraining an ECG-specific encoder with a self-supervised learning\n(SSL) objective, followed by finetuning an LLM for natural language generation\n(NLG) using encoder-derived features. However, these methods face two key\nlimitations: inefficiency due to multi-stage training and challenges in\ninterpreting encoder-generated features. To overcome these issues, we propose\nECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for\nautoregressive language modeling of ECGs. ECG-Byte compresses and encodes ECG\nsignals into tokens, enabling direct end-to-end LLM training by combining ECG\nand text tokens. This approach enhances interpretability, as ECG tokens can be\ndirectly mapped back to the original signals. Leveraging ECG-Byte, we achieve\ncompetitive NLG performance while training 3 times faster and using just 48\\%\nof the data required by traditional two-stage methods."
                },
                "authors": [
                    {
                        "name": "William Han"
                    },
                    {
                        "name": "Chaojing Duan"
                    },
                    {
                        "name": "Michael A. Rosenberg"
                    },
                    {
                        "name": "Emerson Liu"
                    },
                    {
                        "name": "Ding Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Ding Zhao"
                },
                "author": "Ding Zhao",
                "arxiv_comment": "38 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20702v2",
                "updated": "2025-06-30T21:04:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    21,
                    4,
                    58,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-25T17:59:50Z",
                "published_parsed": [
                    2025,
                    6,
                    25,
                    17,
                    59,
                    50,
                    2,
                    176,
                    0
                ],
                "title": "The Singapore Consensus on Global AI Safety Research Priorities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Singapore Consensus on Global AI Safety Research Priorities"
                },
                "summary": "Rapidly improving AI capabilities and autonomy hold significant promise of\ntransformation, but are also driving vigorous debate on how to ensure that AI\nis safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem\nis therefore essential -- it helps people embrace AI with confidence and gives\nmaximal space for innovation while avoiding backlash.\n  The \"2025 Singapore Conference on AI (SCAI): International Scientific\nExchange on AI Safety\" aimed to support research in this space by bringing\ntogether AI scientists across geographies to identify and synthesise research\npriorities in AI safety. This resulting report builds on the International AI\nSafety Report chaired by Yoshua Bengio and backed by 33 governments. By\nadopting a defence-in-depth model, this report organises AI safety research\ndomains into three types: challenges with creating trustworthy AI systems\n(Development), challenges with evaluating their risks (Assessment), and\nchallenges with monitoring and intervening after deployment (Control).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapidly improving AI capabilities and autonomy hold significant promise of\ntransformation, but are also driving vigorous debate on how to ensure that AI\nis safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem\nis therefore essential -- it helps people embrace AI with confidence and gives\nmaximal space for innovation while avoiding backlash.\n  The \"2025 Singapore Conference on AI (SCAI): International Scientific\nExchange on AI Safety\" aimed to support research in this space by bringing\ntogether AI scientists across geographies to identify and synthesise research\npriorities in AI safety. This resulting report builds on the International AI\nSafety Report chaired by Yoshua Bengio and backed by 33 governments. By\nadopting a defence-in-depth model, this report organises AI safety research\ndomains into three types: challenges with creating trustworthy AI systems\n(Development), challenges with evaluating their risks (Assessment), and\nchallenges with monitoring and intervening after deployment (Control)."
                },
                "authors": [
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Tegan Maharaj"
                    },
                    {
                        "name": "Luke Ong"
                    },
                    {
                        "name": "Stuart Russell"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Max Tegmark"
                    },
                    {
                        "name": "Lan Xue"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Wan Sie Lee"
                    },
                    {
                        "name": "Sren Mindermann"
                    },
                    {
                        "name": "Vanessa Wilfred"
                    },
                    {
                        "name": "Vidhisha Balachandran"
                    },
                    {
                        "name": "Fazl Barez"
                    },
                    {
                        "name": "Michael Belinsky"
                    },
                    {
                        "name": "Imane Bello"
                    },
                    {
                        "name": "Malo Bourgon"
                    },
                    {
                        "name": "Mark Brakel"
                    },
                    {
                        "name": "Simon Campos"
                    },
                    {
                        "name": "Duncan Cass-Beggs"
                    },
                    {
                        "name": "Jiahao Chen"
                    },
                    {
                        "name": "Rumman Chowdhury"
                    },
                    {
                        "name": "Kuan Chua Seah"
                    },
                    {
                        "name": "Jeff Clune"
                    },
                    {
                        "name": "Juntao Dai"
                    },
                    {
                        "name": "Agnes Delaborde"
                    },
                    {
                        "name": "Nouha Dziri"
                    },
                    {
                        "name": "Francisco Eiras"
                    },
                    {
                        "name": "Joshua Engels"
                    },
                    {
                        "name": "Jinyu Fan"
                    },
                    {
                        "name": "Adam Gleave"
                    },
                    {
                        "name": "Noah Goodman"
                    },
                    {
                        "name": "Fynn Heide"
                    },
                    {
                        "name": "Johannes Heidecke"
                    },
                    {
                        "name": "Dan Hendrycks"
                    },
                    {
                        "name": "Cyrus Hodes"
                    },
                    {
                        "name": "Bryan Low Kian Hsiang"
                    },
                    {
                        "name": "Minlie Huang"
                    },
                    {
                        "name": "Sami Jawhar"
                    },
                    {
                        "name": "Wang Jingyu"
                    },
                    {
                        "name": "Adam Tauman Kalai"
                    },
                    {
                        "name": "Meindert Kamphuis"
                    },
                    {
                        "name": "Mohan Kankanhalli"
                    },
                    {
                        "name": "Subhash Kantamneni"
                    },
                    {
                        "name": "Mathias Bonde Kirk"
                    },
                    {
                        "name": "Thomas Kwa"
                    },
                    {
                        "name": "Jeffrey Ladish"
                    },
                    {
                        "name": "Kwok-Yan Lam"
                    },
                    {
                        "name": "Wan Lee Sie"
                    },
                    {
                        "name": "Taewhi Lee"
                    },
                    {
                        "name": "Xiaojian Li"
                    },
                    {
                        "name": "Jiajun Liu"
                    },
                    {
                        "name": "Chaochao Lu"
                    },
                    {
                        "name": "Yifan Mai"
                    },
                    {
                        "name": "Richard Mallah"
                    },
                    {
                        "name": "Julian Michael"
                    },
                    {
                        "name": "Nick Mos"
                    },
                    {
                        "name": "Simon Mller"
                    },
                    {
                        "name": "Kihyuk Nam"
                    },
                    {
                        "name": "Kwan Yee Ng"
                    },
                    {
                        "name": "Mark Nitzberg"
                    },
                    {
                        "name": "Besmira Nushi"
                    },
                    {
                        "name": "Sen O higeartaigh"
                    },
                    {
                        "name": "Alejandro Ortega"
                    },
                    {
                        "name": "Pierre Peign"
                    },
                    {
                        "name": "James Petrie"
                    },
                    {
                        "name": "Benjamin Prud'Homme"
                    },
                    {
                        "name": "Reihaneh Rabbany"
                    },
                    {
                        "name": "Nayat Sanchez-Pi"
                    },
                    {
                        "name": "Sarah Schwettmann"
                    },
                    {
                        "name": "Buck Shlegeris"
                    },
                    {
                        "name": "Saad Siddiqui"
                    },
                    {
                        "name": "Aradhana Sinha"
                    },
                    {
                        "name": "Martn Soto"
                    },
                    {
                        "name": "Cheston Tan"
                    },
                    {
                        "name": "Dong Ting"
                    },
                    {
                        "name": "William Tjhi"
                    },
                    {
                        "name": "Robert Trager"
                    },
                    {
                        "name": "Brian Tse"
                    },
                    {
                        "name": "Anthony Tung K. H."
                    },
                    {
                        "name": "Vanessa Wilfred"
                    },
                    {
                        "name": "John Willes"
                    },
                    {
                        "name": "Denise Wong"
                    },
                    {
                        "name": "Wei Xu"
                    },
                    {
                        "name": "Rongwu Xu"
                    },
                    {
                        "name": "Yi Zeng"
                    },
                    {
                        "name": "HongJiang Zhang"
                    },
                    {
                        "name": "Djordje ikeli"
                    }
                ],
                "author_detail": {
                    "name": "Djordje ikeli"
                },
                "author": "Djordje ikeli",
                "arxiv_comment": "Final report from the \"2025 Singapore Conference on AI (SCAI)\" held\n  April 26: https://www.scai.gov.sg/2025/scai2025-report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06096v2",
                "updated": "2025-06-30T19:58:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    19,
                    58,
                    46,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-09T14:44:07Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    44,
                    7,
                    4,
                    129,
                    0
                ],
                "title": "Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog\n  Generation using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Free and Fair Hardware: A Pathway to Copyright Infringement-Free Verilog\n  Generation using LLMs"
                },
                "summary": "Limitations in Large Language Model (LLM) capabilities for hardware design\ntasks, such as generating functional Verilog codes, have motivated various\nfine-tuning optimizations utilizing curated hardware datasets from open-source\nrepositories. However, these datasets remain limited in size and contain\nminimal checks on licensing for reuse, resulting in potential copyright\nviolations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to\nestimate the risk of Verilog-trained LLMs to generate copyright-protected\ncodes. To minimize this risk, we present an open-source Verilog dataset,\nFreeSet, containing over 220k files, along with the automated dataset curation\nframework utilized to provide additional guarantees of fair-use Verilog data.\nWe then execute an LLM fine-tuning framework consisting of continual\npre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our\nresults indicate that FreeV demonstrates the smallest risk of\ncopyright-infringement among prior works, with only a 3% violation rate.\nFurthermore, experimental results demonstrate improvements in Verilog\ngeneration functionality over its baseline model, improving VerilogEval pass@10\nrates by over 10%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limitations in Large Language Model (LLM) capabilities for hardware design\ntasks, such as generating functional Verilog codes, have motivated various\nfine-tuning optimizations utilizing curated hardware datasets from open-source\nrepositories. However, these datasets remain limited in size and contain\nminimal checks on licensing for reuse, resulting in potential copyright\nviolations by fine-tuned LLMs. Therefore, we propose an evaluation benchmark to\nestimate the risk of Verilog-trained LLMs to generate copyright-protected\ncodes. To minimize this risk, we present an open-source Verilog dataset,\nFreeSet, containing over 220k files, along with the automated dataset curation\nframework utilized to provide additional guarantees of fair-use Verilog data.\nWe then execute an LLM fine-tuning framework consisting of continual\npre-training, resulting in a fine-tuned Llama model for Verilog, FreeV. Our\nresults indicate that FreeV demonstrates the smallest risk of\ncopyright-infringement among prior works, with only a 3% violation rate.\nFurthermore, experimental results demonstrate improvements in Verilog\ngeneration functionality over its baseline model, improving VerilogEval pass@10\nrates by over 10%."
                },
                "authors": [
                    {
                        "name": "Sam Bush"
                    },
                    {
                        "name": "Matthew DeLorenzo"
                    },
                    {
                        "name": "Phat Tieu"
                    },
                    {
                        "name": "Jeyavijayan Rajendran"
                    }
                ],
                "author_detail": {
                    "name": "Jeyavijayan Rajendran"
                },
                "author": "Jeyavijayan Rajendran",
                "arxiv_comment": "Accepted at DAC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19089v2",
                "updated": "2025-06-30T19:05:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    19,
                    5,
                    37,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-23T20:06:53Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    20,
                    6,
                    53,
                    0,
                    174,
                    0
                ],
                "title": "Language Models Might Not Understand You: Evaluating Theory of Mind via\n  Story Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Might Not Understand You: Evaluating Theory of Mind via\n  Story Prompting"
                },
                "summary": "We introduce $\\texttt{StorySim}$, a programmable framework for synthetically\ngenerating stories to evaluate the theory of mind (ToM) and world modeling (WM)\ncapabilities of large language models (LLMs). Unlike prior benchmarks that may\nsuffer from contamination in pretraining data, $\\texttt{StorySim}$ produces\nnovel, compositional story prompts anchored by a highly controllable\n$\\texttt{Storyboard}$, enabling precise manipulation of character perspectives\nand events. We use this framework to design first- and second-order ToM tasks\nalongside WM tasks that control for the ability to track and model mental\nstates. Our experiments across a suite of state-of-the-art LLMs reveal that\nmost models perform better on WM tasks than ToM tasks, and that models tend to\nperform better reasoning with humans compared to inanimate objects.\nAdditionally, our framework enabled us to find evidence of heuristic behavior\nsuch as recency bias and an over-reliance on earlier events in the story. All\ncode for generating data and evaluations is freely available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce $\\texttt{StorySim}$, a programmable framework for synthetically\ngenerating stories to evaluate the theory of mind (ToM) and world modeling (WM)\ncapabilities of large language models (LLMs). Unlike prior benchmarks that may\nsuffer from contamination in pretraining data, $\\texttt{StorySim}$ produces\nnovel, compositional story prompts anchored by a highly controllable\n$\\texttt{Storyboard}$, enabling precise manipulation of character perspectives\nand events. We use this framework to design first- and second-order ToM tasks\nalongside WM tasks that control for the ability to track and model mental\nstates. Our experiments across a suite of state-of-the-art LLMs reveal that\nmost models perform better on WM tasks than ToM tasks, and that models tend to\nperform better reasoning with humans compared to inanimate objects.\nAdditionally, our framework enabled us to find evidence of heuristic behavior\nsuch as recency bias and an over-reliance on earlier events in the story. All\ncode for generating data and evaluations is freely available."
                },
                "authors": [
                    {
                        "name": "Nathaniel Getachew"
                    },
                    {
                        "name": "Abulhair Saparov"
                    }
                ],
                "author_detail": {
                    "name": "Abulhair Saparov"
                },
                "author": "Abulhair Saparov",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14051v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14051v2",
                "updated": "2025-06-30T19:01:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    19,
                    1,
                    18,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-19T19:12:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    19,
                    12,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache\n  Compression"
                },
                "summary": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models rely critically on the KV cache to\nefficiently handle extended contexts during the decode phase. Yet, the size of\nthe KV cache grows proportionally with the input length, burdening both memory\nbandwidth and capacity as decoding progresses. To address this challenge, we\npresent RocketKV, a training-free KV cache compression strategy containing two\nconsecutive stages. In the first stage, it performs coarse-grain permanent KV\ncache eviction on the input sequence tokens. In the second stage, it adopts a\nhybrid sparse attention method to conduct fine-grain top-k sparse attention,\napproximating the attention scores by leveraging both head and sequence\ndimensionality reductions. We show that RocketKV provides a compression ratio\nof up to 400$\\times$, end-to-end speedup of up to 3.7$\\times$ as well as peak\nmemory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU\ncompared to the full KV cache baseline, while achieving negligible accuracy\nloss on a variety of long-context tasks. We also propose a variant of RocketKV\nfor multi-turn scenarios, which consistently outperforms other existing methods\nand achieves accuracy nearly on par with an oracle top-k attention scheme."
                },
                "authors": [
                    {
                        "name": "Payman Behnam"
                    },
                    {
                        "name": "Yaosheng Fu"
                    },
                    {
                        "name": "Ritchie Zhao"
                    },
                    {
                        "name": "Po-An Tsai"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Alexey Tumanov"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Tumanov"
                },
                "author": "Alexey Tumanov",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14051v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14051v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01141v3",
                "updated": "2025-06-30T18:26:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    18,
                    26,
                    8,
                    0,
                    181,
                    0
                ],
                "published": "2024-10-02T00:43:10Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    0,
                    43,
                    10,
                    2,
                    276,
                    0
                ],
                "title": "Evaluating Deduplication Techniques for Economic Research Paper Titles\n  with a Focus on Semantic Similarity using NLP and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Deduplication Techniques for Economic Research Paper Titles\n  with a Focus on Semantic Similarity using NLP and LLMs"
                },
                "summary": "This study investigates efficient deduplication techniques for a large NLP\ndataset of economic research paper titles. We explore various pairing methods\nalongside established distance measures (Levenshtein distance, cosine\nsimilarity) and a sBERT model for semantic evaluation. Our findings suggest a\npotentially low prevalence of duplicates based on the observed semantic\nsimilarity across different methods. Further exploration with a human-annotated\nground truth set is completed for a more conclusive assessment. The result\nsupports findings from the NLP, LLM based distance metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates efficient deduplication techniques for a large NLP\ndataset of economic research paper titles. We explore various pairing methods\nalongside established distance measures (Levenshtein distance, cosine\nsimilarity) and a sBERT model for semantic evaluation. Our findings suggest a\npotentially low prevalence of duplicates based on the observed semantic\nsimilarity across different methods. Further exploration with a human-annotated\nground truth set is completed for a more conclusive assessment. The result\nsupports findings from the NLP, LLM based distance metrics."
                },
                "authors": [
                    {
                        "name": "Doohee You"
                    },
                    {
                        "name": "S Fraiberger"
                    }
                ],
                "author_detail": {
                    "name": "S Fraiberger"
                },
                "author": "S Fraiberger",
                "arxiv_comment": "6 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24120v1",
                "updated": "2025-06-30T17:58:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    58,
                    30,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:58:30Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    58,
                    30,
                    0,
                    181,
                    0
                ],
                "title": "Data Uniformity Improves Training Efficiency and More, with a\n  Convergence Framework Beyond the NTK Regime",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data Uniformity Improves Training Efficiency and More, with a\n  Convergence Framework Beyond the NTK Regime"
                },
                "summary": "Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_{\\min}$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_{\\min}$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity."
                },
                "authors": [
                    {
                        "name": "Yuqing Wang"
                    },
                    {
                        "name": "Shangding Gu"
                    }
                ],
                "author_detail": {
                    "name": "Shangding Gu"
                },
                "author": "Shangding Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24118v1",
                "updated": "2025-06-30T17:57:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    57,
                    32,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:57:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    57,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Scaling Human Judgment in Community Notes with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Human Judgment in Community Notes with LLMs"
                },
                "summary": "This paper argues for a new paradigm for Community Notes in the LLM era: an\nopen ecosystem where both humans and LLMs can write notes, and the decision of\nwhich notes are helpful enough to show remains in the hands of humans. This\napproach can accelerate the delivery of notes, while maintaining trust and\nlegitimacy through Community Notes' foundational principle: A community of\ndiverse human raters collectively serve as the ultimate evaluator and arbiter\nof what is helpful. Further, the feedback from this diverse community can be\nused to improve LLMs' ability to produce accurate, unbiased, broadly helpful\nnotes--what we term Reinforcement Learning from Community Feedback (RLCF). This\nbecomes a two-way street: LLMs serve as an asset to humans--helping deliver\ncontext quickly and with minimal effort--while human feedback, in turn,\nenhances the performance of LLMs. This paper describes how such a system can\nwork, its benefits, key new risks and challenges it introduces, and a research\nagenda to solve those challenges and realize the potential of this approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper argues for a new paradigm for Community Notes in the LLM era: an\nopen ecosystem where both humans and LLMs can write notes, and the decision of\nwhich notes are helpful enough to show remains in the hands of humans. This\napproach can accelerate the delivery of notes, while maintaining trust and\nlegitimacy through Community Notes' foundational principle: A community of\ndiverse human raters collectively serve as the ultimate evaluator and arbiter\nof what is helpful. Further, the feedback from this diverse community can be\nused to improve LLMs' ability to produce accurate, unbiased, broadly helpful\nnotes--what we term Reinforcement Learning from Community Feedback (RLCF). This\nbecomes a two-way street: LLMs serve as an asset to humans--helping deliver\ncontext quickly and with minimal effort--while human feedback, in turn,\nenhances the performance of LLMs. This paper describes how such a system can\nwork, its benefits, key new risks and challenges it introduces, and a research\nagenda to solve those challenges and realize the potential of this approach."
                },
                "authors": [
                    {
                        "name": "Haiwen Li"
                    },
                    {
                        "name": "Soham De"
                    },
                    {
                        "name": "Manon Revel"
                    },
                    {
                        "name": "Andreas Haupt"
                    },
                    {
                        "name": "Brad Miller"
                    },
                    {
                        "name": "Keith Coleman"
                    },
                    {
                        "name": "Jay Baxter"
                    },
                    {
                        "name": "Martin Saveski"
                    },
                    {
                        "name": "Michiel A. Bakker"
                    }
                ],
                "author_detail": {
                    "name": "Michiel A. Bakker"
                },
                "author": "Michiel A. Bakker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02113v2",
                "updated": "2025-06-30T17:50:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    50,
                    31,
                    0,
                    181,
                    0
                ],
                "published": "2024-12-03T03:10:12Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    10,
                    12,
                    1,
                    338,
                    0
                ],
                "title": "Trust & Safety of LLMs and LLMs in Trust & Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust & Safety of LLMs and LLMs in Trust & Safety"
                },
                "summary": "In recent years, Large Language Models (LLMs) have garnered considerable\nattention for their remarkable abilities in natural language processing tasks.\nHowever, their widespread adoption has raised concerns pertaining to trust and\nsafety. This systematic review investigates the current research landscape on\ntrust and safety in LLMs, with a particular focus on the novel application of\nLLMs within the field of Trust and Safety itself. We delve into the\ncomplexities of utilizing LLMs in domains where maintaining trust and safety is\nparamount, offering a consolidated perspective on this emerging trend.\\\n  By synthesizing findings from various studies, we identify key challenges and\npotential solutions, aiming to benefit researchers and practitioners seeking to\nunderstand the nuanced interplay between LLMs and Trust and Safety.\n  This review provides insights on best practices for using LLMs in Trust and\nSafety, and explores emerging risks such as prompt injection and jailbreak\nattacks. Ultimately, this study contributes to a deeper understanding of how\nLLMs can be effectively and responsibly utilized to enhance trust and safety in\nthe digital realm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have garnered considerable\nattention for their remarkable abilities in natural language processing tasks.\nHowever, their widespread adoption has raised concerns pertaining to trust and\nsafety. This systematic review investigates the current research landscape on\ntrust and safety in LLMs, with a particular focus on the novel application of\nLLMs within the field of Trust and Safety itself. We delve into the\ncomplexities of utilizing LLMs in domains where maintaining trust and safety is\nparamount, offering a consolidated perspective on this emerging trend.\\\n  By synthesizing findings from various studies, we identify key challenges and\npotential solutions, aiming to benefit researchers and practitioners seeking to\nunderstand the nuanced interplay between LLMs and Trust and Safety.\n  This review provides insights on best practices for using LLMs in Trust and\nSafety, and explores emerging risks such as prompt injection and jailbreak\nattacks. Ultimately, this study contributes to a deeper understanding of how\nLLMs can be effectively and responsibly utilized to enhance trust and safety in\nthe digital realm."
                },
                "authors": [
                    {
                        "name": "Doohee You"
                    },
                    {
                        "name": "Dan Chon"
                    }
                ],
                "author_detail": {
                    "name": "Dan Chon"
                },
                "author": "Dan Chon",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02811v2",
                "updated": "2025-06-30T17:46:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    46,
                    40,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-05T17:39:35Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    17,
                    39,
                    35,
                    0,
                    125,
                    0
                ],
                "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round\n  RAG through Self-Practicing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowing You Don't Know: Learning When to Continue Search in Multi-round\n  RAG through Self-Practicing"
                },
                "summary": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance. This paper aims to\naddress these limitations by introducing a new framework, SIM-RAG, to\nexplicitly enhance RAG systems' self-awareness and multi-round retrieval\ncapabilities. To train SIM-RAG, we first let a RAG system self-practice\nmulti-round retrieval, augmenting existing question-answer pairs with\nintermediate inner monologue reasoning steps to generate synthetic training\ndata. For each pair, the system may explore multiple retrieval paths, which are\nlabeled as successful if they reach the correct answer and unsuccessful\notherwise. Using this data, we train a lightweight information sufficiency\nCritic. At inference time, the Critic evaluates whether the RAG system has\nretrieved sufficient information at each round, guiding retrieval decisions and\nimproving system-level self-awareness through in-context reinforcement\nlearning. Experiments across multiple prominent RAG benchmarks show that\nSIM-RAG is an effective multi-round RAG solution. Furthermore, this framework\nis system-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing\nlanguage models' knowledge and reducing AI generative hallucinations, driving\nits widespread use. However, complex tasks requiring multi-round retrieval\nremain challenging, and early attempts tend to be overly optimistic without a\ngood sense of self-skepticism. Current multi-round RAG systems may continue\nsearching even when enough information has already been retrieved, or they may\nprovide incorrect answers without having sufficient information or knowledge.\nExisting solutions either require large amounts of expensive human-labeled\nprocess supervision data or lead to subpar performance. This paper aims to\naddress these limitations by introducing a new framework, SIM-RAG, to\nexplicitly enhance RAG systems' self-awareness and multi-round retrieval\ncapabilities. To train SIM-RAG, we first let a RAG system self-practice\nmulti-round retrieval, augmenting existing question-answer pairs with\nintermediate inner monologue reasoning steps to generate synthetic training\ndata. For each pair, the system may explore multiple retrieval paths, which are\nlabeled as successful if they reach the correct answer and unsuccessful\notherwise. Using this data, we train a lightweight information sufficiency\nCritic. At inference time, the Critic evaluates whether the RAG system has\nretrieved sufficient information at each round, guiding retrieval decisions and\nimproving system-level self-awareness through in-context reinforcement\nlearning. Experiments across multiple prominent RAG benchmarks show that\nSIM-RAG is an effective multi-round RAG solution. Furthermore, this framework\nis system-efficient, adding a lightweight component to RAG without requiring\nmodifications to existing LLMs or search engines, and data-efficient,\neliminating the need for costly human-annotated mid-step retrieval process\nsupervision data."
                },
                "authors": [
                    {
                        "name": "Diji Yang"
                    },
                    {
                        "name": "Linda Zeng"
                    },
                    {
                        "name": "Jinmeng Rao"
                    },
                    {
                        "name": "Yi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhang"
                },
                "author": "Yi Zhang",
                "arxiv_comment": "Proceedings of the 48th International ACM SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18797v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18797v2",
                "updated": "2025-06-30T17:45:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    45,
                    54,
                    0,
                    181,
                    0
                ],
                "published": "2024-11-27T22:46:08Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    22,
                    46,
                    8,
                    2,
                    332,
                    0
                ],
                "title": "SEUF: Is Unlearning One Expert Enough for Mixture-of-Experts LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEUF: Is Unlearning One Expert Enough for Mixture-of-Experts LLMs?"
                },
                "summary": "Recent advancements in LLMs unlearning have shown remarkable success in\nremoving unwanted data-model influences while preserving the model's utility\nfor legitimate knowledge. Despite these strides, sparse Mixture-of-Experts\n(MoE) LLMs--a key subset of the LLM family--have remained unexplored in the\ncontext of unlearning. As MoE LLMs are celebrated for their exceptional\nperformance, we ask:How can unlearning be performed effectively and efficiently\non MoE LLMs? Our pilot study shows that the dynamic routing nature of MoE LLMs\nintroduces unique challenges, leading to excessive forgetting, uncontrolled\nknowledge erasure and substantial utility drops when existing unlearning\nmethods are applied. To address this, we propose a novel Selected-Expert\nUnlearning Framework (SEUF). Through expert attribution, unlearning is\nconcentrated on the most actively engaged experts for the specified knowledge.\nConcurrently, an anchor loss is applied to the router to stabilize the active\nstate of this targeted expert, ensuring focused and controlled unlearning. SEUF\nis compatible with various standard unlearning algorithms. Extensive\nexperiments demonstrate that SEUF enhances both forget quality up to 5% and\nmodel utility by 35% on MoE LLMs across various benchmarks and LLM\narchitectures (compared to standard unlearning algorithms), while only\nunlearning 0.06% of the model parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLMs unlearning have shown remarkable success in\nremoving unwanted data-model influences while preserving the model's utility\nfor legitimate knowledge. Despite these strides, sparse Mixture-of-Experts\n(MoE) LLMs--a key subset of the LLM family--have remained unexplored in the\ncontext of unlearning. As MoE LLMs are celebrated for their exceptional\nperformance, we ask:How can unlearning be performed effectively and efficiently\non MoE LLMs? Our pilot study shows that the dynamic routing nature of MoE LLMs\nintroduces unique challenges, leading to excessive forgetting, uncontrolled\nknowledge erasure and substantial utility drops when existing unlearning\nmethods are applied. To address this, we propose a novel Selected-Expert\nUnlearning Framework (SEUF). Through expert attribution, unlearning is\nconcentrated on the most actively engaged experts for the specified knowledge.\nConcurrently, an anchor loss is applied to the router to stabilize the active\nstate of this targeted expert, ensuring focused and controlled unlearning. SEUF\nis compatible with various standard unlearning algorithms. Extensive\nexperiments demonstrate that SEUF enhances both forget quality up to 5% and\nmodel utility by 35% on MoE LLMs across various benchmarks and LLM\narchitectures (compared to standard unlearning algorithms), while only\nunlearning 0.06% of the model parameters."
                },
                "authors": [
                    {
                        "name": "Haomin Zhuang"
                    },
                    {
                        "name": "Yihua Zhang"
                    },
                    {
                        "name": "Kehan Guo"
                    },
                    {
                        "name": "Jinghan Jia"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Sijia Liu"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "arxiv_comment": "Accepted to ACL'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18797v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18797v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17677v2",
                "updated": "2025-06-30T17:30:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    30,
                    39,
                    0,
                    181,
                    0
                ],
                "published": "2025-04-24T15:47:20Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    47,
                    20,
                    3,
                    114,
                    0
                ],
                "title": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language\n  Models"
                },
                "summary": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. Based on interviews with teaching staff, this paper\nintroduces INSIGHT, a proof of concept to combine various AI tools to assist\nteaching staff and students in the process of solving exercises. INSIGHT has a\nmodular design that allows it to be integrated into various higher education\ncourses. We analyze students' questions to an LLM by extracting keywords, which\nwe use to dynamically build an FAQ from students' questions and provide new\ninsights for the teaching staff to use for more personalized face-to-face\nsupport. Future work could build upon INSIGHT by using the collected data to\nprovide adaptive learning and adjust content based on student progress and\nlearning styles to offer a more interactive and inclusive learning experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of AI, especially Large Language Models, presents challenges and\nopportunities to integrate such technology into the classroom. AI has the\npotential to revolutionize education by helping teaching staff with various\ntasks, such as personalizing their teaching methods, but it also raises\nconcerns, for example, about the degradation of student-teacher interactions\nand user privacy. Based on interviews with teaching staff, this paper\nintroduces INSIGHT, a proof of concept to combine various AI tools to assist\nteaching staff and students in the process of solving exercises. INSIGHT has a\nmodular design that allows it to be integrated into various higher education\ncourses. We analyze students' questions to an LLM by extracting keywords, which\nwe use to dynamically build an FAQ from students' questions and provide new\ninsights for the teaching staff to use for more personalized face-to-face\nsupport. Future work could build upon INSIGHT by using the collected data to\nprovide adaptive learning and adjust content based on student progress and\nlearning styles to offer a more interactive and inclusive learning experience."
                },
                "authors": [
                    {
                        "name": "Jarne Thys"
                    },
                    {
                        "name": "Sebe Vanbrabant"
                    },
                    {
                        "name": "Davy Vanacken"
                    },
                    {
                        "name": "Gustavo Rovelo Ruiz"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Rovelo Ruiz"
                },
                "author": "Gustavo Rovelo Ruiz",
                "arxiv_comment": "Accepted author version for the D-SAIL Workshop - Transformative\n  Curriculum Design: Digitalisation, Sustainability, and AI Literacy for 21st\n  Century Learning, July 22, 2025, Palermo, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24068v1",
                "updated": "2025-06-30T17:21:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    21,
                    8,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T17:21:08Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    17,
                    21,
                    8,
                    0,
                    181,
                    0
                ],
                "title": "STACK: Adversarial Attacks on LLM Safeguard Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STACK: Adversarial Attacks on LLM Safeguard Pipelines"
                },
                "summary": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks."
                },
                "authors": [
                    {
                        "name": "Ian R. McKenzie"
                    },
                    {
                        "name": "Oskar J. Hollinsworth"
                    },
                    {
                        "name": "Tom Tseng"
                    },
                    {
                        "name": "Xander Davies"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Aaron D. Tucker"
                    },
                    {
                        "name": "Robert Kirk"
                    },
                    {
                        "name": "Adam Gleave"
                    }
                ],
                "author_detail": {
                    "name": "Adam Gleave"
                },
                "author": "Adam Gleave",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05651v2",
                "updated": "2025-06-30T16:54:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    54,
                    48,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-08T17:53:41Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    17,
                    53,
                    41,
                    5,
                    39,
                    0
                ],
                "title": "KMI: A Dataset of Korean Motivational Interviewing Dialogues for\n  Psychotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KMI: A Dataset of Korean Motivational Interviewing Dialogues for\n  Psychotherapy"
                },
                "summary": "The increasing demand for mental health services has led to the rise of\nAI-driven mental health chatbots, though challenges related to privacy, data\ncollection, and expertise persist. Motivational Interviewing (MI) is gaining\nattention as a theoretical basis for boosting expertise in the development of\nthese chatbots. However, existing datasets are showing limitations for training\nchatbots, leading to a substantial demand for publicly available resources in\nthe field of MI and psychotherapy. These challenges are even more pronounced in\nnon-English languages, where they receive less attention. In this paper, we\npropose a novel framework that simulates MI sessions enriched with the\nexpertise of professional therapists. We train an MI forecaster model that\nmimics the behavioral choices of professional therapists and employ Large\nLanguage Models (LLMs) to generate utterances through prompt engineering. Then,\nwe present KMI, the first synthetic dataset theoretically grounded in MI,\ncontaining 1,000 high-quality Korean Motivational Interviewing dialogues.\nThrough an extensive expert evaluation of the generated dataset and the\ndialogue model trained on it, we demonstrate the quality, expertise, and\npracticality of KMI. We also introduce novel metrics derived from MI theory in\norder to evaluate dialogues from the perspective of MI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for mental health services has led to the rise of\nAI-driven mental health chatbots, though challenges related to privacy, data\ncollection, and expertise persist. Motivational Interviewing (MI) is gaining\nattention as a theoretical basis for boosting expertise in the development of\nthese chatbots. However, existing datasets are showing limitations for training\nchatbots, leading to a substantial demand for publicly available resources in\nthe field of MI and psychotherapy. These challenges are even more pronounced in\nnon-English languages, where they receive less attention. In this paper, we\npropose a novel framework that simulates MI sessions enriched with the\nexpertise of professional therapists. We train an MI forecaster model that\nmimics the behavioral choices of professional therapists and employ Large\nLanguage Models (LLMs) to generate utterances through prompt engineering. Then,\nwe present KMI, the first synthetic dataset theoretically grounded in MI,\ncontaining 1,000 high-quality Korean Motivational Interviewing dialogues.\nThrough an extensive expert evaluation of the generated dataset and the\ndialogue model trained on it, we demonstrate the quality, expertise, and\npracticality of KMI. We also introduce novel metrics derived from MI theory in\norder to evaluate dialogues from the perspective of MI."
                },
                "authors": [
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Suyeon Lee"
                    },
                    {
                        "name": "Yeongjae Cho"
                    },
                    {
                        "name": "Eunseo Ryu"
                    },
                    {
                        "name": "Yohan Jo"
                    },
                    {
                        "name": "Suran Seong"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "arxiv_comment": "Accepted at NAACL 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24045v1",
                "updated": "2025-06-30T16:50:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    50,
                    48,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:50:48Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    50,
                    48,
                    0,
                    181,
                    0
                ],
                "title": "Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on\n  Heterogeneous SoC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on\n  Heterogeneous SoC"
                },
                "summary": "The proliferation of agentic Large Language Models (LLMs) on personal devices\nintroduces a new class of workloads characterized by a dichotomy of objectives.\nReactive tasks, initiated by users, demand immediate, low-latency responses,\nwhile proactive tasks operate invisibly and prioritize throughput. Existing\non-device LLM engines, designed for isolated inferences, fail to efficiently\nmanage these concurrent and conflicting requests on consumer-grade\nheterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces\nAgent.xpu, an efficient serving system for agentic LLM workloads on\nmemory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu\nfirst constructs a heterogeneous execution graph, which fuses and chunks model\nkernels for affinity-guided, elastic accelerator mapping with predictive kernel\nannotation. At runtime, its online scheduler enables fine-grained, kernel-level\npreemption to guarantee the responsiveness of reactive tasks. To maximize SoC\nutilization, it adopts slack-aware kernel backfill to opportunistically append\nproactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware\ndispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves\n4.6$\\times$ lower latency for reactive tasks and sustains\n1.6$\\times$-6.8$\\times$ higher throughput for proactive tasks compared to\nstate-of-the-art inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of agentic Large Language Models (LLMs) on personal devices\nintroduces a new class of workloads characterized by a dichotomy of objectives.\nReactive tasks, initiated by users, demand immediate, low-latency responses,\nwhile proactive tasks operate invisibly and prioritize throughput. Existing\non-device LLM engines, designed for isolated inferences, fail to efficiently\nmanage these concurrent and conflicting requests on consumer-grade\nheterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces\nAgent.xpu, an efficient serving system for agentic LLM workloads on\nmemory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu\nfirst constructs a heterogeneous execution graph, which fuses and chunks model\nkernels for affinity-guided, elastic accelerator mapping with predictive kernel\nannotation. At runtime, its online scheduler enables fine-grained, kernel-level\npreemption to guarantee the responsiveness of reactive tasks. To maximize SoC\nutilization, it adopts slack-aware kernel backfill to opportunistically append\nproactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware\ndispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves\n4.6$\\times$ lower latency for reactive tasks and sustains\n1.6$\\times$-6.8$\\times$ higher throughput for proactive tasks compared to\nstate-of-the-art inference engines."
                },
                "authors": [
                    {
                        "name": "Xinming Wei"
                    },
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Haoran Li"
                    },
                    {
                        "name": "Jiayu Chen"
                    },
                    {
                        "name": "Rui Qu"
                    },
                    {
                        "name": "Maoliang Li"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Guojie Luo"
                    }
                ],
                "author_detail": {
                    "name": "Guojie Luo"
                },
                "author": "Guojie Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00306v2",
                "updated": "2025-06-30T16:37:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    37,
                    59,
                    0,
                    181,
                    0
                ],
                "published": "2025-02-01T04:01:18Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    4,
                    1,
                    18,
                    5,
                    32,
                    0
                ],
                "title": "Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Riddle Me This! Stealthy Membership Inference for Retrieval-Augmented\n  Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to\ngenerate grounded responses by leveraging external knowledge databases without\naltering model parameters. Although the absence of weight tuning prevents\nleakage via model parameters, it introduces the risk of inference adversaries\nexploiting retrieved documents in the model's context. Existing methods for\nmembership inference and data extraction often rely on jailbreaking or\ncarefully crafted unnatural queries, which can be easily detected or thwarted\nwith query rewriting techniques common in RAG systems. In this work, we present\nInterrogation Attack (IA), a membership inference technique targeting documents\nin the RAG datastore. By crafting natural-text queries that are answerable only\nwith the target document's presence, our approach demonstrates successful\ninference with just 30 queries while remaining stealthy; straightforward\ndetectors identify adversarial prompts from existing methods up to ~76x more\nfrequently than those generated by our attack. We observe a 2x improvement in\nTPR@1%FPR over prior inference attacks across diverse RAG configurations, all\nwhile costing less than $0.02 per document inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enables Large Language Models (LLMs) to\ngenerate grounded responses by leveraging external knowledge databases without\naltering model parameters. Although the absence of weight tuning prevents\nleakage via model parameters, it introduces the risk of inference adversaries\nexploiting retrieved documents in the model's context. Existing methods for\nmembership inference and data extraction often rely on jailbreaking or\ncarefully crafted unnatural queries, which can be easily detected or thwarted\nwith query rewriting techniques common in RAG systems. In this work, we present\nInterrogation Attack (IA), a membership inference technique targeting documents\nin the RAG datastore. By crafting natural-text queries that are answerable only\nwith the target document's presence, our approach demonstrates successful\ninference with just 30 queries while remaining stealthy; straightforward\ndetectors identify adversarial prompts from existing methods up to ~76x more\nfrequently than those generated by our attack. We observe a 2x improvement in\nTPR@1%FPR over prior inference attacks across diverse RAG configurations, all\nwhile costing less than $0.02 per document inference."
                },
                "authors": [
                    {
                        "name": "Ali Naseh"
                    },
                    {
                        "name": "Yuefeng Peng"
                    },
                    {
                        "name": "Anshuman Suri"
                    },
                    {
                        "name": "Harsh Chaudhari"
                    },
                    {
                        "name": "Alina Oprea"
                    },
                    {
                        "name": "Amir Houmansadr"
                    }
                ],
                "author_detail": {
                    "name": "Amir Houmansadr"
                },
                "author": "Amir Houmansadr",
                "arxiv_comment": "This is the full version (27 pages) of the paper 'Riddle Me This!\n  Stealthy Membership Inference for Retrieval-Augmented Generation' published\n  at CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08842v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08842v2",
                "updated": "2025-06-30T16:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    31,
                    53,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-13T12:58:11Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    58,
                    11,
                    1,
                    133,
                    0
                ],
                "title": "LibVulnWatch: A Deep Assessment Agent System and Leaderboard for\n  Uncovering Hidden Vulnerabilities in Open-Source AI Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LibVulnWatch: A Deep Assessment Agent System and Leaderboard for\n  Uncovering Hidden Vulnerabilities in Open-Source AI Libraries"
                },
                "summary": "Open-source AI libraries are foundational to modern AI systems, yet they\npresent significant, underexamined risks spanning security, licensing,\nmaintenance, supply chain integrity, and regulatory compliance. We introduce\nLibVulnWatch, a system that leverages recent advances in large language models\nand agentic workflows to perform deep, evidence-based evaluations of these\nlibraries. Built on a graph-based orchestration of specialized agents, the\nframework extracts, verifies, and quantifies risk using information from\nrepositories, documentation, and vulnerability databases. LibVulnWatch produces\nreproducible, governance-aligned scores across five critical domains,\npublishing results to a public leaderboard for ongoing ecosystem monitoring.\nApplied to 20 widely used libraries, including ML frameworks, LLM inference\nengines, and agent orchestration tools, our approach covers up to 88% of\nOpenSSF Scorecard checks while surfacing up to 19 additional risks per library,\nsuch as critical RCE vulnerabilities, missing SBOMs, and regulatory gaps. By\nintegrating advanced language technologies with the practical demands of\nsoftware risk assessment, this work demonstrates a scalable, transparent\nmechanism for continuous supply chain evaluation and informed library\nselection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source AI libraries are foundational to modern AI systems, yet they\npresent significant, underexamined risks spanning security, licensing,\nmaintenance, supply chain integrity, and regulatory compliance. We introduce\nLibVulnWatch, a system that leverages recent advances in large language models\nand agentic workflows to perform deep, evidence-based evaluations of these\nlibraries. Built on a graph-based orchestration of specialized agents, the\nframework extracts, verifies, and quantifies risk using information from\nrepositories, documentation, and vulnerability databases. LibVulnWatch produces\nreproducible, governance-aligned scores across five critical domains,\npublishing results to a public leaderboard for ongoing ecosystem monitoring.\nApplied to 20 widely used libraries, including ML frameworks, LLM inference\nengines, and agent orchestration tools, our approach covers up to 88% of\nOpenSSF Scorecard checks while surfacing up to 19 additional risks per library,\nsuch as critical RCE vulnerabilities, missing SBOMs, and regulatory gaps. By\nintegrating advanced language technologies with the practical demands of\nsoftware risk assessment, this work demonstrates a scalable, transparent\nmechanism for continuous supply chain evaluation and informed library\nselection."
                },
                "authors": [
                    {
                        "name": "Zekun Wu"
                    },
                    {
                        "name": "Seonglae Cho"
                    },
                    {
                        "name": "Umar Mohammed"
                    },
                    {
                        "name": "Cristian Munoz"
                    },
                    {
                        "name": "Kleyton Costa"
                    },
                    {
                        "name": "Xin Guan"
                    },
                    {
                        "name": "Theo King"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Emre Kazim"
                    },
                    {
                        "name": "Adriano Koshiyama"
                    }
                ],
                "author_detail": {
                    "name": "Adriano Koshiyama"
                },
                "author": "Adriano Koshiyama",
                "arxiv_comment": "ACL 2025 Student Research Workshop and ICML 2025 TAIG Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08842v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08842v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19577v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19577v3",
                "updated": "2025-06-30T16:21:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    21,
                    25,
                    0,
                    181,
                    0
                ],
                "published": "2025-05-26T06:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    6,
                    47,
                    43,
                    0,
                    146,
                    0
                ],
                "title": "MFA-KWS: Effective Keyword Spotting with Multi-head Frame-asynchronous\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MFA-KWS: Effective Keyword Spotting with Multi-head Frame-asynchronous\n  Decoding"
                },
                "summary": "Keyword spotting (KWS) is essential for voice-driven applications, demanding\nboth accuracy and efficiency. Traditional ASR-based KWS methods, such as greedy\nand beam search, explore the entire search space without explicitly\nprioritizing keyword detection, often leading to suboptimal performance. In\nthis paper, we propose an effective keyword-specific KWS framework by\nintroducing a streaming-oriented CTC-Transducer-combined frame-asynchronous\nsystem with multi-head frame-asynchronous decoding (MFA-KWS). Specifically,\nMFA-KWS employs keyword-specific phone-synchronous decoding for CTC and\nreplaces conventional RNN-T with Token-and-Duration Transducer to enhance both\nperformance and efficiency. Furthermore, we explore various score fusion\nstrategies, including single-frame-based and consistency-based methods.\nExtensive experiments demonstrate the superior performance of MFA-KWS, which\nachieves state-of-the-art results on both fixed keyword and arbitrary keywords\ndatasets, such as Snips, MobvoiHotwords, and LibriKWS-20, while exhibiting\nstrong robustness in noisy environments. Among fusion strategies, the\nconsistency-based CDC-Last method delivers the best performance. Additionally,\nMFA-KWS achieves a 47% to 63% speed-up over the frame-synchronous baselines\nacross various datasets. Extensive experimental results confirm that MFA-KWS is\nan effective and efficient KWS framework, making it well-suited for on-device\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keyword spotting (KWS) is essential for voice-driven applications, demanding\nboth accuracy and efficiency. Traditional ASR-based KWS methods, such as greedy\nand beam search, explore the entire search space without explicitly\nprioritizing keyword detection, often leading to suboptimal performance. In\nthis paper, we propose an effective keyword-specific KWS framework by\nintroducing a streaming-oriented CTC-Transducer-combined frame-asynchronous\nsystem with multi-head frame-asynchronous decoding (MFA-KWS). Specifically,\nMFA-KWS employs keyword-specific phone-synchronous decoding for CTC and\nreplaces conventional RNN-T with Token-and-Duration Transducer to enhance both\nperformance and efficiency. Furthermore, we explore various score fusion\nstrategies, including single-frame-based and consistency-based methods.\nExtensive experiments demonstrate the superior performance of MFA-KWS, which\nachieves state-of-the-art results on both fixed keyword and arbitrary keywords\ndatasets, such as Snips, MobvoiHotwords, and LibriKWS-20, while exhibiting\nstrong robustness in noisy environments. Among fusion strategies, the\nconsistency-based CDC-Last method delivers the best performance. Additionally,\nMFA-KWS achieves a 47% to 63% speed-up over the frame-synchronous baselines\nacross various datasets. Extensive experimental results confirm that MFA-KWS is\nan effective and efficient KWS framework, making it well-suited for on-device\ndeployment."
                },
                "authors": [
                    {
                        "name": "Yu Xi"
                    },
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Xiaoyu Gu"
                    },
                    {
                        "name": "Yidi Jiang"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "Accepted by TASLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19577v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19577v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24015v1",
                "updated": "2025-06-30T16:19:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    19,
                    38,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:19:38Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    19,
                    38,
                    0,
                    181,
                    0
                ],
                "title": "Bug Fixing with Broader Context: Enhancing LLM-Based Program Repair via\n  Layered Knowledge Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bug Fixing with Broader Context: Enhancing LLM-Based Program Repair via\n  Layered Knowledge Injection"
                },
                "summary": "Prompting LLMs with bug-related context (e.g., error messages, stack traces)\nimproves automated program repair, but many bugs still remain unresolved. In\nreal-world projects, developers often rely on broader repository and\nproject-level context beyond the local code to resolve such bugs. In this\npaper, we investigate how automatically extracting and providing such knowledge\ncan improve LLM-based program repair. We propose a layered knowledge injection\nframework that incrementally augments LLMs with structured context. It starts\nwith the Bug Knowledge Layer, which includes information such as the buggy\nfunction and failing tests; expands to the Repository Knowledge Layer, which\nadds structural dependencies, related files, and commit history; and finally\ninjects the Project Knowledge Layer, which incorporates relevant details from\ndocumentation and previously fixed bugs. We evaluate this framework on a\ndataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),\nand analyze fix rates across six bug types. By progressively injecting\nknowledge across layers, our approach achieves a fix rate of 79% (250/314)\nusing Llama 3.3, a significant improvement of 23% over previous work. All bug\ntypes show improvement with the addition of repository-level context, while\nonly a subset benefit further from project-level knowledge, highlighting that\ndifferent bug types require different levels of contextual information for\neffective repair. We also analyze the remaining unresolved bugs and find that\nmore complex and structurally isolated bugs, such as Program Anomaly and GUI\nbugs, remain difficult even after injecting all available information. Our\nresults show that layered context injection improves program repair and suggest\nthe need for interactive and adaptive APR systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting LLMs with bug-related context (e.g., error messages, stack traces)\nimproves automated program repair, but many bugs still remain unresolved. In\nreal-world projects, developers often rely on broader repository and\nproject-level context beyond the local code to resolve such bugs. In this\npaper, we investigate how automatically extracting and providing such knowledge\ncan improve LLM-based program repair. We propose a layered knowledge injection\nframework that incrementally augments LLMs with structured context. It starts\nwith the Bug Knowledge Layer, which includes information such as the buggy\nfunction and failing tests; expands to the Repository Knowledge Layer, which\nadds structural dependencies, related files, and commit history; and finally\ninjects the Project Knowledge Layer, which incorporates relevant details from\ndocumentation and previously fixed bugs. We evaluate this framework on a\ndataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),\nand analyze fix rates across six bug types. By progressively injecting\nknowledge across layers, our approach achieves a fix rate of 79% (250/314)\nusing Llama 3.3, a significant improvement of 23% over previous work. All bug\ntypes show improvement with the addition of repository-level context, while\nonly a subset benefit further from project-level knowledge, highlighting that\ndifferent bug types require different levels of contextual information for\neffective repair. We also analyze the remaining unresolved bugs and find that\nmore complex and structurally isolated bugs, such as Program Anomaly and GUI\nbugs, remain difficult even after injecting all available information. Our\nresults show that layered context injection improves program repair and suggest\nthe need for interactive and adaptive APR systems."
                },
                "authors": [
                    {
                        "name": "Ramtin Ehsani"
                    },
                    {
                        "name": "Esteban Parra"
                    },
                    {
                        "name": "Sonia Haiduc"
                    },
                    {
                        "name": "Preetha Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Preetha Chatterjee"
                },
                "author": "Preetha Chatterjee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.24006v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.24006v1",
                "updated": "2025-06-30T16:10:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    10,
                    42,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:10:42Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    10,
                    42,
                    0,
                    181,
                    0
                ],
                "title": "Large Language Models Don't Make Sense of Word Problems. A Scoping\n  Review from a Mathematics Education Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Don't Make Sense of Word Problems. A Scoping\n  Review from a Mathematics Education Perspective"
                },
                "summary": "The progress of Large Language Models (LLMs) like ChatGPT raises the question\nof how they can be integrated into education. One hope is that they can support\nmathematics learning, including word-problem solving. Since LLMs can handle\ntextual input with ease, they appear well-suited for solving mathematical word\nproblems. Yet their real competence, whether they can make sense of the\nreal-world context, and the implications for classrooms remain unclear. We\nconducted a scoping review from a mathematics-education perspective, including\nthree parts: a technical overview, a systematic review of word problems used in\nresearch, and a state-of-the-art empirical evaluation of LLMs on mathematical\nword problems. First, in the technical overview, we contrast the\nconceptualization of word problems and their solution processes between LLMs\nand students. In computer-science research this is typically labeled\nmathematical reasoning, a term that does not align with usage in mathematics\neducation. Second, our literature review of 213 studies shows that the most\npopular word-problem corpora are dominated by s-problems, which do not require\na consideration of realities of their real-world context. Finally, our\nevaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems\nshows that most recent LLMs solve these s-problems with near-perfect accuracy,\nincluding a perfect score on 20 problems from PISA. LLMs still showed\nweaknesses in tackling problems where the real-world context is problematic or\nnon-sensical. In sum, we argue based on all three aspects that LLMs have\nmastered a superficial solution process but do not make sense of word problems,\nwhich potentially limits their value as instructional tools in mathematics\nclassrooms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The progress of Large Language Models (LLMs) like ChatGPT raises the question\nof how they can be integrated into education. One hope is that they can support\nmathematics learning, including word-problem solving. Since LLMs can handle\ntextual input with ease, they appear well-suited for solving mathematical word\nproblems. Yet their real competence, whether they can make sense of the\nreal-world context, and the implications for classrooms remain unclear. We\nconducted a scoping review from a mathematics-education perspective, including\nthree parts: a technical overview, a systematic review of word problems used in\nresearch, and a state-of-the-art empirical evaluation of LLMs on mathematical\nword problems. First, in the technical overview, we contrast the\nconceptualization of word problems and their solution processes between LLMs\nand students. In computer-science research this is typically labeled\nmathematical reasoning, a term that does not align with usage in mathematics\neducation. Second, our literature review of 213 studies shows that the most\npopular word-problem corpora are dominated by s-problems, which do not require\na consideration of realities of their real-world context. Finally, our\nevaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems\nshows that most recent LLMs solve these s-problems with near-perfect accuracy,\nincluding a perfect score on 20 problems from PISA. LLMs still showed\nweaknesses in tackling problems where the real-world context is problematic or\nnon-sensical. In sum, we argue based on all three aspects that LLMs have\nmastered a superficial solution process but do not make sense of word problems,\nwhich potentially limits their value as instructional tools in mathematics\nclassrooms."
                },
                "authors": [
                    {
                        "name": "Anselm R. Strohmaier"
                    },
                    {
                        "name": "Wim Van Dooren"
                    },
                    {
                        "name": "Kathrin Seler"
                    },
                    {
                        "name": "Brian Greer"
                    },
                    {
                        "name": "Lieven Verschaffel"
                    }
                ],
                "author_detail": {
                    "name": "Lieven Verschaffel"
                },
                "author": "Lieven Verschaffel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.24006v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.24006v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.HO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23998v1",
                "updated": "2025-06-30T16:02:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    2,
                    28,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T16:02:28Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    16,
                    2,
                    28,
                    0,
                    181,
                    0
                ],
                "title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via\n  Multi-Agent Large Language Models with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via\n  Multi-Agent Large Language Models with Reinforcement Learning"
                },
                "summary": "Congenital heart disease (CHD) presents complex, lifelong challenges often\nunderrepresented in traditional clinical metrics. While unstructured narratives\noffer rich insights into patient and caregiver experiences, manual thematic\nanalysis (TA) remains labor-intensive and unscalable. We propose a fully\nautomated large language model (LLM) pipeline that performs end-to-end TA on\nclinical narratives, which eliminates the need for manual coding or full\ntranscript review. Our system employs a novel multi-agent framework, where\nspecialized LLM agents assume roles to enhance theme quality and alignment with\nhuman analysis. To further improve thematic relevance, we optionally integrate\nreinforcement learning from human feedback (RLHF). This supports scalable,\npatient-centered analysis of large qualitative datasets and allows LLMs to be\nfine-tuned for specific clinical contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Congenital heart disease (CHD) presents complex, lifelong challenges often\nunderrepresented in traditional clinical metrics. While unstructured narratives\noffer rich insights into patient and caregiver experiences, manual thematic\nanalysis (TA) remains labor-intensive and unscalable. We propose a fully\nautomated large language model (LLM) pipeline that performs end-to-end TA on\nclinical narratives, which eliminates the need for manual coding or full\ntranscript review. Our system employs a novel multi-agent framework, where\nspecialized LLM agents assume roles to enhance theme quality and alignment with\nhuman analysis. To further improve thematic relevance, we optionally integrate\nreinforcement learning from human feedback (RLHF). This supports scalable,\npatient-centered analysis of large qualitative datasets and allows LLMs to be\nfine-tuned for specific clinical contexts."
                },
                "authors": [
                    {
                        "name": "Seungjun Yi"
                    },
                    {
                        "name": "Joakim Nguyen"
                    },
                    {
                        "name": "Huimin Xu"
                    },
                    {
                        "name": "Terence Lim"
                    },
                    {
                        "name": "Andrew Well"
                    },
                    {
                        "name": "Mia Markey"
                    },
                    {
                        "name": "Ying Ding"
                    }
                ],
                "author_detail": {
                    "name": "Ying Ding"
                },
                "author": "Ying Ding",
                "arxiv_comment": "Presented at ACL 2025 SRW",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16084v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16084v3",
                "updated": "2025-06-30T15:59:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    59,
                    26,
                    0,
                    181,
                    0
                ],
                "published": "2025-04-22T17:59:56Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    17,
                    59,
                    56,
                    1,
                    112,
                    0
                ],
                "title": "TTRL: Test-Time Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTRL: Test-Time Reinforcement Learning"
                },
                "summary": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the maj@n metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model maj@n,\nand approach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates Reinforcement Learning (RL) on data without explicit\nlabels for reasoning tasks in Large Language Models (LLMs). The core challenge\nof the problem is reward estimation during inference while not having access to\nground-truth information. While this setting appears elusive, we find that\ncommon practices in Test-Time Scaling (TTS), such as majority voting, yield\nsurprisingly effective rewards suitable for driving RL training. In this work,\nwe introduce Test-Time Reinforcement Learning (TTRL), a novel method for\ntraining LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs\nby utilizing the priors in the pre-trained models. Our experiments demonstrate\nthat TTRL consistently improves performance across a variety of tasks and\nmodels. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by\napproximately 211% on the AIME 2024 with only unlabeled test data. Furthermore,\nalthough TTRL is only supervised by the maj@n metric, TTRL has demonstrated\nperformance to consistently surpass the upper limit of the initial model maj@n,\nand approach the performance of models trained directly on test data with\nground-truth labels. Our experimental findings validate the general\neffectiveness of TTRL across various tasks and highlight TTRL's potential for\nbroader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL"
                },
                "authors": [
                    {
                        "name": "Yuxin Zuo"
                    },
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Li Sheng"
                    },
                    {
                        "name": "Shang Qu"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Haozhan Li"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Xinwei Long"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Youbang Sun"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Lifan Yuan"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16084v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16084v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23995v1",
                "updated": "2025-06-30T15:58:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    58,
                    10,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:58:10Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    58,
                    10,
                    0,
                    181,
                    0
                ],
                "title": "STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems"
                },
                "summary": "Autonomous Driving System (ADS) testing is essential to ensure the safety and\nreliability of autonomous vehicles (AVs) before deployment. However, existing\ntechniques primarily focus on evaluating ADS functionalities in single-AV\nsettings. As ADSs are increasingly deployed in multi-AV traffic, it becomes\ncrucial to assess their cooperative performance, particularly regarding\ndeadlocks, a fundamental coordination failure in which multiple AVs enter a\ncircular waiting state indefinitely, resulting in motion planning failures.\nDespite its importance, the cooperative capability of ADSs to prevent deadlocks\nremains insufficiently underexplored. To address this gap, we propose the first\ndedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,\nSTCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs\ncontrolled by the ADS under test are in a circular wait state. STCLocker\nconsists of three key components: Deadlock Oracle, Conflict Feedback, and\nConflict-aware Scenario Generation. Deadlock Oracle provides a reliable\nblack-box mechanism for detecting deadlock cycles among multiple AVs within a\ngiven scenario. Conflict Feedback and Conflict-aware Scenario Generation\ncollaborate to actively guide AVs into simultaneous competition over spatial\nconflict resources (i.e., shared passing regions) and temporal competitive\nbehaviors (i.e., reaching the conflict region at the same time), thereby\nincreasing the effectiveness of generating conflict-prone deadlocks. We\nevaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,\na module-based ADS supporting cooperative communication. Experimental results\nshow that, on average, STCLocker generates more DLS than the best-performing\nbaseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Driving System (ADS) testing is essential to ensure the safety and\nreliability of autonomous vehicles (AVs) before deployment. However, existing\ntechniques primarily focus on evaluating ADS functionalities in single-AV\nsettings. As ADSs are increasingly deployed in multi-AV traffic, it becomes\ncrucial to assess their cooperative performance, particularly regarding\ndeadlocks, a fundamental coordination failure in which multiple AVs enter a\ncircular waiting state indefinitely, resulting in motion planning failures.\nDespite its importance, the cooperative capability of ADSs to prevent deadlocks\nremains insufficiently underexplored. To address this gap, we propose the first\ndedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,\nSTCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs\ncontrolled by the ADS under test are in a circular wait state. STCLocker\nconsists of three key components: Deadlock Oracle, Conflict Feedback, and\nConflict-aware Scenario Generation. Deadlock Oracle provides a reliable\nblack-box mechanism for detecting deadlock cycles among multiple AVs within a\ngiven scenario. Conflict Feedback and Conflict-aware Scenario Generation\ncollaborate to actively guide AVs into simultaneous competition over spatial\nconflict resources (i.e., shared passing regions) and temporal competitive\nbehaviors (i.e., reaching the conflict region at the same time), thereby\nincreasing the effectiveness of generating conflict-prone deadlocks. We\nevaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,\na module-based ADS supporting cooperative communication. Experimental results\nshow that, on average, STCLocker generates more DLS than the best-performing\nbaseline."
                },
                "authors": [
                    {
                        "name": "Mingfei Cheng"
                    },
                    {
                        "name": "Renzhi Wang"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23979v1",
                "updated": "2025-06-30T15:45:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    45,
                    28,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:45:28Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    45,
                    28,
                    0,
                    181,
                    0
                ],
                "title": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference\n  Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference\n  Data Generation"
                },
                "summary": "Conducting supervised fine-tuning and preference fine-tuning on large\nlanguage models (LLMs) requires high-quality datasets to improve their ability\nto follow instructions and align with human preferences and values. However,\nconstructing such datasets is resource-intensive, and most available datasets\nfor supervised and preference fine-tuning are in English. To address these\nchallenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided\n\\underline{\\textbf{P}}reference Data Generation (TaP) framework, which\nfacilitates automated and scalable construction of preference datasets across\nvarious languages. TaP is grounded in a structured taxonomy that allows\nfine-grained control over dataset composition, thereby ensuring both diversity\nand comprehensive coverage. We employ TaP-generated datasets to perform\nsupervised and preference fine-tuning on various LLMs. Experimental results\ndemonstrate that LLMs trained on TaP-generated datasets outperform those\ntrained on existing open-source datasets. Remarkably, LLMs trained on\nTaP-generated datasets surpass the performance of those trained on an\nopen-source dataset that is 180 times larger.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conducting supervised fine-tuning and preference fine-tuning on large\nlanguage models (LLMs) requires high-quality datasets to improve their ability\nto follow instructions and align with human preferences and values. However,\nconstructing such datasets is resource-intensive, and most available datasets\nfor supervised and preference fine-tuning are in English. To address these\nchallenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided\n\\underline{\\textbf{P}}reference Data Generation (TaP) framework, which\nfacilitates automated and scalable construction of preference datasets across\nvarious languages. TaP is grounded in a structured taxonomy that allows\nfine-grained control over dataset composition, thereby ensuring both diversity\nand comprehensive coverage. We employ TaP-generated datasets to perform\nsupervised and preference fine-tuning on various LLMs. Experimental results\ndemonstrate that LLMs trained on TaP-generated datasets outperform those\ntrained on existing open-source datasets. Remarkably, LLMs trained on\nTaP-generated datasets surpass the performance of those trained on an\nopen-source dataset that is 180 times larger."
                },
                "authors": [
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Tianhao Shen"
                    },
                    {
                        "name": "Xinwei Wu"
                    },
                    {
                        "name": "Dan Shi"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Wuwei Huang"
                    },
                    {
                        "name": "Quandong Wang"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "arxiv_comment": "33 pages, 15 tables, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23978v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23978v1",
                "updated": "2025-06-30T15:45:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    45,
                    17,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:45:17Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    45,
                    17,
                    0,
                    181,
                    0
                ],
                "title": "LLM Agents Are the Antidote to Walled Gardens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Agents Are the Antidote to Walled Gardens"
                },
                "summary": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security."
                },
                "authors": [
                    {
                        "name": "Samuele Marro"
                    },
                    {
                        "name": "Philip Torr"
                    }
                ],
                "author_detail": {
                    "name": "Philip Torr"
                },
                "author": "Philip Torr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23978v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23978v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68M10, 91B26",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.2.7; H.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23967v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23967v1",
                "updated": "2025-06-30T15:36:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    36,
                    53,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:36:53Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    36,
                    53,
                    0,
                    181,
                    0
                ],
                "title": "Green Metrics Tool: Measuring for fun and profit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Green Metrics Tool: Measuring for fun and profit"
                },
                "summary": "The environmental impact of software is gaining increasing attention as the\ndemand for computational resources continues to rise. In order to optimize\nsoftware resource consumption and reduce carbon emissions, measuring and\nevaluating software is a first essential step. In this paper we discuss what\nmetrics are important for fact base decision making. We introduce the Green\nMetrics Tool (GMT), a novel framework for accurately measuring the resource\nconsumption of software. The tool provides a containerized, controlled, and\nreproducible life cycle-based approach, assessing the resource use of software\nduring key phases. Finally, we discuss GMT features like visualization,\ncomparability and rule- and LLM-based optimisations highlighting its potential\nto guide developers and researchers in reducing the environmental impact of\ntheir software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The environmental impact of software is gaining increasing attention as the\ndemand for computational resources continues to rise. In order to optimize\nsoftware resource consumption and reduce carbon emissions, measuring and\nevaluating software is a first essential step. In this paper we discuss what\nmetrics are important for fact base decision making. We introduce the Green\nMetrics Tool (GMT), a novel framework for accurately measuring the resource\nconsumption of software. The tool provides a containerized, controlled, and\nreproducible life cycle-based approach, assessing the resource use of software\nduring key phases. Finally, we discuss GMT features like visualization,\ncomparability and rule- and LLM-based optimisations highlighting its potential\nto guide developers and researchers in reducing the environmental impact of\ntheir software."
                },
                "authors": [
                    {
                        "name": "Geerd-Dietger Hoffmann"
                    },
                    {
                        "name": "Verena Majuntke"
                    }
                ],
                "author_detail": {
                    "name": "Verena Majuntke"
                },
                "author": "Verena Majuntke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23967v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23967v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23964v1",
                "updated": "2025-06-30T15:36:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    36,
                    22,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:36:22Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    36,
                    22,
                    0,
                    181,
                    0
                ],
                "title": "Learning Constraints Directly from Network Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Constraints Directly from Network Data"
                },
                "summary": "Network data conforms to a wide range of rules that arise from protocols,\ndesign principles, and deployment decisions (e.g., a packet's queuing delay\nmust be less than its end-to-end delay). Formalizing such rules as logic\nconstraints can (i) improve the quality of synthetic data, (ii) reduce the\nbrittleness of machine learning (ML) models, and (iii) improve semantic\nunderstanding of network measurements. However, these benefits remain out of\nreach if rule extraction is manual or solely reliant on ML, as both approaches\nyield incomplete, unreliable, and/or inaccurate rules.\n  This paper formulates rule extraction as a constraint modeling problem and\nintroduces NetNomos that learns propositional logic constraints directly from\nraw network measurements. Constraint modeling in this domain is uniquely\nchallenging due to the scale of the data, the inherent learning complexity and\npassive environment, and the lack of ground truth supervision. NetNomos\naddresses these challenges via a lattice-based search structured by constraint\nspecificity and succinctness. Our approach reduces learning complexity from\nsuperquadratic to logarithmic and enables efficient traversal in combinatorial\nsearch space.\n  Our evaluations on diverse network datasets show that NetNomos learns all\nbenchmark rules, including those associated with as little as 0.01% of data\npoints, in under three hours. In contrast, baseline methods discover less than\n25% of the rules and require several days to run. Through three case studies,\nwe show that: NetNomos (i) finds rule violations in the outputs of all seven\nsynthetic traffic generators, hence can be used to assess and guide their\ngeneration process; (ii) detects semantic differences in traffic, hence can be\nused for anomaly detection; and (iii) automatically finds rules used for\ntelemetry imputation, hence can support monitoring through inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network data conforms to a wide range of rules that arise from protocols,\ndesign principles, and deployment decisions (e.g., a packet's queuing delay\nmust be less than its end-to-end delay). Formalizing such rules as logic\nconstraints can (i) improve the quality of synthetic data, (ii) reduce the\nbrittleness of machine learning (ML) models, and (iii) improve semantic\nunderstanding of network measurements. However, these benefits remain out of\nreach if rule extraction is manual or solely reliant on ML, as both approaches\nyield incomplete, unreliable, and/or inaccurate rules.\n  This paper formulates rule extraction as a constraint modeling problem and\nintroduces NetNomos that learns propositional logic constraints directly from\nraw network measurements. Constraint modeling in this domain is uniquely\nchallenging due to the scale of the data, the inherent learning complexity and\npassive environment, and the lack of ground truth supervision. NetNomos\naddresses these challenges via a lattice-based search structured by constraint\nspecificity and succinctness. Our approach reduces learning complexity from\nsuperquadratic to logarithmic and enables efficient traversal in combinatorial\nsearch space.\n  Our evaluations on diverse network datasets show that NetNomos learns all\nbenchmark rules, including those associated with as little as 0.01% of data\npoints, in under three hours. In contrast, baseline methods discover less than\n25% of the rules and require several days to run. Through three case studies,\nwe show that: NetNomos (i) finds rule violations in the outputs of all seven\nsynthetic traffic generators, hence can be used to assess and guide their\ngeneration process; (ii) detects semantic differences in traffic, hence can be\nused for anomaly detection; and (iii) automatically finds rules used for\ntelemetry imputation, hence can support monitoring through inference."
                },
                "authors": [
                    {
                        "name": "Hongyu H"
                    },
                    {
                        "name": "Minhao Jin"
                    },
                    {
                        "name": "Maria Apostolaki"
                    }
                ],
                "author_detail": {
                    "name": "Maria Apostolaki"
                },
                "author": "Maria Apostolaki",
                "arxiv_comment": "13 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.3; I.2.6; I.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23951v1",
                "updated": "2025-06-30T15:18:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    18,
                    50,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T15:18:50Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    15,
                    18,
                    50,
                    0,
                    181,
                    0
                ],
                "title": "Unveiling Decision-Making in LLMs for Text Classification : Extraction\n  of influential and interpretable concepts with Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Decision-Making in LLMs for Text Classification : Extraction\n  of influential and interpretable concepts with Sparse Autoencoders"
                },
                "summary": "Sparse Autoencoders (SAEs) have been successfully used to probe Large\nLanguage Models (LLMs) and extract interpretable concepts from their internal\nrepresentations. These concepts are linear combinations of neuron activations\nthat correspond to human-interpretable features. In this paper, we investigate\nthe effectiveness of SAE-based explainability approaches for sentence\nclassification, a domain where such methods have not been extensively explored.\nWe present a novel SAE-based architecture tailored for text classification,\nleveraging a specialized classifier head and incorporating an activation rate\nsparsity loss. We benchmark this architecture against established methods such\nas ConceptShap, Independent Component Analysis, and other SAE-based concept\nextraction techniques. Our evaluation covers two classification benchmarks and\nfour fine-tuned LLMs from the Pythia family. We further enrich our analysis\nwith two novel metrics for measuring the precision of concept-based\nexplanations, using an external sentence encoder. Our empirical results show\nthat our architecture improves both the causality and interpretability of the\nextracted features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Autoencoders (SAEs) have been successfully used to probe Large\nLanguage Models (LLMs) and extract interpretable concepts from their internal\nrepresentations. These concepts are linear combinations of neuron activations\nthat correspond to human-interpretable features. In this paper, we investigate\nthe effectiveness of SAE-based explainability approaches for sentence\nclassification, a domain where such methods have not been extensively explored.\nWe present a novel SAE-based architecture tailored for text classification,\nleveraging a specialized classifier head and incorporating an activation rate\nsparsity loss. We benchmark this architecture against established methods such\nas ConceptShap, Independent Component Analysis, and other SAE-based concept\nextraction techniques. Our evaluation covers two classification benchmarks and\nfour fine-tuned LLMs from the Pythia family. We further enrich our analysis\nwith two novel metrics for measuring the precision of concept-based\nexplanations, using an external sentence encoder. Our empirical results show\nthat our architecture improves both the causality and interpretability of the\nextracted features."
                },
                "authors": [
                    {
                        "name": "Mathis Le Bail"
                    },
                    {
                        "name": "Jrmie Dentan"
                    },
                    {
                        "name": "Davide Buscaldi"
                    },
                    {
                        "name": "Sonia Vanier"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Vanier"
                },
                "author": "Sonia Vanier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23930v1",
                "updated": "2025-06-30T14:59:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    59,
                    25,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:59:25Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    59,
                    25,
                    0,
                    181,
                    0
                ],
                "title": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection\n  in Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection\n  in Low-Resource Languages"
                },
                "summary": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time."
                },
                "authors": [
                    {
                        "name": "Ruhina Tabasshum Prome"
                    },
                    {
                        "name": "Tarikul Islam Tamiti"
                    },
                    {
                        "name": "Anomadarshi Barua"
                    }
                ],
                "author_detail": {
                    "name": "Anomadarshi Barua"
                },
                "arxiv_affiliation": "George Mason University",
                "author": "Anomadarshi Barua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23929v1",
                "updated": "2025-06-30T14:58:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    58,
                    23,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:58:23Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    58,
                    23,
                    0,
                    181,
                    0
                ],
                "title": "IMPACT: Inflectional Morphology Probes Across Complex Typologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IMPACT: Inflectional Morphology Probes Across Complex Typologies"
                },
                "summary": "Large Language Models (LLMs) have shown significant progress on various\nmultilingual benchmarks and are increasingly used to generate and evaluate text\nin non-English languages. However, while they may produce fluent outputs, it\nremains unclear to what extent these models truly grasp the underlying\nlinguistic complexity of those languages, particularly in morphology. To\ninvestigate this, we introduce IMPACT, a synthetically generated evaluation\nframework focused on inflectional morphology, which we publicly release,\ndesigned to evaluate LLM performance across five morphologically rich\nlanguages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes\nunit-test-style cases covering both shared and language-specific phenomena,\nfrom basic verb inflections (e.g., tense, number, gender) to unique features\nlike Arabic's reverse gender agreement and vowel harmony in Finnish and\nTurkish. We assess eight multilingual LLMs that, despite strong English\nperformance, struggle with other languages and uncommon morphological patterns,\nespecially when judging ungrammatical examples. We also show that Chain of\nThought and Thinking Models can degrade performance. Our work exposes gaps in\nLLMs' handling of linguistic complexity, pointing to clear room for\nimprovement. To support further research, we publicly release the IMPACT\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant progress on various\nmultilingual benchmarks and are increasingly used to generate and evaluate text\nin non-English languages. However, while they may produce fluent outputs, it\nremains unclear to what extent these models truly grasp the underlying\nlinguistic complexity of those languages, particularly in morphology. To\ninvestigate this, we introduce IMPACT, a synthetically generated evaluation\nframework focused on inflectional morphology, which we publicly release,\ndesigned to evaluate LLM performance across five morphologically rich\nlanguages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes\nunit-test-style cases covering both shared and language-specific phenomena,\nfrom basic verb inflections (e.g., tense, number, gender) to unique features\nlike Arabic's reverse gender agreement and vowel harmony in Finnish and\nTurkish. We assess eight multilingual LLMs that, despite strong English\nperformance, struggle with other languages and uncommon morphological patterns,\nespecially when judging ungrammatical examples. We also show that Chain of\nThought and Thinking Models can degrade performance. Our work exposes gaps in\nLLMs' handling of linguistic complexity, pointing to clear room for\nimprovement. To support further research, we publicly release the IMPACT\nframework."
                },
                "authors": [
                    {
                        "name": "Mohammed J. Saeed"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Evgeny Fedoseev"
                    },
                    {
                        "name": "Sevil Caliskan"
                    },
                    {
                        "name": "Tatiana Vodolazova"
                    }
                ],
                "author_detail": {
                    "name": "Tatiana Vodolazova"
                },
                "author": "Tatiana Vodolazova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23924v1",
                "updated": "2025-06-30T14:54:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    54,
                    15,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:54:15Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    54,
                    15,
                    0,
                    181,
                    0
                ],
                "title": "Performance of LLMs on Stochastic Modeling Operations Research Problems:\n  From Theory to Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of LLMs on Stochastic Modeling Operations Research Problems:\n  From Theory to Practice"
                },
                "summary": "Large language models (LLMs) have exhibited expert-level capabilities across\nvarious domains. However, their abilities to solve problems in Operations\nResearch (OR) -- the analysis and optimization of mathematical models derived\nfrom real-world problems or their verbal descriptions -- remain underexplored.\nIn this work, we take a first step toward evaluating LLMs' abilities to solve\nstochastic modeling problems, a core class of OR problems characterized by\nuncertainty and typically involving tools from probability, statistics, and\nstochastic processes. We manually procure a representative set of\ngraduate-level homework and doctoral qualification-exam problems and test LLMs'\nabilities to solve them. We further leverage SimOpt, an open-source library of\nsimulation-optimization problems and solvers, to investigate LLMs' abilities to\nmake real-world decisions under uncertainty. Our results show that, though a\nnontrivial amount of work is still needed to reliably automate the stochastic\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\npar with human experts in both classroom and practical settings. These findings\nhighlight the potential of building AI agents that assist OR researchers and\namplify the real-world impact of OR through automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited expert-level capabilities across\nvarious domains. However, their abilities to solve problems in Operations\nResearch (OR) -- the analysis and optimization of mathematical models derived\nfrom real-world problems or their verbal descriptions -- remain underexplored.\nIn this work, we take a first step toward evaluating LLMs' abilities to solve\nstochastic modeling problems, a core class of OR problems characterized by\nuncertainty and typically involving tools from probability, statistics, and\nstochastic processes. We manually procure a representative set of\ngraduate-level homework and doctoral qualification-exam problems and test LLMs'\nabilities to solve them. We further leverage SimOpt, an open-source library of\nsimulation-optimization problems and solvers, to investigate LLMs' abilities to\nmake real-world decisions under uncertainty. Our results show that, though a\nnontrivial amount of work is still needed to reliably automate the stochastic\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\npar with human experts in both classroom and practical settings. These findings\nhighlight the potential of building AI agents that assist OR researchers and\namplify the real-world impact of OR through automation."
                },
                "authors": [
                    {
                        "name": "Akshit Kumar"
                    },
                    {
                        "name": "Tianyi Peng"
                    },
                    {
                        "name": "Yuhang Wu"
                    },
                    {
                        "name": "Assaf Zeevi"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Zeevi"
                },
                "author": "Assaf Zeevi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23921v1",
                "updated": "2025-06-30T14:49:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    49,
                    28,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:49:28Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    49,
                    28,
                    0,
                    181,
                    0
                ],
                "title": "The Trilemma of Truth in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Trilemma of Truth in Large Language Models"
                },
                "summary": "We often attribute human characteristics to large language models (LLMs) and\nclaim that they \"know\" certain things. LLMs have an internal probabilistic\nknowledge that represents information retained during training. How can we\nassess the veracity of this knowledge? We examine two common methods for\nprobing the veracity of LLMs and discover several assumptions that are flawed.\nTo address these flawed assumptions, we introduce sAwMIL (short for Sparse\nAware Multiple-Instance Learning), a probing method that utilizes the internal\nactivations of LLMs to separate statements into true, false, and neither.\nsAwMIL is based on multiple-instance learning and conformal prediction. We\nevaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including\nboth default and chat-based variants, as well as on 3 new datasets. Among the\ninsights we provide are: (1) the veracity signal is often concentrated in the\nthird quarter of an LLM's depth; (2) truth and falsehood signals are not always\nsymmetric; (3) linear probes perform better on chat models than on default\nmodels; (4) nonlinear probes may be required to capture veracity signals for\nsome LLMs with reinforcement learning from human feedback or knowledge\ndistillation; and (5) LLMs capture a third type of signal that is distinct from\ntrue and false and is neither true nor false. These findings provide a reliable\nmethod for verifying what LLMs \"know\" and how certain they are of their\nprobabilistic internal knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We often attribute human characteristics to large language models (LLMs) and\nclaim that they \"know\" certain things. LLMs have an internal probabilistic\nknowledge that represents information retained during training. How can we\nassess the veracity of this knowledge? We examine two common methods for\nprobing the veracity of LLMs and discover several assumptions that are flawed.\nTo address these flawed assumptions, we introduce sAwMIL (short for Sparse\nAware Multiple-Instance Learning), a probing method that utilizes the internal\nactivations of LLMs to separate statements into true, false, and neither.\nsAwMIL is based on multiple-instance learning and conformal prediction. We\nevaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including\nboth default and chat-based variants, as well as on 3 new datasets. Among the\ninsights we provide are: (1) the veracity signal is often concentrated in the\nthird quarter of an LLM's depth; (2) truth and falsehood signals are not always\nsymmetric; (3) linear probes perform better on chat models than on default\nmodels; (4) nonlinear probes may be required to capture veracity signals for\nsome LLMs with reinforcement learning from human feedback or knowledge\ndistillation; and (5) LLMs capture a third type of signal that is distinct from\ntrue and false and is neither true nor false. These findings provide a reliable\nmethod for verifying what LLMs \"know\" and how certain they are of their\nprobabilistic internal knowledge."
                },
                "authors": [
                    {
                        "name": "Germans Savcisens"
                    },
                    {
                        "name": "Tina Eliassi-Rad"
                    }
                ],
                "author_detail": {
                    "name": "Tina Eliassi-Rad"
                },
                "author": "Tina Eliassi-Rad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19592v2",
                "updated": "2025-06-30T14:40:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    40,
                    24,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-24T13:02:06Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    13,
                    2,
                    6,
                    1,
                    175,
                    0
                ],
                "title": "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to\n  Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to\n  Task Planning"
                },
                "summary": "We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a\nmulti-agent framework that integrates Large Language Models (LLMs) with\nsymbolic planning to solve complex tasks without the need for manually defined\nenvironment models. TAPAS employs specialized LLM-based agents that\ncollaboratively generate and adapt domain models, initial states, and goal\nspecifications as needed using structured tool-calling mechanisms. Through this\ntool-based interaction, downstream agents can request modifications from\nupstream agents, enabling adaptation to novel attributes and constraints\nwithout manual domain redefinition. A ReAct (Reason+Act)-style execution agent,\ncoupled with natural language plan translation, bridges the gap between\ndynamically generated plans and real-world robot capabilities. TAPAS\ndemonstrates strong performance in benchmark planning domains and in the\nVirtualHome simulated real-world environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a\nmulti-agent framework that integrates Large Language Models (LLMs) with\nsymbolic planning to solve complex tasks without the need for manually defined\nenvironment models. TAPAS employs specialized LLM-based agents that\ncollaboratively generate and adapt domain models, initial states, and goal\nspecifications as needed using structured tool-calling mechanisms. Through this\ntool-based interaction, downstream agents can request modifications from\nupstream agents, enabling adaptation to novel attributes and constraints\nwithout manual domain redefinition. A ReAct (Reason+Act)-style execution agent,\ncoupled with natural language plan translation, bridges the gap between\ndynamically generated plans and real-world robot capabilities. TAPAS\ndemonstrates strong performance in benchmark planning domains and in the\nVirtualHome simulated real-world environment."
                },
                "authors": [
                    {
                        "name": "Harisankar Babu"
                    },
                    {
                        "name": "Philipp Schillinger"
                    },
                    {
                        "name": "Tamim Asfour"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Asfour"
                },
                "author": "Tamim Asfour",
                "arxiv_comment": "Accepted at IEEE CASE 2025, 8 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23888v1",
                "updated": "2025-06-30T14:18:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    18,
                    35,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T14:18:35Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    14,
                    18,
                    35,
                    0,
                    181,
                    0
                ],
                "title": "Advancing Multi-Step Mathematical Reasoning in Large Language Models\n  through Multi-Layered Self-Reflection with Auto-Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Multi-Step Mathematical Reasoning in Large Language Models\n  through Multi-Layered Self-Reflection with Auto-Prompting"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their problem-solving capabilities. However, these models still\nstruggle when faced with complex multi-step reasoning tasks. In this paper, we\npropose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,\na novel approach designed to enhance multi-step mathematical reasoning in LLMs\nby integrating techniques such as Chain of Thought (CoT), Self-Reflection, and\nAuto-Prompting. Unlike traditional static prompting methods, MAPS employs an\niterative refinement process. Initially, the model generates a solution using\nCoT prompting. When errors are detected, an adaptive self-reflection mechanism\nidentifies and analyzes them, generating tailored prompts to guide corrections.\nThese dynamically adjusted prompts enable the model to iteratively refine its\nreasoning. Experiments on four well-established benchmarks across multiple LLMs\nshow that MAPS significantly outperforms standard CoT and achieves competitive\nresults with reasoning-optimized models. In addition, MAPS enables\ngeneral-purpose LLMs to reach performance levels comparable to specialized\nreasoning models. While deeper reflection layers improve accuracy, they also\nincrease token usage and costs. To balance this trade-off, MAPS strategically\nlimits reflection depth, ensuring an optimal balance between cost and reasoning\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their problem-solving capabilities. However, these models still\nstruggle when faced with complex multi-step reasoning tasks. In this paper, we\npropose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,\na novel approach designed to enhance multi-step mathematical reasoning in LLMs\nby integrating techniques such as Chain of Thought (CoT), Self-Reflection, and\nAuto-Prompting. Unlike traditional static prompting methods, MAPS employs an\niterative refinement process. Initially, the model generates a solution using\nCoT prompting. When errors are detected, an adaptive self-reflection mechanism\nidentifies and analyzes them, generating tailored prompts to guide corrections.\nThese dynamically adjusted prompts enable the model to iteratively refine its\nreasoning. Experiments on four well-established benchmarks across multiple LLMs\nshow that MAPS significantly outperforms standard CoT and achieves competitive\nresults with reasoning-optimized models. In addition, MAPS enables\ngeneral-purpose LLMs to reach performance levels comparable to specialized\nreasoning models. While deeper reflection layers improve accuracy, they also\nincrease token usage and costs. To balance this trade-off, MAPS strategically\nlimits reflection depth, ensuring an optimal balance between cost and reasoning\nperformance."
                },
                "authors": [
                    {
                        "name": "Andr de Souza Loureiro"
                    },
                    {
                        "name": "Jorge Valverde-Rebaza"
                    },
                    {
                        "name": "Julieta Noguez"
                    },
                    {
                        "name": "David Escarcega"
                    },
                    {
                        "name": "Ricardo Marcacini"
                    }
                ],
                "author_detail": {
                    "name": "Ricardo Marcacini"
                },
                "author": "Ricardo Marcacini",
                "arxiv_comment": "Accepted for publication in: European Conference on Machine Learning\n  and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD\n  2025). Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23864v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23864v1",
                "updated": "2025-06-30T13:57:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    57,
                    28,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T13:57:28Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    57,
                    28,
                    0,
                    181,
                    0
                ],
                "title": "Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What\n  to Do About It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What\n  to Do About It"
                },
                "summary": "We conduct a systematic audit of three widely used reasoning benchmarks,\nSocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark\nitems and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and\nLLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic\nissues in benchmark design (e.g., duplicated items, ambiguous wording, and\nimplausible answers), as well as scoring procedures that prioritize output form\nover reasoning process. Through systematic human annotation and re-evaluation\non cleaned benchmark subsets, we find that model scores often improve not due\nto due to erratic surface wording variations and not to improved reasoning.\nInfact, further analyses show that model performance is highly sensitive to\nminor input variations such as context availability and phrasing, revealing\nthat high scores may reflect alignment with format-specific cues rather than\nconsistent inference based on the input. These findings challenge the validity\nof current benchmark-based claims about reasoning in LLMs, and highlight the\nneed for evaluation protocols that assess reasoning as a process of drawing\ninference from available information, rather than as static output selection.\nWe release audited data and evaluation tools to support more interpretable and\ndiagnostic assessments of model reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We conduct a systematic audit of three widely used reasoning benchmarks,\nSocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark\nitems and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and\nLLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic\nissues in benchmark design (e.g., duplicated items, ambiguous wording, and\nimplausible answers), as well as scoring procedures that prioritize output form\nover reasoning process. Through systematic human annotation and re-evaluation\non cleaned benchmark subsets, we find that model scores often improve not due\nto due to erratic surface wording variations and not to improved reasoning.\nInfact, further analyses show that model performance is highly sensitive to\nminor input variations such as context availability and phrasing, revealing\nthat high scores may reflect alignment with format-specific cues rather than\nconsistent inference based on the input. These findings challenge the validity\nof current benchmark-based claims about reasoning in LLMs, and highlight the\nneed for evaluation protocols that assess reasoning as a process of drawing\ninference from available information, rather than as static output selection.\nWe release audited data and evaluation tools to support more interpretable and\ndiagnostic assessments of model reasoning."
                },
                "authors": [
                    {
                        "name": "Seyed Mahed Mousavi"
                    },
                    {
                        "name": "Edoardo Cecchinato"
                    },
                    {
                        "name": "Lucia Hornikova"
                    },
                    {
                        "name": "Giuseppe Riccardi"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Riccardi"
                },
                "author": "Giuseppe Riccardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23864v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23864v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23862v1",
                "updated": "2025-06-30T13:57:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    57,
                    11,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T13:57:11Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    57,
                    11,
                    0,
                    181,
                    0
                ],
                "title": "Large Language Models for Statistical Inference: Context Augmentation\n  with Applications to the Two-Sample Problem and Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Statistical Inference: Context Augmentation\n  with Applications to the Two-Sample Problem and Regression"
                },
                "summary": "We introduce context augmentation, a data-augmentation approach that uses\nlarge language models (LLMs) to generate contexts around observed strings as a\nmeans of facilitating valid frequentist inference. These generated contexts\nserve to reintroduce uncertainty, incorporate auxiliary information, and\nfacilitate interpretability. For example, in the two-sample test, we compare\nthe log-probability of strings under contexts from its own versus the other\ngroup. We show on synthetic data that the method's t-statistics exhibit the\nexpected null behaviour while maintaining power and, through a replication,\nthat the method is powerful and interpretable. We next introduce text-on-text\nregression. Contexts generated around the predictor string are treated as\nmediating variables between the predictor and outcome strings. Using negative\ncontrols, we then distinguish between semantic and syntactic dimensions of\nprediction. Analysis of real-world dialogic data illustrates behaviour\npredicted from a psycholinguistic framework. Theoretically, we provide\nidentification conditions, derive an influence-function decomposition, and show\nthat repeated cross-fitting of a pivotal statistic yields higher-order\nefficiency. We derive bounds linking estimation error, context count, and\nnumber of cross-fits. Taken together, context augmentation offers the ability\nto connect LLMs with longstanding statistical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce context augmentation, a data-augmentation approach that uses\nlarge language models (LLMs) to generate contexts around observed strings as a\nmeans of facilitating valid frequentist inference. These generated contexts\nserve to reintroduce uncertainty, incorporate auxiliary information, and\nfacilitate interpretability. For example, in the two-sample test, we compare\nthe log-probability of strings under contexts from its own versus the other\ngroup. We show on synthetic data that the method's t-statistics exhibit the\nexpected null behaviour while maintaining power and, through a replication,\nthat the method is powerful and interpretable. We next introduce text-on-text\nregression. Contexts generated around the predictor string are treated as\nmediating variables between the predictor and outcome strings. Using negative\ncontrols, we then distinguish between semantic and syntactic dimensions of\nprediction. Analysis of real-world dialogic data illustrates behaviour\npredicted from a psycholinguistic framework. Theoretically, we provide\nidentification conditions, derive an influence-function decomposition, and show\nthat repeated cross-fitting of a pivotal statistic yields higher-order\nefficiency. We derive bounds linking estimation error, context count, and\nnumber of cross-fits. Taken together, context augmentation offers the ability\nto connect LLMs with longstanding statistical practice."
                },
                "authors": [
                    {
                        "name": "Marc Ratkovic"
                    }
                ],
                "author_detail": {
                    "name": "Marc Ratkovic"
                },
                "author": "Marc Ratkovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23850v1",
                "updated": "2025-06-30T13:39:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    39,
                    54,
                    0,
                    181,
                    0
                ],
                "published": "2025-06-30T13:39:54Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    13,
                    39,
                    54,
                    0,
                    181,
                    0
                ],
                "title": "Email as the Interface to Generative AI Models: Seamless Administrative\n  Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Email as the Interface to Generative AI Models: Seamless Administrative\n  Automation"
                },
                "summary": "This paper introduces a novel architectural framework that integrates Large\nLanguage Models (LLMs) with email interfaces to automate administrative tasks,\nspecifically targeting accessibility barriers in enterprise environments. The\nsystem connects email communication channels with Optical Character Recognition\n(OCR) and intelligent automation, enabling non-technical administrative staff\nto delegate complex form-filling and document processing tasks using familiar\nemail interfaces. By treating the email body as a natural language prompt and\nattachments as contextual information, the workflow bridges the gap between\nadvanced AI capabilities and practical usability. Empirical evaluation shows\nthat the system can complete complex administrative forms in under 8 seconds of\nautomated processing, with human supervision reducing total staff time by a\nfactor of three to four compared to manual workflows. The top-performing LLM\naccurately filled 16 out of 29 form fields and reduced the total cost per\nprocessed form by 64% relative to manual completion. These findings demonstrate\nthat email-based LLM integration is a viable and cost-effective approach for\ndemocratizing advanced automation in organizational settings, supporting\nwidespread adoption without requiring specialized technical knowledge or major\nworkflow changes. This aligns with broader trends in leveraging LLMs to enhance\naccessibility and automate complex tasks for non-technical users, making\ntechnology more inclusive and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel architectural framework that integrates Large\nLanguage Models (LLMs) with email interfaces to automate administrative tasks,\nspecifically targeting accessibility barriers in enterprise environments. The\nsystem connects email communication channels with Optical Character Recognition\n(OCR) and intelligent automation, enabling non-technical administrative staff\nto delegate complex form-filling and document processing tasks using familiar\nemail interfaces. By treating the email body as a natural language prompt and\nattachments as contextual information, the workflow bridges the gap between\nadvanced AI capabilities and practical usability. Empirical evaluation shows\nthat the system can complete complex administrative forms in under 8 seconds of\nautomated processing, with human supervision reducing total staff time by a\nfactor of three to four compared to manual workflows. The top-performing LLM\naccurately filled 16 out of 29 form fields and reduced the total cost per\nprocessed form by 64% relative to manual completion. These findings demonstrate\nthat email-based LLM integration is a viable and cost-effective approach for\ndemocratizing advanced automation in organizational settings, supporting\nwidespread adoption without requiring specialized technical knowledge or major\nworkflow changes. This aligns with broader trends in leveraging LLMs to enhance\naccessibility and automate complex tasks for non-technical users, making\ntechnology more inclusive and efficient."
                },
                "authors": [
                    {
                        "name": "Andres Navarro"
                    },
                    {
                        "name": "Carlos de Quinto"
                    },
                    {
                        "name": "Jos Alberto Hernndez"
                    }
                ],
                "author_detail": {
                    "name": "Jos Alberto Hernndez"
                },
                "author": "Jos Alberto Hernndez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]