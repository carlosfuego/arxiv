[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.13532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13532v1",
                "updated": "2024-11-20T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "title": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures"
                },
                "summary": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers."
                },
                "authors": [
                    {
                        "name": "Semih Akkurt"
                    },
                    {
                        "name": "SÃ©bastien Lemaire"
                    },
                    {
                        "name": "Paul Bartholomew"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "arxiv_comment": "42 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v1",
                "updated": "2024-11-20T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_comment": "6 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is a\n  preprint for the \"Fusion Engineering and Design\" journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v3",
                "updated": "2024-11-19T18:24:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    24,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and\n  Retrieval"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12430v1",
                "updated": "2024-11-19T11:40:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T11:40:56Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    11,
                    40,
                    56,
                    1,
                    324,
                    0
                ],
                "title": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Eulerian approach to regularized JKO scheme with low-rank tensor\n  decompositions for Bayesian inversion"
                },
                "summary": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of using the Eulerian discretization for the problem of\nmodelling high-dimensional distributions and sampling, is studied. The problem\nis posed as a minimization problem over the space of probability measures with\nrespect to the Wasserstein distance and solved with entropy-regularized JKO\nscheme. Each proximal step can be formulated as a fixed-point equation and\nsolved with accelerated methods, such as Anderson's. The usage of low-rank\nTensor Train format allows to overcome the \\emph{curse of dimensionality}, i.e.\nthe exponential growth of degrees of freedom with dimension, inherent to\nEulerian approaches. The resulting method requires only pointwise computations\nof the unnormalized posterior and is, in particular, gradient-free. Fixed\nEulerian grid allows to employ a caching strategy, significally reducing the\nexpensive evaluations of the posterior. When the Eulerian model of the target\ndistribution is fitted, the passage back to the Lagrangian perspective can also\nbe made, allowing to approximately sample from it. We test our method both for\nsynthetic target distributions and particular Bayesian inverse problems and\nreport comparable or better performance than the baseline Metropolis-Hastings\nMCMC with same amount of resources. Finally, the fitted model can be modified\nto facilitate the solution of certain associated problems, which we demonstrate\nby fitting an importance distribution for a particular quantity of interest."
                },
                "authors": [
                    {
                        "name": "Vitalii Aksenov"
                    },
                    {
                        "name": "Martin Eigel"
                    }
                ],
                "author_detail": {
                    "name": "Martin Eigel"
                },
                "author": "Martin Eigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "46E27, 49Q22, 62F15, 68W25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12161v1",
                "updated": "2024-11-19T01:55:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T01:55:26Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    1,
                    55,
                    26,
                    1,
                    324,
                    0
                ],
                "title": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Management for Complex Storage Systems Using\n  CNN-LSTM-Based Spatiotemporal Prediction"
                },
                "summary": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an intelligent cache management strategy based on\nCNN-LSTM to improve the performance and cache hit rate of storage systems.\nThrough comparative experiments with traditional algorithms (such as LRU and\nLFU) and other deep learning models (such as RNN, GRU-RNN and LSTM), the\nresults show that the CNN-LSTM model has significant advantages in cache demand\nprediction. The MSE and MAE values of this model are significantly reduced,\nproving its effectiveness under complex data access patterns. This study not\nonly verifies the potential of deep learning technology in storage system\noptimization, but also provides direction and reference for further optimizing\nand improving cache management strategies. This intelligent cache management\nstrategy performs well in complex storage environments. By combining the\nspatial feature extraction capabilities of convolutional neural networks and\nthe time series modeling capabilities of long short-term memory networks, the\nCNN-LSTM model can more accurately predict cache needs, thereby Dynamically\noptimize cache allocation to improve system response speed and resource\nutilization. This research provides theoretical support and practical reference\nfor cache optimization under large-scale data access modes, and is of great\nsignificance to improving the performance of future storage systems."
                },
                "authors": [
                    {
                        "name": "Xiaoye Wang"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Tingyi Ruan"
                    },
                    {
                        "name": "Pochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Pochun Li"
                },
                "author": "Pochun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v1",
                "updated": "2024-11-18T18:59:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical selective state-space model (SSM) of Mamba addresses several\nlimitations of Transformers, such as quadratic computational complexity with\nsequence length and significant inference-time memory requirements due to the\nkey-value cache. However, the growing size of Mamba models continues to pose\ntraining and deployment challenges and raises environmental concerns due to\nconsiderable energy consumption. In this work, we introduce Bi-Mamba, a\nscalable and powerful 1-bit Mamba architecture designed for more efficient\nlarge language models with multiple sizes across 780M, 1.3B, and 2.7B. Bi-Mamba\nmodels are trained from scratch on data volume as regular LLM pertaining using\nan autoregressive distillation loss. Extensive experimental results on language\nmodeling demonstrate that Bi-Mamba achieves performance comparable to its\nfull-precision counterparts (e.g., FP16 or BF16) and much better accuracy than\npost-training-binarization (PTB) Mamba baselines, while significantly reducing\nmemory footprint and energy consumption compared to the original Mamba model.\nOur study pioneers a new linear computational complexity LLM framework under\nlow-bit representation and facilitates the future design of specialized\nhardware tailored for efficient 1-bit Mamba-based LLMs."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11739v1",
                "updated": "2024-11-18T17:08:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T17:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    17,
                    8,
                    35,
                    0,
                    323,
                    0
                ],
                "title": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou"
                },
                "summary": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the significant evolution of multi-modal large models,\nmany recommender researchers realized the potential of multi-modal information\nfor user interest modeling. In industry, a wide-used modeling architecture is a\ncascading paradigm: (1) first pre-training a multi-modal model to provide\nomnipotent representations for downstream services; (2) The downstream\nrecommendation model takes the multi-modal representation as additional input\nto fit real user-item behaviours. Although such paradigm achieves remarkable\nimprovements, however, there still exist two problems that limit model\nperformance: (1) Representation Unmatching: The pre-trained multi-modal model\nis always supervised by the classic NLP/CV tasks, while the recommendation\nmodels are supervised by real user-item interaction. As a result, the two\nfundamentally different tasks' goals were relatively separate, and there was a\nlack of consistent objective on their representations; (2) Representation\nUnlearning: The generated multi-modal representations are always stored in\ncache store and serve as extra fixed input of recommendation model, thus could\nnot be updated by recommendation model gradient, further unfriendly for\ndownstream training. Inspired by the two difficulties challenges in downstream\ntasks usage, we introduce a quantitative multi-modal framework to customize the\nspecialized and trainable multi-modal information for different downstream\nmodels."
                },
                "authors": [
                    {
                        "name": "Xinchen Luo"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "Jinkai Yu"
                    },
                    {
                        "name": "Rui Huang"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Hezheng Lin"
                    },
                    {
                        "name": "Yichen Zheng"
                    },
                    {
                        "name": "Shiyao Wang"
                    },
                    {
                        "name": "Qigen Hu"
                    },
                    {
                        "name": "Changqing Qiu"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Zhiheng Yan"
                    },
                    {
                        "name": "Jingming Zhang"
                    },
                    {
                        "name": "Simin Zhang"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v1",
                "updated": "2024-11-18T11:12:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11300v1",
                "updated": "2024-11-18T05:50:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-18T05:50:58Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    5,
                    50,
                    58,
                    0,
                    323,
                    0
                ],
                "title": "Accelerating spherical K-means clustering for large-scale sparse\n  document data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating spherical K-means clustering for large-scale sparse\n  document data"
                },
                "summary": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an accelerated spherical K-means clustering algorithm for\nlarge-scale and high-dimensional sparse document data sets. We design an\nalgorithm working in an architecture-friendly manner (AFM), which is a\nprocedure of suppressing performance-degradation factors such as the numbers of\ninstructions, branch mispredictions, and cache misses in CPUs of a modern\ncomputer system. For the AFM operation, we leverage unique universal\ncharacteristics (UCs) of a data-object and a cluster's mean set, which are\nskewed distributions on data relationships such as Zipf's law and a\nfeature-value concentration phenomenon. The UCs indicate that the most part of\nthe number of multiplications for similarity calculations is executed regarding\nterms with high document frequencies (df) and the most part of a similarity\nbetween an object- and a mean-feature vector is obtained by the multiplications\nregarding a few high mean-feature values. Our proposed algorithm applies an\ninverted-index data structure to a mean set, extracts the specific region with\nhigh-df terms and high mean-feature values in the mean-inverted index by newly\nintroduced two structural parameters, and exploits the index divided into three\nparts for efficient pruning. The algorithm determines the two structural\nparameters by minimizing the approximate number of multiplications related to\nthat of instructions, reduces the branch mispredictions by sharing the index\nstructure including the two parameters with all the objects, and suppressing\nthe cache misses by keeping in the caches the frequently used data in the\nforegoing specific region, resulting in working in the AFM. We experimentally\ndemonstrate that our algorithm efficiently achieves superior speed performance\nin large-scale documents compared with algorithms using the state-of-the-art\ntechniques."
                },
                "authors": [
                    {
                        "name": "Kazuo Aoyama"
                    },
                    {
                        "name": "Kazumi Saito"
                    }
                ],
                "author_detail": {
                    "name": "Kazumi Saito"
                },
                "author": "Kazumi Saito",
                "arxiv_comment": "28 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06392v2",
                "updated": "2024-11-18T02:10:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    18,
                    2,
                    10,
                    28,
                    0,
                    323,
                    0
                ],
                "published": "2024-11-10T08:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    8,
                    31,
                    18,
                    6,
                    315,
                    0
                ],
                "title": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSMGraph: A High-Performance Dynamic Graph Storage System with\n  Multi-Level CSR"
                },
                "summary": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing volume of graph data may exhaust the main memory. It is crucial\nto design a disk-based graph storage system to ingest updates and analyze\ngraphs efficiently. However, existing dynamic graph storage systems suffer from\nread or write amplification and face the challenge of optimizing both read and\nwrite performance simultaneously. To address this challenge, we propose\nLSMGraph, a novel dynamic graph storage system that combines the write-friendly\nLSM-tree and the read-friendly CSR. It leverages the multi-level structure of\nLSM-trees to optimize write performance while utilizing the compact CSR\nstructures embedded in the LSM-trees to boost read performance. LSMGraph uses a\nnew memory structure, MemGraph, to efficiently cache graph updates and uses a\nmulti-level index to speed up reads within the multi-level structure.\nFurthermore, LSMGraph incorporates a vertex-grained version control mechanism\nto mitigate the impact of LSM-tree compaction on read performance and ensure\nthe correctness of concurrent read and write operations. Our evaluation shows\nthat LSMGraph significantly outperforms state-of-the-art (graph) storage\nsystems on both graph update and graph analytical workloads."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Pengxi Liu"
                    },
                    {
                        "name": "Zhixin Zhang"
                    },
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Xiaojian Luo"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11091v1",
                "updated": "2024-11-17T14:47:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-17T14:47:15Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    14,
                    47,
                    15,
                    6,
                    322,
                    0
                ],
                "title": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Tandem -- a Modular Approach to Building High-Speed LSM Storage\n  Engines"
                },
                "summary": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present~\\emph{KV-Tandem}, a modular architecture for building LSM-based\nstorage engines on top of simple, non-ordered persistent key-value stores\n(KVSs). KV-Tandem enables advanced functionalities such as range queries and\nsnapshot reads, while maintaining the native KVS performance for random reads\nand writes. Its modular design offers better performance trade-offs compared to\nprevious KV-separation solutions, which struggle to decompose the monolithic\nLSM structure. Central to KV-Tandem is~\\emph{LSM bypass} -- a novel algorithm\nthat offers a fast path to basic operations while ensuring the correctness of\nadvanced APIs.\n  We implement KV-Tandem in \\emph{XDP-Rocks}, a RocksDB-compatible storage\nengine that leverages the XDP KVS and incorporates practical design\noptimizations for real-world deployment. Through extensive microbenchmark and\nsystem-level comparisons, we demonstrate that XDP-Rocks achieves 3x to 4x\nperformance improvements over RocksDB across various workloads. XDP-Rocks is\nalready deployed in production, delivering significant operator cost savings\nconsistent with these performance gains."
                },
                "authors": [
                    {
                        "name": "Edward Bortnikov"
                    },
                    {
                        "name": "Michael Azran"
                    },
                    {
                        "name": "Asa Bornstein"
                    },
                    {
                        "name": "Shmuel Dashevsky"
                    },
                    {
                        "name": "Dennis Huang"
                    },
                    {
                        "name": "Omer Kepten"
                    },
                    {
                        "name": "Michael Pan"
                    },
                    {
                        "name": "Gali Sheffi"
                    },
                    {
                        "name": "Moshe Twitto"
                    },
                    {
                        "name": "Tamar Weiss Orzech"
                    },
                    {
                        "name": "Idit Keidar"
                    },
                    {
                        "name": "Guy Gueta"
                    },
                    {
                        "name": "Roey Maor"
                    },
                    {
                        "name": "Niv Dayan"
                    }
                ],
                "author_detail": {
                    "name": "Niv Dayan"
                },
                "author": "Niv Dayan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07635v3",
                "updated": "2024-11-17T12:56:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    17,
                    12,
                    56,
                    16,
                    6,
                    322,
                    0
                ],
                "published": "2024-11-12T08:30:59Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    30,
                    59,
                    1,
                    317,
                    0
                ],
                "title": "Breaking the Low-Rank Dilemma of Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Low-Rank Dilemma of Linear Attention"
                },
                "summary": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Softmax attention mechanism in Transformer models is notoriously\ncomputationally expensive, particularly due to its quadratic complexity, posing\nsignificant challenges in vision applications. In contrast, linear attention\nprovides a far more efficient solution by reducing the complexity to linear\nlevels. However, compared to Softmax attention, linear attention often\nexperiences significant performance degradation. Our experiments indicate that\nthis performance drop is due to the low-rank nature of linear attention's\nfeature map, which hinders its ability to adequately model complex spatial\ninformation. In this paper, to break the low-rank dilemma of linear attention,\nwe conduct rank analysis from two perspectives: the KV buffer and the output\nfeatures. Consequently, we introduce Rank-Augmented Linear Attention (RALA),\nwhich rivals the performance of Softmax attention while maintaining linear\ncomplexity and high efficiency. Based on RALA, we construct the Rank-Augmented\nVision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT\nachieves excellent performance across various vision tasks. Specifically,\nwithout using any additional labels, data, or supervision during training,\nRAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters\nand 4.6G FLOPs. This result significantly surpasses previous linear attention\nmechanisms, fully illustrating the potential of RALA. Code will be available at\nhttps://github.com/qhfan/RALA."
                },
                "authors": [
                    {
                        "name": "Qihang Fan"
                    },
                    {
                        "name": "Huaibo Huang"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v1",
                "updated": "2024-11-16T20:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13112v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13112v3",
                "updated": "2024-11-16T20:39:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    39,
                    46,
                    5,
                    321,
                    0
                ],
                "published": "2024-03-19T19:27:23Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    19,
                    27,
                    23,
                    1,
                    79,
                    0
                ],
                "title": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Encoder-Decoder Transformer Decoding for Decomposable Tasks"
                },
                "summary": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based NLP models are powerful but have high computational costs\nthat limit deployment. Finetuned encoder-decoder models are popular in\nspecialized domains and can outperform larger more generalized decoder-only\nmodels, such as GPT-4. We introduce a new configuration for encoder-decoder\nmodels that improves efficiency on structured output and decomposable tasks\nwhere multiple outputs are required for a single shared input. Our method,\nprompt-in-decoder (PiD), encodes the input once and decodes the output in\nparallel, boosting both training and inference efficiency by avoiding duplicate\ninput encoding and increasing the operational intensity (ratio of numbers of\narithmetic operation to memory access) of decoding process by sharing the input\nkey-value cache. We achieve computation reduction that roughly scales with the\nnumber of subtasks, gaining up to 4.6x speed-up over state-of-the-art models\nfor dialogue state tracking, summarization, and question-answering tasks, with\ncomparable or better performance."
                },
                "authors": [
                    {
                        "name": "Bo-Ru Lu"
                    },
                    {
                        "name": "Nikita Haduong"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Noah A. Smith"
                    },
                    {
                        "name": "Mari Ostendorf"
                    }
                ],
                "author_detail": {
                    "name": "Mari Ostendorf"
                },
                "author": "Mari Ostendorf",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13112v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13112v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10803v1",
                "updated": "2024-11-16T13:45:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T13:45:33Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    13,
                    45,
                    33,
                    5,
                    321,
                    0
                ],
                "title": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Stage Vision Token Dropping: Towards Efficient Multimodal Large\n  Language Model"
                },
                "summary": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision tokens in multimodal large language models usually exhibit\nsignificant spatial and temporal redundancy and take up most of the input\ntokens, which harms their inference efficiency. To solve this problem, some\nrecent works were introduced to drop the unimportant tokens during inference\nwhere the importance of each token is decided only by the information in either\nthe vision encoding stage or the prefilling stage. In this paper, we propose\nMulti-stage Token Dropping (MustDrop) to measure the importance of each token\nfrom the whole lifecycle, including the vision encoding stage, prefilling\nstage, and decoding stage. Concretely, in the visual encoding stage, MustDrop\nmerges spatially adjacent tokens with high similarity, and establishes a key\ntoken set to retain the most vision-critical tokens, preventing them from being\ndiscarded in later stages. In the prefilling stage, MustDrop further compresses\nvision tokens by the guidance of text semantics, with a dual-attention\nfiltering strategy. In the decoding stage, an output-aware cache policy is\nproposed to further reduce the size of the KV cache. By leveraging tailored\nstrategies in the multi-stage process, MustDrop can more precisely recognize\nthe important and redundant tokens, thus achieving an optimal balance between\nperformance and efficiency. For instance, MustDrop reduces about 88.5\\% FLOPs\non LLaVA with a compression ratio of 92.2\\% while maintaining comparable\naccuracy. Our codes are available at\n\\url{https://github.com/liuting20/MustDrop}."
                },
                "authors": [
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Liangtao Shi"
                    },
                    {
                        "name": "Richang Hong"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Quanjun Yin"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "8 pages, 4figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01733v2",
                "updated": "2024-11-16T07:43:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    7,
                    43,
                    28,
                    5,
                    321,
                    0
                ],
                "published": "2024-06-03T18:49:57Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    18,
                    49,
                    57,
                    0,
                    155,
                    0
                ],
                "title": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching"
                },
                "summary": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have recently demonstrated unprecedented generative\ncapabilities for various tasks. The encouraging results, however, come with the\ncost of slow inference, since each denoising step requires inference on a\ntransformer model with a large scale of parameters. In this study, we make an\ninteresting and somehow surprising observation: the computation of a large\nproportion of layers in the diffusion transformer, through introducing a\ncaching mechanism, can be readily removed even without updating the model\nparameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68%\nof the computation in the cache steps (46.84% for all steps), with less than\n0.01 drop in FID. To achieve this, we introduce a novel scheme, named\nLearning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for\ndiffusion transformers. Specifically, by leveraging the identical structure of\nlayers in transformers and the sequential nature of diffusion, we explore\nredundant computations between timesteps by treating each layer as the\nfundamental unit for caching. To address the challenge of the exponential\nsearch space in deep models for identifying layers to cache and remove, we\npropose a novel differentiable optimization objective. An input-invariant yet\ntimestep-variant router is then optimized, which can finally produce a static\ncomputation graph. Experimental results show that L2C largely outperforms\nsamplers such as DDIM and DPM-Solver, alongside prior cache-based methods at\nthe same inference speed. Code is available at\nhttps://github.com/horseee/learning-to-cache"
                },
                "authors": [
                    {
                        "name": "Xinyin Ma"
                    },
                    {
                        "name": "Gongfan Fang"
                    },
                    {
                        "name": "Michael Bi Mi"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v1",
                "updated": "2024-11-16T01:39:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v4",
                "updated": "2024-11-15T22:37:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    37,
                    48,
                    4,
                    320,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v2",
                "updated": "2024-11-15T22:30:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    22,
                    30,
                    38,
                    4,
                    320,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10510v1",
                "updated": "2024-11-15T16:24:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T16:24:02Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    16,
                    24,
                    2,
                    4,
                    320,
                    0
                ],
                "title": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmoothCache: A Universal Inference Acceleration Technique for Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as powerful generative models for\nvarious tasks, including image, video, and speech synthesis. However, their\ninference process remains computationally expensive due to the repeated\nevaluation of resource-intensive attention and feed-forward modules. To address\nthis, we introduce SmoothCache, a model-agnostic inference acceleration\ntechnique for DiT architectures. SmoothCache leverages the observed high\nsimilarity between layer outputs across adjacent diffusion timesteps. By\nanalyzing layer-wise representation errors from a small calibration set,\nSmoothCache adaptively caches and reuses key features during inference. Our\nexperiments demonstrate that SmoothCache achieves 8% to 71% speed up while\nmaintaining or even improving generation quality across diverse modalities. We\nshowcase its effectiveness on DiT-XL for image generation, Open-Sora for\ntext-to-video, and Stable Audio Open for text-to-audio, highlighting its\npotential to enable real-time applications and broaden the accessibility of\npowerful DiT models."
                },
                "authors": [
                    {
                        "name": "Joseph Liu"
                    },
                    {
                        "name": "Joshua Geddes"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Haomiao Jiang"
                    },
                    {
                        "name": "Mahesh Kumar Nandwana"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Kumar Nandwana"
                },
                "author": "Mahesh Kumar Nandwana",
                "arxiv_comment": "Code can be found at https://github.com/Roblox/SmoothCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v2",
                "updated": "2024-11-15T07:25:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    7,
                    25,
                    54,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09859v1",
                "updated": "2024-11-15T00:37:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "published": "2024-11-15T00:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    15,
                    0,
                    37,
                    31,
                    4,
                    320,
                    0
                ],
                "title": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures"
                },
                "summary": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The factorization of skew-symmetric matrices is a critically understudied\narea of dense linear algebra (DLA), particularly in comparison to that of\nsymmetric matrices. While some algorithms can be adapted from the symmetric\ncase, the cost of algorithms can be reduced by exploiting skew-symmetry. A\nmotivating example is the factorization $X=LTL^T$ of a skew-symmetric matrix\n$X$, which is used in practical applications as a means of determining the\ndeterminant of $X$ as the square of the (cheaply-computed) Pfaffian of the\nskew-symmetric tridiagonal matrix $T$, for example in fields such as quantum\nelectronic structure and machine learning. Such applications also often require\npivoting in order to improve numerical stability. In this work we explore a\ncombination of known literature algorithms and new algorithms recently derived\nusing formal methods. High-performance parallel CPU implementations are\ncreated, leveraging the concept of fusion at multiple levels in order to reduce\nmemory traffic overhead, as well as the BLIS framework which provides\nhigh-performance GEMM kernels, hierarchical parallelism, and cache blocking. We\nfind that operation fusion and improved use of available bandwidth via\nparallelization of bandwidth-bound (level-2 BLAS) operations are essential for\nobtaining high performance, while a concise C++ implementation provides a clear\nand close connection to the formal derivation process without sacrificing\nperformance."
                },
                "authors": [
                    {
                        "name": "Ishna Satyarth"
                    },
                    {
                        "name": "Chao Yin"
                    },
                    {
                        "name": "RuQing G. Xu"
                    },
                    {
                        "name": "Devin A. Matthews"
                    }
                ],
                "author_detail": {
                    "name": "Devin A. Matthews"
                },
                "author": "Devin A. Matthews",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09812v1",
                "updated": "2024-11-14T21:01:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T21:01:29Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    21,
                    1,
                    29,
                    3,
                    319,
                    0
                ],
                "title": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Caching Optimization with PPO and Transfer Learning for Dynamic\n  Environments"
                },
                "summary": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of edge caching in dynamic environments,\nwhere rising traffic loads strain backhaul links and core networks. We propose\na Proximal Policy Optimization (PPO)-based caching strategy that fully\nincorporates key file attributes such as size, lifetime, importance, and\npopularity, while also considering random file request arrivals, reflecting\nmore realistic edge caching scenarios. In dynamic environments, changes such as\nshifts in content popularity and variations in request rates frequently occur,\nmaking previously learned policies less effective as they were optimized for\nearlier conditions. Without adaptation, caching efficiency and response times\ncan degrade. While learning a new policy from scratch in a new environment is\nan option, it is highly inefficient and computationally expensive. Thus,\nadapting an existing policy to these changes is critical. To address this, we\ndevelop a mechanism that detects changes in content popularity and request\nrates, ensuring timely adjustments to the caching strategy. We also propose a\ntransfer learning-based PPO algorithm that accelerates convergence in new\nenvironments by leveraging prior knowledge. Simulation results demonstrate the\nsignificant effectiveness of our approach, outperforming a recent Deep\nReinforcement Learning (DRL)-based method."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Wang"
                },
                "author": "Ping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v1",
                "updated": "2024-11-14T18:54:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v2",
                "updated": "2024-11-14T17:46:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    17,
                    46,
                    4,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the $KV$ cache by nearly 50\\%. Comprehensive\nempirical evidence demonstrates that ResFormer mitigates attention\nconcentration problem in deeper layers and enhances representation across most\nlayers, outperforming the vanilla Transformer, DenseFormer, and NeuTRENO in\ntraining error as well as downstream tasks. Further visualization results\nsuggest that Resformer alleviates attention sinks through avoiding value-state\ndrains. SVFormer trains significantly faster than the vanilla Transformer and\nperforms better than other methods like GQA and CLA, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09546v1",
                "updated": "2024-11-14T16:01:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T16:01:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    16,
                    1,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Exploration of Application-Specific Resonant SRAM\n  Compute-in-Memory (rCiM)"
                },
                "summary": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While general-purpose computing follows Von Neumann's architecture, the data\nmovement between memory and processor elements dictates the processor's\nperformance. The evolving compute-in-memory (CiM) paradigm tackles this issue\nby facilitating simultaneous processing and storage within static random-access\nmemory (SRAM) elements. Numerous design decisions taken at different levels of\nhierarchy affect the figure of merits (FoMs) of SRAM, such as power,\nperformance, area, and yield. The absence of a rapid assessment mechanism for\nthe impact of changes at different hierarchy levels on global FoMs poses a\nchallenge to accurately evaluating innovative SRAM designs. This paper presents\nan automation tool designed to optimize the energy and latency of SRAM designs\nincorporating diverse implementation strategies for executing logic operations\nwithin the SRAM. The tool structure allows easy comparison across different\narray topologies and various design strategies to result in energy-efficient\nimplementations. Our study involves a comprehensive comparison of over 6900+\ndistinct design implementation strategies for EPFL combinational benchmark\ncircuits on the energy-recycling resonant compute-in-memory (rCiM) architecture\ndesigned using TSMC 28 nm technology. When provided with a combinational\ncircuit, the tool aims to generate an energy-efficient implementation strategy\ntailored to the specified input memory and latency constraints. The tool\nreduces 80.9% of energy consumption on average across all benchmarks while\nusing the six-topology implementation compared to baseline implementation of\nsingle-macro topology by considering the parallel processing capability of rCiM\ncache size ranging from 4KB to 192KB."
                },
                "authors": [
                    {
                        "name": "Dhandeep Challagundla"
                    },
                    {
                        "name": "Ignatius Bezzam"
                    },
                    {
                        "name": "Riadul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Riadul Islam"
                },
                "author": "Riadul Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09473v1",
                "updated": "2024-11-14T14:28:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T14:28:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    14,
                    28,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Scalability and Performance in Influence Maximization with\n  Optimized Parallel Processing"
                },
                "summary": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Influence Maximization (IM) is vital in viral marketing and biological\nnetwork analysis for identifying key influencers. Given its NP-hard nature,\napproximate solutions are employed. This paper addresses scalability challenges\nin scale-out shared memory system by focusing on the state-of-the-art Influence\nMaximization via Martingales (IMM) benchmark. To enhance the work efficiency of\nthe current IMM implementation, we propose EFFICIENTIMM with key strategies,\nincluding new parallelization scheme, NUMA-aware memory usage, dynamic load\nbalancing and fine-grained adaptive data structures. Benchmarking on a 128-core\nCPU system with 8 NUMA nodes, EFFICIENTIMM demonstrated significant performance\nimprovements, achieving an average 5.9x speedup over Ripples across 8 diverse\nSNAP datasets, when compared to the best execution times of the original\nRipples framework. Additionally, on the Youtube graph, EFFICIENTIMM\ndemonstrates a better memory access pattern with 357.4x reduction in L1+L2\ncache misses as compared to Ripples."
                },
                "authors": [
                    {
                        "name": "Hanjiang Wu"
                    },
                    {
                        "name": "Huan Xu"
                    },
                    {
                        "name": "Joongun Park"
                    },
                    {
                        "name": "Jesmin Jahan Tithi"
                    },
                    {
                        "name": "Fabio Checconi"
                    },
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    },
                    {
                        "name": "Tushar Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Tushar Krishna"
                },
                "author": "Tushar Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v1",
                "updated": "2024-11-14T13:22:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09317v1",
                "updated": "2024-11-14T09:50:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T09:50:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    9,
                    50,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "Pie: Pooling CPU Memory for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: Pooling CPU Memory for LLM Inference"
                },
                "summary": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of LLMs has revolutionized natural language processing and\nAI analysis, but their increasing size and memory demands present significant\nchallenges. A common solution is to spill over to CPU memory; however,\ntraditional GPU-CPU memory swapping often results in higher latency and lower\nthroughput.\n  This paper introduces Pie, an LLM inference framework that addresses these\nchallenges with performance-transparent swapping and adaptive expansion. By\nleveraging predictable memory access patterns and the high bandwidth of modern\nhardware like the NVIDIA GH200 Grace Hopper Superchip, Pie enables concurrent\ndata swapping without affecting foreground computation, expanding effective\nmemory without added latency. Adaptive expansion dynamically adjusts CPU memory\nallocation based on real-time information, optimizing memory usage and\nperformance under varying conditions.\n  Pie maintains low computation latency, high throughput, and high elasticity.\nOur experimental evaluation demonstrates that Pie achieves optimal swapping\npolicy during cache warmup and effectively balances increased memory capacity\nwith negligible impact on computation. With its extended capacity, Pie\noutperforms vLLM by up to 1.9X in throughput and 2X in latency. Additionally,\nPie can reduce GPU memory usage by up to 1.67X while maintaining the same\nperformance. Compared to FlexGen, an offline profiling-based swapping solution,\nPie achieves magnitudes lower latency and 9.4X higher throughput."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v1",
                "updated": "2024-11-14T08:25:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Pkd-tree: Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pkd-tree: Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v3",
                "updated": "2024-11-14T01:56:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    14,
                    1,
                    56,
                    11,
                    3,
                    319,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v4",
                "updated": "2024-11-13T16:33:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    16,
                    33,
                    33,
                    2,
                    318,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The personalized recommendation system's continuous size growth poses new\nchallenges for model inference. Although weight-sharing algorithms have been\nproposed to reduce embedding table capacity, they increase memory access.\nRecent advancements in processing-in-memory (PIM) successfully enhance the\nrecommendation system's throughput by exploiting memory parallelism, but our\nanalysis shows that those algorithms introduce CPU-PIM communication overhead\ninto prior PIM systems, compromising the PIM throughput. We propose\nProactivePIM, a specialized memory architecture integrated with PIM technology\ntailored to accelerate the weight-sharing algorithms. ProacitvePIM integrates\nan SRAM cache within the PIM with an efficient prefetching scheme to leverage a\nunique locality of the algorithm and eliminate CPU-PIM communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The personalized recommendation system's continuous size growth poses new\nchallenges for model inference. Although weight-sharing algorithms have been\nproposed to reduce embedding table capacity, they increase memory access.\nRecent advancements in processing-in-memory (PIM) successfully enhance the\nrecommendation system's throughput by exploiting memory parallelism, but our\nanalysis shows that those algorithms introduce CPU-PIM communication overhead\ninto prior PIM systems, compromising the PIM throughput. We propose\nProactivePIM, a specialized memory architecture integrated with PIM technology\ntailored to accelerate the weight-sharing algorithms. ProacitvePIM integrates\nan SRAM cache within the PIM with an efficient prefetching scheme to leverage a\nunique locality of the algorithm and eliminate CPU-PIM communication."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "7 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08672v1",
                "updated": "2024-11-13T15:07:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T15:07:15Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    15,
                    7,
                    15,
                    2,
                    318,
                    0
                ],
                "title": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Model Caching and Resource Allocation in Generative AI-Enabled\n  Wireless Edge Networks"
                },
                "summary": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of artificial intelligence (AI), generative AI\n(GenAI) has emerged as a transformative tool, enabling customized and\npersonalized AI-generated content (AIGC) services. However, GenAI models with\nbillions of parameters require substantial memory capacity and computational\npower for deployment and execution, presenting significant challenges to\nresource-limited edge networks. In this paper, we address the joint model\ncaching and resource allocation problem in GenAI-enabled wireless edge\nnetworks. Our objective is to balance the trade-off between delivering\nhigh-quality AIGC and minimizing the delay in AIGC service provisioning. To\ntackle this problem, we employ a deep deterministic policy gradient\n(DDPG)-based reinforcement learning approach, capable of efficiently\ndetermining optimal model caching and resource allocation decisions for AIGC\nservices in response to user mobility and time-varying channel conditions.\nNumerical results demonstrate that DDPG achieves a higher model hit ratio and\nprovides superior-quality, lower-latency AIGC services compared to other\nbenchmark solutions."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Zhibin Gao"
                    },
                    {
                        "name": "Dusit Niyato"
                    }
                ],
                "author_detail": {
                    "name": "Dusit Niyato"
                },
                "author": "Dusit Niyato",
                "arxiv_comment": "conference paper with 6 pages and 5 figures. arXiv admin note: text\n  overlap with arXiv:2411.01458",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08312v1",
                "updated": "2024-11-13T03:28:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "published": "2024-11-13T03:28:44Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    3,
                    28,
                    44,
                    2,
                    318,
                    0
                ],
                "title": "A Novel Extensible Simulation Framework for CXL-Enabled Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Extensible Simulation Framework for CXL-Enabled Systems"
                },
                "summary": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) serves as a rising industry standard, delivering\nhigh-speed cache-coherent links to a variety of devices, including host CPUs,\ncomputational accelerators, and memory devices. It is designed to promote\nsystem scalability, enable peer-to-peer exchanges, and accelerate data\ntransmissions. To achieve these objectives, the most recent CXL protocol has\nbrought forth several innovative features, such as port-focused routing,\ndevice-handled coherence, and PCIe 6.0 compatibility. However, due to the\nlimited availability of hardware prototypes and simulators compatible with CXL,\nearlier CXL research has largely depended on emulating CXL devices using remote\nNUMA nodes. Unfortunately, these NUMA-based emulators have difficulties in\naccurately representing the new features due to fundamental differences in\nhardware and protocols. Moreover, the absence of support for non-tree topology\nand PCIe links makes it complex to merely adapt existing simulators for CXL\nsimulation. To overcome these problems, we introduce ESF, a simulation\nframework specifically designed for CXL systems. ESF has been developed to\naccurately reflect the unique features of the latest CXL protocol from the\nground up. It uses a specialized interconnect layer to facilitate connections\nwithin a wide range of system topologies and also includes key components to\ncarry out specific functions required by these features. By utilizing ESF, we\nthoroughly investigate various aspects of CXL systems, including system\ntopology, device-handled coherence, and the effects of PCIe characteristics,\nleading to important findings that can guide the creation of high-performance\nCXL systems. The ESF source codes are fully open-source and can be accessed at\nhttps://anonymous.4open.science/r/ESF-1CE3."
                },
                "authors": [
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Bo Mao"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Mingzhe Zhang"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Guangyu Sun"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "arxiv_affiliation": "Peking University",
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08203v1",
                "updated": "2024-11-12T21:50:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T21:50:03Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    21,
                    50,
                    3,
                    1,
                    317,
                    0
                ],
                "title": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaaS and Furious: abstractions and differential caching for efficient\n  data pre-processing"
                },
                "summary": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data pre-processing pipelines are the bread and butter of any successful AI\nproject. We introduce a novel programming model for pipelines in a data\nlakehouse, allowing users to interact declaratively with assets in object\nstorage. Motivated by real-world industry usage patterns, we exploit these new\nabstractions with a columnar and differential cache to maximize iteration speed\nfor data scientists, who spent most of their time in pre-processing - adding or\nremoving features, restricting or relaxing time windows, wrangling current or\nolder datasets. We show how the new cache works transparently across\nprogramming languages, schemas and time windows, and provide preliminary\nevidence on its efficiency on standard data workloads."
                },
                "authors": [
                    {
                        "name": "Jacopo Tagliabue"
                    },
                    {
                        "name": "Ryan Curtin"
                    },
                    {
                        "name": "Ciro Greco"
                    }
                ],
                "author_detail": {
                    "name": "Ciro Greco"
                },
                "author": "Ciro Greco",
                "arxiv_comment": "Pre-print of the paper accepted at DEMAI@IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06219v3",
                "updated": "2024-11-12T08:18:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    18,
                    45,
                    1,
                    317,
                    0
                ],
                "published": "2024-05-10T03:06:24Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    3,
                    6,
                    24,
                    4,
                    131,
                    0
                ],
                "title": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKVQ: Sliding-window Key and Value Cache Quantization for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can now handle longer sequences of tokens,\nenabling complex tasks like book understanding and generating lengthy novels.\nHowever, the key-value (KV) cache required for LLMs consumes substantial memory\nas context length increasing, becoming the bottleneck for deployment. In this\npaper, we present a strategy called SKVQ, which stands for sliding-window KV\ncache quantization, to address the issue of extremely low bitwidth KV cache\nquantization. To achieve this, SKVQ rearranges the channels of the KV cache in\norder to improve the similarity of channels in quantization groups, and applies\nclipped dynamic quantization at the group level. Additionally, SKVQ ensures\nthat the most recent window tokens in the KV cache are preserved with high\nprecision. This helps maintain the accuracy of a small but important portion of\nthe KV cache.SKVQ achieves high compression ratios while maintaining accuracy.\nOur evaluation on LLMs demonstrates that SKVQ surpasses previous quantization\napproaches, allowing for quantization of the KV cache to 2-bit keys and 1.5-bit\nvalues with minimal loss of accuracy. With SKVQ, it is possible to process\ncontext lengths of up to 1M on an 80GB memory GPU for a 7b model and up to 7\ntimes faster decoding."
                },
                "authors": [
                    {
                        "name": "Haojie Duanmu"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Jiangfei Duan"
                    },
                    {
                        "name": "Xingcheng Zhang"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v1",
                "updated": "2024-11-12T08:17:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06681v1",
                "updated": "2024-11-11T02:48:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:48:00Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    48,
                    0,
                    0,
                    316,
                    0
                ],
                "title": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WDMoE: Wireless Distributed Mixture of Experts for Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved significant success in various\nnatural language processing tasks, but the role of wireless networks in\nsupporting LLMs has not been thoroughly explored. In this paper, we propose a\nwireless distributed Mixture of Experts (WDMoE) architecture to enable\ncollaborative deployment of LLMs across edge servers at the base station (BS)\nand mobile devices in wireless networks. Specifically, we decompose the MoE\nlayer in LLMs by placing the gating network and the preceding neural network\nlayer at BS, while distributing the expert networks among the devices. This\ndeployment leverages the parallel inference capabilities of expert networks on\nmobile devices, effectively utilizing the limited computing and caching\nresources of these devices. Accordingly, we develop a performance metric for\nWDMoE-based LLMs, which accounts for both model capability and latency. To\nminimize the latency while maintaining accuracy, we jointly optimize expert\nselection and bandwidth allocation based on the performance metric. Moreover,\nwe build a hardware testbed using NVIDIA Jetson kits to validate the\neffectiveness of WDMoE. Both theoretical simulations and practical hardware\nexperiments demonstrate that the proposed method can significantly reduce the\nlatency without compromising LLM performance."
                },
                "authors": [
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Liang Qian"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Wenjun Zhang"
                    },
                    {
                        "name": "Ping Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Zhang"
                },
                "author": "Ping Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06680v1",
                "updated": "2024-11-11T02:47:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T02:47:05Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    2,
                    47,
                    5,
                    0,
                    316,
                    0
                ],
                "title": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchor Attention, Small Cache: Code Generation with Large Language\n  Models"
                },
                "summary": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has revolutionized automated\ncode generation. However, their high demand of computation resources has\nhindered a broader deployment and raised environmental concerns. A common\nstrategy for diminishing computational demands is to cache Key-Value (KV)\nstates from the attention mechanism which is adopted predominately by\nmainstream LLMs. It can mitigate the need of repeated attention computations,\nbut brings significant memory overhead. Current practices in NLP often use\nsparse attention which may, unfortunately, lead to substantial inaccuracies, or\nhallucinations, in code generation tasks. In this paper, we analyze the\nattention weights distribution within code generation models via an empirical\nstudy, uncovering a sparsity pattern, i.e., the aggregation of information at\nspecific anchor points. Based on this observation, we propose a novel approach,\nAnchorCoder, which features token-wise anchor attention designed to extract and\ncompress the contextual information, and layer-wise anchor attention enabling\ncross-layer communication to mitigate the issue of excessive superposition\ncaused by the compression. The extensive experiments across multiple benchmark\ndatasets confirm the effectiveness of AnchorCoder, which can consistently\nachieve a significant (at least 70%) reduction in KV cache requirements, while\npreserving the majority of model's performance."
                },
                "authors": [
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Harald C. Gall"
                    },
                    {
                        "name": "Taolue Chen"
                    }
                ],
                "author_detail": {
                    "name": "Taolue Chen"
                },
                "author": "Taolue Chen",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N19",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06659v1",
                "updated": "2024-11-11T01:53:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "published": "2024-11-11T01:53:14Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    1,
                    53,
                    14,
                    0,
                    316,
                    0
                ],
                "title": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Memory Module for Graph Few-Shot Class-Incremental Learning"
                },
                "summary": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incremental graph learning has gained significant attention for its ability\nto address the catastrophic forgetting problem in graph representation\nlearning. However, traditional methods often rely on a large number of labels\nfor node classification, which is impractical in real-world applications. This\nmakes few-shot incremental learning on graphs a pressing need. Current methods\ntypically require extensive training samples from meta-learning to build memory\nand perform intensive fine-tuning of GNN parameters, leading to high memory\nconsumption and potential loss of previously learned knowledge. To tackle these\nchallenges, we introduce Mecoin, an efficient method for building and\nmaintaining memory. Mecoin employs Structured Memory Units to cache prototypes\nof learned categories, as well as Memory Construction Modules to update these\nprototypes for new categories through interactions between the nodes and the\ncached prototypes. Additionally, we have designed a Memory Representation\nAdaptation Module to store probabilities associated with each class prototype,\nreducing the need for parameter fine-tuning and lowering the forgetting rate.\nWhen a sample matches its corresponding class prototype, the relevant\nprobabilities are retrieved from the MRaM. Knowledge is then distilled back\ninto the GNN through a Graph Knowledge Distillation Module, preserving the\nmodel's memory. We analyze the effectiveness of Mecoin in terms of\ngeneralization error and explore the impact of different distillation\nstrategies on model performance through experiments and VC-dimension analysis.\nCompared to other related works, Mecoin shows superior performance in accuracy\nand forgetting rate. Our code is publicly available on the\nhttps://github.com/Arvin0313/Mecoin-GFSCIL.git ."
                },
                "authors": [
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Junqi Gao"
                    },
                    {
                        "name": "Biqing Qi"
                    }
                ],
                "author_detail": {
                    "name": "Biqing Qi"
                },
                "author": "Biqing Qi",
                "arxiv_comment": "16 pages, 6 figures, 38th Conference on Neural Information Processing\n  Systems, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01783v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01783v2",
                "updated": "2024-11-10T23:04:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    23,
                    4,
                    12,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-04T04:15:36Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    4,
                    15,
                    36,
                    0,
                    309,
                    0
                ],
                "title": "Context Parallelism for Scalable Million-Token Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Parallelism for Scalable Million-Token Inference"
                },
                "summary": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth."
                },
                "authors": [
                    {
                        "name": "Amy Yang"
                    },
                    {
                        "name": "Jingyi Yang"
                    },
                    {
                        "name": "Aya Ibrahim"
                    },
                    {
                        "name": "Xinfeng Xie"
                    },
                    {
                        "name": "Bangsheng Tang"
                    },
                    {
                        "name": "Grigory Sizov"
                    },
                    {
                        "name": "Jeremy Reizenstein"
                    },
                    {
                        "name": "Jongsoo Park"
                    },
                    {
                        "name": "Jianyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Huang"
                },
                "author": "Jianyu Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01783v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01783v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v4",
                "updated": "2024-11-10T15:58:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    15,
                    58,
                    7,
                    6,
                    315,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04873v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04873v2",
                "updated": "2024-11-10T10:08:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    10,
                    8,
                    37,
                    6,
                    315,
                    0
                ],
                "published": "2024-06-07T12:12:25Z",
                "published_parsed": [
                    2024,
                    6,
                    7,
                    12,
                    12,
                    25,
                    4,
                    159,
                    0
                ],
                "title": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-VE: Training-Free Consistent Video Editing Using Adaptive Motion\n  Prior"
                },
                "summary": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-to-video synthesis poses significant challenges in maintaining\ncharacter consistency, smooth temporal transitions, and preserving visual\nquality during fast motion. While recent fully cross-frame self-attention\nmechanisms have improved character consistency across multiple frames, they\ncome with high computational costs and often include redundant operations,\nespecially for videos with higher frame rates. To address these inefficiencies,\nwe propose an adaptive motion-guided cross-frame attention mechanism that\nselectively reduces redundant computations. This enables a greater number of\ncross-frame attentions over more frames within the same computational budget,\nthereby enhancing both video quality and temporal coherence. Our method\nleverages optical flow to focus on moving regions while sparsely attending to\nstationary areas, allowing for the joint editing of more frames without\nincreasing computational demands. Traditional frame interpolation techniques\nstruggle with motion blur and flickering in intermediate frames, which\ncompromises visual fidelity. To mitigate this, we introduce KV-caching for\njointly edited frames, reusing keys and values across intermediate frames to\npreserve visual quality and maintain temporal consistency throughout the video.\nWith our adaptive cross-frame self-attention approach, we achieve a threefold\nincrease in the number of keyframes processed compared to existing methods, all\nwithin the same computational budget as fully cross-frame attention baselines.\nThis results in significant improvements in prediction accuracy and temporal\nconsistency, outperforming state-of-the-art approaches. Code will be made\npublicly available at https://github.com/tanvir-utexas/AdaVE/tree/main"
                },
                "authors": [
                    {
                        "name": "Tanvir Mahmud"
                    },
                    {
                        "name": "Mustafa Munir"
                    },
                    {
                        "name": "Radu Marculescu"
                    },
                    {
                        "name": "Diana Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Diana Marculescu"
                },
                "author": "Diana Marculescu",
                "arxiv_comment": "Accepted in WACV 2025. Project page:\n  https://tanvir-utexas.github.io/AdaVE_Demo/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04873v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04873v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06364v1",
                "updated": "2024-11-10T05:12:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "published": "2024-11-10T05:12:51Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    5,
                    12,
                    51,
                    6,
                    315,
                    0
                ],
                "title": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in\n  LLM Serving"
                },
                "summary": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to grow, reducing costs and\nalleviating GPU demands has become increasingly critical. However, existing\nschedulers primarily target either GPU compute or Key-Value Cache (KVC)\nutilization, failing to fully optimize both GPU compute and KVC usage during\neach iteration or guarantee timely KVC allocations when needed. To address\nthese challenges, we conducted a trace-based experimental analysis and made\ninsightful observations, leading to the design of a system called EcoServe.\nEcoServe maximizes multi-resource utilization while ensuring service-level\nobjective (SLO) guarantees in LLM serving. To enable adding prompts to a batch\nto maximize GPU utilization in each iteration, EcoServe maintains separate\nwaiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It\nbatches GTs with the same predicted response lengths (RL) to save scheduling\ntime and allocates KVC space for the predicted RL to avoid KVC allocation\nfailures. It further has a novel KVC pipelining method, allowing sharing\nallocated but unused KVC space to enhance KVC utilization. In addition, it\nprioritizes queued requests that occupy more KVC to release KVC earlier and\nsatisfy request service-level-objective (SLO). Experimental results demonstrate\nthat EcoServe increases throughput by up to 4$\\times$ with the same level of\nlatency, generates up to 91\\% lower job completion time and up to 91\\% higher\nSLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs\nused in DistServe by up to 78\\% while maintaining the same level of goodput."
                },
                "authors": [
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Tanmoy Sen"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Sen"
                },
                "author": "Tanmoy Sen",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v2",
                "updated": "2024-11-08T16:29:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    16,
                    29,
                    33,
                    4,
                    313,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance. Code is available at\nhttps://github.com/UtkarshSaxena1/EigenAttn."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05555v1",
                "updated": "2024-11-08T13:24:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T13:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    13,
                    24,
                    1,
                    4,
                    313,
                    0
                ],
                "title": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AcceLLM: Accelerating LLM Inference using Redundancy for Load Balancing\n  and Data Locality"
                },
                "summary": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference on large-scale systems is expected to\ndominate future cloud infrastructures. Efficient LLM inference in cloud\nenvironments with numerous AI accelerators is challenging, necessitating\nextensive optimizations for optimal performance. Current systems batch prefill\nand decoding to boost throughput but encounter latency issues, while others\ndisaggregate these phases, leading to resource underutilization. We propose\nAcceLLM, a novel method addressing latency and load balancing, inspired by the\ncache data management. It strategically utilizes redundant data to enhance\ninference via load balancing and optimal hardware use. Simulated evaluations on\nNvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art\nsystems up to 30% in latency and efficiency, handling diverse workloads\neffectively."
                },
                "authors": [
                    {
                        "name": "Ilias Bournias"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    },
                    {
                        "name": "Georgios Zacharopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Georgios Zacharopoulos"
                },
                "author": "Georgios Zacharopoulos",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v1",
                "updated": "2024-11-08T02:21:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT (Radford et al., 2019), have\nsignificantly advanced artificial intelligence by enabling sophisticated\nnatural language understanding and generation. However, the high computational\nand financial costs associated with frequent API calls to these models present\na substantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique reduces operational costs and improves response times, enhancing\nthe efficiency of LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02542v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02542v2",
                "updated": "2024-11-07T18:58:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    58,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-06-04T17:58:03Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    58,
                    3,
                    1,
                    156,
                    0
                ],
                "title": "Loki: Low-rank Keys for Efficient Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loki: Low-rank Keys for Efficient Sparse Attention"
                },
                "summary": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference on large language models (LLMs) can be expensive in terms of the\ncompute and memory costs involved, especially when long sequence lengths are\nused. In particular, the self-attention mechanism used in LLM inference\ncontributes significantly to these costs, which has sparked an interest in\napproximating the self-attention computation to reduce such costs. In this\nwork, we propose to approximate self-attention by focusing on the\ndimensionality of key vectors computed in the attention block. Our analysis\nreveals that key vectors lie in a significantly lower-dimensional space,\nconsistently across several datasets and models. Exploiting this observation,\nwe propose Loki, a novel sparse attention method that ranks and selects tokens\nin the KV-cache based on attention scores computed in low-dimensional space.\nOur evaluations show that Loki is able to speed up the attention computation\ndue to reduced data movement (load/store) and compute costs while maintaining\nthe efficacy of the models better than other popular approximation methods."
                },
                "authors": [
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Siddharth Singh"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Soheil Feizi"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    }
                ],
                "author_detail": {
                    "name": "Abhinav Bhatele"
                },
                "author": "Abhinav Bhatele",
                "arxiv_comment": "Proceedings of the Thirty-Eighth Annual Conference on Neural\n  Information Processing Systems (Main Conference Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02542v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02542v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04965v1",
                "updated": "2024-11-07T18:41:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T18:41:50Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    41,
                    50,
                    3,
                    312,
                    0
                ],
                "title": "BitNet a4.8: 4-bit Activations for 1-bit LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitNet a4.8: 4-bit Activations for 1-bit LLMs"
                },
                "summary": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research on the 1-bit Large Language Models (LLMs), such as BitNet\nb1.58, presents a promising direction for reducing the inference cost of LLMs\nwhile maintaining their performance. In this work, we introduce BitNet a4.8,\nenabling 4-bit activations for 1-bit LLMs. BitNet a4.8 employs a hybrid\nquantization and sparsification strategy to mitigate the quantization errors\nintroduced by the outlier channels. Specifically, we utilize 4-bit activations\nfor inputs to the attention and feed-forward network layers, while sparsifying\nintermediate states followed with 8-bit quantization. Extensive experiments\ndemonstrate that BitNet a4.8 achieves performance comparable to BitNet b1.58\nwith equivalent training costs, while being faster in inference with enabling\n4-bit (INT4/FP4) kernels. Additionally, BitNet a4.8 activates only 55% of\nparameters and supports 3-bit KV cache, further enhancing the efficiency of\nlarge-scale LLM deployment and inference."
                },
                "authors": [
                    {
                        "name": "Hongyu Wang"
                    },
                    {
                        "name": "Shuming Ma"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02397v2",
                "updated": "2024-11-07T17:06:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    17,
                    6,
                    32,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-04T18:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    18,
                    59,
                    44,
                    0,
                    309,
                    0
                ],
                "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Caching for Faster Video Generation with Diffusion Transformers"
                },
                "summary": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating temporally-consistent high-fidelity videos can be computationally\nexpensive, especially over longer temporal spans. More-recent Diffusion\nTransformers (DiTs) -- despite making significant headway in this context --\nhave only heightened such challenges as they rely on larger models and heavier\nattention mechanisms, resulting in slower inference speeds. In this paper, we\nintroduce a training-free method to accelerate video DiTs, termed Adaptive\nCaching (AdaCache), which is motivated by the fact that \"not all videos are\ncreated equal\": meaning, some videos require fewer denoising steps to attain a\nreasonable quality than others. Building on this, we not only cache\ncomputations through the diffusion process, but also devise a caching schedule\ntailored to each video generation, maximizing the quality-latency trade-off. We\nfurther introduce a Motion Regularization (MoReg) scheme to utilize video\ninformation within AdaCache, essentially controlling the compute allocation\nbased on motion content. Altogether, our plug-and-play contributions grant\nsignificant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video\ngeneration) without sacrificing the generation quality, across multiple video\nDiT baselines."
                },
                "authors": [
                    {
                        "name": "Kumara Kahatapitiya"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Sen He"
                    },
                    {
                        "name": "Ding Liu"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Chenyang Zhang"
                    },
                    {
                        "name": "Michael S. Ryoo"
                    },
                    {
                        "name": "Tian Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tian Xie"
                },
                "author": "Tian Xie",
                "arxiv_comment": "Project-page is available at https://adacache-dit.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v1",
                "updated": "2024-11-07T14:59:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16591v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16591v2",
                "updated": "2024-11-07T09:33:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    9,
                    33,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-05-26T14:50:40Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    14,
                    50,
                    40,
                    6,
                    147,
                    0
                ],
                "title": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CapS-Adapter: Caption-based MultiModal Adapter in Zero-Shot\n  Classification"
                },
                "summary": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language foundational models, such as CLIP, have\ndemonstrated significant strides in zero-shot classification. However, the\nextensive parameterization of models like CLIP necessitates a\nresource-intensive fine-tuning process. In response, TIP-Adapter and SuS-X have\nintroduced training-free methods aimed at bolstering the efficacy of downstream\ntasks. While these approaches incorporate support sets to maintain data\ndistribution consistency between knowledge cache and test sets, they often fall\nshort in terms of generalization on the test set, particularly when faced with\ntest data exhibiting substantial distributional variations. In this work, we\npresent CapS-Adapter, an innovative method that employs a caption-based support\nset, effectively harnessing both image and caption features to exceed existing\nstate-of-the-art techniques in training-free scenarios. CapS-Adapter adeptly\nconstructs support sets that closely mirror target distributions, utilizing\ninstance-level distribution features extracted from multimodal large models. By\nleveraging CLIP's single and cross-modal strengths, CapS-Adapter enhances\npredictive accuracy through the use of multimodal support sets. Our method\nachieves outstanding zero-shot classification results across 19 benchmark\ndatasets, improving accuracy by 2.19\\% over the previous leading method. Our\ncontributions are substantiated through extensive validation on multiple\nbenchmark datasets, demonstrating superior performance and robust\ngeneralization capabilities. Our code is made publicly available at\nhttps://github.com/WLuLi/CapS-Adapter."
                },
                "authors": [
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Guandu Liu"
                    },
                    {
                        "name": "Bin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Wang"
                },
                "author": "Bin Wang",
                "arxiv_doi": "10.1145/3664647.3681566",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3681566",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.16591v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16591v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Multimedia 2024 Poster",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v2",
                "updated": "2024-11-07T06:40:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    7,
                    6,
                    40,
                    40,
                    3,
                    312,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02265v3",
                "updated": "2024-11-06T09:15:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    9,
                    15,
                    27,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-04T16:56:26Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    16,
                    56,
                    26,
                    0,
                    309,
                    0
                ],
                "title": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated\n  Parameters by Tencent"
                },
                "summary": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Hunyuan-Large, which is currently the largest\nopen-source Transformer-based mixture of experts model, with a total of 389\nbillion parameters and 52 billion activation parameters, capable of handling up\nto 256K tokens. We conduct a thorough evaluation of Hunyuan-Large's superior\nperformance across various benchmarks including language understanding and\ngeneration, logical reasoning, mathematical problem-solving, coding,\nlong-context, and aggregated tasks, where it outperforms LLama3.1-70B and\nexhibits comparable performance when compared to the significantly larger\nLLama3.1-405B model. Key practice of Hunyuan-Large include large-scale\nsynthetic data that is orders larger than in previous literature, a mixed\nexpert routing strategy, a key-value cache compression technique, and an\nexpert-specific learning rate strategy. Additionally, we also investigate the\nscaling laws and learning rate schedule of mixture of experts models, providing\nvaluable insights and guidances for future model development and optimization.\nThe code and checkpoints of Hunyuan-Large are released to facilitate future\ninnovations and applications.\n  Codes: https://github.com/Tencent/Hunyuan-Large\n  Models: https://huggingface.co/tencent/Tencent-Hunyuan-Large"
                },
                "authors": [
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Yanfeng Chen"
                    },
                    {
                        "name": "Yiqing Huang"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "Jiaqi Zhu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Shuaipeng Li"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Jonny Han"
                    },
                    {
                        "name": "Xiaobo Shu"
                    },
                    {
                        "name": "Jiahao Bu"
                    },
                    {
                        "name": "Zhongzhi Chen"
                    },
                    {
                        "name": "Xuemeng Huang"
                    },
                    {
                        "name": "Fengzong Lian"
                    },
                    {
                        "name": "Saiyong Yang"
                    },
                    {
                        "name": "Jianfeng Yan"
                    },
                    {
                        "name": "Yuyuan Zeng"
                    },
                    {
                        "name": "Xiaoqin Ren"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Lulu Wu"
                    },
                    {
                        "name": "Yue Mao"
                    },
                    {
                        "name": "Jun Xia"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Suncong Zheng"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Dian Jiao"
                    },
                    {
                        "name": "Jinbao Xue"
                    },
                    {
                        "name": "Xipeng Zhang"
                    },
                    {
                        "name": "Decheng Wu"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Dengpeng Wu"
                    },
                    {
                        "name": "Guanghui Xu"
                    },
                    {
                        "name": "Shaohua Chen"
                    },
                    {
                        "name": "Shuang Chen"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yigeng Hong"
                    },
                    {
                        "name": "Junqiang Zheng"
                    },
                    {
                        "name": "Chengcheng Xu"
                    },
                    {
                        "name": "Zongwei Li"
                    },
                    {
                        "name": "Xiong Kuang"
                    },
                    {
                        "name": "Jianglu Hu"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Yuchi Deng"
                    },
                    {
                        "name": "Guiyang Li"
                    },
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Chenchen Zhang"
                    },
                    {
                        "name": "Shihui Hu"
                    },
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Zifan Wu"
                    },
                    {
                        "name": "Yao Ding"
                    },
                    {
                        "name": "Weichao Wang"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Roberts Wang"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Ze Zhao"
                    },
                    {
                        "name": "Xun Cao"
                    },
                    {
                        "name": "Hai Wang"
                    },
                    {
                        "name": "Fusheng Xiang"
                    },
                    {
                        "name": "Mengyuan Huang"
                    },
                    {
                        "name": "Zhiyuan Xiong"
                    },
                    {
                        "name": "Bin Hu"
                    },
                    {
                        "name": "Xuebin Hou"
                    },
                    {
                        "name": "Lei Jiang"
                    },
                    {
                        "name": "Jianqiang Ma"
                    },
                    {
                        "name": "Jiajia Wu"
                    },
                    {
                        "name": "Yaping Deng"
                    },
                    {
                        "name": "Yi Shen"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Liang Dong"
                    },
                    {
                        "name": "Weiwen Jia"
                    },
                    {
                        "name": "Hu Chen"
                    },
                    {
                        "name": "Feifei Liu"
                    },
                    {
                        "name": "Rui Yuan"
                    },
                    {
                        "name": "Huilin Xu"
                    },
                    {
                        "name": "Zhenxiang Yan"
                    },
                    {
                        "name": "Tengfei Cao"
                    },
                    {
                        "name": "Zhichao Hu"
                    },
                    {
                        "name": "Xinhua Feng"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Tinghao Yu"
                    },
                    {
                        "name": "Yangyu Tao"
                    },
                    {
                        "name": "Feng Zhang"
                    },
                    {
                        "name": "Jianchen Zhu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Xirui Li"
                    },
                    {
                        "name": "Chong Zha"
                    },
                    {
                        "name": "Wen Ouyang"
                    },
                    {
                        "name": "Yinben Xia"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Zekun He"
                    },
                    {
                        "name": "Rongpeng Chen"
                    },
                    {
                        "name": "Jiawei Song"
                    },
                    {
                        "name": "Ruibin Chen"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Chongqing Zhao"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Hao Gong"
                    },
                    {
                        "name": "Rong Gan"
                    },
                    {
                        "name": "Winston Hu"
                    },
                    {
                        "name": "Zhanhui Kang"
                    },
                    {
                        "name": "Yong Yang"
                    },
                    {
                        "name": "Yuhong Liu"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Jie Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Jiang"
                },
                "author": "Jie Jiang",
                "arxiv_comment": "17 pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03731v1",
                "updated": "2024-11-06T07:53:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-06T07:53:04Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    53,
                    4,
                    2,
                    311,
                    0
                ],
                "title": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Hyperparameter Tuning Costs in ML, Vision and Language Model\n  Training Pipelines via Memoization-Awareness"
                },
                "summary": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The training or fine-tuning of machine learning, vision, and language models\nis often implemented as a pipeline: a sequence of stages encompassing data\npreparation, model training and evaluation. In this paper, we exploit pipeline\nstructures to reduce the cost of hyperparameter tuning for model\ntraining/fine-tuning, which is particularly valuable for language models given\ntheir high costs in GPU-days. We propose a \"memoization-aware\" Bayesian\nOptimization (BO) algorithm, EEIPU, that works in tandem with a pipeline\ncaching system, allowing it to evaluate significantly more hyperparameter\ncandidates per GPU-day than other tuning algorithms. The result is\nbetter-quality hyperparameters in the same amount of search time, or\nequivalently, reduced search time to reach the same hyperparameter quality. In\nour benchmarks on machine learning (model ensembles), vision (convolutional\narchitecture) and language (T5 architecture) pipelines, we compare EEIPU\nagainst recent BO algorithms: EEIPU produces an average of $103\\%$ more\nhyperparameter candidates (within the same budget), and increases the\nvalidation metric by an average of $108\\%$ more than other algorithms (where\nthe increase is measured starting from the end of warm-up iterations)."
                },
                "authors": [
                    {
                        "name": "Abdelmajid Essofi"
                    },
                    {
                        "name": "Ridwan Salahuddeen"
                    },
                    {
                        "name": "Munachiso Nwadike"
                    },
                    {
                        "name": "Elnura Zhalieva"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Qirong Ho"
                    }
                ],
                "author_detail": {
                    "name": "Qirong Ho"
                },
                "author": "Qirong Ho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v2",
                "updated": "2024-11-06T07:12:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    7,
                    12,
                    55,
                    2,
                    311,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01433v2",
                "updated": "2024-11-06T01:49:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    6,
                    1,
                    49,
                    45,
                    2,
                    311,
                    0
                ],
                "published": "2024-11-03T04:25:46Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    4,
                    25,
                    46,
                    6,
                    308,
                    0
                ],
                "title": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HOBBIT: A Mixed Precision Expert Offloading System for Fast MoE\n  Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has demonstrated significant\nadvantages in the era of Large Language Models (LLMs), offering enhanced\ncapabilities with reduced inference costs. However, deploying MoE-based LLMs on\nmemoryconstrained edge devices remains challenging due to their substantial\nmemory requirements. While existing expertoffloading methods alleviate the\nmemory requirements, they often incur significant expert-loading costs or\ncompromise model accuracy. We present HOBBIT, a mixed precision expert\noffloading system to enable flexible and efficient MoE inference. Our key\ninsight is that dynamically replacing less critical cache-miss experts with low\nprecision versions can substantially reduce expert-loading latency while\npreserving model accuracy. HOBBIT introduces three innovative techniques that\nmap the natural hierarchy of MoE computation: (1) a token-level dynamic expert\nloading mechanism, (2) a layer-level adaptive expert prefetching technique, and\n(3) a sequence-level multidimensional expert caching policy. These innovations\nfully leverage the benefits of mixedprecision expert inference. By implementing\nHOBBIT on top of the renowned LLM inference framework Llama.cpp, we evaluate\nits performance across different edge devices with representative MoE models.\nThe results demonstrate that HOBBIT achieves up to a 9.93x speedup in decoding\ncompared to state-of-the-art MoE offloading systems."
                },
                "authors": [
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xiaofeng Hou"
                    },
                    {
                        "name": "Yifei Pu"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Pheng-Ann Heng"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05591v3",
                "updated": "2024-11-05T08:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    8,
                    34,
                    44,
                    1,
                    310,
                    0
                ],
                "published": "2023-08-10T13:57:37Z",
                "published_parsed": [
                    2023,
                    8,
                    10,
                    13,
                    57,
                    37,
                    3,
                    222,
                    0
                ],
                "title": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless Edge Content Broadcast via Integrated Terrestrial and\n  Non-terrestrial Networks"
                },
                "summary": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-terrestrial networks (NTN) have emerged as a transformative solution to\nbridge the digital divide and deliver essential services to remote and\nunderserved areas. In this context, low Earth orbit (LEO) satellite\nconstellations offer remarkable potential for efficient cache content broadcast\nin remote regions, thereby extending the reach of digital services. In this\npaper, we introduce a novel approach to optimize wireless edge content\nplacement using NTN. Despite wide coverage, the varying NTN transmission\ncapabilities must be carefully aligned with each content placement to maximize\nbroadcast efficiency. In this paper, we introduce a novel approach to optimize\nwireless edge content placement using NTN, positioning NTN as a complement to\nTN for achieving optimal content broadcasting. Specifically, we dynamically\nselect content for placement via NTN links. This selection is based on\npopularity and suitability for delivery through NTN, while considering the\norbital motion of LEO satellites. Our system-level case studies, based on a\npractical LEO constellation, demonstrate the significant improvement in\nplacement speed compared to existing methods, which neglect network mobility.\nWe also demonstrate that NTN links significantly outperform standalone wireless\nTN solutions, particularly in the early stages of content delivery. This\nadvantage is amplified when there is a higher correlation of content popularity\nacross geographical regions."
                },
                "authors": [
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Giovanni Geraci"
                    },
                    {
                        "name": "Lingxiang Li"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "This work is expanded on our paper presented at IEEE Globecom 2023:\n  F. Wang, G. Geraci and T. Q. S. Quek, \"Optimizing Cache Content Placement in\n  Integrated Terrestrial and Non-terrestrial Networks,\" GLOBECOM 2023 - 2023\n  IEEE Global Communications Conference, Kuala Lumpur, Malaysia, 2023, pp.\n  6609-6614",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v1",
                "updated": "2024-11-05T07:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), the ability to handle\nlonger contexts has become a key capability for Web applications such as\ncross-document understanding and LLM-powered search systems. However, this\nprogress faces two major challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues hinder the\napplication of LLMs in long-context scenarios. In this paper, we propose\nDynamic Token-Level KV Cache Selection (TokenSelect), a model-agnostic,\ntraining-free method for efficient and accurate long-context inference.\nTokenSelect builds upon the observation of non-contiguous attention sparsity,\nusing Query-Key dot products to measure per-head KV Cache criticality at\ntoken-level. By per-head soft voting mechanism, TokenSelect selectively\ninvolves a small number of critical KV cache tokens in the attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesigned the Selection Cache based on observations of consecutive Query\nsimilarity and implemented efficient dot product kernel, significantly reducing\nthe overhead of token selection. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v1",
                "updated": "2024-11-05T05:41:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: Enhancing Cross-LLM Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: Enhancing Cross-LLM Communication"
                },
                "summary": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent systems utilizing Large Language Models (LLMs), communication\nbetween agents traditionally relies on natural language. This communication\noften includes the full context of the query so far, which can introduce\nsignificant prefill-phase latency, especially with long contexts.\n  We introduce DroidSpeak, a novel framework to target this cross-LLM\ncommunication by leveraging the reuse of intermediate data, such as input\nembeddings (E-cache) and key-value caches (KV-cache). We efficiently bypass the\nneed to reprocess entire contexts for fine-tuned versions of the same\nfoundational model. This approach allows faster context integration while\nmaintaining the quality of task performance. Experimental evaluations\ndemonstrate DroidSpeak's ability to significantly accelerate inter-agent\ncommunication, achieving up to a 2.78x speedup in prefill latency with\nnegligible loss in accuracy. Our findings underscore the potential to create\nmore efficient and scalable multi-agent systems."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Esha Choukse"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Madan Musuvathi"
                    }
                ],
                "author_detail": {
                    "name": "Madan Musuvathi"
                },
                "author": "Madan Musuvathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02295v1",
                "updated": "2024-11-04T17:21:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T17:21:58Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    21,
                    58,
                    0,
                    309,
                    0
                ],
                "title": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt Pyroelectric Voltage Generation and Electrostatic Actuation\n  With Fluidic Heating"
                },
                "summary": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated micro power generators are crucial components for micro robotic\nplatforms to demonstrate untethered operation and to achieve autonomy. Current\nmicro robotic electrostatic actuators typically require hundreds to thousands\nof voltages to output sufficient work. Pyroelectricity is one such source of\nhigh voltages that can be scaled to small form factors. This paper demonstrates\na distributed pyroelectric high voltage generation mechanism to power kV\nactuators using alternating exposure of crystals to hot and cold water (300C to\n900C water temperature). Using this fluidic temperature control, a\npyroelectrically generated voltage of 2470 V was delivered to a 2 pF storage\ncapacitor yielding a 6.10 {\\mu}J stored energy. A maximum energy of 17.46\n{\\mu}J was delivered to a 47 pF capacitor at 861 V. The recirculating water can\nbe used to heat a distributed array of converters to generate electricity in\ndistant robotic actuator sections. The development of this distributed system\nwould enable untethered micro-robot to be operated with a flexible body and\nfree of battery recharging, which advances its applications in the real world."
                },
                "authors": [
                    {
                        "name": "Di Ni"
                    },
                    {
                        "name": "Ved Gund"
                    },
                    {
                        "name": "Landon Ivy"
                    },
                    {
                        "name": "Amit Lal"
                    }
                ],
                "author_detail": {
                    "name": "Amit Lal"
                },
                "author": "Amit Lal",
                "arxiv_doi": "10.31438/trf.hh2022.16",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.31438/trf.hh2022.16",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.02295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and published at Hilton Head Workshop 2022: A Solid-State\n  Sensors, Actuators and Microsystems Workshop",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v2",
                "updated": "2024-11-04T12:14:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    12,
                    14,
                    7,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "arxiv_comment": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium, February 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00601v2",
                "updated": "2024-11-04T09:40:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    9,
                    40,
                    27,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-01T14:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    3,
                    21,
                    4,
                    306,
                    0
                ],
                "title": "Diversity in Network-Friendly Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diversity in Network-Friendly Recommendations"
                },
                "summary": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms."
                },
                "authors": [
                    {
                        "name": "Evangelia Tzimpimpaki"
                    },
                    {
                        "name": "Thrasyvoulos Spyropoulos"
                    }
                ],
                "author_detail": {
                    "name": "Thrasyvoulos Spyropoulos"
                },
                "author": "Thrasyvoulos Spyropoulos",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01754v1",
                "updated": "2024-11-04T02:35:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "published": "2024-11-04T02:35:03Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    35,
                    3,
                    0,
                    309,
                    0
                ],
                "title": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental demonstration of dark current mitigation by an\n  over-inserted plug in a normal conducting VHF gun"
                },
                "summary": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The room temperature continuous wave (CW) very-high-frequency (VHF) gun is\none of the candidates for the electron gun of the high-repetition-rate\nfree-electron lasers (FELs). The VHF gun operates with a cathode gradient of ~\n20 MV/m and an accelerating voltage of ~ 750 kV. The gun dark current emission\nleads to beam loss along the FEL machine, therefore is a critical parameter for\nthe performance of the CW gun. In this paper, we presents a systematic study of\nthe dark current reduction of the VHF gun, including cathode region\noptimizations, dark current tracking simulations and measurements.\nOver-inserted cathode plugs were tested in two VHF guns of different\nacceleration gap sizes, and both demonstrated significant dark current\nreduction ratios of more than two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "X. -H. Wang"
                    },
                    {
                        "name": "G. Shu"
                    },
                    {
                        "name": "H. Qian"
                    },
                    {
                        "name": "X. Li"
                    },
                    {
                        "name": "Z. Liu"
                    },
                    {
                        "name": "Z. Jiang"
                    },
                    {
                        "name": "H. Meng"
                    },
                    {
                        "name": "C. Xing"
                    },
                    {
                        "name": "Q. Zhou"
                    },
                    {
                        "name": "H. Deng"
                    }
                ],
                "author_detail": {
                    "name": "H. Deng"
                },
                "author": "H. Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v2",
                "updated": "2024-11-04T02:08:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    4,
                    2,
                    8,
                    55,
                    0,
                    309,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training KV-Cache compression methods typically either sample a subset\nof effectual tokens or quantize the data into lower numerical bit width.\nHowever, these methods cannot exploit redundancy in the hidden dimension of the\nKV tensors. This paper presents a hidden dimension compression approach called\nPalu, a KV-Cache compression framework that utilizes low-rank projection to\nreduce inference-time LLM memory usage. Palu decomposes the linear layers into\nlow-rank matrices, caches compressed intermediate states, and reconstructs the\nfull keys and values on the fly. To improve accuracy, compression rate, and\nefficiency, Palu further encompasses (1) a medium-grained low-rank\ndecomposition scheme, (2) an efficient rank search algorithm, (3)\nlow-rank-aware quantization compatibility enhancements, and (4) optimized GPU\nkernels with operators fusion. Extensive experiments with popular LLMs show\nthat Palu compresses KV-Cache by 50% while maintaining strong accuracy and\ndelivering up to 1.89x on the RoPE-based attention module. When combined with\nquantization, Palu's inherent quantization-friendly design yields small to\nnegligible extra accuracy degradation while saving additional memory than\nquantization-only methods and achieving up to 2.91x speedup for the RoPE-based\nattention. Moreover, it maintains comparable or even better accuracy (up to\n1.19 lower perplexity) compared to quantization-only methods. These results\ndemonstrate Palu's superior capability to effectively address the efficiency\nand memory challenges of LLM inference posed by KV-Cache. Our code is publicly\navailable at: https://github.com/shadowpa0327/Palu"
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v4",
                "updated": "2024-11-03T09:42:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    42,
                    35,
                    6,
                    308,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01458v1",
                "updated": "2024-11-03T07:01:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "published": "2024-11-03T07:01:13Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    7,
                    1,
                    13,
                    6,
                    308,
                    0
                ],
                "title": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Timescale Model Caching and Resource Allocation for Edge-Enabled\n  AI-Generated Content Services"
                },
                "summary": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI (GenAI) has emerged as a transformative technology, enabling\ncustomized and personalized AI-generated content (AIGC) services. In this\npaper, we address challenges of edge-enabled AIGC service provisioning, which\nremain underexplored in the literature. These services require executing GenAI\nmodels with billions of parameters, posing significant obstacles to\nresource-limited wireless edge. We subsequently introduce the formulation of\njoint model caching and resource allocation for AIGC services to balance a\ntrade-off between AIGC quality and latency metrics. We obtain mathematical\nrelationships of these metrics with the computational resources required by\nGenAI models via experimentation. Afterward, we decompose the formulation into\na model caching subproblem on a long-timescale and a resource allocation\nsubproblem on a short-timescale. Since the variables to be solved are discrete\nand continuous, respectively, we leverage a double deep Q-network (DDQN)\nalgorithm to solve the former subproblem and propose a diffusion-based deep\ndeterministic policy gradient (D3PG) algorithm to solve the latter. The\nproposed D3PG algorithm makes an innovative use of diffusion models as the\nactor network to determine optimal resource allocation decisions. Consequently,\nwe integrate these two learning methods within the overarching two-timescale\ndeep reinforcement learning (T2DRL) algorithm, the performance of which is\nstudied through comparative numerical simulations."
                },
                "authors": [
                    {
                        "name": "Zhang Liu"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Xiangwang Hou"
                    },
                    {
                        "name": "Lianfen Huang"
                    },
                    {
                        "name": "Seyyedali Hosseinalipour"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Khaled Ben Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled Ben Letaief"
                },
                "author": "Khaled Ben Letaief",
                "arxiv_comment": "14 pages, 8 figures, 39 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01269v1",
                "updated": "2024-11-02T14:40:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T14:40:36Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    14,
                    40,
                    36,
                    5,
                    307,
                    0
                ],
                "title": "Disaggregated Database Management Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Database Management Systems"
                },
                "summary": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern applications demand high performance and cost efficient database\nmanagement systems (DBMSs). Their workloads may be diverse, ranging from online\ntransaction processing to analytics and decision support. The cloud\ninfrastructure enables disaggregation of monolithic DBMSs into components that\nfacilitate software-hardware co-design. This is realized using pools of\nhardware resources, i.e., CPUs, GPUs, memory, FPGA, NVM, etc., connected using\nhigh-speed networks. This disaggregation trend is being adopted by cloud DBMSs\nbecause hardware re-provisioning can be achieved by simply invoking software\nAPIs. Disaggregated DBMSs separate processing from storage, enabling each to\nscale elastically and independently. They may disaggregate compute usage based\non functionality, e.g., compute needed for writes from compute needed for\nqueries and compute needed for compaction. They may also use disaggregated\nmemory, e.g., for intermediate results in a shuffle or for remote caching. The\nDBMS monitors the characteristics of a workload and dynamically assembles its\ncomponents that are most efficient and cost effective for the workload. This\npaper is a summary of a panel session that discussed the capability,\nchallenges, and opportunities of these emerging DBMSs and disaggregated\nhardware systems."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Philip A. Bernstein"
                    },
                    {
                        "name": "Dhruba Borthakur"
                    },
                    {
                        "name": "Haoyu Huang"
                    },
                    {
                        "name": "Jai Menon"
                    },
                    {
                        "name": "Sumit Puri"
                    }
                ],
                "author_detail": {
                    "name": "Sumit Puri"
                },
                "author": "Sumit Puri",
                "arxiv_comment": "This paper appeared in the {\\em Performance Evaluation and\n  Benchmarking} - 14th TPC Technology Conference, TPCTC 2022, Sydney, NSW,\n  Australia, September 5, 2022, Revised Selected Papers. Lecture Notes in\n  Computer Science 13860, Springer 2023, ISBN 978-3-031-29575-1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01246v1",
                "updated": "2024-11-02T13:52:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T13:52:49Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    13,
                    52,
                    49,
                    5,
                    307,
                    0
                ],
                "title": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMP: A Cost Adaptive Multi-Queue Eviction Policy for Key-Value Stores"
                },
                "summary": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost Adaptive Multi-queue eviction Policy (CAMP) is an algorithm for a\ngeneral purpose key-value store (KVS) that manages key-value pairs computed by\napplications with different access patterns, key-value sizes, and varying costs\nfor each key-value pair. CAMP is an approximation of the Greedy Dual Size (GDS)\nalgorithm that can be implemented as efficiently as LRU. In particular, CAMP's\neviction policies are as effective as those of GDS but require only a small\nfraction of the updates to an internal data structure in order to make those\ndecisions. Similar to an implementation of LRU using queues, it adapts to\nchanging workload patterns based on the history of requests for different\nkey-value pairs. It is superior to LRU because it considers both the size and\ncost of key-value pairs to maximize the utility of the available memory across\ncompeting applications. We compare CAMP with both LRU and an alternative that\nrequires human intervention to partition memory into pools and assign grouping\nof key-value pairs to different pools. The results demonstrate CAMP is as fast\nas LRU while outperforming both LRU and the pooled alternative. We also present\nresults from an implementation of CAMP using Twitter's version of memcached."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    },
                    {
                        "name": "Jason Yap"
                    }
                ],
                "author_detail": {
                    "name": "Jason Yap"
                },
                "author": "Jason Yap",
                "arxiv_doi": "10.1145/2663165.2663317",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2663165.2663317",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.01246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version of CAMP appeared in the Proceedings of the\n  ACM/IFIP/USENIX Middleware Conference, Bordeaux, France, December 2014. See\n  https://github.com/scdblab/CAMP for an implementation",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01142v1",
                "updated": "2024-11-02T05:15:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "published": "2024-11-02T05:15:44Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    5,
                    15,
                    44,
                    5,
                    307,
                    0
                ],
                "title": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NEO: Saving GPU Memory Crisis with CPU Offloading for Online LLM\n  Inference"
                },
                "summary": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online LLM inference powers many exciting applications such as intelligent\nchatbots and autonomous agents. Modern LLM inference engines widely rely on\nrequest batching to improve inference throughput, aiming to make it\ncost-efficient when running on expensive GPU accelerators. However, the limited\nGPU memory has largely limited the batch size achieved in practice, leaving\nsignificant GPU compute resources wasted.\n  We present NEO, an online LLM inference system that offloads part of\nattention compute and KV cache states from the GPU to the local host CPU,\neffectively increasing the GPU batch size and thus inference throughput. To\nthis end, NEO proposes asymmetric GPU-CPU pipelining and load-aware scheduling\nto balance GPU and CPU loads and fully utilize their compute and memory\nresources. We evaluate NEO on a wide range of workloads (i.e., code generation,\ntext summarization), GPUs (i.e., T4, A10G, H100), and LLM models (i.e., 7B, 8B,\n70B). NEO achieves up to 7.5$\\times$, 26%, and 14% higher throughput compared\nto GPU-only approach on T4, A10G, and H100 GPUs, respectively, while\nmaintaining the same latency; with more powerful CPUs, NEO achieves up to 79.3%\nthroughput gain on A10G GPU."
                },
                "authors": [
                    {
                        "name": "Xuanlin Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v3",
                "updated": "2024-11-01T14:56:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    14,
                    56,
                    52,
                    4,
                    306,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "JoÃ£o Monteiro"
                    },
                    {
                        "name": "Ãtienne Marcotte"
                    },
                    {
                        "name": "Pierre-AndrÃ© NoÃ«l"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David VÃ¡zquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02657v2",
                "updated": "2024-11-01T08:52:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    1,
                    8,
                    52,
                    18,
                    4,
                    306,
                    0
                ],
                "published": "2024-06-04T17:45:26Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    45,
                    26,
                    1,
                    156,
                    0
                ],
                "title": "Block Transformer: Global-to-Local Language Modeling for Fast Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block Transformer: Global-to-Local Language Modeling for Fast Inference"
                },
                "summary": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Block Transformer which adopts hierarchical global-to-local\nmodeling to autoregressive transformers to mitigate the inference bottlenecks\nassociated with self-attention. Self-attention requires the key-value (KV)\ncache of all previous sequences to be retrieved from memory at every decoding\nstep to retrieve context information, leading to two primary bottlenecks during\nbatch inference. First, there is a significant delay in obtaining the first\ntoken, as the information of the entire prompt must first be processed to\nprefill the KV cache. Second, computation of subsequent tokens is bottlenecked\nby the high memory I/O demand of fetching the entire KV cache, which grows\nlinearly with sequence length, incurring quadratic memory reads overall. We\ndesign the Block Transformer to strategically mitigate these costs, by\nincorporating coarsity and locality into an integrated global-to-local\narchitecture. At the lower layers, we aggregate tokens into fixed size blocks\nto apply attention across the entire sequence at coarse-grained detail, to\ncapture the global context while minimizing KV cache overhead. At upper layers,\nwe apply attention within each block to decode individual tokens, to model\nfine-grained details with a lightweight local KV cache. We pretrain vanilla and\nBlock Transformers from scratch and demonstrate that Block Transformers reach\n10--20x inference throughput compared to vanilla transformers with equivalent\nperplexity and zero-shot task performance. Code is available at\nhttps://github.com/itsnamgyu/block-transformer."
                },
                "authors": [
                    {
                        "name": "Namgyu Ho"
                    },
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Taehyeon Kim"
                    },
                    {
                        "name": "Hyunjik Jo"
                    },
                    {
                        "name": "Yireun Kim"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "James Thorne"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "37 pages, 24 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00131v1",
                "updated": "2024-10-31T18:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T18:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    31,
                    13,
                    3,
                    305,
                    0
                ],
                "title": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two Dimensional Hidden Surface Removal with Frame-to-frame Coherence"
                },
                "summary": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe a hidden surface removal algorithm for two-dimensional layered\nscenes built from arbitrary primitives, particularly suited to interaction and\nanimation in rich scenes (for example, in illustration). The method makes use\nof a set-based raster representation to implement a front-to-back rendering\nmodel which analyses and dramatically reduces the amount of rasterization and\ncomposition required to render a scene. The method is extended to add\nframe-to-frame coherence analysis and caching for interactive or animated\nscenes. A powerful system of primitive-combiners called filters is described,\nwhich preserves the efficiencies of the algorithm in highly complicated scenes.\nThe set representation is extended to solve the problem of correlated mattes,\nleading to an efficient solution for high quality antialiasing. A prototype\nimplementation has been prepared."
                },
                "authors": [
                    {
                        "name": "John Whitington"
                    }
                ],
                "author_detail": {
                    "name": "John Whitington"
                },
                "author": "John Whitington",
                "arxiv_doi": "10.1145/2788539.27885",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/2788539.27885",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.00131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 14 figures",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.24174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.24174v1",
                "updated": "2024-10-31T17:41:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T17:41:14Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    17,
                    41,
                    14,
                    3,
                    305,
                    0
                ],
                "title": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Novel Architecture for Distributed Travel Data Integration and Service\n  Provision Using Microservices"
                },
                "summary": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a microservices architecture for the purpose of\nenhancing the flexibility and performance of an airline reservation system. The\narchitectural design incorporates Redis cache technologies, two different\nmessaging systems (Kafka and RabbitMQ), two types of storages (MongoDB, and\nPostgreSQL). It also introduces authorization techniques, including secure\ncommunication through OAuth2 and JWT which is essential with the management of\nhigh-demand travel services. According to selected indicators, the architecture\nprovides an impressive level of data consistency at 99.5% and a latency of data\npropagation of less than 75 ms allowing rapid and reliable intercommunication\nbetween microservices. A system throughput of 1050 events per second was\nachieved so that the acceptability level was maintained even during peak time.\nRedis caching reduced a 92% cache hit ratio on the database thereby lowering\nthe burden on the database and increasing the speed of response. Further\nimprovement of the systems scalability was done through the use of Docker and\nKubernetes which enabled services to be expanded horizontally to cope with the\nchanges in demand. The error rates were very low, at 0.2% further enhancing the\nefficiency of the system in handling real-time data integration. This approach\nis suggested to meet the specific needs of the airline reservation system. It\nis secure, fast, scalable, all serving to improve the user experience as well\nas the efficiency of operations. The low latency and high data integration\nlevels and prevaiing efficient usage of the resources demonstrates the\narchitecture ability to offer continued support in the ever growing high demand\nsituations."
                },
                "authors": [
                    {
                        "name": "Biman Barua"
                    },
                    {
                        "name": "M. Shamim Kaiser"
                    }
                ],
                "author_detail": {
                    "name": "M. Shamim Kaiser"
                },
                "author": "M. Shamim Kaiser",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.24174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.24174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23805v1",
                "updated": "2024-10-31T10:45:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T10:45:02Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    10,
                    45,
                    2,
                    3,
                    305,
                    0
                ],
                "title": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemANNS: Enhancing Billion-Scale ANNS Efficiency with Practical PIM\n  Hardware"
                },
                "summary": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In numerous production environments, Approximate Nearest Neighbor Search\n(ANNS) plays an indispensable role, particularly when dealing with massive\ndatasets that can contain billions of entries. The necessity for rapid response\ntimes in these applications makes the efficiency of ANNS algorithms crucial.\nHowever, traditional ANNS approaches encounter substantial challenges at the\nbillion-scale level. CPU-based methods are hindered by the limitations of\nmemory bandwidth, while GPU-based methods struggle with memory capacity and\nresource utilization efficiency. This paper introduces MemANNS, an innovative\nframework that utilizes UPMEM PIM architecture to address the memory\nbottlenecks in ANNS algorithms at scale. We concentrate on optimizing a\nwell-known ANNS algorithm, IVFPQ, for PIM hardware through several techniques.\nFirst, we introduce an architecture-aware strategy for data placement and query\nscheduling that ensures an even distribution of workload across PIM chips,\nthereby maximizing the use of aggregated memory bandwidth. Additionally, we\nhave developed an efficient thread scheduling mechanism that capitalizes on\nPIM's multi-threading capabilities and enhances memory management to boost\ncache efficiency. Moreover, we have recognized that real-world datasets often\nfeature vectors with frequently co-occurring items. To address this, we propose\na novel encoding method for IVFPQ that minimizes memory accesses during query\nprocessing. Our comprehensive evaluation using actual PIM hardware and\nreal-world datasets at the billion-scale, show that MemANNS offers a\nsignificant 4.3x increase in QPS over CPU-based Faiss, and it matches the\nperformance of GPU-based Faiss implementations. Additionally, MemANNS improves\nenergy efficiency, with a 2.3x enhancement in QPS/Watt compared to GPU\nsolutions."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Xin Yao"
                    }
                ],
                "author_detail": {
                    "name": "Xin Yao"
                },
                "author": "Xin Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23537v1",
                "updated": "2024-10-31T00:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "published": "2024-10-31T00:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    0,
                    58,
                    11,
                    3,
                    305,
                    0
                ],
                "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling"
                },
                "summary": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Youpeng Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "ICCAD 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v6",
                "updated": "2024-10-30T21:22:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    21,
                    22,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.14576v3",
                "updated": "2024-10-30T16:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    16,
                    6,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2024-02-08T17:17:46Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    17,
                    17,
                    46,
                    3,
                    39,
                    0
                ],
                "title": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Enhanced Prioritized Proximal Policy Optimization for Adaptive\n  Edge Caching"
                },
                "summary": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the growing issue of excessive data transmission in\nnetworks. With increasing traffic, backhaul links and core networks are under\nsignificant traffic, leading to the investigation of caching solutions at edge\nrouters. Many existing studies utilize Markov Decision Processes (MDP) to\ntackle caching problems, often assuming decision points at fixed intervals;\nhowever, real-world environments are characterized by random request arrivals.\nAdditionally, critical file attributes such as lifetime, size, and priority\nsignificantly impact the effectiveness of caching policies, yet existing\nresearch fails to integrate all these attributes in policy design. In this\nwork, we model the caching problem using a Semi-Markov Decision Process (SMDP)\nto better capture the continuous-time nature of real-world applications,\nenabling caching decisions to be triggered by random file requests. We then\nintroduce a Proximal Policy Optimization (PPO)--based caching strategy that\nfully considers file attributes like lifetime, size, and priority. Simulations\nshow that our method outperforms a recent Deep Reinforcement Learning-based\ntechnique. To further advance our research, we improved the convergence rate of\nPPO by prioritizing transitions within the replay buffer through an attention\nmechanism. This mechanism evaluates the similarity between the current state\nand all stored transitions, assigning higher priorities to transitions that\nexhibit greater similarity."
                },
                "authors": [
                    {
                        "name": "Farnaz Niknia"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Zixu Wang"
                    },
                    {
                        "name": "Aakash Agarwal"
                    },
                    {
                        "name": "Adib S. Rezaei"
                    }
                ],
                "author_detail": {
                    "name": "Adib S. Rezaei"
                },
                "author": "Adib S. Rezaei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23079v1",
                "updated": "2024-10-30T14:53:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T14:53:37Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    14,
                    53,
                    37,
                    2,
                    304,
                    0
                ],
                "title": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BUZZ: Beehive-structured Sparse KV Cache with Segmented Heavy Hitters\n  for Efficient LLM Inference"
                },
                "summary": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are essential in natural language processing but\noften struggle with inference speed and computational efficiency, limiting\nreal-time deployment. The key-value (KV) cache mechanism reduces computational\noverhead in transformer models, but challenges in maintaining contextual\nunderstanding remain. In this paper, we propose BUZZ, a novel KV caching\nalgorithm that leverages structured contextual information to minimize cache\nmemory usage while enhancing inference speed. BUZZ employs a beehive-structured\nsparse cache, incorporating a sliding window to capture recent information and\ndynamically segmenting historical tokens into chunks to prioritize important\ntokens in local neighborhoods. We evaluate BUZZ on four real-world datasets:\nCNN/Daily Mail, XSUM, Wikitext, and 10-QA. Our results demonstrate that BUZZ\n(1) reduces cache memory usage by $\\textbf{2.5}\\times$ in LLM inference while\nmaintaining over 99% accuracy in long-text summarization, and (2) surpasses\nstate-of-the-art performance in multi-document question answering by\n$\\textbf{7.69%}$ under the same memory limit, where full cache methods\nencounter out-of-memory issues. Additionally, BUZZ achieves significant\ninference speedup with a $\\log{n}$ time complexity. The code is available at\nhttps://github.com/JunqiZhao888/buzz-llm."
                },
                "authors": [
                    {
                        "name": "Junqi Zhao"
                    },
                    {
                        "name": "Zhijin Fang"
                    },
                    {
                        "name": "Shu Li"
                    },
                    {
                        "name": "Shaohui Yang"
                    },
                    {
                        "name": "Shichao He"
                    }
                ],
                "author_detail": {
                    "name": "Shichao He"
                },
                "author": "Shichao He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v2",
                "updated": "2024-10-30T03:31:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    31,
                    9,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v1",
                "updated": "2024-10-30T02:36:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "The code is coming soon! For sure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08043v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08043v1",
                "updated": "2024-10-30T02:18:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-30T02:18:59Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    18,
                    59,
                    2,
                    304,
                    0
                ],
                "title": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-GIC: A Smart and Parallelized Geomagnetically Induced Current\n  Modelling Algorithm Based on Graph Theory for Space Weather Applications"
                },
                "summary": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geomagnetically Induced Current (GIC) refers to the electromagnetic response\nof the Earth and its conductive modern infrastructures to space weather and\nwould pose a significant threat to high-voltage power grids designed for the\nalternative current operation. To assess the impact of space weather on the\npower grid, one needs to calculate the GIC on a national or continental scale.\nIn this study, we developed a smart and parallelized GIC modelling algorithm,\nGraph GIC. This algorithm deploys a graph representing a power grid in a\nsingle-line diagram, in which substations/transformers act as nodes and\ntransmission lines as edges. With these denotations, a power grid and its\nelectric parameters are mathematically represented with an adjacency matrix and\nan admittance matrix. We used sparse matrix and parallelisation techniques to\nexpedite the intensive computation in cases of large-scale power grids. The\nGraph GIC was validated with a benchmark grid, applied to the GIC calculation\nof the 500 kV power grid of Guangdong, China, and conducted preliminary\nanalysis on the grid's susceptibility to geomagnetic storms. The Graph GIC\nalgorithm has the advantage of an intuitive and highly scalable graph\nrepresentation of a power grid at any scale. It achieves high-accuracy\ncalculation and a speedup of about 18 times after parallelisation. This\nalgorithm could be applied to assess the impact of space weather on a power\ngrid up to continental scales and could be incorporated into global space\nweather modelling frameworks."
                },
                "authors": [
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Ding Yuan"
                    },
                    {
                        "name": "Xueshang Feng"
                    },
                    {
                        "name": "Stefaan Poedts"
                    },
                    {
                        "name": "Zhengyang Zou"
                    },
                    {
                        "name": "Song Feng"
                    },
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Tong Yin"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yin"
                },
                "author": "Tong Yin",
                "arxiv_comment": "19 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08043v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08043v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.space-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23317v1",
                "updated": "2024-10-29T20:04:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T20:04:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    20,
                    4,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for\n  Vision-Language Model Inference Acceleration"
                },
                "summary": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have demonstrated impressive performance across\na versatile set of tasks. A key challenge in accelerating VLMs is storing and\naccessing the large Key-Value (KV) cache that encodes long visual contexts,\nsuch as images or videos. While existing KV cache compression methods are\neffective for Large Language Models (LLMs), directly migrating them to VLMs\nyields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache,\na novel KV cache compression recipe tailored for accelerating VLM inference. In\nthis paper, we first investigate the unique sparsity pattern of VLM attention\nby distinguishing visual and text tokens in prefill and decoding phases. Based\non these observations, we introduce a layer-adaptive sparsity-aware cache\nbudget allocation method that effectively distributes the limited cache budget\nacross different layers, further reducing KV cache size without compromising\naccuracy. Additionally, we develop a modality-aware token scoring policy to\nbetter evaluate the token importance. Empirical results on multiple benchmark\ndatasets demonstrate that retaining only 10% of KV cache achieves accuracy\ncomparable to that with full cache. In a speed benchmark, our method\naccelerates end-to-end latency of generating 100 tokens by up to 2.33x and\nspeeds up decoding by up to 7.08x, while reducing the memory footprint of KV\ncache in GPU by 90%."
                },
                "authors": [
                    {
                        "name": "Dezhan Tu"
                    },
                    {
                        "name": "Danylo Vashchilenko"
                    },
                    {
                        "name": "Yuzhe Lu"
                    },
                    {
                        "name": "Panpan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Panpan Xu"
                },
                "author": "Panpan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.01801v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.01801v4",
                "updated": "2024-10-29T18:26:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    18,
                    26,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-03T05:17:08Z",
                "published_parsed": [
                    2023,
                    10,
                    3,
                    5,
                    17,
                    8,
                    1,
                    276,
                    0
                ],
                "title": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs"
                },
                "summary": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce adaptive KV cache compression, a plug-and-play\nmethod that reduces the memory footprint of generative inference for Large\nLanguage Models (LLMs). Different from the conventional KV cache that retains\nkey and value vectors for all context tokens, we conduct targeted profiling to\ndiscern the intrinsic structure of attention modules. Based on the recognized\nstructure, we then construct the KV cache in an adaptive manner: evicting\nlong-range contexts on attention heads emphasizing local contexts, discarding\nnon-special tokens on attention heads centered on special tokens, and only\nemploying the standard KV cache for attention heads that broadly attend to all\ntokens. Moreover, with the lightweight attention profiling used to guide the\nconstruction of the adaptive KV cache, FastGen can be deployed without\nresource-intensive fine-tuning or re-training. In our experiments across\nvarious asks, FastGen demonstrates substantial reduction on GPU memory\nconsumption with negligible generation quality loss. We will release our code\nand the compatible CUDA kernel for reproducibility."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Jianfeng Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jianfeng Gao"
                },
                "author": "Jianfeng Gao",
                "arxiv_comment": "ICLR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.01801v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.01801v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v3",
                "updated": "2024-10-29T02:52:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    2,
                    52,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v1",
                "updated": "2024-10-28T19:08:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.13552v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13552v1",
                "updated": "2024-11-20T18:59:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    59,
                    52,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:59:52Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    59,
                    52,
                    2,
                    325,
                    0
                ],
                "title": "REDUCIO! Generating 1024$\\times$1024 Video within 16 Seconds using\n  Extremely Compressed Motion Latents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REDUCIO! Generating 1024$\\times$1024 Video within 16 Seconds using\n  Extremely Compressed Motion Latents"
                },
                "summary": "Commercial video generation models have exhibited realistic, high-fidelity\nresults but are still restricted to limited access. One crucial obstacle for\nlarge-scale applications is the expensive training and inference cost. In this\npaper, we argue that videos contain much more redundant information than\nimages, thus can be encoded by very few motion latents based on a content\nimage. Towards this goal, we design an image-conditioned VAE to encode a video\nto an extremely compressed motion latent space. This magic Reducio charm\nenables 64x reduction of latents compared to a common 2D VAE, without\nsacrificing the quality. Training diffusion models on such a compact\nrepresentation easily allows for generating 1K resolution videos. We then adopt\na two-stage video generation paradigm, which performs text-to-image and\ntext-image-to-video sequentially. Extensive experiments show that our\nReducio-DiT achieves strong performance in evaluation, though trained with\nlimited GPU resources. More importantly, our method significantly boost the\nefficiency of video LDMs both in training and inference. We train Reducio-DiT\nin around 3.2K training hours in total and generate a 16-frame 1024*1024 video\nclip within 15.5 seconds on a single A100 GPU. Code released at\nhttps://github.com/microsoft/Reducio-VAE .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commercial video generation models have exhibited realistic, high-fidelity\nresults but are still restricted to limited access. One crucial obstacle for\nlarge-scale applications is the expensive training and inference cost. In this\npaper, we argue that videos contain much more redundant information than\nimages, thus can be encoded by very few motion latents based on a content\nimage. Towards this goal, we design an image-conditioned VAE to encode a video\nto an extremely compressed motion latent space. This magic Reducio charm\nenables 64x reduction of latents compared to a common 2D VAE, without\nsacrificing the quality. Training diffusion models on such a compact\nrepresentation easily allows for generating 1K resolution videos. We then adopt\na two-stage video generation paradigm, which performs text-to-image and\ntext-image-to-video sequentially. Extensive experiments show that our\nReducio-DiT achieves strong performance in evaluation, though trained with\nlimited GPU resources. More importantly, our method significantly boost the\nefficiency of video LDMs both in training and inference. We train Reducio-DiT\nin around 3.2K training hours in total and generate a 16-frame 1024*1024 video\nclip within 15.5 seconds on a single A100 GPU. Code released at\nhttps://github.com/microsoft/Reducio-VAE ."
                },
                "authors": [
                    {
                        "name": "Rui Tian"
                    },
                    {
                        "name": "Qi Dai"
                    },
                    {
                        "name": "Jianmin Bao"
                    },
                    {
                        "name": "Kai Qiu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Chong Luo"
                    },
                    {
                        "name": "Zuxuan Wu"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "arxiv_comment": "Code available at https://github.com/microsoft/Reducio-VAE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13552v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13548v1",
                "updated": "2024-11-20T18:56:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    56,
                    24,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    56,
                    24,
                    2,
                    325,
                    0
                ],
                "title": "HF-Diff: High-Frequency Perceptual Loss and Distribution Matching for\n  One-Step Diffusion-Based Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HF-Diff: High-Frequency Perceptual Loss and Distribution Matching for\n  One-Step Diffusion-Based Image Super-Resolution"
                },
                "summary": "Although recent diffusion-based single-step super-resolution methods achieve\nbetter performance as compared to SinSR, they are computationally complex. To\nimprove the performance of SinSR, we investigate preserving the high-frequency\ndetail features during super-resolution (SR) because the downgraded images lack\ndetailed information. For this purpose, we introduce a high-frequency\nperceptual loss by utilizing an invertible neural network (INN) pretrained on\nthe ImageNet dataset. Different feature maps of pretrained INN produce\ndifferent high-frequency aspects of an image. During the training phase, we\nimpose to preserve the high-frequency features of super-resolved and ground\ntruth (GT) images that improve the SR image quality during inference.\nFurthermore, we also utilize the Jenson-Shannon divergence between GT and SR\nimages in the pretrained DINO-v2 embedding space to match their distribution.\nBy introducing the $\\textbf{h}igh$- $\\textbf{f}requency$ preserving loss and\ndistribution matching constraint in the single-step $\\textbf{diff}usion-based$\nSR ($\\textbf{HF-Diff}$), we achieve a state-of-the-art CLIPIQA score in the\nbenchmark RealSR, RealSet65, DIV2K-Val, and ImageNet datasets. Furthermore, the\nexperimental results in several datasets demonstrate that our high-frequency\nperceptual loss yields better SR image quality than LPIPS and VGG-based\nperceptual losses. Our code will be released at\nhttps://github.com/shoaib-sami/HF-Diff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although recent diffusion-based single-step super-resolution methods achieve\nbetter performance as compared to SinSR, they are computationally complex. To\nimprove the performance of SinSR, we investigate preserving the high-frequency\ndetail features during super-resolution (SR) because the downgraded images lack\ndetailed information. For this purpose, we introduce a high-frequency\nperceptual loss by utilizing an invertible neural network (INN) pretrained on\nthe ImageNet dataset. Different feature maps of pretrained INN produce\ndifferent high-frequency aspects of an image. During the training phase, we\nimpose to preserve the high-frequency features of super-resolved and ground\ntruth (GT) images that improve the SR image quality during inference.\nFurthermore, we also utilize the Jenson-Shannon divergence between GT and SR\nimages in the pretrained DINO-v2 embedding space to match their distribution.\nBy introducing the $\\textbf{h}igh$- $\\textbf{f}requency$ preserving loss and\ndistribution matching constraint in the single-step $\\textbf{diff}usion-based$\nSR ($\\textbf{HF-Diff}$), we achieve a state-of-the-art CLIPIQA score in the\nbenchmark RealSR, RealSet65, DIV2K-Val, and ImageNet datasets. Furthermore, the\nexperimental results in several datasets demonstrate that our high-frequency\nperceptual loss yields better SR image quality than LPIPS and VGG-based\nperceptual losses. Our code will be released at\nhttps://github.com/shoaib-sami/HF-Diff."
                },
                "authors": [
                    {
                        "name": "Shoaib Meraj Sami"
                    },
                    {
                        "name": "Md Mahedi Hasan"
                    },
                    {
                        "name": "Jeremy Dawson"
                    },
                    {
                        "name": "Nasser Nasrabadi"
                    }
                ],
                "author_detail": {
                    "name": "Nasser Nasrabadi"
                },
                "author": "Nasser Nasrabadi",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13547v1",
                "updated": "2024-11-20T18:56:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    56,
                    22,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:56:22Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    56,
                    22,
                    2,
                    325,
                    0
                ],
                "title": "SpecTool: A Benchmark for Characterizing Errors in Tool-Use LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecTool: A Benchmark for Characterizing Errors in Tool-Use LLMs"
                },
                "summary": "Evaluating the output of Large Language Models (LLMs) is one of the most\ncritical aspects of building a performant compound AI system. Since the output\nfrom LLMs propagate to downstream steps, identifying LLM errors is crucial to\nsystem performance. A common task for LLMs in AI systems is tool use. While\nthere are several benchmark environments for evaluating LLMs on this task, they\ntypically only give a success rate without any explanation of the failure\ncases. To solve this problem, we introduce SpecTool, a new benchmark to\nidentify error patterns in LLM output on tool-use tasks. Our benchmark data set\ncomprises of queries from diverse environments that can be used to test for the\npresence of seven newly characterized error patterns. Using SPECTOOL , we show\nthat even the most prominent LLMs exhibit these error patterns in their\noutputs. Researchers can use the analysis and insights from SPECTOOL to guide\ntheir error mitigation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the output of Large Language Models (LLMs) is one of the most\ncritical aspects of building a performant compound AI system. Since the output\nfrom LLMs propagate to downstream steps, identifying LLM errors is crucial to\nsystem performance. A common task for LLMs in AI systems is tool use. While\nthere are several benchmark environments for evaluating LLMs on this task, they\ntypically only give a success rate without any explanation of the failure\ncases. To solve this problem, we introduce SpecTool, a new benchmark to\nidentify error patterns in LLM output on tool-use tasks. Our benchmark data set\ncomprises of queries from diverse environments that can be used to test for the\npresence of seven newly characterized error patterns. Using SPECTOOL , we show\nthat even the most prominent LLMs exhibit these error patterns in their\noutputs. Researchers can use the analysis and insights from SPECTOOL to guide\ntheir error mitigation strategies."
                },
                "authors": [
                    {
                        "name": "Shirley Kokane"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Tulika Awalgaonkar"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Thai Hoang"
                    },
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Liangwei Yang"
                    },
                    {
                        "name": "Juntao Tan"
                    },
                    {
                        "name": "Rithesh Murthy"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Silivo Savarese"
                    }
                ],
                "author_detail": {
                    "name": "Silivo Savarese"
                },
                "author": "Silivo Savarese",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13543v1",
                "updated": "2024-11-20T18:54:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    54,
                    32,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:54:32Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    54,
                    32,
                    2,
                    325,
                    0
                ],
                "title": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games"
                },
                "summary": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess\nextensive knowledge and exhibit promising reasoning abilities; however, they\nstill struggle to perform well in complex, dynamic environments. Real-world\ntasks require handling intricate interactions, advanced spatial reasoning,\nlong-term planning, and continuous exploration of new strategies-areas in which\nwe lack effective methodologies for comprehensively evaluating these\ncapabilities. To address this gap, we introduce BALROG, a novel benchmark\ndesigned to assess the agentic capabilities of LLMs and VLMs through a diverse\nset of challenging games. Our benchmark incorporates a range of existing\nreinforcement learning environments with varying levels of difficulty,\nincluding tasks that are solvable by non-expert humans in seconds to extremely\nchallenging ones that may take years to master (e.g., the NetHack Learning\nEnvironment). We devise fine-grained metrics to measure performance and conduct\nan extensive evaluation of several popular open-source and closed-source LLMs\nand VLMs. Our findings indicate that while current models achieve partial\nsuccess in the easier games, they struggle significantly with more challenging\ntasks. Notably, we observe severe deficiencies in vision-based decision-making,\nas models perform worse when visual representations of the environments are\nprovided. We release BALROG as an open and user-friendly benchmark to\nfacilitate future research and development in the agentic community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess\nextensive knowledge and exhibit promising reasoning abilities; however, they\nstill struggle to perform well in complex, dynamic environments. Real-world\ntasks require handling intricate interactions, advanced spatial reasoning,\nlong-term planning, and continuous exploration of new strategies-areas in which\nwe lack effective methodologies for comprehensively evaluating these\ncapabilities. To address this gap, we introduce BALROG, a novel benchmark\ndesigned to assess the agentic capabilities of LLMs and VLMs through a diverse\nset of challenging games. Our benchmark incorporates a range of existing\nreinforcement learning environments with varying levels of difficulty,\nincluding tasks that are solvable by non-expert humans in seconds to extremely\nchallenging ones that may take years to master (e.g., the NetHack Learning\nEnvironment). We devise fine-grained metrics to measure performance and conduct\nan extensive evaluation of several popular open-source and closed-source LLMs\nand VLMs. Our findings indicate that while current models achieve partial\nsuccess in the easier games, they struggle significantly with more challenging\ntasks. Notably, we observe severe deficiencies in vision-based decision-making,\nas models perform worse when visual representations of the environments are\nprovided. We release BALROG as an open and user-friendly benchmark to\nfacilitate future research and development in the agentic community."
                },
                "authors": [
                    {
                        "name": "Davide Paglieri"
                    },
                    {
                        "name": "BartÅomiej CupiaÅ"
                    },
                    {
                        "name": "Samuel Coward"
                    },
                    {
                        "name": "Ulyana Piterbarg"
                    },
                    {
                        "name": "Maciej Wolczyk"
                    },
                    {
                        "name": "Akbir Khan"
                    },
                    {
                        "name": "Eduardo Pignatelli"
                    },
                    {
                        "name": "Åukasz KuciÅski"
                    },
                    {
                        "name": "Lerrel Pinto"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Jakob Nicolaus Foerster"
                    },
                    {
                        "name": "Jack Parker-Holder"
                    },
                    {
                        "name": "Tim RocktÃ¤schel"
                    }
                ],
                "author_detail": {
                    "name": "Tim RocktÃ¤schel"
                },
                "author": "Tim RocktÃ¤schel",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13537v1",
                "updated": "2024-11-20T18:41:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    41,
                    3,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:41:03Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    41,
                    3,
                    2,
                    325,
                    0
                ],
                "title": "Metacognition for Unknown Situations and Environments (MUSE)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metacognition for Unknown Situations and Environments (MUSE)"
                },
                "summary": "Metacognition--the awareness and regulation of one's cognitive processes--is\ncentral to human adaptability in unknown situations. In contrast, current\nautonomous agents often struggle in novel environments due to their limited\ncapacity for adaptation. We hypothesize that metacognition is a critical\nmissing ingredient in adaptive autonomous systems, equipping them with the\ncognitive flexibility needed to tackle unfamiliar challenges. Given the broad\nscope of metacognitive abilities, we focus on two key aspects: competence\nawareness and strategy selection for novel tasks. To this end, we propose the\nMetacognition for Unknown Situations and Environments (MUSE) framework, which\nintegrates metacognitive processes--specifically self-awareness and\nself-regulation--into autonomous agents. We present two initial implementations\nof MUSE: one based on world modeling and another leveraging large language\nmodels (LLMs), both instantiating the metacognitive cycle. Our system\ncontinuously learns to assess its competence on a given task and uses this\nself-awareness to guide iterative cycles of strategy selection. MUSE agents\nshow significant improvements in self-awareness and self-regulation, enabling\nthem to solve novel, out-of-distribution tasks more effectively compared to\nDreamer-v3-based reinforcement learning and purely prompt-based LLM agent\napproaches. This work highlights the promise of approaches inspired by\ncognitive and neural systems in enabling autonomous systems to adapt to new\nenvironments, overcoming the limitations of current methods that rely heavily\non extensive training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metacognition--the awareness and regulation of one's cognitive processes--is\ncentral to human adaptability in unknown situations. In contrast, current\nautonomous agents often struggle in novel environments due to their limited\ncapacity for adaptation. We hypothesize that metacognition is a critical\nmissing ingredient in adaptive autonomous systems, equipping them with the\ncognitive flexibility needed to tackle unfamiliar challenges. Given the broad\nscope of metacognitive abilities, we focus on two key aspects: competence\nawareness and strategy selection for novel tasks. To this end, we propose the\nMetacognition for Unknown Situations and Environments (MUSE) framework, which\nintegrates metacognitive processes--specifically self-awareness and\nself-regulation--into autonomous agents. We present two initial implementations\nof MUSE: one based on world modeling and another leveraging large language\nmodels (LLMs), both instantiating the metacognitive cycle. Our system\ncontinuously learns to assess its competence on a given task and uses this\nself-awareness to guide iterative cycles of strategy selection. MUSE agents\nshow significant improvements in self-awareness and self-regulation, enabling\nthem to solve novel, out-of-distribution tasks more effectively compared to\nDreamer-v3-based reinforcement learning and purely prompt-based LLM agent\napproaches. This work highlights the promise of approaches inspired by\ncognitive and neural systems in enabling autonomous systems to adapt to new\nenvironments, overcoming the limitations of current methods that rely heavily\non extensive training data."
                },
                "authors": [
                    {
                        "name": "Rodolfo Valiente"
                    },
                    {
                        "name": "Praveen K. Pilly"
                    }
                ],
                "author_detail": {
                    "name": "Praveen K. Pilly"
                },
                "author": "Praveen K. Pilly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13528v1",
                "updated": "2024-11-20T18:24:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    24,
                    11,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:24:11Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    24,
                    11,
                    2,
                    325,
                    0
                ],
                "title": "Entropy Bootstrapping for Weakly Supervised Nuclei Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy Bootstrapping for Weakly Supervised Nuclei Detection"
                },
                "summary": "Microscopy structure segmentation, such as detecting cells or nuclei,\ngenerally requires a human to draw a ground truth contour around each instance.\nWeakly supervised approaches (e.g. consisting of only single point labels) have\nthe potential to reduce this workload significantly. Our approach uses\nindividual point labels for an entropy estimation to approximate an underlying\ndistribution of cell pixels. We infer full cell masks from this distribution,\nand use Mask-RCNN to produce an instance segmentation output. We compare this\npoint--annotated approach with training on the full ground truth masks. We show\nthat our method achieves a comparatively good level of performance, despite a\n95% reduction in pixel labels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microscopy structure segmentation, such as detecting cells or nuclei,\ngenerally requires a human to draw a ground truth contour around each instance.\nWeakly supervised approaches (e.g. consisting of only single point labels) have\nthe potential to reduce this workload significantly. Our approach uses\nindividual point labels for an entropy estimation to approximate an underlying\ndistribution of cell pixels. We infer full cell masks from this distribution,\nand use Mask-RCNN to produce an instance segmentation output. We compare this\npoint--annotated approach with training on the full ground truth masks. We show\nthat our method achieves a comparatively good level of performance, despite a\n95% reduction in pixel labels."
                },
                "authors": [
                    {
                        "name": "James Willoughby"
                    },
                    {
                        "name": "Irina Voiculescu"
                    }
                ],
                "author_detail": {
                    "name": "Irina Voiculescu"
                },
                "author": "Irina Voiculescu",
                "arxiv_comment": "Submitted for CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03753v2",
                "updated": "2024-11-20T18:20:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    20,
                    18,
                    2,
                    325,
                    0
                ],
                "published": "2024-06-06T05:30:42Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    5,
                    30,
                    42,
                    3,
                    158,
                    0
                ],
                "title": "VisTR: Visualizations as Representations for Time-series Table Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisTR: Visualizations as Representations for Time-series Table Reasoning"
                },
                "summary": "Table reasoning involves transforming natural language questions into\ncorresponding answers based on the provided data table. Recent research\nexploits large language models (LLMs) to facilitate table reasoning, which\nhowever struggle with pattern recognition and lack support for visual-based\npattern exploration. To address these limitations, we propose VisTR, a\nframework that leverages visualizations as representations to facilitate data\npattern recognition and support cross-modal exploration. We describe VisTR as a\nprocess consisting of four major modules: 1) visualization alignment that\nutilizes multimodal LLMs to align visualizations across various modalities,\nincluding chart, text, and sketch; 2) visualization referencing that decomposes\na table into multifaceted visualization references that comprehensively\nrepresent the table; 3) visualization pruning that incorporates data and\nretrieval pruning to excise visualization references with poor information and\nenhance retrieval efficiency; and 4) visualization interaction that offers an\ninteractive visual interface with multimodal interactions for user-friendly\ntable reasoning. Quantitative evaluation with existing multimodal LLMs\ndemonstrates the effectiveness of the alignment model in cross-modal\nvisualization pairings. We further illustrate the applicability of the proposed\nframework in various time-series table reasoning and exploration tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table reasoning involves transforming natural language questions into\ncorresponding answers based on the provided data table. Recent research\nexploits large language models (LLMs) to facilitate table reasoning, which\nhowever struggle with pattern recognition and lack support for visual-based\npattern exploration. To address these limitations, we propose VisTR, a\nframework that leverages visualizations as representations to facilitate data\npattern recognition and support cross-modal exploration. We describe VisTR as a\nprocess consisting of four major modules: 1) visualization alignment that\nutilizes multimodal LLMs to align visualizations across various modalities,\nincluding chart, text, and sketch; 2) visualization referencing that decomposes\na table into multifaceted visualization references that comprehensively\nrepresent the table; 3) visualization pruning that incorporates data and\nretrieval pruning to excise visualization references with poor information and\nenhance retrieval efficiency; and 4) visualization interaction that offers an\ninteractive visual interface with multimodal interactions for user-friendly\ntable reasoning. Quantitative evaluation with existing multimodal LLMs\ndemonstrates the effectiveness of the alignment model in cross-modal\nvisualization pairings. We further illustrate the applicability of the proposed\nframework in various time-series table reasoning and exploration tasks."
                },
                "authors": [
                    {
                        "name": "Jianing Hao"
                    },
                    {
                        "name": "Zhuowen Liang"
                    },
                    {
                        "name": "Chunting Li"
                    },
                    {
                        "name": "Yuyu Luo"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Wei Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zeng"
                },
                "author": "Wei Zeng",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16838v2",
                "updated": "2024-11-20T17:57:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    57,
                    26,
                    2,
                    325,
                    0
                ],
                "published": "2024-06-24T17:45:59Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    17,
                    45,
                    59,
                    0,
                    176,
                    0
                ],
                "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Decoding to Meta-Generation: Inference-time Algorithms for Large\n  Language Models"
                },
                "summary": "One of the most striking findings in modern research on large language models\n(LLMs) is that scaling up compute during training leads to better results.\nHowever, less attention has been given to the benefits of scaling compute\nduring inference. This survey focuses on these inference-time approaches. We\nexplore three areas under a unified mathematical formalism: token-level\ngeneration algorithms, meta-generation algorithms, and efficient generation.\nToken-level generation algorithms, often called decoding algorithms, operate by\nsampling a single token at a time or constructing a token-level search space\nand then selecting an output. These methods typically assume access to a\nlanguage model's logits, next-token distributions, or probability scores.\nMeta-generation algorithms work on partial or full sequences, incorporating\ndomain knowledge, enabling backtracking, and integrating external information.\nEfficient generation methods aim to reduce token costs and improve the speed of\ngeneration. Our survey unifies perspectives from three research communities:\ntraditional natural language processing, modern LLMs, and machine learning\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the most striking findings in modern research on large language models\n(LLMs) is that scaling up compute during training leads to better results.\nHowever, less attention has been given to the benefits of scaling compute\nduring inference. This survey focuses on these inference-time approaches. We\nexplore three areas under a unified mathematical formalism: token-level\ngeneration algorithms, meta-generation algorithms, and efficient generation.\nToken-level generation algorithms, often called decoding algorithms, operate by\nsampling a single token at a time or constructing a token-level search space\nand then selecting an output. These methods typically assume access to a\nlanguage model's logits, next-token distributions, or probability scores.\nMeta-generation algorithms work on partial or full sequences, incorporating\ndomain knowledge, enabling backtracking, and integrating external information.\nEfficient generation methods aim to reduce token costs and improve the speed of\ngeneration. Our survey unifies perspectives from three research communities:\ntraditional natural language processing, modern LLMs, and machine learning\nsystems."
                },
                "authors": [
                    {
                        "name": "Sean Welleck"
                    },
                    {
                        "name": "Amanda Bertsch"
                    },
                    {
                        "name": "Matthew Finlayson"
                    },
                    {
                        "name": "Hailey Schoelkopf"
                    },
                    {
                        "name": "Alex Xie"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Zaid Harchaoui"
                    }
                ],
                "author_detail": {
                    "name": "Zaid Harchaoui"
                },
                "author": "Zaid Harchaoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13504v1",
                "updated": "2024-11-20T17:55:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    55,
                    38,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T17:55:38Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    55,
                    38,
                    2,
                    325,
                    0
                ],
                "title": "Disentangling Memory and Reasoning Ability in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Memory and Reasoning Ability in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong performance in handling\ncomplex tasks requiring both extensive knowledge and reasoning abilities.\nHowever, the existing LLM inference pipeline operates as an opaque process\nwithout explicit separation between knowledge retrieval and reasoning steps,\nmaking the model's decision-making process unclear and disorganized. This\nambiguity can lead to issues such as hallucinations and knowledge forgetting,\nwhich significantly impact the reliability of LLMs in high-stakes domains. In\nthis paper, we propose a new inference paradigm that decomposes the complex\ninference process into two distinct and clear actions: (1) memory recall: which\nretrieves relevant knowledge, and (2) reasoning: which performs logical steps\nbased on the recalled knowledge. To facilitate this decomposition, we introduce\ntwo special tokens memory and reason, guiding the model to distinguish between\nsteps that require knowledge retrieval and those that involve reasoning. Our\nexperiment results show that this decomposition not only improves model\nperformance but also enhances the interpretability of the inference process,\nenabling users to identify sources of error and refine model responses\neffectively. The code is available at\nhttps://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong performance in handling\ncomplex tasks requiring both extensive knowledge and reasoning abilities.\nHowever, the existing LLM inference pipeline operates as an opaque process\nwithout explicit separation between knowledge retrieval and reasoning steps,\nmaking the model's decision-making process unclear and disorganized. This\nambiguity can lead to issues such as hallucinations and knowledge forgetting,\nwhich significantly impact the reliability of LLMs in high-stakes domains. In\nthis paper, we propose a new inference paradigm that decomposes the complex\ninference process into two distinct and clear actions: (1) memory recall: which\nretrieves relevant knowledge, and (2) reasoning: which performs logical steps\nbased on the recalled knowledge. To facilitate this decomposition, we introduce\ntwo special tokens memory and reason, guiding the model to distinguish between\nsteps that require knowledge retrieval and those that involve reasoning. Our\nexperiment results show that this decomposition not only improves model\nperformance but also enhances the interpretability of the inference process,\nenabling users to identify sources of error and refine model responses\neffectively. The code is available at\nhttps://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning."
                },
                "authors": [
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Weidi Luo"
                    },
                    {
                        "name": "Sitao Cheng"
                    },
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Ruixiang Tang"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12578v2",
                "updated": "2024-11-20T17:53:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    53,
                    3,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-19T15:43:36Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    15,
                    43,
                    36,
                    1,
                    324,
                    0
                ],
                "title": "Robust Inference for High-dimensional Linear Models with Heavy-tailed\n  Errors via Partial Gini Covariance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Inference for High-dimensional Linear Models with Heavy-tailed\n  Errors via Partial Gini Covariance"
                },
                "summary": "This paper introduces the partial Gini covariance, a novel dependence measure\nthat addresses the challenges of high-dimensional inference with heavy-tailed\nerrors, often encountered in fields like finance, insurance, climate, and\nbiology. Conventional high-dimensional regression inference methods suffer from\ninaccurate type I errors and reduced power in heavy-tailed contexts, limiting\ntheir effectiveness. Our proposed approach leverages the partial Gini\ncovariance to construct a robust statistical inference framework that requires\nminimal tuning and does not impose restrictive moment conditions on error\ndistributions. Unlike traditional methods, it circumvents the need for\nestimating the density of random errors and enhances the computational\nfeasibility and robustness. Extensive simulations demonstrate the proposed\nmethod's superior power and robustness over standard high-dimensional inference\napproaches, such as those based on the debiased Lasso. The asymptotic relative\nefficiency analysis provides additional theoretical insight on the improved\nefficiency of the new approach in the heavy-tailed setting. Additionally, the\npartial Gini covariance extends to the multivariate setting, enabling\nchi-square testing for a group of coefficients. We illustrate the method's\npractical application with a real-world data example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces the partial Gini covariance, a novel dependence measure\nthat addresses the challenges of high-dimensional inference with heavy-tailed\nerrors, often encountered in fields like finance, insurance, climate, and\nbiology. Conventional high-dimensional regression inference methods suffer from\ninaccurate type I errors and reduced power in heavy-tailed contexts, limiting\ntheir effectiveness. Our proposed approach leverages the partial Gini\ncovariance to construct a robust statistical inference framework that requires\nminimal tuning and does not impose restrictive moment conditions on error\ndistributions. Unlike traditional methods, it circumvents the need for\nestimating the density of random errors and enhances the computational\nfeasibility and robustness. Extensive simulations demonstrate the proposed\nmethod's superior power and robustness over standard high-dimensional inference\napproaches, such as those based on the debiased Lasso. The asymptotic relative\nefficiency analysis provides additional theoretical insight on the improved\nefficiency of the new approach in the heavy-tailed setting. Additionally, the\npartial Gini covariance extends to the multivariate setting, enabling\nchi-square testing for a group of coefficients. We illustrate the method's\npractical application with a real-world data example."
                },
                "authors": [
                    {
                        "name": "Yilin Zhang"
                    },
                    {
                        "name": "Songshan Yang"
                    },
                    {
                        "name": "Yunan Wu"
                    },
                    {
                        "name": "Lan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Lan Wang"
                },
                "author": "Lan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13491v1",
                "updated": "2024-11-20T17:39:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    39,
                    29,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T17:39:29Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    39,
                    29,
                    2,
                    325,
                    0
                ],
                "title": "Neural machine translation of seismic waves for petrophysical inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural machine translation of seismic waves for petrophysical inversion"
                },
                "summary": "Effective structural assessment of urban infrastructure is essential for\nsustainable land use and resilience to climate change and natural hazards.\nSeismic wave methods are widely applied in these areas for subsurface\ncharacterization and monitoring, yet they often rely on time-consuming\ninversion techniques that fall short in delivering comprehensive geological,\nhydrogeological, and geomechanical descriptions. Here, we explore the\neffectiveness of a passive seismic approach coupled with artificial\nintelligence (AI) for monitoring geological structures and hydrogeological\nconditions in the context of sinkhole hazard assessment. We introduce a\ndeterministic petrophysical inversion technique based on a language model that\ndecodes seismic wave velocity measurements to infer soil petrophysical and\nmechanical parameters as textual descriptions. Results successfully delineate\n3D subsurface structures with their respective soil nature and mechanical\ncharacteristics, while accurately predicting daily water table levels.\nValidation demonstrates high accuracy, with a normalized root mean square error\nof 8%, closely rivaling with conventional stochastic seismic inversion methods,\nwhile delivering broader insights into subsurface conditions 2,000 times\nfaster. These findings underscore the potential of advanced AI techniques to\nsignificantly enhance subsurface characterization across diverse scales,\nsupporting decision-making for natural hazard mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective structural assessment of urban infrastructure is essential for\nsustainable land use and resilience to climate change and natural hazards.\nSeismic wave methods are widely applied in these areas for subsurface\ncharacterization and monitoring, yet they often rely on time-consuming\ninversion techniques that fall short in delivering comprehensive geological,\nhydrogeological, and geomechanical descriptions. Here, we explore the\neffectiveness of a passive seismic approach coupled with artificial\nintelligence (AI) for monitoring geological structures and hydrogeological\nconditions in the context of sinkhole hazard assessment. We introduce a\ndeterministic petrophysical inversion technique based on a language model that\ndecodes seismic wave velocity measurements to infer soil petrophysical and\nmechanical parameters as textual descriptions. Results successfully delineate\n3D subsurface structures with their respective soil nature and mechanical\ncharacteristics, while accurately predicting daily water table levels.\nValidation demonstrates high accuracy, with a normalized root mean square error\nof 8%, closely rivaling with conventional stochastic seismic inversion methods,\nwhile delivering broader insights into subsurface conditions 2,000 times\nfaster. These findings underscore the potential of advanced AI techniques to\nsignificantly enhance subsurface characterization across diverse scales,\nsupporting decision-making for natural hazard mitigation."
                },
                "authors": [
                    {
                        "name": "JosÃ© Cunha Teixeira"
                    },
                    {
                        "name": "Ludovic Bodet"
                    },
                    {
                        "name": "AgnÃ¨s RiviÃ¨re"
                    },
                    {
                        "name": "Santiago G. Solazzi"
                    },
                    {
                        "name": "AmÃ©lie Hallier"
                    },
                    {
                        "name": "Alexandrine Gesret"
                    },
                    {
                        "name": "Sanae El Janyani"
                    },
                    {
                        "name": "Marine Dangeard"
                    },
                    {
                        "name": "Amine Dhemaied"
                    },
                    {
                        "name": "JosÃ©phine Boisson Gaboriau"
                    }
                ],
                "author_detail": {
                    "name": "JosÃ©phine Boisson Gaboriau"
                },
                "author": "JosÃ©phine Boisson Gaboriau",
                "arxiv_comment": "70 pages, 33 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13485v1",
                "updated": "2024-11-20T17:35:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    35,
                    21,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T17:35:21Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    35,
                    21,
                    2,
                    325,
                    0
                ],
                "title": "Utilizing Large Language Models to Synthesize Product Desirability\n  Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models to Synthesize Product Desirability\n  Datasets"
                },
                "summary": "This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production."
                },
                "authors": [
                    {
                        "name": "John D. Hastings"
                    },
                    {
                        "name": "Sherri Weitl-Harms"
                    },
                    {
                        "name": "Joseph Doty"
                    },
                    {
                        "name": "Zachary L. Myers"
                    },
                    {
                        "name": "Warren Thompson"
                    }
                ],
                "author_detail": {
                    "name": "Warren Thompson"
                },
                "author": "Warren Thompson",
                "arxiv_comment": "9 pages, 2 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.2.6; H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13477v1",
                "updated": "2024-11-20T17:23:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    23,
                    40,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T17:23:40Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    23,
                    40,
                    2,
                    325,
                    0
                ],
                "title": "PatentEdits: Framing Patent Novelty as Textual Entailment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatentEdits: Framing Patent Novelty as Textual Entailment"
                },
                "summary": "A patent must be deemed novel and non-obvious in order to be granted by the\nUS Patent Office (USPTO). If it is not, a US patent examiner will cite the\nprior work, or prior art, that invalidates the novelty and issue a non-final\nrejection. Predicting what claims of the invention should change given the\nprior art is an essential and crucial step in securing invention rights, yet\nhas not been studied before as a learnable task. In this work we introduce the\nPatentEdits dataset, which contains 105K examples of successful revisions that\novercome objections to novelty. We design algorithms to label edits sentence by\nsentence, then establish how well these edits can be predicted with large\nlanguage models (LLMs). We demonstrate that evaluating textual entailment\nbetween cited references and draft sentences is especially effective in\npredicting which inventive claims remained unchanged or are novel in relation\nto prior art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A patent must be deemed novel and non-obvious in order to be granted by the\nUS Patent Office (USPTO). If it is not, a US patent examiner will cite the\nprior work, or prior art, that invalidates the novelty and issue a non-final\nrejection. Predicting what claims of the invention should change given the\nprior art is an essential and crucial step in securing invention rights, yet\nhas not been studied before as a learnable task. In this work we introduce the\nPatentEdits dataset, which contains 105K examples of successful revisions that\novercome objections to novelty. We design algorithms to label edits sentence by\nsentence, then establish how well these edits can be predicted with large\nlanguage models (LLMs). We demonstrate that evaluating textual entailment\nbetween cited references and draft sentences is especially effective in\npredicting which inventive claims remained unchanged or are novel in relation\nto prior art."
                },
                "authors": [
                    {
                        "name": "Ryan Lee"
                    },
                    {
                        "name": "Alexander Spangher"
                    },
                    {
                        "name": "Xuezhe Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xuezhe Ma"
                },
                "author": "Xuezhe Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13476v1",
                "updated": "2024-11-20T17:22:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    22,
                    31,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T17:22:31Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    22,
                    31,
                    2,
                    325,
                    0
                ],
                "title": "When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context\n  Training"
                },
                "summary": "Extending context window sizes allows large language models (LLMs) to process\nlonger sequences and handle more complex tasks. Rotary Positional Embedding\n(RoPE) has become the de facto standard due to its relative positional encoding\nproperties that benefit long-context training. However, we observe that using\nRoPE with BFloat16 format results in numerical issues, causing it to deviate\nfrom its intended relative positional encoding, especially in long-context\nscenarios. This issue arises from BFloat16's limited precision and accumulates\nas context length increases, with the first token contributing significantly to\nthis problem. To address this, we develop AnchorAttention, a plug-and-play\nattention method that alleviates numerical issues caused by BFloat16, improves\nlong-context capabilities, and speeds up training. AnchorAttention reduces\nunnecessary attention computations, maintains semantic coherence, and boosts\ncomputational efficiency by treating the first token as a shared anchor with a\nconsistent position ID, making it visible to all documents within the training\ncontext. Experiments on three types of LLMs demonstrate that AnchorAttention\nsignificantly improves long-context performance and reduces training time by\nover 50\\% compared to standard full attention mechanisms, while preserving the\noriginal LLM's capabilities on general tasks. Our code is available at\nhttps://github.com/haonan3/AnchorContext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending context window sizes allows large language models (LLMs) to process\nlonger sequences and handle more complex tasks. Rotary Positional Embedding\n(RoPE) has become the de facto standard due to its relative positional encoding\nproperties that benefit long-context training. However, we observe that using\nRoPE with BFloat16 format results in numerical issues, causing it to deviate\nfrom its intended relative positional encoding, especially in long-context\nscenarios. This issue arises from BFloat16's limited precision and accumulates\nas context length increases, with the first token contributing significantly to\nthis problem. To address this, we develop AnchorAttention, a plug-and-play\nattention method that alleviates numerical issues caused by BFloat16, improves\nlong-context capabilities, and speeds up training. AnchorAttention reduces\nunnecessary attention computations, maintains semantic coherence, and boosts\ncomputational efficiency by treating the first token as a shared anchor with a\nconsistent position ID, making it visible to all documents within the training\ncontext. Experiments on three types of LLMs demonstrate that AnchorAttention\nsignificantly improves long-context performance and reduces training time by\nover 50\\% compared to standard full attention mechanisms, while preserving the\noriginal LLM's capabilities on general tasks. Our code is available at\nhttps://github.com/haonan3/AnchorContext."
                },
                "authors": [
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Tianyu Pang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Pang"
                },
                "author": "Tianyu Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13459v1",
                "updated": "2024-11-20T17:08:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    8,
                    38,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T17:08:38Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    8,
                    38,
                    2,
                    325,
                    0
                ],
                "title": "SoK: A Systems Perspective on Compound AI Threats and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: A Systems Perspective on Compound AI Threats and Countermeasures"
                },
                "summary": "Large language models (LLMs) used across enterprises often use proprietary\nmodels and operate on sensitive inputs and data. The wide range of attack\nvectors identified in prior research - targeting various software and hardware\ncomponents used in training and inference - makes it extremely challenging to\nenforce confidentiality and integrity policies.\n  As we advance towards constructing compound AI inference pipelines that\nintegrate multiple large language models (LLMs), the attack surfaces expand\nsignificantly. Attackers now focus on the AI algorithms as well as the software\nand hardware components associated with these systems. While current research\noften examines these elements in isolation, we find that combining cross-layer\nattack observations can enable powerful end-to-end attacks with minimal\nassumptions about the threat model. Given, the sheer number of existing attacks\nat each layer, we need a holistic and systemized understanding of different\nattack vectors at each layer.\n  This SoK discusses different software and hardware attacks applicable to\ncompound AI systems and demonstrates how combining multiple attack mechanisms\ncan reduce the threat model assumptions required for an isolated attack. Next,\nwe systematize the ML attacks in lines with the Mitre Att&ck framework to\nbetter position each attack based on the threat model. Finally, we outline the\nexisting countermeasures for both software and hardware layers and discuss the\nnecessity of a comprehensive defense strategy to enable the secure and\nhigh-performance deployment of compound AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) used across enterprises often use proprietary\nmodels and operate on sensitive inputs and data. The wide range of attack\nvectors identified in prior research - targeting various software and hardware\ncomponents used in training and inference - makes it extremely challenging to\nenforce confidentiality and integrity policies.\n  As we advance towards constructing compound AI inference pipelines that\nintegrate multiple large language models (LLMs), the attack surfaces expand\nsignificantly. Attackers now focus on the AI algorithms as well as the software\nand hardware components associated with these systems. While current research\noften examines these elements in isolation, we find that combining cross-layer\nattack observations can enable powerful end-to-end attacks with minimal\nassumptions about the threat model. Given, the sheer number of existing attacks\nat each layer, we need a holistic and systemized understanding of different\nattack vectors at each layer.\n  This SoK discusses different software and hardware attacks applicable to\ncompound AI systems and demonstrates how combining multiple attack mechanisms\ncan reduce the threat model assumptions required for an isolated attack. Next,\nwe systematize the ML attacks in lines with the Mitre Att&ck framework to\nbetter position each attack based on the threat model. Finally, we outline the\nexisting countermeasures for both software and hardware layers and discuss the\nnecessity of a comprehensive defense strategy to enable the secure and\nhigh-performance deployment of compound AI systems."
                },
                "authors": [
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Anjo Vahldiek-Oberwagner"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "arxiv_comment": "13 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14923v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14923v2",
                "updated": "2024-11-20T16:53:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    53,
                    28,
                    2,
                    325,
                    0
                ],
                "published": "2024-09-23T11:19:19Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    11,
                    19,
                    19,
                    0,
                    267,
                    0
                ],
                "title": "A NICER View of PSR J1231$-$1411: A Complex Case",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A NICER View of PSR J1231$-$1411: A Complex Case"
                },
                "summary": "Recent constraints on neutron star mass and radius have advanced our\nunderstanding of the equation of state (EOS) of cold dense matter. Some of them\nhave been obtained by modeling the pulses of three millisecond X-ray pulsars\nobserved by the Neutron Star Interior Composition Explorer (NICER). Here, we\npresent a Bayesian parameter inference for a fourth pulsar, PSR J1231$-$1411,\nusing the same technique with NICER and XMM-Newton data. When applying a broad\nmass-inclination prior from radio timing measurements and the emission region\ngeometry model that can best explain the data, we find likely converged results\nonly when using a limited radius prior. If limiting the radius to be consistent\nwith the previous observational constraints and EOS analyses, we infer the\nradius to be $12.6 \\pm 0.3$ km and the mass to be $1.04_{-0.03}^{+0.05}$\n$M_\\odot$, each reported as the posterior credible interval bounded by the\n$16\\,\\%$ and $84\\,\\%$ quantiles. If using an uninformative prior but limited\nbetween $10$ and $14$ km, we find otherwise similar results, but\n$R_{\\mathrm{eq}} = 13.5_{-0.5}^{+0.3}$ km for the radius. In both cases, we\nfind a nonantipodal hot region geometry where one emitting spot is at the\nequator or slightly above, surrounded by a large colder region, and where a\nnoncircular hot region lies close to southern rotational pole. If using a wider\nradius prior, we only find solutions that fit the data significantly worse. We\ndiscuss the challenges in finding the better fitting solutions, possibly\nrelated to the weak interpulse feature in the pulse profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent constraints on neutron star mass and radius have advanced our\nunderstanding of the equation of state (EOS) of cold dense matter. Some of them\nhave been obtained by modeling the pulses of three millisecond X-ray pulsars\nobserved by the Neutron Star Interior Composition Explorer (NICER). Here, we\npresent a Bayesian parameter inference for a fourth pulsar, PSR J1231$-$1411,\nusing the same technique with NICER and XMM-Newton data. When applying a broad\nmass-inclination prior from radio timing measurements and the emission region\ngeometry model that can best explain the data, we find likely converged results\nonly when using a limited radius prior. If limiting the radius to be consistent\nwith the previous observational constraints and EOS analyses, we infer the\nradius to be $12.6 \\pm 0.3$ km and the mass to be $1.04_{-0.03}^{+0.05}$\n$M_\\odot$, each reported as the posterior credible interval bounded by the\n$16\\,\\%$ and $84\\,\\%$ quantiles. If using an uninformative prior but limited\nbetween $10$ and $14$ km, we find otherwise similar results, but\n$R_{\\mathrm{eq}} = 13.5_{-0.5}^{+0.3}$ km for the radius. In both cases, we\nfind a nonantipodal hot region geometry where one emitting spot is at the\nequator or slightly above, surrounded by a large colder region, and where a\nnoncircular hot region lies close to southern rotational pole. If using a wider\nradius prior, we only find solutions that fit the data significantly worse. We\ndiscuss the challenges in finding the better fitting solutions, possibly\nrelated to the weak interpulse feature in the pulse profile."
                },
                "authors": [
                    {
                        "name": "Tuomo Salmi"
                    },
                    {
                        "name": "Julia S. Deneva"
                    },
                    {
                        "name": "Paul S. Ray"
                    },
                    {
                        "name": "Anna L. Watts"
                    },
                    {
                        "name": "Devarshi Choudhury"
                    },
                    {
                        "name": "Yves Kini"
                    },
                    {
                        "name": "Serena Vinciguerra"
                    },
                    {
                        "name": "H. Thankful Cromartie"
                    },
                    {
                        "name": "Michael T. Wolff"
                    },
                    {
                        "name": "Zaven Arzoumanian"
                    },
                    {
                        "name": "Slavko Bogdanov"
                    },
                    {
                        "name": "Keith Gendreau"
                    },
                    {
                        "name": "Sebastien Guillot"
                    },
                    {
                        "name": "Wynn C. G. Ho"
                    },
                    {
                        "name": "Sharon M. Morsink"
                    },
                    {
                        "name": "IsmaÃ«l Cognard"
                    },
                    {
                        "name": "Lucas Guillemot"
                    },
                    {
                        "name": "Gilles Theureau"
                    },
                    {
                        "name": "Matthew Kerr"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Kerr"
                },
                "author": "Matthew Kerr",
                "arxiv_doi": "10.3847/1538-4357/ad81d2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ad81d2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.14923v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14923v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, 13 figures (2 of which are figure sets), 3 tables,\n  published in ApJ",
                "arxiv_journal_ref": "ApJ 976 58 (2024)",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20886v2",
                "updated": "2024-11-20T16:47:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    47,
                    44,
                    2,
                    325,
                    0
                ],
                "published": "2024-10-28T10:12:06Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    10,
                    12,
                    6,
                    0,
                    302,
                    0
                ],
                "title": "CODES: Benchmarking Coupled ODE Surrogates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CODES: Benchmarking Coupled ODE Surrogates"
                },
                "summary": "We introduce CODES, a benchmark for comprehensive evaluation of surrogate\narchitectures for coupled ODE systems. Besides standard metrics like mean\nsquared error (MSE) and inference time, CODES provides insights into surrogate\nbehaviour across multiple dimensions like interpolation, extrapolation, sparse\ndata, uncertainty quantification and gradient correlation. The benchmark\nemphasizes usability through features such as integrated parallel training, a\nweb-based configuration generator, and pre-implemented baseline models and\ndatasets. Extensive documentation ensures sustainability and provides the\nfoundation for collaborative improvement. By offering a fair and multi-faceted\ncomparison, CODES helps researchers select the most suitable surrogate for\ntheir specific dataset and application while deepening our understanding of\nsurrogate learning behaviour.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce CODES, a benchmark for comprehensive evaluation of surrogate\narchitectures for coupled ODE systems. Besides standard metrics like mean\nsquared error (MSE) and inference time, CODES provides insights into surrogate\nbehaviour across multiple dimensions like interpolation, extrapolation, sparse\ndata, uncertainty quantification and gradient correlation. The benchmark\nemphasizes usability through features such as integrated parallel training, a\nweb-based configuration generator, and pre-implemented baseline models and\ndatasets. Extensive documentation ensures sustainability and provides the\nfoundation for collaborative improvement. By offering a fair and multi-faceted\ncomparison, CODES helps researchers select the most suitable surrogate for\ntheir specific dataset and application while deepening our understanding of\nsurrogate learning behaviour."
                },
                "authors": [
                    {
                        "name": "Robin Janssen"
                    },
                    {
                        "name": "Immanuel Sulzer"
                    },
                    {
                        "name": "Tobias Buck"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Buck"
                },
                "author": "Tobias Buck",
                "arxiv_comment": "13 pages, 10 figures, accepted for the Machine Learning and the\n  Physical Sciences workshop at NeurIPS 2024, source code available on GitHub\n  at https://github.com/robin-janssen/CODES-Benchmark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13425v1",
                "updated": "2024-11-20T16:09:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    9,
                    22,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T16:09:22Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    9,
                    22,
                    2,
                    325,
                    0
                ],
                "title": "WaterPark: A Robustness Assessment of Language Model Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaterPark: A Robustness Assessment of Language Model Watermarking"
                },
                "summary": "To mitigate the misuse of large language models (LLMs), such as\ndisinformation, automated phishing, and academic cheating, there is a pressing\nneed for the capability of identifying LLM-generated texts. Watermarking\nemerges as one promising solution: it plants statistical signals into LLMs'\ngenerative processes and subsequently verifies whether LLMs produce given\ntexts. Various watermarking methods (``watermarkers'') have been proposed; yet,\ndue to the lack of unified evaluation platforms, many critical questions remain\nunder-explored: i) What are the strengths/limitations of various watermarkers,\nespecially their attack robustness? ii) How do various design choices impact\ntheir robustness? iii) How to optimally operate watermarkers in adversarial\nenvironments?\n  To fill this gap, we systematize existing LLM watermarkers and watermark\nremoval attacks, mapping out their design spaces. We then develop WaterPark, a\nunified platform that integrates 10 state-of-the-art watermarkers and 12\nrepresentative attacks. More importantly, leveraging WaterPark, we conduct a\ncomprehensive assessment of existing watermarkers, unveiling the impact of\nvarious design choices on their attack robustness. For instance, a\nwatermarker's resilience to increasingly intensive attacks hinges on its\ncontext dependency. We further explore the best practices to operate\nwatermarkers in adversarial environments. For instance, using a generic\ndetector alongside a watermark-specific detector improves the security of\nvulnerable watermarkers. We believe our study sheds light on current LLM\nwatermarking techniques while WaterPark serves as a valuable testbed to\nfacilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To mitigate the misuse of large language models (LLMs), such as\ndisinformation, automated phishing, and academic cheating, there is a pressing\nneed for the capability of identifying LLM-generated texts. Watermarking\nemerges as one promising solution: it plants statistical signals into LLMs'\ngenerative processes and subsequently verifies whether LLMs produce given\ntexts. Various watermarking methods (``watermarkers'') have been proposed; yet,\ndue to the lack of unified evaluation platforms, many critical questions remain\nunder-explored: i) What are the strengths/limitations of various watermarkers,\nespecially their attack robustness? ii) How do various design choices impact\ntheir robustness? iii) How to optimally operate watermarkers in adversarial\nenvironments?\n  To fill this gap, we systematize existing LLM watermarkers and watermark\nremoval attacks, mapping out their design spaces. We then develop WaterPark, a\nunified platform that integrates 10 state-of-the-art watermarkers and 12\nrepresentative attacks. More importantly, leveraging WaterPark, we conduct a\ncomprehensive assessment of existing watermarkers, unveiling the impact of\nvarious design choices on their attack robustness. For instance, a\nwatermarker's resilience to increasingly intensive attacks hinges on its\ncontext dependency. We further explore the best practices to operate\nwatermarkers in adversarial environments. For instance, using a generic\ndetector alongside a watermark-specific detector improves the security of\nvulnerable watermarkers. We believe our study sheds light on current LLM\nwatermarking techniques while WaterPark serves as a valuable testbed to\nfacilitate future research."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Lauren Hong"
                    },
                    {
                        "name": "Shouling Ji"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13415v1",
                "updated": "2024-11-20T16:02:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    2,
                    14,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T16:02:14Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    2,
                    14,
                    2,
                    325,
                    0
                ],
                "title": "Unleashing the Power of Large Language Models for Group POI\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Power of Large Language Models for Group POI\n  Recommendations"
                },
                "summary": "Group Point-of-Interest (POI) recommendations aim to predict the next POI\nthat satisfies the diverse preferences of a group of users. This task is more\nchallenging than traditional individual POI recommendations due to complex\ngroup decision-making and extremely sparse group-level check-in data. Existing\nmethods for group POI recommendations primarily rely on single ID-based\nfeatures from check-in data, capturing only statistical correlations and\nfailing to fully utilize the rich semantic information contained in the\ncheck-ins, resulting in suboptimal performance. To this end, we propose a\nframework that unleashes the power of the Large Language Model (LLM) for\ncontext-aware group POI recommendations (LLMGPR). Our approach first introduces\nPOI tokens alongside the original word tokens of the LLM, which are initialized\nby applying the LLM to the rich information of each POI. We then propose a\nnovel sequencing adapter guided by Quantized Low-Rank Adaptation (QLORA) to\nmodify the LLM. The enhanced LLM can learn sequence representations by\ncombining semantic-enhanced POI tokens and rich contextual information\nincluding positional encodings and spatio-temporal differences. This approach\ncan be adapted for learning either group or user representations depending on\nthe sequence type. Furthermore, we enhance group representations by aggregating\nindividual member representations with another QLORA-based aggregation adapter\nand introducing a self-supervised learning task that predicts the purpose of\ncheck-in sequences, alleviating the data sparsity issue. Our experimental\nresults demonstrate that LLMGPR outperforms existing methods, effectively\naddressing group-level data sparsity and providing superior recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Point-of-Interest (POI) recommendations aim to predict the next POI\nthat satisfies the diverse preferences of a group of users. This task is more\nchallenging than traditional individual POI recommendations due to complex\ngroup decision-making and extremely sparse group-level check-in data. Existing\nmethods for group POI recommendations primarily rely on single ID-based\nfeatures from check-in data, capturing only statistical correlations and\nfailing to fully utilize the rich semantic information contained in the\ncheck-ins, resulting in suboptimal performance. To this end, we propose a\nframework that unleashes the power of the Large Language Model (LLM) for\ncontext-aware group POI recommendations (LLMGPR). Our approach first introduces\nPOI tokens alongside the original word tokens of the LLM, which are initialized\nby applying the LLM to the rich information of each POI. We then propose a\nnovel sequencing adapter guided by Quantized Low-Rank Adaptation (QLORA) to\nmodify the LLM. The enhanced LLM can learn sequence representations by\ncombining semantic-enhanced POI tokens and rich contextual information\nincluding positional encodings and spatio-temporal differences. This approach\ncan be adapted for learning either group or user representations depending on\nthe sequence type. Furthermore, we enhance group representations by aggregating\nindividual member representations with another QLORA-based aggregation adapter\nand introducing a self-supervised learning task that predicts the purpose of\ncheck-in sequences, alleviating the data sparsity issue. Our experimental\nresults demonstrate that LLMGPR outperforms existing methods, effectively\naddressing group-level data sparsity and providing superior recommendations."
                },
                "authors": [
                    {
                        "name": "Jing Long"
                    },
                    {
                        "name": "Liang Qu"
                    },
                    {
                        "name": "Guanhua Ye"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Quoc Viet Hung Nguyen"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13410v1",
                "updated": "2024-11-20T15:52:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    52,
                    3,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T15:52:03Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    52,
                    3,
                    2,
                    325,
                    0
                ],
                "title": "A Survey On Enhancing Reinforcement Learning in Complex Environments:\n  Insights from Human and LLM Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey On Enhancing Reinforcement Learning in Complex Environments:\n  Insights from Human and LLM Feedback"
                },
                "summary": "Reinforcement learning (RL) is one of the active fields in machine learning,\ndemonstrating remarkable potential in tackling real-world challenges. Despite\nits promising prospects, this methodology has encountered with issues and\nchallenges, hindering it from achieving the best performance. In particular,\nthese approaches lack decent performance when navigating environments and\nsolving tasks with large observation space, often resulting in\nsample-inefficiency and prolonged learning times. This issue, commonly referred\nto as the curse of dimensionality, complicates decision-making for RL agents,\nnecessitating a careful balance between attention and decision-making. RL\nagents, when augmented with human or large language models' (LLMs) feedback,\nmay exhibit resilience and adaptability, leading to enhanced performance and\naccelerated learning. Such feedback, conveyed through various modalities or\ngranularities including natural language, serves as a guide for RL agents,\naiding them in discerning relevant environmental cues and optimizing\ndecision-making processes. In this survey paper, we mainly focus on problems of\ntwo-folds: firstly, we focus on humans or an LLMs assistance, investigating the\nways in which these entities may collaborate with the RL agent in order to\nfoster optimal behavior and expedite learning; secondly, we delve into the\nresearch papers dedicated to addressing the intricacies of environments\ncharacterized by large observation space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is one of the active fields in machine learning,\ndemonstrating remarkable potential in tackling real-world challenges. Despite\nits promising prospects, this methodology has encountered with issues and\nchallenges, hindering it from achieving the best performance. In particular,\nthese approaches lack decent performance when navigating environments and\nsolving tasks with large observation space, often resulting in\nsample-inefficiency and prolonged learning times. This issue, commonly referred\nto as the curse of dimensionality, complicates decision-making for RL agents,\nnecessitating a careful balance between attention and decision-making. RL\nagents, when augmented with human or large language models' (LLMs) feedback,\nmay exhibit resilience and adaptability, leading to enhanced performance and\naccelerated learning. Such feedback, conveyed through various modalities or\ngranularities including natural language, serves as a guide for RL agents,\naiding them in discerning relevant environmental cues and optimizing\ndecision-making processes. In this survey paper, we mainly focus on problems of\ntwo-folds: firstly, we focus on humans or an LLMs assistance, investigating the\nways in which these entities may collaborate with the RL agent in order to\nfoster optimal behavior and expedite learning; secondly, we delve into the\nresearch papers dedicated to addressing the intricacies of environments\ncharacterized by large observation space."
                },
                "authors": [
                    {
                        "name": "Alireza Rashidi Laleh"
                    },
                    {
                        "name": "Majid Nili Ahmadabadi"
                    }
                ],
                "author_detail": {
                    "name": "Majid Nili Ahmadabadi"
                },
                "author": "Majid Nili Ahmadabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13409v1",
                "updated": "2024-11-20T15:48:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    48,
                    21,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T15:48:21Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    48,
                    21,
                    2,
                    325,
                    0
                ],
                "title": "Unification of Balti and trans-border sister dialects in the essence of\n  LLMs and AI Technology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unification of Balti and trans-border sister dialects in the essence of\n  LLMs and AI Technology"
                },
                "summary": "The language called Balti belongs to the Sino-Tibetan, specifically the\nTibeto-Burman language family. It is understood with variations, across\npopulations in India, China, Pakistan, Nepal, Tibet, Burma, and Bhutan,\ninfluenced by local cultures and producing various dialects. Considering the\ndiverse cultural, socio-political, religious, and geographical impacts, it is\nimportant to step forward unifying the dialects, the basis of common root,\nlexica, and phonological perspectives, is vital. In the era of globalization\nand the increasingly frequent developments in AI technology, understanding the\ndiversity and the efforts of dialect unification is important to understanding\ncommonalities and shortening the gaps impacted by unavoidable circumstances.\nThis article analyzes and examines how artificial intelligence AI in the\nessence of Large Language Models LLMs, can assist in analyzing, documenting,\nand standardizing the endangered Balti Language, based on the efforts made in\ndifferent dialects so far.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The language called Balti belongs to the Sino-Tibetan, specifically the\nTibeto-Burman language family. It is understood with variations, across\npopulations in India, China, Pakistan, Nepal, Tibet, Burma, and Bhutan,\ninfluenced by local cultures and producing various dialects. Considering the\ndiverse cultural, socio-political, religious, and geographical impacts, it is\nimportant to step forward unifying the dialects, the basis of common root,\nlexica, and phonological perspectives, is vital. In the era of globalization\nand the increasingly frequent developments in AI technology, understanding the\ndiversity and the efforts of dialect unification is important to understanding\ncommonalities and shortening the gaps impacted by unavoidable circumstances.\nThis article analyzes and examines how artificial intelligence AI in the\nessence of Large Language Models LLMs, can assist in analyzing, documenting,\nand standardizing the endangered Balti Language, based on the efforts made in\ndifferent dialects so far."
                },
                "authors": [
                    {
                        "name": "Muhammad Sharif"
                    },
                    {
                        "name": "Jiangyan Yi"
                    },
                    {
                        "name": "Muhammad Shoaib"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shoaib"
                },
                "author": "Muhammad Shoaib",
                "arxiv_comment": "Accepted by IEEE conference ISCSLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13407v1",
                "updated": "2024-11-20T15:46:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    46,
                    48,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T15:46:48Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    46,
                    48,
                    2,
                    325,
                    0
                ],
                "title": "Transformer-Based Contextualized Language Models Joint with Neural\n  Networks for Natural Language Inference in Vietnamese",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-Based Contextualized Language Models Joint with Neural\n  Networks for Natural Language Inference in Vietnamese"
                },
                "summary": "Natural Language Inference (NLI) is a task within Natural Language Processing\n(NLP) that holds value for various AI applications. However, there have been\nlimited studies on Natural Language Inference in Vietnamese that explore the\nconcept of joint models. Therefore, we conducted experiments using various\ncombinations of contextualized language models (CLM) and neural networks. We\nuse CLM to create contextualized work presentations and use Neural Networks for\nclassification. Furthermore, we have evaluated the strengths and weaknesses of\neach joint model and identified the model failure points in the Vietnamese\ncontext. The highest F1 score in this experiment, up to 82.78\\% in the\nbenchmark dataset (ViNLI). By conducting experiments with various models, the\nmost considerable size of the CLM is XLM-R (355M). That combination has\nconsistently demonstrated superior performance compared to fine-tuning strong\npre-trained language models like PhoBERT (+6.58\\%), mBERT (+19.08\\%), and XLM-R\n(+0.94\\%) in terms of F1-score. This article aims to introduce a novel approach\nor model that attains improved performance for Vietnamese NLI. Overall, we find\nthat the joint approach of CLM and neural networks is simple yet capable of\nachieving high-quality performance, which makes it suitable for applications\nthat require efficient resource utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Inference (NLI) is a task within Natural Language Processing\n(NLP) that holds value for various AI applications. However, there have been\nlimited studies on Natural Language Inference in Vietnamese that explore the\nconcept of joint models. Therefore, we conducted experiments using various\ncombinations of contextualized language models (CLM) and neural networks. We\nuse CLM to create contextualized work presentations and use Neural Networks for\nclassification. Furthermore, we have evaluated the strengths and weaknesses of\neach joint model and identified the model failure points in the Vietnamese\ncontext. The highest F1 score in this experiment, up to 82.78\\% in the\nbenchmark dataset (ViNLI). By conducting experiments with various models, the\nmost considerable size of the CLM is XLM-R (355M). That combination has\nconsistently demonstrated superior performance compared to fine-tuning strong\npre-trained language models like PhoBERT (+6.58\\%), mBERT (+19.08\\%), and XLM-R\n(+0.94\\%) in terms of F1-score. This article aims to introduce a novel approach\nor model that attains improved performance for Vietnamese NLI. Overall, we find\nthat the joint approach of CLM and neural networks is simple yet capable of\nachieving high-quality performance, which makes it suitable for applications\nthat require efficient resource utilization."
                },
                "authors": [
                    {
                        "name": "Dat Van-Thanh Nguyen"
                    },
                    {
                        "name": "Tin Van Huynh"
                    },
                    {
                        "name": "Kiet Van Nguyen"
                    },
                    {
                        "name": "Ngan Luu-Thuy Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Ngan Luu-Thuy Nguyen"
                },
                "author": "Ngan Luu-Thuy Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13405v1",
                "updated": "2024-11-20T15:45:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    45,
                    8,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T15:45:08Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    45,
                    8,
                    2,
                    325,
                    0
                ],
                "title": "On the Way to LLM Personalization: Learning to Remember User\n  Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Way to LLM Personalization: Learning to Remember User\n  Conversations"
                },
                "summary": "Large Language Models (LLMs) have quickly become an invaluable assistant for\na variety of tasks. However, their effectiveness is constrained by their\nability to tailor responses to human preferences and behaviors via\npersonalization. Prior work in LLM personalization has largely focused on style\ntransfer or incorporating small factoids about the user, as knowledge injection\nremains an open challenge. In this paper, we explore injecting knowledge of\nprior conversations into LLMs to enable future work on less redundant,\npersonalized conversations. We identify two real-world constraints: (1)\nconversations are sequential in time and must be treated as such during\ntraining, and (2) per-user personalization is only viable in\nparameter-efficient settings. To this aim, we propose PLUM, a pipeline\nperforming data augmentation for up-sampling conversations as question-answer\npairs, that are then used to finetune a low-rank adaptation adapter with a\nweighted cross entropy loss. Even in this first exploration of the problem, we\nperform competitively with baselines such as RAG, attaining an accuracy of\n81.5% across 100 conversations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have quickly become an invaluable assistant for\na variety of tasks. However, their effectiveness is constrained by their\nability to tailor responses to human preferences and behaviors via\npersonalization. Prior work in LLM personalization has largely focused on style\ntransfer or incorporating small factoids about the user, as knowledge injection\nremains an open challenge. In this paper, we explore injecting knowledge of\nprior conversations into LLMs to enable future work on less redundant,\npersonalized conversations. We identify two real-world constraints: (1)\nconversations are sequential in time and must be treated as such during\ntraining, and (2) per-user personalization is only viable in\nparameter-efficient settings. To this aim, we propose PLUM, a pipeline\nperforming data augmentation for up-sampling conversations as question-answer\npairs, that are then used to finetune a low-rank adaptation adapter with a\nweighted cross entropy loss. Even in this first exploration of the problem, we\nperform competitively with baselines such as RAG, attaining an accuracy of\n81.5% across 100 conversations."
                },
                "authors": [
                    {
                        "name": "Lucie Charlotte Magister"
                    },
                    {
                        "name": "Katherine Metcalf"
                    },
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Maartje ter Hoeve"
                    }
                ],
                "author_detail": {
                    "name": "Maartje ter Hoeve"
                },
                "author": "Maartje ter Hoeve",
                "arxiv_comment": "16 pages, 6 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13404v1",
                "updated": "2024-11-20T15:43:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    43,
                    58,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T15:43:58Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    43,
                    58,
                    2,
                    325,
                    0
                ],
                "title": "Issues with Input-Space Representation in Nonlinear Data-Based\n  Dissipativity Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Issues with Input-Space Representation in Nonlinear Data-Based\n  Dissipativity Estimation"
                },
                "summary": "In data-based control, dissipativity can be a powerful tool for attaining\nstability guarantees for nonlinear systems if that dissipativity can be\ninferred from data. This work provides a tutorial on several existing methods\nfor data-based dissipativity estimation of nonlinear systems. The interplay\nbetween the underlying assumptions of these methods and their sample complexity\nis investigated. It is shown that methods based on delta-covering result in an\nintractable trade-off between sample complexity and robustness. A new method is\nproposed to quantify the robustness of machine learning-based dissipativity\nestimation. It is shown that this method achieves a more tractable trade-off\nbetween robustness and sample complexity. Several numerical case studies\ndemonstrate the results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In data-based control, dissipativity can be a powerful tool for attaining\nstability guarantees for nonlinear systems if that dissipativity can be\ninferred from data. This work provides a tutorial on several existing methods\nfor data-based dissipativity estimation of nonlinear systems. The interplay\nbetween the underlying assumptions of these methods and their sample complexity\nis investigated. It is shown that methods based on delta-covering result in an\nintractable trade-off between sample complexity and robustness. A new method is\nproposed to quantify the robustness of machine learning-based dissipativity\nestimation. It is shown that this method achieves a more tractable trade-off\nbetween robustness and sample complexity. Several numerical case studies\ndemonstrate the results."
                },
                "authors": [
                    {
                        "name": "Ethan LoCicero"
                    },
                    {
                        "name": "Alex Penne"
                    },
                    {
                        "name": "Leila Bridgeman"
                    }
                ],
                "author_detail": {
                    "name": "Leila Bridgeman"
                },
                "author": "Leila Bridgeman",
                "arxiv_comment": "Preprint of conference manuscript, currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08435v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08435v3",
                "updated": "2024-11-20T15:41:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    41,
                    38,
                    2,
                    325,
                    0
                ],
                "published": "2024-09-13T00:03:19Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    0,
                    3,
                    19,
                    4,
                    257,
                    0
                ],
                "title": "When Context Leads but Parametric Memory Follows in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Context Leads but Parametric Memory Follows in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable progress in\nleveraging diverse knowledge sources. This study investigates how nine widely\nused LLMs allocate knowledge between local context and global parameters when\nanswering open-ended questions in knowledge-consistent scenarios. We introduce\na novel dataset, WikiAtomic, and systematically vary context sizes to analyze\nhow LLMs prioritize and utilize the provided information and their parametric\nknowledge in knowledge-consistent scenarios. Additionally, we also study their\ntendency to hallucinate under varying context sizes. Our findings reveal\nconsistent patterns across models, including a consistent reliance on both\ncontextual (around 70%) and parametric (around 30%) knowledge, and a decrease\nin hallucinations with increasing context. These insights highlight the\nimportance of more effective context organization and developing models that\nuse input more deterministically for robust performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable progress in\nleveraging diverse knowledge sources. This study investigates how nine widely\nused LLMs allocate knowledge between local context and global parameters when\nanswering open-ended questions in knowledge-consistent scenarios. We introduce\na novel dataset, WikiAtomic, and systematically vary context sizes to analyze\nhow LLMs prioritize and utilize the provided information and their parametric\nknowledge in knowledge-consistent scenarios. Additionally, we also study their\ntendency to hallucinate under varying context sizes. Our findings reveal\nconsistent patterns across models, including a consistent reliance on both\ncontextual (around 70%) and parametric (around 30%) knowledge, and a decrease\nin hallucinations with increasing context. These insights highlight the\nimportance of more effective context organization and developing models that\nuse input more deterministically for robust performance."
                },
                "authors": [
                    {
                        "name": "Yufei Tao"
                    },
                    {
                        "name": "Adam Hiatt"
                    },
                    {
                        "name": "Erik Haake"
                    },
                    {
                        "name": "Antonie J. Jetter"
                    },
                    {
                        "name": "Ameeta Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Ameeta Agrawal"
                },
                "author": "Ameeta Agrawal",
                "arxiv_comment": "Accepted by EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08435v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08435v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13402v1",
                "updated": "2024-11-20T15:41:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    41,
                    14,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T15:41:14Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    41,
                    14,
                    2,
                    325,
                    0
                ],
                "title": "Extraction of gravitational wave signals in realistic LISA data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extraction of gravitational wave signals in realistic LISA data"
                },
                "summary": "The Laser Interferometer Space Antenna (LISA) mission is being developed by\nESA with NASA participation. As it has recently passed the Mission Adoption\nmilestone, models of the instruments and noise performance are becoming more\ndetailed, and likewise prototype data analyses must as well. Assumptions such\nas Gaussianity, Stationarity, and continuous data continuity are unrealistic,\nand must be replaced with physically motivated data simulations, and data\nanalysis methods adapted to accommodate such likely imperfections. To this end,\nthe LISA Data Challenges have produced datasets featuring time-varying and\nunequal constellation armlength, and measurement artifacts including data\ninterruptions and instrumental transients. In this work, we assess the impact\nof these data artifacts on the inference of Galactic Binary and Massive Black\nHole properties. Our analysis shows that the treatment of noise transients and\ngaps is necessary for effective parameter estimation. We find that\nstraightforward mitigation techniques can significantly suppress artifacts,\nalbeit leaving a non-negligible impact on aspects of the science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Laser Interferometer Space Antenna (LISA) mission is being developed by\nESA with NASA participation. As it has recently passed the Mission Adoption\nmilestone, models of the instruments and noise performance are becoming more\ndetailed, and likewise prototype data analyses must as well. Assumptions such\nas Gaussianity, Stationarity, and continuous data continuity are unrealistic,\nand must be replaced with physically motivated data simulations, and data\nanalysis methods adapted to accommodate such likely imperfections. To this end,\nthe LISA Data Challenges have produced datasets featuring time-varying and\nunequal constellation armlength, and measurement artifacts including data\ninterruptions and instrumental transients. In this work, we assess the impact\nof these data artifacts on the inference of Galactic Binary and Massive Black\nHole properties. Our analysis shows that the treatment of noise transients and\ngaps is necessary for effective parameter estimation. We find that\nstraightforward mitigation techniques can significantly suppress artifacts,\nalbeit leaving a non-negligible impact on aspects of the science."
                },
                "authors": [
                    {
                        "name": "Eleonora Castelli"
                    },
                    {
                        "name": "Quentin Baghi"
                    },
                    {
                        "name": "John G. Baker"
                    },
                    {
                        "name": "Jacob Slutsky"
                    },
                    {
                        "name": "JÃ©rÃ´me Bobin"
                    },
                    {
                        "name": "Nikolaos Karnesis"
                    },
                    {
                        "name": "Antoine Petiteau"
                    },
                    {
                        "name": "Orion Sauter"
                    },
                    {
                        "name": "Peter Wass"
                    },
                    {
                        "name": "William J. Weber"
                    }
                ],
                "author_detail": {
                    "name": "William J. Weber"
                },
                "author": "William J. Weber",
                "arxiv_comment": "28 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13383v1",
                "updated": "2024-11-20T15:13:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    13,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T15:13:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    13,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Adversarial Diffusion Compression for Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Diffusion Compression for Real-World Image Super-Resolution"
                },
                "summary": "Real-world image super-resolution (Real-ISR) aims to reconstruct\nhigh-resolution images from low-resolution inputs degraded by complex, unknown\nprocesses. While many Stable Diffusion (SD)-based Real-ISR methods have\nachieved remarkable success, their slow, multi-step inference hinders practical\ndeployment. Recent SD-based one-step networks like OSEDiff and S3Diff alleviate\nthis issue but still incur high computational costs due to their reliance on\nlarge pretrained SD models. This paper proposes a novel Real-ISR method, AdcSR,\nby distilling the one-step diffusion network OSEDiff into a streamlined\ndiffusion-GAN model under our Adversarial Diffusion Compression (ADC)\nframework. We meticulously examine the modules of OSEDiff, categorizing them\ninto two types: (1) Removable (VAE encoder, prompt extractor, text encoder,\netc.) and (2) Prunable (denoising UNet and VAE decoder). Since direct removal\nand pruning can degrade the model's generation capability, we pretrain our\npruned VAE decoder to restore its ability to decode images and employ\nadversarial distillation to compensate for performance loss. This ADC-based\ndiffusion-GAN hybrid design effectively reduces complexity by 73% in inference\ntime, 78% in computation, and 74% in parameters, while preserving the model's\ngeneration capability. Experiments manifest that our proposed AdcSR achieves\ncompetitive recovery quality on both synthetic and real-world datasets,\noffering up to 9.3$\\times$ speedup over previous one-step diffusion-based\nmethods. Code and models will be made available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world image super-resolution (Real-ISR) aims to reconstruct\nhigh-resolution images from low-resolution inputs degraded by complex, unknown\nprocesses. While many Stable Diffusion (SD)-based Real-ISR methods have\nachieved remarkable success, their slow, multi-step inference hinders practical\ndeployment. Recent SD-based one-step networks like OSEDiff and S3Diff alleviate\nthis issue but still incur high computational costs due to their reliance on\nlarge pretrained SD models. This paper proposes a novel Real-ISR method, AdcSR,\nby distilling the one-step diffusion network OSEDiff into a streamlined\ndiffusion-GAN model under our Adversarial Diffusion Compression (ADC)\nframework. We meticulously examine the modules of OSEDiff, categorizing them\ninto two types: (1) Removable (VAE encoder, prompt extractor, text encoder,\netc.) and (2) Prunable (denoising UNet and VAE decoder). Since direct removal\nand pruning can degrade the model's generation capability, we pretrain our\npruned VAE decoder to restore its ability to decode images and employ\nadversarial distillation to compensate for performance loss. This ADC-based\ndiffusion-GAN hybrid design effectively reduces complexity by 73% in inference\ntime, 78% in computation, and 74% in parameters, while preserving the model's\ngeneration capability. Experiments manifest that our proposed AdcSR achieves\ncompetitive recovery quality on both synthetic and real-world datasets,\noffering up to 9.3$\\times$ speedup over previous one-step diffusion-based\nmethods. Code and models will be made available."
                },
                "authors": [
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Gehui Li"
                    },
                    {
                        "name": "Rongyuan Wu"
                    },
                    {
                        "name": "Xindong Zhang"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13372v1",
                "updated": "2024-11-20T14:52:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    9,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:09Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    9,
                    2,
                    325,
                    0
                ],
                "title": "Clustering with Potential Multidimensionality: Inference and Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustering with Potential Multidimensionality: Inference and Practice"
                },
                "summary": "We show how clustering standard errors in one or more dimensions can be\njustified in M-estimation when there is sampling or assignment uncertainty.\nSince existing procedures for variance estimation are either conservative or\ninvalid, we propose a variance estimator that refines a conservative procedure\nand remains valid. We then interpret environments where clustering is\nfrequently employed in empirical work from our design-based perspective and\nprovide insights on their estimands and inference procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show how clustering standard errors in one or more dimensions can be\njustified in M-estimation when there is sampling or assignment uncertainty.\nSince existing procedures for variance estimation are either conservative or\ninvalid, we propose a variance estimator that refines a conservative procedure\nand remains valid. We then interpret environments where clustering is\nfrequently employed in empirical work from our design-based perspective and\nprovide insights on their estimands and inference procedures."
                },
                "authors": [
                    {
                        "name": "Ruonan Xu"
                    },
                    {
                        "name": "Luther Yap"
                    }
                ],
                "author_detail": {
                    "name": "Luther Yap"
                },
                "author": "Luther Yap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13361v1",
                "updated": "2024-11-20T14:35:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    35,
                    16,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:35:16Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    35,
                    16,
                    2,
                    325,
                    0
                ],
                "title": "Integration of Active Learning and MCMC Sampling for Efficient Bayesian\n  Calibration of Mechanical Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integration of Active Learning and MCMC Sampling for Efficient Bayesian\n  Calibration of Mechanical Properties"
                },
                "summary": "Recent advancements in Markov chain Monte Carlo (MCMC) sampling and surrogate\nmodelling have significantly enhanced the feasibility of Bayesian analysis\nacross engineering fields. However, the selection and integration of surrogate\nmodels and cutting-edge MCMC algorithms, often depend on ad-hoc decisions. A\nsystematic assessment of their combined influence on analytical accuracy and\nefficiency is notably lacking. The present work offers a comprehensive\ncomparative study, employing a scalable case study in computational mechanics\nfocused on the inference of spatially varying material parameters, that sheds\nlight on the impact of methodological choices for surrogate modelling and\nsampling. We show that a priori training of the surrogate model introduces\nlarge errors in the posterior estimation even in low to moderate dimensions. We\nintroduce a simple active learning strategy based on the path of the MCMC\nalgorithm that is superior to all a priori trained models, and determine its\ntraining data requirements. We demonstrate that the choice of the MCMC\nalgorithm has only a small influence on the amount of training data but no\nsignificant influence on the accuracy of the resulting surrogate model.\nFurther, we show that the accuracy of the posterior estimation largely depends\non the surrogate model, but not even a tailored surrogate guarantees\nconvergence of the MCMC.Finally, we identify the forward model as the\nbottleneck in the inference process, not the MCMC algorithm. While related\nworks focus on employing advanced MCMC algorithms, we demonstrate that the\ntraining data requirements render the surrogate modelling approach infeasible\nbefore the benefits of these gradient-based MCMC algorithms on cheap models can\nbe reaped.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Markov chain Monte Carlo (MCMC) sampling and surrogate\nmodelling have significantly enhanced the feasibility of Bayesian analysis\nacross engineering fields. However, the selection and integration of surrogate\nmodels and cutting-edge MCMC algorithms, often depend on ad-hoc decisions. A\nsystematic assessment of their combined influence on analytical accuracy and\nefficiency is notably lacking. The present work offers a comprehensive\ncomparative study, employing a scalable case study in computational mechanics\nfocused on the inference of spatially varying material parameters, that sheds\nlight on the impact of methodological choices for surrogate modelling and\nsampling. We show that a priori training of the surrogate model introduces\nlarge errors in the posterior estimation even in low to moderate dimensions. We\nintroduce a simple active learning strategy based on the path of the MCMC\nalgorithm that is superior to all a priori trained models, and determine its\ntraining data requirements. We demonstrate that the choice of the MCMC\nalgorithm has only a small influence on the amount of training data but no\nsignificant influence on the accuracy of the resulting surrogate model.\nFurther, we show that the accuracy of the posterior estimation largely depends\non the surrogate model, but not even a tailored surrogate guarantees\nconvergence of the MCMC.Finally, we identify the forward model as the\nbottleneck in the inference process, not the MCMC algorithm. While related\nworks focus on employing advanced MCMC algorithms, we demonstrate that the\ntraining data requirements render the surrogate modelling approach infeasible\nbefore the benefits of these gradient-based MCMC algorithms on cheap models can\nbe reaped."
                },
                "authors": [
                    {
                        "name": "Leon Riccius"
                    },
                    {
                        "name": "Iuri B. C. M. Rocha"
                    },
                    {
                        "name": "Joris Bierkens"
                    },
                    {
                        "name": "Hanne Kekkonen"
                    },
                    {
                        "name": "Frans P. van der Meer"
                    }
                ],
                "author_detail": {
                    "name": "Frans P. van der Meer"
                },
                "author": "Frans P. van der Meer",
                "arxiv_comment": "28 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "74S60",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13343v1",
                "updated": "2024-11-20T14:15:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    15,
                    18,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:15:18Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    15,
                    18,
                    2,
                    325,
                    0
                ],
                "title": "Fact-Level Confidence Calibration and Self-Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact-Level Confidence Calibration and Self-Correction"
                },
                "summary": "Confidence calibration in LLMs, i.e., aligning their self-assessed confidence\nwith the actual accuracy of their responses, enabling them to self-evaluate the\ncorrectness of their outputs. However, current calibration methods for LLMs\ntypically estimate two scalars to represent overall response confidence and\ncorrectness, which is inadequate for long-form generation where the response\nincludes multiple atomic facts and may be partially confident and correct.\nThese methods also overlook the relevance of each fact to the query. To address\nthese challenges, we propose a Fact-Level Calibration framework that operates\nat a finer granularity, calibrating confidence to relevance-weighted\ncorrectness at the fact level. Furthermore, comprehensive analysis under the\nframework inspired the development of Confidence-Guided Fact-level\nSelf-Correction ($\\textbf{ConFix}$), which uses high-confidence facts within a\nresponse as additional knowledge to improve low-confidence ones. Extensive\nexperiments across four datasets and six models demonstrate that ConFix\neffectively mitigates hallucinations without requiring external knowledge\nsources such as retrieval systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence calibration in LLMs, i.e., aligning their self-assessed confidence\nwith the actual accuracy of their responses, enabling them to self-evaluate the\ncorrectness of their outputs. However, current calibration methods for LLMs\ntypically estimate two scalars to represent overall response confidence and\ncorrectness, which is inadequate for long-form generation where the response\nincludes multiple atomic facts and may be partially confident and correct.\nThese methods also overlook the relevance of each fact to the query. To address\nthese challenges, we propose a Fact-Level Calibration framework that operates\nat a finer granularity, calibrating confidence to relevance-weighted\ncorrectness at the fact level. Furthermore, comprehensive analysis under the\nframework inspired the development of Confidence-Guided Fact-level\nSelf-Correction ($\\textbf{ConFix}$), which uses high-confidence facts within a\nresponse as additional knowledge to improve low-confidence ones. Extensive\nexperiments across four datasets and six models demonstrate that ConFix\neffectively mitigates hallucinations without requiring external knowledge\nsources such as retrieval systems."
                },
                "authors": [
                    {
                        "name": "Yige Yuan"
                    },
                    {
                        "name": "Bingbing Xu"
                    },
                    {
                        "name": "Hexiang Tan"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Teng Xiao"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Code is available at https://github.com/yuanyige/fact-calibration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13333v1",
                "updated": "2024-11-20T14:02:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    2,
                    53,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:02:53Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    2,
                    53,
                    2,
                    325,
                    0
                ],
                "title": "Reanalyzing the ringdown signal of GW150914 using the F-statistic method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reanalyzing the ringdown signal of GW150914 using the F-statistic method"
                },
                "summary": "The ringdown phase of a gravitational wave (GW) signal from a binary black\nhole merger provides valuable insights into the properties of the final black\nhole and serves as a critical test of general relativity in the strong-field\nregime. A key aspect of this investigation is to determine whether the first\novertone mode exists in real GW data, as its presence would offer significant\nimplications for our understanding of general relativity under extreme\nconditions. To address this, we conducted a reanalysis of the ringdown signal\nfrom GW150914, using the newly proposed F-statistic method to search for the\nfirst overtone mode. Our results are consistent with those obtained through\nclassical time-domain Bayesian inference, indicating that there is no evidence\nof the first overtone mode in the ringdown signal of GW150914. However, our\nresults show the potentiality of utilizing the F-statistic methodology to\nunearth nuanced features within GW signals, thereby contributing novel insights\ninto black hole properties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ringdown phase of a gravitational wave (GW) signal from a binary black\nhole merger provides valuable insights into the properties of the final black\nhole and serves as a critical test of general relativity in the strong-field\nregime. A key aspect of this investigation is to determine whether the first\novertone mode exists in real GW data, as its presence would offer significant\nimplications for our understanding of general relativity under extreme\nconditions. To address this, we conducted a reanalysis of the ringdown signal\nfrom GW150914, using the newly proposed F-statistic method to search for the\nfirst overtone mode. Our results are consistent with those obtained through\nclassical time-domain Bayesian inference, indicating that there is no evidence\nof the first overtone mode in the ringdown signal of GW150914. However, our\nresults show the potentiality of utilizing the F-statistic methodology to\nunearth nuanced features within GW signals, thereby contributing novel insights\ninto black hole properties."
                },
                "authors": [
                    {
                        "name": "Hai-Tian Wang"
                    },
                    {
                        "name": "Ziming Wang"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Garvin Yim"
                    },
                    {
                        "name": "Lijing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Lijing Shao"
                },
                "author": "Lijing Shao",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13323v1",
                "updated": "2024-11-20T13:46:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    13,
                    46,
                    4,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T13:46:04Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    13,
                    46,
                    4,
                    2,
                    325,
                    0
                ],
                "title": "Are Large Language Models Memorizing Bug Benchmarks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Memorizing Bug Benchmarks?"
                },
                "summary": "Large Language Models (LLMs) have become integral to various software\nengineering tasks, including code generation, bug detection, and repair. To\nevaluate model performance in these domains, numerous bug benchmarks containing\nreal-world bugs from software projects have been developed. However, a growing\nconcern within the software engineering community is that these benchmarks may\nnot reliably reflect true LLM performance due to the risk of data leakage.\nDespite this concern, limited research has been conducted to quantify the\nimpact of potential leakage.\n  In this paper, we systematically evaluate popular LLMs to assess their\nsusceptibility to data leakage from widely used bug benchmarks. To identify\npotential leakage, we use multiple metrics, including a study of benchmark\nmembership within commonly used training datasets, as well as analyses of\nnegative log-likelihood and n-gram accuracy. Our findings show that certain\nmodels, in particular codegen-multi, exhibit significant evidence of\nmemorization in widely used benchmarks like Defects4J, while newer models\ntrained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.\nThese results highlight the need for careful benchmark selection and the\nadoption of robust metrics to adequately assess models capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become integral to various software\nengineering tasks, including code generation, bug detection, and repair. To\nevaluate model performance in these domains, numerous bug benchmarks containing\nreal-world bugs from software projects have been developed. However, a growing\nconcern within the software engineering community is that these benchmarks may\nnot reliably reflect true LLM performance due to the risk of data leakage.\nDespite this concern, limited research has been conducted to quantify the\nimpact of potential leakage.\n  In this paper, we systematically evaluate popular LLMs to assess their\nsusceptibility to data leakage from widely used bug benchmarks. To identify\npotential leakage, we use multiple metrics, including a study of benchmark\nmembership within commonly used training datasets, as well as analyses of\nnegative log-likelihood and n-gram accuracy. Our findings show that certain\nmodels, in particular codegen-multi, exhibit significant evidence of\nmemorization in widely used benchmarks like Defects4J, while newer models\ntrained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.\nThese results highlight the need for careful benchmark selection and the\nadoption of robust metrics to adequately assess models capabilities."
                },
                "authors": [
                    {
                        "name": "Daniel Ramos"
                    },
                    {
                        "name": "Claudia Mamede"
                    },
                    {
                        "name": "Kush Jain"
                    },
                    {
                        "name": "Paulo Canelas"
                    },
                    {
                        "name": "Catarina Gamboa"
                    },
                    {
                        "name": "Claire Le Goues"
                    }
                ],
                "author_detail": {
                    "name": "Claire Le Goues"
                },
                "author": "Claire Le Goues",
                "arxiv_comment": "pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11730v2",
                "updated": "2024-11-20T13:01:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    13,
                    1,
                    18,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-18T16:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    16,
                    59,
                    44,
                    0,
                    323,
                    0
                ],
                "title": "Lifted Model Construction without Normalisation: A Vectorised Approach\n  to Exploit Symmetries in Factor Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifted Model Construction without Normalisation: A Vectorised Approach\n  to Exploit Symmetries in Factor Graphs"
                },
                "summary": "Lifted probabilistic inference exploits symmetries in a probabilistic model\nto allow for tractable probabilistic inference with respect to domain sizes of\nlogical variables. We found that the current state-of-the-art algorithm to\nconstruct a lifted representation in form of a parametric factor graph misses\nsymmetries between factors that are exchangeable but scaled differently,\nthereby leading to a less compact representation. In this paper, we propose a\ngeneralisation of the advanced colour passing (ACP) algorithm, which is the\nstate of the art to construct a parametric factor graph. Our proposed algorithm\nallows for potentials of factors to be scaled arbitrarily and efficiently\ndetects more symmetries than the original ACP algorithm. By detecting strictly\nmore symmetries than ACP, our algorithm significantly reduces online query\ntimes for probabilistic inference when the resulting model is applied, which we\nalso confirm in our experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifted probabilistic inference exploits symmetries in a probabilistic model\nto allow for tractable probabilistic inference with respect to domain sizes of\nlogical variables. We found that the current state-of-the-art algorithm to\nconstruct a lifted representation in form of a parametric factor graph misses\nsymmetries between factors that are exchangeable but scaled differently,\nthereby leading to a less compact representation. In this paper, we propose a\ngeneralisation of the advanced colour passing (ACP) algorithm, which is the\nstate of the art to construct a parametric factor graph. Our proposed algorithm\nallows for potentials of factors to be scaled arbitrarily and efficiently\ndetects more symmetries than the original ACP algorithm. By detecting strictly\nmore symmetries than ACP, our algorithm significantly reduces online query\ntimes for probabilistic inference when the resulting model is applied, which we\nalso confirm in our experiments."
                },
                "authors": [
                    {
                        "name": "Malte Luttermann"
                    },
                    {
                        "name": "Ralf MÃ¶ller"
                    },
                    {
                        "name": "Marcel Gehrke"
                    }
                ],
                "author_detail": {
                    "name": "Marcel Gehrke"
                },
                "author": "Marcel Gehrke",
                "arxiv_comment": "Accepted to the Proceedings of the 3rd Learning on Graphs Conference\n  (LoG 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13280v1",
                "updated": "2024-11-20T12:48:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    48,
                    29,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T12:48:29Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    48,
                    29,
                    2,
                    325,
                    0
                ],
                "title": "Unlocking the Power of Gradient Guidance for Structure-Based Molecule\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking the Power of Gradient Guidance for Structure-Based Molecule\n  Optimization"
                },
                "summary": "Structure-based molecule optimization (SBMO) aims to optimize molecules with\nboth continuous coordinates and discrete types against protein targets. A\npromising direction is to exert gradient guidance on generative models given\nits remarkable success in images, but it is challenging to guide discrete data\nand risks inconsistencies between modalities. To this end, we leverage a\ncontinuous and differentiable space derived through Bayesian inference,\npresenting Molecule Joint Optimization (MolJO), the first gradient-based SBMO\nframework that facilitates joint guidance signals across different modalities\nwhile preserving SE(3)-equivariance. We introduce a novel backward correction\nstrategy that optimizes within a sliding window of the past histories, allowing\nfor a seamless trade-off between explore-and-exploit during optimization. Our\nproposed MolJO achieves state-of-the-art performance on CrossDocked2020\nbenchmark (Success Rate 51.3% , Vina Dock -9.05 and SA 0.78), more than 4x\nimprovement in Success Rate compared to the gradient-based counterpart, and 2x\n\"Me-Better\" Ratio as much as 3D baselines. Furthermore, we extend MolJO to a\nwide range of optimization settings, including multi-objective optimization and\nchallenging tasks in drug design such as R-group optimization and scaffold\nhopping, further underscoring its versatility and potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structure-based molecule optimization (SBMO) aims to optimize molecules with\nboth continuous coordinates and discrete types against protein targets. A\npromising direction is to exert gradient guidance on generative models given\nits remarkable success in images, but it is challenging to guide discrete data\nand risks inconsistencies between modalities. To this end, we leverage a\ncontinuous and differentiable space derived through Bayesian inference,\npresenting Molecule Joint Optimization (MolJO), the first gradient-based SBMO\nframework that facilitates joint guidance signals across different modalities\nwhile preserving SE(3)-equivariance. We introduce a novel backward correction\nstrategy that optimizes within a sliding window of the past histories, allowing\nfor a seamless trade-off between explore-and-exploit during optimization. Our\nproposed MolJO achieves state-of-the-art performance on CrossDocked2020\nbenchmark (Success Rate 51.3% , Vina Dock -9.05 and SA 0.78), more than 4x\nimprovement in Success Rate compared to the gradient-based counterpart, and 2x\n\"Me-Better\" Ratio as much as 3D baselines. Furthermore, we extend MolJO to a\nwide range of optimization settings, including multi-objective optimization and\nchallenging tasks in drug design such as R-group optimization and scaffold\nhopping, further underscoring its versatility and potential."
                },
                "authors": [
                    {
                        "name": "Keyue Qiu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Jie Yu"
                    },
                    {
                        "name": "Hongbo Ma"
                    },
                    {
                        "name": "Ziyao Cao"
                    },
                    {
                        "name": "Zhilong Zhang"
                    },
                    {
                        "name": "Yushuai Wu"
                    },
                    {
                        "name": "Mingyue Zheng"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Ying Ma"
                },
                "author": "Wei-Ying Ma",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13278v1",
                "updated": "2024-11-20T12:46:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    46,
                    41,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T12:46:41Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    46,
                    41,
                    2,
                    325,
                    0
                ],
                "title": "Introducing Schema Inference as a Scalable SQL Function [Extended\n  Version]",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing Schema Inference as a Scalable SQL Function [Extended\n  Version]"
                },
                "summary": "This paper introduces a novel approach to schema inference as an on-demand\nfunction integrated directly within a DBMS, targeting NoSQL databases where\nschema flexibility can create challenges. Unlike previous methods relying on\nexternal frameworks like Apache Spark, our solution enables schema inference as\na SQL function, allowing users to infer schemas natively within the DBMS.\nImplemented in Apache AsterixDB, it performs schema discovery in two phases,\nlocal inference and global schema merging, leveraging internal resources for\nimproved performance. Experiments with real world datasets show up to a two\norders of magnitude performance boost over external methods, enhancing\nusability and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach to schema inference as an on-demand\nfunction integrated directly within a DBMS, targeting NoSQL databases where\nschema flexibility can create challenges. Unlike previous methods relying on\nexternal frameworks like Apache Spark, our solution enables schema inference as\na SQL function, allowing users to infer schemas natively within the DBMS.\nImplemented in Apache AsterixDB, it performs schema discovery in two phases,\nlocal inference and global schema merging, leveraging internal resources for\nimproved performance. Experiments with real world datasets show up to a two\norders of magnitude performance boost over external methods, enhancing\nusability and scalability."
                },
                "authors": [
                    {
                        "name": "Calvin Dani"
                    },
                    {
                        "name": "Shiva Jahangiri"
                    },
                    {
                        "name": "Thomas HÃ¼tter"
                    }
                ],
                "author_detail": {
                    "name": "Thomas HÃ¼tter"
                },
                "author": "Thomas HÃ¼tter",
                "arxiv_comment": "Extended version of EDBT 2025 submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13277v1",
                "updated": "2024-11-20T12:45:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    45,
                    51,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T12:45:51Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    45,
                    51,
                    2,
                    325,
                    0
                ],
                "title": "Functional normalizing flow for statistical inverse problems of partial\n  differential equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional normalizing flow for statistical inverse problems of partial\n  differential equations"
                },
                "summary": "Inverse problems of partial differential equations are ubiquitous across\nvarious scientific disciplines and can be formulated as statistical inference\nproblems using Bayes' theorem. To address large-scale problems, it is crucial\nto develop discretization-invariant algorithms, which can be achieved by\nformulating methods directly in infinite-dimensional space. We propose a novel\nnormalizing flow based infinite-dimensional variational inference method\n(NF-iVI) to extract posterior information efficiently. Specifically, by\nintroducing well-defined transformations, the prior in Bayes' formula is\ntransformed into post-transformed measures that approximate the true posterior.\nTo circumvent the issue of mutually singular probability measures, we formulate\ngeneral conditions for the employed transformations. As guiding principles,\nthese conditions yield four concrete transformations. Additionally, to minimize\ncomputational demands, we have developed a conditional normalizing flow\nvariant, termed CNF-iVI, which is adept at processing measurement data of\nvarying dimensions while requiring minimal computational resources. We apply\nthe proposed algorithms to two typical inverse problems governed by a simple\nsmooth equation and the steady-state Darcy flow equation. Numerical results\nconfirm our theoretical findings, illustrate the efficiency of our algorithms,\nand verify the discretization-invariant property.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse problems of partial differential equations are ubiquitous across\nvarious scientific disciplines and can be formulated as statistical inference\nproblems using Bayes' theorem. To address large-scale problems, it is crucial\nto develop discretization-invariant algorithms, which can be achieved by\nformulating methods directly in infinite-dimensional space. We propose a novel\nnormalizing flow based infinite-dimensional variational inference method\n(NF-iVI) to extract posterior information efficiently. Specifically, by\nintroducing well-defined transformations, the prior in Bayes' formula is\ntransformed into post-transformed measures that approximate the true posterior.\nTo circumvent the issue of mutually singular probability measures, we formulate\ngeneral conditions for the employed transformations. As guiding principles,\nthese conditions yield four concrete transformations. Additionally, to minimize\ncomputational demands, we have developed a conditional normalizing flow\nvariant, termed CNF-iVI, which is adept at processing measurement data of\nvarying dimensions while requiring minimal computational resources. We apply\nthe proposed algorithms to two typical inverse problems governed by a simple\nsmooth equation and the steady-state Darcy flow equation. Numerical results\nconfirm our theoretical findings, illustrate the efficiency of our algorithms,\nand verify the discretization-invariant property."
                },
                "authors": [
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Haoyu Lu"
                    },
                    {
                        "name": "Junxiong Jia"
                    },
                    {
                        "name": "Tao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tao Zhou"
                },
                "author": "Tao Zhou",
                "arxiv_comment": "52 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65L09, 49N45, 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13269v1",
                "updated": "2024-11-20T12:38:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    38,
                    17,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T12:38:17Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    38,
                    17,
                    2,
                    325,
                    0
                ],
                "title": "Towards Specification-Driven LLM-Based Generation of Embedded Automotive\n  Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Specification-Driven LLM-Based Generation of Embedded Automotive\n  Software"
                },
                "summary": "The paper studies how code generation by LLMs can be combined with formal\nverification to produce critical embedded software. The first contribution is a\ngeneral framework, spec2code, in which LLMs are combined with different types\nof critics that produce feedback for iterative backprompting and fine-tuning.\nThe second contribution presents a first feasibility study, where a\nminimalistic instantiation of spec2code, without iterative backprompting and\nfine-tuning, is empirically evaluated using three industrial case studies from\nthe heavy vehicle manufacturer Scania. The goal is to automatically generate\nindustrial-quality code from specifications only. Different combinations of\nformal ACSL specifications and natural language specifications are explored.\nThe results indicate that formally correct code can be generated even without\nthe application of iterative backprompting and fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper studies how code generation by LLMs can be combined with formal\nverification to produce critical embedded software. The first contribution is a\ngeneral framework, spec2code, in which LLMs are combined with different types\nof critics that produce feedback for iterative backprompting and fine-tuning.\nThe second contribution presents a first feasibility study, where a\nminimalistic instantiation of spec2code, without iterative backprompting and\nfine-tuning, is empirically evaluated using three industrial case studies from\nthe heavy vehicle manufacturer Scania. The goal is to automatically generate\nindustrial-quality code from specifications only. Different combinations of\nformal ACSL specifications and natural language specifications are explored.\nThe results indicate that formally correct code can be generated even without\nthe application of iterative backprompting and fine-tuning."
                },
                "authors": [
                    {
                        "name": "Minal Suresh Patil"
                    },
                    {
                        "name": "Gustav Ung"
                    },
                    {
                        "name": "Mattias Nyberg"
                    }
                ],
                "author_detail": {
                    "name": "Mattias Nyberg"
                },
                "author": "Mattias Nyberg",
                "arxiv_comment": "21 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13268v1",
                "updated": "2024-11-20T12:37:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    37,
                    31,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T12:37:31Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    37,
                    31,
                    2,
                    325,
                    0
                ],
                "title": "Enhanced Gas Source Localization Using Distributed IoT Sensors and\n  Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Gas Source Localization Using Distributed IoT Sensors and\n  Bayesian Inference"
                },
                "summary": "Identifying a gas source in turbulent environments presents a significant\nchallenge for critical applications such as environmental monitoring and\nemergency response. This issue is addressed through an approach that combines\ndistributed IoT smart sensors with an algorithm based on Bayesian inference and\nMonte Carlo sampling techniques. Employing a probabilistic model of the\nenvironment, such an algorithm interprets the gas readings obtained from an\narray of static sensors to estimate the location of the source. The performance\nof our methodology is evaluated by its ability to estimate the source's\nlocation within a given time frame. To test the robustness and practical\napplications of the methods under real-world conditions, we deployed an\nadvanced distributed sensors network to gather water vapor data from a\ncontrolled source. The proposed methodology performs well when using both the\nsynthetic data generated by the model of the environment and those measured in\nthe real experiment, with the source localization error consistently lower than\nthe distance between one sensor and the next in the array.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying a gas source in turbulent environments presents a significant\nchallenge for critical applications such as environmental monitoring and\nemergency response. This issue is addressed through an approach that combines\ndistributed IoT smart sensors with an algorithm based on Bayesian inference and\nMonte Carlo sampling techniques. Employing a probabilistic model of the\nenvironment, such an algorithm interprets the gas readings obtained from an\narray of static sensors to estimate the location of the source. The performance\nof our methodology is evaluated by its ability to estimate the source's\nlocation within a given time frame. To test the robustness and practical\napplications of the methods under real-world conditions, we deployed an\nadvanced distributed sensors network to gather water vapor data from a\ncontrolled source. The proposed methodology performs well when using both the\nsynthetic data generated by the model of the environment and those measured in\nthe real experiment, with the source localization error consistently lower than\nthe distance between one sensor and the next in the array."
                },
                "authors": [
                    {
                        "name": "Leonardo Balocchi"
                    },
                    {
                        "name": "Lorenzo Piro"
                    },
                    {
                        "name": "Luca Biferale"
                    },
                    {
                        "name": "Stefania Bonafoni"
                    },
                    {
                        "name": "Massimo Cencini"
                    },
                    {
                        "name": "Iacopo Nannipieri"
                    },
                    {
                        "name": "Andrea Ria"
                    },
                    {
                        "name": "Luca Roselli"
                    }
                ],
                "author_detail": {
                    "name": "Luca Roselli"
                },
                "author": "Luca Roselli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13262v1",
                "updated": "2024-11-20T12:28:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    28,
                    13,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T12:28:13Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    28,
                    13,
                    2,
                    325,
                    0
                ],
                "title": "FASTNav: Fine-tuned Adaptive Small-language-models Trained for\n  Multi-point Robot Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FASTNav: Fine-tuned Adaptive Small-language-models Trained for\n  Multi-point Robot Navigation"
                },
                "summary": "With the rapid development of large language models (LLM), robots are\nstarting to enjoy the benefits of new interaction methods that large language\nmodels bring. Because edge computing fulfills the needs for rapid response,\nprivacy, and network autonomy, we believe it facilitates the extensive\ndeployment of large models for robot navigation across various industries. To\nenable local deployment of language models on edge devices, we adopt some model\nboosting methods. In this paper, we propose FASTNav - a method for boosting\nlightweight LLMs, also known as small language models (SLMs), for robot\nnavigation. The proposed method contains three modules: fine-tuning,\nteacher-student iteration, and language-based multi-point robot navigation. We\ntrain and evaluate models with FASTNav in both simulation and real robots,\nproving that we can deploy them with low cost, high accuracy and low response\ntime. Compared to other model compression methods, FASTNav shows potential in\nthe local deployment of language models and tends to be a promising solution\nfor language-guided robot navigation on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLM), robots are\nstarting to enjoy the benefits of new interaction methods that large language\nmodels bring. Because edge computing fulfills the needs for rapid response,\nprivacy, and network autonomy, we believe it facilitates the extensive\ndeployment of large models for robot navigation across various industries. To\nenable local deployment of language models on edge devices, we adopt some model\nboosting methods. In this paper, we propose FASTNav - a method for boosting\nlightweight LLMs, also known as small language models (SLMs), for robot\nnavigation. The proposed method contains three modules: fine-tuning,\nteacher-student iteration, and language-based multi-point robot navigation. We\ntrain and evaluate models with FASTNav in both simulation and real robots,\nproving that we can deploy them with low cost, high accuracy and low response\ntime. Compared to other model compression methods, FASTNav shows potential in\nthe local deployment of language models and tends to be a promising solution\nfor language-guided robot navigation on edge devices."
                },
                "authors": [
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Yixin Han"
                    },
                    {
                        "name": "Xiao Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Li"
                },
                "author": "Xiao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02910v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02910v2",
                "updated": "2024-11-20T12:19:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    19,
                    0,
                    2,
                    325,
                    0
                ],
                "published": "2024-10-03T19:00:00Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    19,
                    0,
                    0,
                    3,
                    277,
                    0
                ],
                "title": "Systematics in tests of general relativity using LISA massive black hole\n  binaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematics in tests of general relativity using LISA massive black hole\n  binaries"
                },
                "summary": "Our current understanding is that an environment - mainly consisting of gas\nor stars - is required to bring massive black hole binaries (MBHBs) with total\nredshifted mass $M_z\\sim[10^{4},10^7]~{\\rm M}_\\odot$ to the LISA band from\nparsec separation. Even in the gravitational wave (GW) dominated final\ninspiral, realistic environments can non-negligibly speed up or slow down the\nbinary evolution, or leave residual, measurable eccentricity in the LISA band.\nDespite this fact, most of the literature does not consider environmental\neffects or orbital eccentricity in modelling GWs from near-equal mass MBHBs.\nConsidering either a circular MBHB embedded in a circumbinary disc or a vacuum\neccentric binary, we explore if ignoring either secular gas effects (migration\nand accretion) or eccentric corrections to the GW waveform can mimic a failure\nof General Relativity (GR). We use inspiral-only aligned-spin 3.5\npost-Newtonian waveforms, a complete LISA response model, and Bayesian\ninference to perform a parameterized test of GR. For a four-year LISA\nobservation of an MBHB with $M_z=10^{5}~{\\rm M}_\\odot$, primary-to-secondary\nmass ratio $q=8$, and component BHs' dimensionless spins $\\chi_{1,2}=0.9$ at\nredshift $z=1$, even a moderate gas-disc imprint (Eddington ratio ${\\rm f}_{\\rm\nEdd}\\sim0.1$) or low initial eccentricity ($e_0\\sim10^{-2.5}$) causes a false\nviolation of GR in several PN orders. However, correctly modelling either\neffect can mitigate systematics while avoiding significant biases in vacuum\ncircular systems. The adoption of LISA makes it urgent to consider gas imprints\nand eccentricity in waveform models to ensure accurate inference for MBHBs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our current understanding is that an environment - mainly consisting of gas\nor stars - is required to bring massive black hole binaries (MBHBs) with total\nredshifted mass $M_z\\sim[10^{4},10^7]~{\\rm M}_\\odot$ to the LISA band from\nparsec separation. Even in the gravitational wave (GW) dominated final\ninspiral, realistic environments can non-negligibly speed up or slow down the\nbinary evolution, or leave residual, measurable eccentricity in the LISA band.\nDespite this fact, most of the literature does not consider environmental\neffects or orbital eccentricity in modelling GWs from near-equal mass MBHBs.\nConsidering either a circular MBHB embedded in a circumbinary disc or a vacuum\neccentric binary, we explore if ignoring either secular gas effects (migration\nand accretion) or eccentric corrections to the GW waveform can mimic a failure\nof General Relativity (GR). We use inspiral-only aligned-spin 3.5\npost-Newtonian waveforms, a complete LISA response model, and Bayesian\ninference to perform a parameterized test of GR. For a four-year LISA\nobservation of an MBHB with $M_z=10^{5}~{\\rm M}_\\odot$, primary-to-secondary\nmass ratio $q=8$, and component BHs' dimensionless spins $\\chi_{1,2}=0.9$ at\nredshift $z=1$, even a moderate gas-disc imprint (Eddington ratio ${\\rm f}_{\\rm\nEdd}\\sim0.1$) or low initial eccentricity ($e_0\\sim10^{-2.5}$) causes a false\nviolation of GR in several PN orders. However, correctly modelling either\neffect can mitigate systematics while avoiding significant biases in vacuum\ncircular systems. The adoption of LISA makes it urgent to consider gas imprints\nand eccentricity in waveform models to ensure accurate inference for MBHBs."
                },
                "authors": [
                    {
                        "name": "Mudit Garg"
                    },
                    {
                        "name": "Laura Sberna"
                    },
                    {
                        "name": "Lorenzo Speri"
                    },
                    {
                        "name": "Francisco Duque"
                    },
                    {
                        "name": "Jonathan Gair"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Gair"
                },
                "author": "Jonathan Gair",
                "arxiv_comment": "11 pages, 8 figures. Accepted by MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02910v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02910v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08202v2",
                "updated": "2024-11-20T12:15:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    15,
                    8,
                    2,
                    325,
                    0
                ],
                "published": "2024-10-10T17:59:22Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    59,
                    22,
                    3,
                    284,
                    0
                ],
                "title": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large\n  Language Models with Endogenous Visual Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large\n  Language Models with Endogenous Visual Pre-training"
                },
                "summary": "In this paper, we focus on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nIn particular, we identify that existing pre-training strategies for monolithic\nMLLMs often suffer from unstable optimization or catastrophic forgetting. To\naddress this issue, our core idea is to embed a new visual parameter space into\na pre-trained LLM, thereby stably learning visual knowledge from noisy data\nwhile freezing the LLM. Based on this principle, we present Mono-InternVL, a\nnovel monolithic MLLM that seamlessly integrates a set of visual experts via a\nmultimodal mixture-of-experts structure. Moreover, we propose an innovative\npre-training strategy to maximize the visual capability of Mono-InternVL,\nnamely Endogenous Visual Pre-training (EViP). In particular, EViP is designed\nas a progressive learning process for visual experts, which aims to fully\nexploit the visual knowledge from noisy data to high-quality data. To validate\nour approach, we conduct extensive experiments on 16 benchmarks. Experimental\nresults confirm the superior performance of Mono-InternVL than existing\nmonolithic MLLMs on 13 of 16 multimodal benchmarks, e.g., +80 points over Emu3\non OCRBench. Compared to the modular baseline, i.e., InternVL-1.5,\nMono-InternVL still retains comparable multimodal performance while reducing up\nto 67% first token latency. Code and model are released at\nhttps://huggingface.co/OpenGVLab/Mono-InternVL-2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we focus on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nIn particular, we identify that existing pre-training strategies for monolithic\nMLLMs often suffer from unstable optimization or catastrophic forgetting. To\naddress this issue, our core idea is to embed a new visual parameter space into\na pre-trained LLM, thereby stably learning visual knowledge from noisy data\nwhile freezing the LLM. Based on this principle, we present Mono-InternVL, a\nnovel monolithic MLLM that seamlessly integrates a set of visual experts via a\nmultimodal mixture-of-experts structure. Moreover, we propose an innovative\npre-training strategy to maximize the visual capability of Mono-InternVL,\nnamely Endogenous Visual Pre-training (EViP). In particular, EViP is designed\nas a progressive learning process for visual experts, which aims to fully\nexploit the visual knowledge from noisy data to high-quality data. To validate\nour approach, we conduct extensive experiments on 16 benchmarks. Experimental\nresults confirm the superior performance of Mono-InternVL than existing\nmonolithic MLLMs on 13 of 16 multimodal benchmarks, e.g., +80 points over Emu3\non OCRBench. Compared to the modular baseline, i.e., InternVL-1.5,\nMono-InternVL still retains comparable multimodal performance while reducing up\nto 67% first token latency. Code and model are released at\nhttps://huggingface.co/OpenGVLab/Mono-InternVL-2B."
                },
                "authors": [
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Wenhan Dou"
                    },
                    {
                        "name": "Zhaokai Wang"
                    },
                    {
                        "name": "Jiawen Liu"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Xizhou Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xizhou Zhu"
                },
                "author": "Xizhou Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13257v1",
                "updated": "2024-11-20T12:14:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    14,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T12:14:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    14,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Inference for Multiple and Conditional Observers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Multiple and Conditional Observers"
                },
                "summary": "We consider models for inference which involve observers which may have\nmultiple copies, such as in the Sleeping Beauty problem. We establish a\nframework for describing these problems on a probability space satisfying\nKolmogorov's axioms, and this enables the main competing solutions to be\ncompared precisely.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider models for inference which involve observers which may have\nmultiple copies, such as in the Sleeping Beauty problem. We establish a\nframework for describing these problems on a probability space satisfying\nKolmogorov's axioms, and this enables the main competing solutions to be\ncompared precisely."
                },
                "authors": [
                    {
                        "name": "Martin T. Barlow"
                    }
                ],
                "author_detail": {
                    "name": "Martin T. Barlow"
                },
                "author": "Martin T. Barlow",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "Primary 62A01 Secondary 60A05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13244v1",
                "updated": "2024-11-20T12:03:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    3,
                    17,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T12:03:17Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    3,
                    17,
                    2,
                    325,
                    0
                ],
                "title": "Leveraging Prior Experience: An Expandable Auxiliary Knowledge Base for\n  Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Prior Experience: An Expandable Auxiliary Knowledge Base for\n  Text-to-SQL"
                },
                "summary": "Large Language Models (LLMs) exhibit impressive problem-solving skills across\nmany tasks, but they still underperform compared to humans in various\ndownstream applications, such as text-to-SQL. On the BIRD benchmark\nleaderboard, human performance achieves an accuracy of 92.96\\%, whereas the\ntop-performing method reaches only 72.39\\%. Notably, these state-of-the-art\n(SoTA) methods predominantly rely on in-context learning to simulate human-like\nreasoning. However, they overlook a critical human skill: continual learning.\nInspired by the educational practice of maintaining mistake notebooks during\nour formative years, we propose LPE-SQL (Leveraging Prior Experience: An\nExpandable Auxiliary Knowledge Base for Text-to-SQL), a novel framework\ndesigned to augment LLMs by enabling continual learning without requiring\nparameter fine-tuning. LPE-SQL consists of four modules that \\textbf{i)}\nretrieve relevant entries, \\textbf{ii)} efficient sql generation, \\textbf{iii)}\ngenerate the final result through a cross-consistency mechanism and\n\\textbf{iv)} log successful and failed tasks along with their reasoning\nprocesses or reflection-generated tips. Importantly, the core module of LPE-SQL\nis the fourth one, while the other modules employ foundational methods,\nallowing LPE-SQL to be easily integrated with SoTA technologies to further\nenhance performance. Our experimental results demonstrate that this continual\nlearning approach yields substantial performance gains, with the smaller\nLlama-3.1-70B model with surpassing the performance of the larger\nLlama-3.1-405B model using SoTA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit impressive problem-solving skills across\nmany tasks, but they still underperform compared to humans in various\ndownstream applications, such as text-to-SQL. On the BIRD benchmark\nleaderboard, human performance achieves an accuracy of 92.96\\%, whereas the\ntop-performing method reaches only 72.39\\%. Notably, these state-of-the-art\n(SoTA) methods predominantly rely on in-context learning to simulate human-like\nreasoning. However, they overlook a critical human skill: continual learning.\nInspired by the educational practice of maintaining mistake notebooks during\nour formative years, we propose LPE-SQL (Leveraging Prior Experience: An\nExpandable Auxiliary Knowledge Base for Text-to-SQL), a novel framework\ndesigned to augment LLMs by enabling continual learning without requiring\nparameter fine-tuning. LPE-SQL consists of four modules that \\textbf{i)}\nretrieve relevant entries, \\textbf{ii)} efficient sql generation, \\textbf{iii)}\ngenerate the final result through a cross-consistency mechanism and\n\\textbf{iv)} log successful and failed tasks along with their reasoning\nprocesses or reflection-generated tips. Importantly, the core module of LPE-SQL\nis the fourth one, while the other modules employ foundational methods,\nallowing LPE-SQL to be easily integrated with SoTA technologies to further\nenhance performance. Our experimental results demonstrate that this continual\nlearning approach yields substantial performance gains, with the smaller\nLlama-3.1-70B model with surpassing the performance of the larger\nLlama-3.1-405B model using SoTA methods."
                },
                "authors": [
                    {
                        "name": "Zhibo Chu"
                    },
                    {
                        "name": "Zichong Wang"
                    },
                    {
                        "name": "Qitao Qin"
                    }
                ],
                "author_detail": {
                    "name": "Qitao Qin"
                },
                "author": "Qitao Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13239v1",
                "updated": "2024-11-20T11:57:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    57,
                    43,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T11:57:43Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    57,
                    43,
                    2,
                    325,
                    0
                ],
                "title": "Transforming the Hybrid Cloud for Emerging AI Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming the Hybrid Cloud for Emerging AI Workloads"
                },
                "summary": "This white paper, developed through close collaboration between IBM Research\nand UIUC researchers within the IIDAI Institute, envisions transforming hybrid\ncloud systems to meet the growing complexity of AI workloads through\ninnovative, full-stack co-design approaches, emphasizing usability,\nmanageability, affordability, adaptability, efficiency, and scalability. By\nintegrating cutting-edge technologies such as generative and agentic AI,\ncross-layer automation and optimization, unified control plane, and composable\nand adaptive system architecture, the proposed framework addresses critical\nchallenges in energy efficiency, performance, and cost-effectiveness.\nIncorporating quantum computing as it matures will enable quantum-accelerated\nsimulations for materials science, climate modeling, and other high-impact\ndomains. Collaborative efforts between academia and industry are central to\nthis vision, driving advancements in foundation models for material design and\nclimate solutions, scalable multimodal data processing, and enhanced\nphysics-based AI emulators for applications like weather forecasting and carbon\nsequestration. Research priorities include advancing AI agentic systems, LLM as\nan Abstraction (LLMaaA), AI model optimization and unified abstractions across\nheterogeneous infrastructure, end-to-end edge-cloud transformation, efficient\nprogramming model, middleware and platform, secure infrastructure,\napplication-adaptive cloud systems, and new quantum-classical collaborative\nworkflows. These ideas and solutions encompass both theoretical and practical\nresearch questions, requiring coordinated input and support from the research\ncommunity. This joint initiative aims to establish hybrid clouds as secure,\nefficient, and sustainable platforms, fostering breakthroughs in AI-driven\napplications and scientific discovery across academia, industry, and society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This white paper, developed through close collaboration between IBM Research\nand UIUC researchers within the IIDAI Institute, envisions transforming hybrid\ncloud systems to meet the growing complexity of AI workloads through\ninnovative, full-stack co-design approaches, emphasizing usability,\nmanageability, affordability, adaptability, efficiency, and scalability. By\nintegrating cutting-edge technologies such as generative and agentic AI,\ncross-layer automation and optimization, unified control plane, and composable\nand adaptive system architecture, the proposed framework addresses critical\nchallenges in energy efficiency, performance, and cost-effectiveness.\nIncorporating quantum computing as it matures will enable quantum-accelerated\nsimulations for materials science, climate modeling, and other high-impact\ndomains. Collaborative efforts between academia and industry are central to\nthis vision, driving advancements in foundation models for material design and\nclimate solutions, scalable multimodal data processing, and enhanced\nphysics-based AI emulators for applications like weather forecasting and carbon\nsequestration. Research priorities include advancing AI agentic systems, LLM as\nan Abstraction (LLMaaA), AI model optimization and unified abstractions across\nheterogeneous infrastructure, end-to-end edge-cloud transformation, efficient\nprogramming model, middleware and platform, secure infrastructure,\napplication-adaptive cloud systems, and new quantum-classical collaborative\nworkflows. These ideas and solutions encompass both theoretical and practical\nresearch questions, requiring coordinated input and support from the research\ncommunity. This joint initiative aims to establish hybrid clouds as secure,\nefficient, and sustainable platforms, fostering breakthroughs in AI-driven\napplications and scientific discovery across academia, industry, and society."
                },
                "authors": [
                    {
                        "name": "Deming Chen"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Ruchi Pendse"
                    },
                    {
                        "name": "AndrÃ© Schleife"
                    },
                    {
                        "name": "Bryan K. Clark"
                    },
                    {
                        "name": "Hendrik Hamann"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Teodoro Laino"
                    },
                    {
                        "name": "Lav Varshney"
                    },
                    {
                        "name": "Yuxiong Wang"
                    },
                    {
                        "name": "Avirup Sil"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Volodymyr Kindratenko"
                    },
                    {
                        "name": "Carlos Costa"
                    },
                    {
                        "name": "Sarita Adve"
                    },
                    {
                        "name": "Charith Mendis"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Santiago NÃºÃ±ez-Corrales"
                    },
                    {
                        "name": "Raghu Ganti"
                    },
                    {
                        "name": "Mudhakar Srivatsa"
                    },
                    {
                        "name": "Nam Sung Kim"
                    },
                    {
                        "name": "Josep Torrellas"
                    },
                    {
                        "name": "Jian Huang"
                    },
                    {
                        "name": "Seetharami Seelam"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    },
                    {
                        "name": "Tarek Abdelzaher"
                    },
                    {
                        "name": "Tamar Eilam"
                    },
                    {
                        "name": "Huimin Zhao"
                    },
                    {
                        "name": "Matteo Manica"
                    },
                    {
                        "name": "Ravishankar Iyer"
                    },
                    {
                        "name": "Martin Hirzel"
                    },
                    {
                        "name": "Vikram Adve"
                    },
                    {
                        "name": "Darko Marinov"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Hanghang Tong"
                    },
                    {
                        "name": "Elizabeth Ainsworth"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Deepak Vasisht"
                    },
                    {
                        "name": "Minh Do"
                    },
                    {
                        "name": "Fabio Oliveira"
                    },
                    {
                        "name": "Giovanni Pacifici"
                    },
                    {
                        "name": "Ruchir Puri"
                    },
                    {
                        "name": "Priya Nagpurkar"
                    }
                ],
                "author_detail": {
                    "name": "Priya Nagpurkar"
                },
                "author": "Priya Nagpurkar",
                "arxiv_comment": "70 pages, 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13226v1",
                "updated": "2024-11-20T11:41:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    41,
                    8,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T11:41:08Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    41,
                    8,
                    2,
                    325,
                    0
                ],
                "title": "AIDBench: A benchmark for evaluating the authorship identification\n  capability of large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIDBench: A benchmark for evaluating the authorship identification\n  capability of large language models"
                },
                "summary": "As large language models (LLMs) rapidly advance and integrate into daily\nlife, the privacy risks they pose are attracting increasing attention. We focus\non a specific privacy risk where LLMs may help identify the authorship of\nanonymous texts, which challenges the effectiveness of anonymity in real-world\nsystems such as anonymous peer review systems. To investigate these risks, we\npresent AIDBench, a new benchmark that incorporates several author\nidentification datasets, including emails, blogs, reviews, articles, and\nresearch papers. AIDBench utilizes two evaluation methods: one-to-one\nauthorship identification, which determines whether two texts are from the same\nauthor; and one-to-many authorship identification, which, given a query text\nand a list of candidate texts, identifies the candidate most likely written by\nthe same author as the query text. We also introduce a Retrieval-Augmented\nGeneration (RAG)-based method to enhance the large-scale authorship\nidentification capabilities of LLMs, particularly when input lengths exceed the\nmodels' context windows, thereby establishing a new baseline for authorship\nidentification using LLMs. Our experiments with AIDBench demonstrate that LLMs\ncan correctly guess authorship at rates well above random chance, revealing new\nprivacy risks posed by these powerful models. The source code and data will be\nmade publicly available after acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) rapidly advance and integrate into daily\nlife, the privacy risks they pose are attracting increasing attention. We focus\non a specific privacy risk where LLMs may help identify the authorship of\nanonymous texts, which challenges the effectiveness of anonymity in real-world\nsystems such as anonymous peer review systems. To investigate these risks, we\npresent AIDBench, a new benchmark that incorporates several author\nidentification datasets, including emails, blogs, reviews, articles, and\nresearch papers. AIDBench utilizes two evaluation methods: one-to-one\nauthorship identification, which determines whether two texts are from the same\nauthor; and one-to-many authorship identification, which, given a query text\nand a list of candidate texts, identifies the candidate most likely written by\nthe same author as the query text. We also introduce a Retrieval-Augmented\nGeneration (RAG)-based method to enhance the large-scale authorship\nidentification capabilities of LLMs, particularly when input lengths exceed the\nmodels' context windows, thereby establishing a new baseline for authorship\nidentification using LLMs. Our experiments with AIDBench demonstrate that LLMs\ncan correctly guess authorship at rates well above random chance, revealing new\nprivacy risks posed by these powerful models. The source code and data will be\nmade publicly available after acceptance."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Dadi Guo"
                    },
                    {
                        "name": "Huishuai Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huishuai Zhang"
                },
                "author": "Huishuai Zhang",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13223v1",
                "updated": "2024-11-20T11:35:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    35,
                    22,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T11:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    35,
                    22,
                    2,
                    325,
                    0
                ],
                "title": "Existential Conversations with Large Language Models: Content,\n  Community, and Culture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existential Conversations with Large Language Models: Content,\n  Community, and Culture"
                },
                "summary": "Contemporary conversational AI systems based on large language models (LLMs)\ncan engage users on a wide variety of topics, including philosophy,\nspirituality, and religion. Suitably prompted, LLMs can be coaxed into\ndiscussing such existentially significant matters as their own putative\nconsciousness and the role of artificial intelligence in the fate of the\nCosmos. Here we examine two lengthy conversations of this type. We trace likely\nsources, both ancient and modern, for the extensive repertoire of images,\nmyths, metaphors, and conceptual esoterica that the language model draws on\nduring these conversations, and foreground the contemporary communities and\ncultural movements that deploy related motifs, especially in their online\nactivity. Finally, we consider the larger societal impacts of such engagements\nwith LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary conversational AI systems based on large language models (LLMs)\ncan engage users on a wide variety of topics, including philosophy,\nspirituality, and religion. Suitably prompted, LLMs can be coaxed into\ndiscussing such existentially significant matters as their own putative\nconsciousness and the role of artificial intelligence in the fate of the\nCosmos. Here we examine two lengthy conversations of this type. We trace likely\nsources, both ancient and modern, for the extensive repertoire of images,\nmyths, metaphors, and conceptual esoterica that the language model draws on\nduring these conversations, and foreground the contemporary communities and\ncultural movements that deploy related motifs, especially in their online\nactivity. Finally, we consider the larger societal impacts of such engagements\nwith LLMs."
                },
                "authors": [
                    {
                        "name": "Murray Shanahan"
                    },
                    {
                        "name": "Beth Singler"
                    }
                ],
                "author_detail": {
                    "name": "Beth Singler"
                },
                "author": "Beth Singler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13212v1",
                "updated": "2024-11-20T11:19:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    19,
                    35,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T11:19:35Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    19,
                    35,
                    2,
                    325,
                    0
                ],
                "title": "On the Statistical Significance with Relevance Assessments of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Statistical Significance with Relevance Assessments of Large\n  Language Models"
                },
                "summary": "Test collections are an integral part of Information Retrieval (IR) research.\nThey allow researchers to evaluate and compare ranking algorithms in a quick,\neasy and reproducible way. However, constructing these datasets requires great\nefforts in manual labelling and logistics, and having only few human relevance\njudgements can introduce biases in the comparison. Recent research has explored\nthe use of Large Language Models (LLMs) for labelling the relevance of\ndocuments for building new retrieval test collections. Their strong\ntext-understanding capabilities and low cost compared to human-made judgements\nmakes them an appealing tool for gathering relevance judgements. Results\nsuggest that LLM-generated labels are promising for IR evaluation in terms of\nranking correlation, but nothing is said about the implications in terms of\nstatistical significance. In this work, we look at how LLM-generated judgements\npreserve the same pairwise significance evaluation as human judgements. Our\nresults show that LLM judgements detect most of the significant differences\nwhile maintaining acceptable numbers of false positives. However, we also show\nthat some systems are treated differently under LLM-generated labels,\nsuggesting that evaluation with LLM judgements might not be entirely fair. Our\nwork represents a step forward in the evaluation of statistical testing results\nprovided by LLM judgements. We hope that this will serve as a basis for other\nresearchers to develop reliable models for automatic relevance assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test collections are an integral part of Information Retrieval (IR) research.\nThey allow researchers to evaluate and compare ranking algorithms in a quick,\neasy and reproducible way. However, constructing these datasets requires great\nefforts in manual labelling and logistics, and having only few human relevance\njudgements can introduce biases in the comparison. Recent research has explored\nthe use of Large Language Models (LLMs) for labelling the relevance of\ndocuments for building new retrieval test collections. Their strong\ntext-understanding capabilities and low cost compared to human-made judgements\nmakes them an appealing tool for gathering relevance judgements. Results\nsuggest that LLM-generated labels are promising for IR evaluation in terms of\nranking correlation, but nothing is said about the implications in terms of\nstatistical significance. In this work, we look at how LLM-generated judgements\npreserve the same pairwise significance evaluation as human judgements. Our\nresults show that LLM judgements detect most of the significant differences\nwhile maintaining acceptable numbers of false positives. However, we also show\nthat some systems are treated differently under LLM-generated labels,\nsuggesting that evaluation with LLM judgements might not be entirely fair. Our\nwork represents a step forward in the evaluation of statistical testing results\nprovided by LLM judgements. We hope that this will serve as a basis for other\nresearchers to develop reliable models for automatic relevance assessments."
                },
                "authors": [
                    {
                        "name": "David Otero"
                    },
                    {
                        "name": "Javier Parapar"
                    },
                    {
                        "name": "Ãlvaro Barreiro"
                    }
                ],
                "author_detail": {
                    "name": "Ãlvaro Barreiro"
                },
                "author": "Ãlvaro Barreiro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13207v1",
                "updated": "2024-11-20T11:09:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    9,
                    55,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T11:09:55Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    9,
                    55,
                    2,
                    325,
                    0
                ],
                "title": "The Information Security Awareness of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Information Security Awareness of Large Language Models"
                },
                "summary": "The popularity of large language models (LLMs) continues to increase, and\nLLM-based assistants have become ubiquitous, assisting people of diverse\nbackgrounds in many aspects of life. Significant resources have been invested\nin the safety of LLMs and their alignment with social norms. However, research\nexamining their behavior from the information security awareness (ISA)\nperspective is lacking. Chatbots and LLM-based assistants may put unwitting\nusers in harm's way by facilitating unsafe behavior. We observe that the ISA\ninherent in some of today's most popular LLMs varies significantly, with most\nmodels requiring user prompts with a clear security context to utilize their\nsecurity knowledge and provide safe responses to users. Based on this\nobservation, we created a comprehensive set of 30 scenarios to assess the ISA\nof LLMs. These scenarios benchmark the evaluated models with respect to all\nfocus areas defined in a mobile ISA taxonomy. Among our findings is that ISA is\nmildly affected by changing the model's temperature, whereas adjusting the\nsystem prompt can substantially impact it. This underscores the necessity of\nsetting the right system prompt to mitigate ISA weaknesses. Our findings also\nhighlight the importance of ISA assessment for the development of future\nLLM-based assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The popularity of large language models (LLMs) continues to increase, and\nLLM-based assistants have become ubiquitous, assisting people of diverse\nbackgrounds in many aspects of life. Significant resources have been invested\nin the safety of LLMs and their alignment with social norms. However, research\nexamining their behavior from the information security awareness (ISA)\nperspective is lacking. Chatbots and LLM-based assistants may put unwitting\nusers in harm's way by facilitating unsafe behavior. We observe that the ISA\ninherent in some of today's most popular LLMs varies significantly, with most\nmodels requiring user prompts with a clear security context to utilize their\nsecurity knowledge and provide safe responses to users. Based on this\nobservation, we created a comprehensive set of 30 scenarios to assess the ISA\nof LLMs. These scenarios benchmark the evaluated models with respect to all\nfocus areas defined in a mobile ISA taxonomy. Among our findings is that ISA is\nmildly affected by changing the model's temperature, whereas adjusting the\nsystem prompt can substantially impact it. This underscores the necessity of\nsetting the right system prompt to mitigate ISA weaknesses. Our findings also\nhighlight the importance of ISA assessment for the development of future\nLLM-based assistants."
                },
                "authors": [
                    {
                        "name": "Ofir Cohen"
                    },
                    {
                        "name": "Gil Ari Agmon"
                    },
                    {
                        "name": "Asaf Shabtai"
                    },
                    {
                        "name": "Rami Puzis"
                    }
                ],
                "author_detail": {
                    "name": "Rami Puzis"
                },
                "author": "Rami Puzis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13203v1",
                "updated": "2024-11-20T11:04:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    4,
                    50,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T11:04:50Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    4,
                    50,
                    2,
                    325,
                    0
                ],
                "title": "A computational framework for integrating Predictive processes with\n  evidence Accumulation Models (PAM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A computational framework for integrating Predictive processes with\n  evidence Accumulation Models (PAM)"
                },
                "summary": "Evidence Accumulation Models (EAMs) have been widely used to investigate\nspeeded decision-making processes, but they have largely neglected the role of\npredictive processes emphasized by theories of the predictive brain. In this\npaper, we present the Predictive evidence Accumulation Models (PAM), a novel\ncomputational framework that integrates predictive processes into EAMs.\nGrounded in the \"observing the observer\" framework, PAM combines models of\nBayesian perceptual inference, such as the Hierarchical Gaussian Filter, with\nthree established EAMs (the Diffusion Decision Model, Lognormal Race Model, and\nRace Diffusion Model) to model decision-making under uncertainty. We validate\nPAM through parameter recovery simulations, demonstrating its accuracy and\ncomputational efficiency across various decision-making scenarios.\nAdditionally, we provide a step-by-step tutorial using real data to illustrate\nPAM's application and discuss its theoretical implications. PAM represents a\nsignificant advancement in the computational modeling of decision-making,\nbridging the gap between predictive brain theories and EAMs, and offers a\npromising tool for future empirical research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evidence Accumulation Models (EAMs) have been widely used to investigate\nspeeded decision-making processes, but they have largely neglected the role of\npredictive processes emphasized by theories of the predictive brain. In this\npaper, we present the Predictive evidence Accumulation Models (PAM), a novel\ncomputational framework that integrates predictive processes into EAMs.\nGrounded in the \"observing the observer\" framework, PAM combines models of\nBayesian perceptual inference, such as the Hierarchical Gaussian Filter, with\nthree established EAMs (the Diffusion Decision Model, Lognormal Race Model, and\nRace Diffusion Model) to model decision-making under uncertainty. We validate\nPAM through parameter recovery simulations, demonstrating its accuracy and\ncomputational efficiency across various decision-making scenarios.\nAdditionally, we provide a step-by-step tutorial using real data to illustrate\nPAM's application and discuss its theoretical implications. PAM represents a\nsignificant advancement in the computational modeling of decision-making,\nbridging the gap between predictive brain theories and EAMs, and offers a\npromising tool for future empirical research."
                },
                "authors": [
                    {
                        "name": "Antonino Visalli"
                    },
                    {
                        "name": "Francesco Maria Calistroni"
                    },
                    {
                        "name": "Margherita Calderan"
                    },
                    {
                        "name": "Francesco Donnarumma"
                    },
                    {
                        "name": "Marco Zorzi"
                    },
                    {
                        "name": "Ettore Ambrosini"
                    }
                ],
                "author_detail": {
                    "name": "Ettore Ambrosini"
                },
                "author": "Ettore Ambrosini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13187v1",
                "updated": "2024-11-20T10:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    40,
                    8,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T10:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    40,
                    8,
                    2,
                    325,
                    0
                ],
                "title": "Engagement-Driven Content Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engagement-Driven Content Generation with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/."
                },
                "authors": [
                    {
                        "name": "Erica Coppolillo"
                    },
                    {
                        "name": "Marco Minici"
                    },
                    {
                        "name": "Federico Cinus"
                    },
                    {
                        "name": "Francesco Bonchi"
                    },
                    {
                        "name": "Giuseppe Manco"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Manco"
                },
                "author": "Giuseppe Manco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07267v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07267v4",
                "updated": "2024-11-20T10:34:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    34,
                    21,
                    2,
                    325,
                    0
                ],
                "published": "2024-09-11T13:43:01Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    43,
                    1,
                    2,
                    255,
                    0
                ],
                "title": "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D\n  Features as Text Tokens for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D\n  Features as Text Tokens for Autonomous Driving"
                },
                "summary": "Vision-language models (VLMs) serve as general-purpose end-to-end models in\nautonomous driving, performing subtasks such as prediction, planning, and\nperception through question-and-answer interactions. However, most existing\nmethods rely on computationally expensive visual encoders and large language\nmodels (LLMs), making them difficult to deploy in real-world scenarios and\nreal-time applications. Meanwhile, most existing VLMs lack the ability to\nprocess multiple images, making it difficult to adapt to multi-camera\nperception in autonomous driving. To address these issues, we propose a novel\nframework called MiniDrive, which incorporates our proposed Feature Engineering\nMixture of Experts (FE-MoE) module and Dynamic Instruction Adapter\n(DI-Adapter). The FE-MoE effectively maps 2D features into visual token\nembeddings before being input into the language model. The DI-Adapter enables\nthe visual token embeddings to dynamically change with the instruction text\nembeddings, resolving the issue of static visual token embeddings for the same\nimage in previous approaches. Compared to previous works, MiniDrive achieves\nstate-of-the-art performance in terms of parameter size, floating point\noperations, and response efficiency, with the smallest version containing only\n83M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) serve as general-purpose end-to-end models in\nautonomous driving, performing subtasks such as prediction, planning, and\nperception through question-and-answer interactions. However, most existing\nmethods rely on computationally expensive visual encoders and large language\nmodels (LLMs), making them difficult to deploy in real-world scenarios and\nreal-time applications. Meanwhile, most existing VLMs lack the ability to\nprocess multiple images, making it difficult to adapt to multi-camera\nperception in autonomous driving. To address these issues, we propose a novel\nframework called MiniDrive, which incorporates our proposed Feature Engineering\nMixture of Experts (FE-MoE) module and Dynamic Instruction Adapter\n(DI-Adapter). The FE-MoE effectively maps 2D features into visual token\nembeddings before being input into the language model. The DI-Adapter enables\nthe visual token embeddings to dynamically change with the instruction text\nembeddings, resolving the issue of static visual token embeddings for the same\nimage in previous approaches. Compared to previous works, MiniDrive achieves\nstate-of-the-art performance in terms of parameter size, floating point\noperations, and response efficiency, with the smallest version containing only\n83M parameters."
                },
                "authors": [
                    {
                        "name": "Enming Zhang"
                    },
                    {
                        "name": "Xingyuan Dai"
                    },
                    {
                        "name": "Yisheng Lv"
                    },
                    {
                        "name": "Qinghai Miao"
                    }
                ],
                "author_detail": {
                    "name": "Qinghai Miao"
                },
                "author": "Qinghai Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07267v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07267v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13173v1",
                "updated": "2024-11-20T10:17:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    17,
                    9,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T10:17:09Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    17,
                    9,
                    2,
                    325,
                    0
                ],
                "title": "Writing Style Matters: An Examination of Bias and Fairness in\n  Information Retrieval Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing Style Matters: An Examination of Bias and Fairness in\n  Information Retrieval Systems"
                },
                "summary": "The rapid advancement of Language Model technologies has opened new\nopportunities, but also introduced new challenges related to bias and fairness.\nThis paper explores the uncharted territory of potential biases in\nstate-of-the-art universal text embedding models towards specific document and\nquery writing styles within Information Retrieval (IR) systems. Our\ninvestigation reveals that different embedding models exhibit different\npreferences of document writing style, while more informal and emotive styles\nare less favored by most embedding models. In terms of query writing styles,\nmany embedding models tend to match the style of the query with the style of\nthe retrieved documents, but some show a consistent preference for specific\nstyles. Text embedding models fine-tuned on synthetic data generated by LLMs\ndisplay a consistent preference for certain style of generated data. These\nbiases in text embedding based IR systems can inadvertently silence or\nmarginalize certain communication styles, thereby posing a significant threat\nto fairness in information retrieval. Finally, we also compare the answer\nstyles of Retrieval Augmented Generation (RAG) systems based on different LLMs\nand find out that most text embedding models are biased towards LLM's answer\nstyles when used as evaluation metrics for answer correctness. This study sheds\nlight on the critical issue of writing style based bias in IR systems, offering\nvaluable insights for the development of more fair and robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Language Model technologies has opened new\nopportunities, but also introduced new challenges related to bias and fairness.\nThis paper explores the uncharted territory of potential biases in\nstate-of-the-art universal text embedding models towards specific document and\nquery writing styles within Information Retrieval (IR) systems. Our\ninvestigation reveals that different embedding models exhibit different\npreferences of document writing style, while more informal and emotive styles\nare less favored by most embedding models. In terms of query writing styles,\nmany embedding models tend to match the style of the query with the style of\nthe retrieved documents, but some show a consistent preference for specific\nstyles. Text embedding models fine-tuned on synthetic data generated by LLMs\ndisplay a consistent preference for certain style of generated data. These\nbiases in text embedding based IR systems can inadvertently silence or\nmarginalize certain communication styles, thereby posing a significant threat\nto fairness in information retrieval. Finally, we also compare the answer\nstyles of Retrieval Augmented Generation (RAG) systems based on different LLMs\nand find out that most text embedding models are biased towards LLM's answer\nstyles when used as evaluation metrics for answer correctness. This study sheds\nlight on the critical issue of writing style based bias in IR systems, offering\nvaluable insights for the development of more fair and robust models."
                },
                "authors": [
                    {
                        "name": "Hongliu Cao"
                    }
                ],
                "author_detail": {
                    "name": "Hongliu Cao"
                },
                "author": "Hongliu Cao",
                "arxiv_doi": "10.1145/3701551.3703514",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701551.3703514",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the Eighteenth ACM International Conference on Web\n  Search and Data Mining (WSDM 25)",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12449v2",
                "updated": "2024-11-20T10:06:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    6,
                    5,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-19T12:17:43Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    17,
                    43,
                    1,
                    324,
                    0
                ],
                "title": "Neon: News Entity-Interaction Extraction for Enhanced Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neon: News Entity-Interaction Extraction for Enhanced Question Answering"
                },
                "summary": "Capturing fresh information in near real-time and using it to augment\nexisting large language models (LLMs) is essential to generate up-to-date,\ngrounded, and reliable output. This problem becomes particularly challenging\nwhen LLMs are used for informational tasks in rapidly evolving fields, such as\nWeb search related to recent or unfolding events involving entities, where\ngenerating temporally relevant responses requires access to up-to-the-hour news\nsources. However, the information modeled by the parametric memory of LLMs is\noften outdated, and Web results from prototypical retrieval systems may fail to\ncapture the latest relevant information and struggle to handle conflicting\nreports in evolving news. To address this challenge, we present the NEON\nframework, designed to extract emerging entity interactions -- such as events\nor activities -- as described in news articles. NEON constructs an\nentity-centric timestamped knowledge graph that captures such interactions,\nthereby facilitating enhanced QA capabilities related to news events. Our\nframework innovates by integrating open Information Extraction (openIE) style\ntuples into LLMs to enable in-context retrieval-augmented generation. This\nintegration demonstrates substantial improvements in QA performance when\ntackling temporal, entity-centric search queries. Through NEON, LLMs can\ndeliver more accurate, reliable, and up-to-date responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing fresh information in near real-time and using it to augment\nexisting large language models (LLMs) is essential to generate up-to-date,\ngrounded, and reliable output. This problem becomes particularly challenging\nwhen LLMs are used for informational tasks in rapidly evolving fields, such as\nWeb search related to recent or unfolding events involving entities, where\ngenerating temporally relevant responses requires access to up-to-the-hour news\nsources. However, the information modeled by the parametric memory of LLMs is\noften outdated, and Web results from prototypical retrieval systems may fail to\ncapture the latest relevant information and struggle to handle conflicting\nreports in evolving news. To address this challenge, we present the NEON\nframework, designed to extract emerging entity interactions -- such as events\nor activities -- as described in news articles. NEON constructs an\nentity-centric timestamped knowledge graph that captures such interactions,\nthereby facilitating enhanced QA capabilities related to news events. Our\nframework innovates by integrating open Information Extraction (openIE) style\ntuples into LLMs to enable in-context retrieval-augmented generation. This\nintegration demonstrates substantial improvements in QA performance when\ntackling temporal, entity-centric search queries. Through NEON, LLMs can\ndeliver more accurate, reliable, and up-to-date responses."
                },
                "authors": [
                    {
                        "name": "Sneha Singhania"
                    },
                    {
                        "name": "Silviu Cucerzan"
                    },
                    {
                        "name": "Allen Herring"
                    },
                    {
                        "name": "Sujay Kumar Jauhar"
                    }
                ],
                "author_detail": {
                    "name": "Sujay Kumar Jauhar"
                },
                "author": "Sujay Kumar Jauhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13163v1",
                "updated": "2024-11-20T09:59:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    59,
                    12,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T09:59:12Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    59,
                    12,
                    2,
                    325,
                    0
                ],
                "title": "Unlocking Historical Clinical Trial Data with ALIGN: A Compositional\n  Large Language Model System for Medical Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Historical Clinical Trial Data with ALIGN: A Compositional\n  Large Language Model System for Medical Coding"
                },
                "summary": "The reuse of historical clinical trial data has significant potential to\naccelerate medical research and drug development. However, interoperability\nchallenges, particularly with missing medical codes, hinders effective data\nintegration across studies. While Large Language Models (LLMs) offer a\npromising solution for automated coding without labeled data, current\napproaches face challenges on complex coding tasks. We introduce ALIGN, a novel\ncompositional LLM-based system for automated, zero-shot medical coding. ALIGN\nfollows a three-step process: (1) diverse candidate code generation; (2)\nself-evaluation of codes and (3) confidence scoring and uncertainty estimation\nenabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing\nmedication terms into Anatomical Therapeutic Chemical (ATC) and medical history\nterms into Medical Dictionary for Regulatory Activities (MedDRA) codes\nextracted from 22 immunology trials. ALIGN outperformed the LLM baselines,\nwhile also providing capabilities for trustworthy deployment. For MedDRA\ncoding, ALIGN achieved high accuracy across all levels, matching RAG and\nexcelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN\ndemonstrated superior performance, particularly at lower hierarchy levels (ATC\nLevel 4), with 72-73% overall accuracy and 86-89% accuracy for common\nmedications, outperforming baselines by 7-22%. ALIGN's uncertainty-based\ndeferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably\nenhancing performance on uncommon medications. ALIGN achieves this\ncost-efficiently at \\$0.0007 and \\$0.02 per code for GPT-4o-mini and GPT-4o,\nreducing barriers to clinical adoption. ALIGN advances automated medical coding\nfor clinical trial data, contributing to enhanced data interoperability and\nreusability, positioning it as a promising tool to improve clinical research\nand accelerate drug development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reuse of historical clinical trial data has significant potential to\naccelerate medical research and drug development. However, interoperability\nchallenges, particularly with missing medical codes, hinders effective data\nintegration across studies. While Large Language Models (LLMs) offer a\npromising solution for automated coding without labeled data, current\napproaches face challenges on complex coding tasks. We introduce ALIGN, a novel\ncompositional LLM-based system for automated, zero-shot medical coding. ALIGN\nfollows a three-step process: (1) diverse candidate code generation; (2)\nself-evaluation of codes and (3) confidence scoring and uncertainty estimation\nenabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing\nmedication terms into Anatomical Therapeutic Chemical (ATC) and medical history\nterms into Medical Dictionary for Regulatory Activities (MedDRA) codes\nextracted from 22 immunology trials. ALIGN outperformed the LLM baselines,\nwhile also providing capabilities for trustworthy deployment. For MedDRA\ncoding, ALIGN achieved high accuracy across all levels, matching RAG and\nexcelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN\ndemonstrated superior performance, particularly at lower hierarchy levels (ATC\nLevel 4), with 72-73% overall accuracy and 86-89% accuracy for common\nmedications, outperforming baselines by 7-22%. ALIGN's uncertainty-based\ndeferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably\nenhancing performance on uncommon medications. ALIGN achieves this\ncost-efficiently at \\$0.0007 and \\$0.02 per code for GPT-4o-mini and GPT-4o,\nreducing barriers to clinical adoption. ALIGN advances automated medical coding\nfor clinical trial data, contributing to enhanced data interoperability and\nreusability, positioning it as a promising tool to improve clinical research\nand accelerate drug development."
                },
                "authors": [
                    {
                        "name": "Nabeel Seedat"
                    },
                    {
                        "name": "Caterina Tozzi"
                    },
                    {
                        "name": "Andrea Hita Ardiaca"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    },
                    {
                        "name": "James Weatherall"
                    },
                    {
                        "name": "Adam Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Adam Taylor"
                },
                "author": "Adam Taylor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13159v1",
                "updated": "2024-11-20T09:49:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    49,
                    37,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T09:49:37Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    49,
                    37,
                    2,
                    325,
                    0
                ],
                "title": "Hard-Synth: Synthesizing Diverse Hard Samples for ASR using Zero-Shot\n  TTS and LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hard-Synth: Synthesizing Diverse Hard Samples for ASR using Zero-Shot\n  TTS and LLM"
                },
                "summary": "Text-to-speech (TTS) models have been widely adopted to enhance automatic\nspeech recognition (ASR) systems using text-only corpora, thereby reducing the\ncost of labeling real speech data. Existing research primarily utilizes\nadditional text data and predefined speech styles supported by TTS models. In\nthis paper, we propose Hard-Synth, a novel ASR data augmentation method that\nleverages large language models (LLMs) and advanced zero-shot TTS. Our approach\nemploys LLMs to generate diverse in-domain text through rewriting, without\nrelying on additional text data. Rather than using predefined speech styles, we\nintroduce a hard prompt selection method with zero-shot TTS to clone speech\nstyles that the ASR model finds challenging to recognize. Experiments\ndemonstrate that Hard-Synth significantly enhances the Conformer model,\nachieving relative word error rate (WER) reductions of 6.5\\%/4.4\\% on\nLibriSpeech dev/test-other subsets. Additionally, we show that Hard-Synth is\ndata-efficient and capable of reducing bias in ASR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-speech (TTS) models have been widely adopted to enhance automatic\nspeech recognition (ASR) systems using text-only corpora, thereby reducing the\ncost of labeling real speech data. Existing research primarily utilizes\nadditional text data and predefined speech styles supported by TTS models. In\nthis paper, we propose Hard-Synth, a novel ASR data augmentation method that\nleverages large language models (LLMs) and advanced zero-shot TTS. Our approach\nemploys LLMs to generate diverse in-domain text through rewriting, without\nrelying on additional text data. Rather than using predefined speech styles, we\nintroduce a hard prompt selection method with zero-shot TTS to clone speech\nstyles that the ASR model finds challenging to recognize. Experiments\ndemonstrate that Hard-Synth significantly enhances the Conformer model,\nachieving relative word error rate (WER) reductions of 6.5\\%/4.4\\% on\nLibriSpeech dev/test-other subsets. Additionally, we show that Hard-Synth is\ndata-efficient and capable of reducing bias in ASR."
                },
                "authors": [
                    {
                        "name": "Jiawei Yu"
                    },
                    {
                        "name": "Yuang Li"
                    },
                    {
                        "name": "Xiaosong Qiao"
                    },
                    {
                        "name": "Huan Zhao"
                    },
                    {
                        "name": "Xiaofeng Zhao"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13157v1",
                "updated": "2024-11-20T09:46:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    46,
                    30,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T09:46:30Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    46,
                    30,
                    2,
                    325,
                    0
                ],
                "title": "Closer Look at Efficient Inference Methods: A Survey of Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closer Look at Efficient Inference Methods: A Survey of Speculative\n  Decoding"
                },
                "summary": "Efficient inference in large language models (LLMs) has become a critical\nfocus as their scale and complexity grow. Traditional autoregressive decoding,\nwhile effective, suffers from computational inefficiencies due to its\nsequential token generation process. Speculative decoding addresses this\nbottleneck by introducing a two-stage framework: drafting and verification. A\nsmaller, efficient model generates a preliminary draft, which is then refined\nby a larger, more sophisticated model. This paper provides a comprehensive\nsurvey of speculative decoding methods, categorizing them into draft-centric\nand model-centric approaches. We discuss key ideas associated with each method,\nhighlighting their potential for scaling LLM inference. This survey aims to\nguide future research in optimizing speculative decoding and its integration\ninto real-world LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference in large language models (LLMs) has become a critical\nfocus as their scale and complexity grow. Traditional autoregressive decoding,\nwhile effective, suffers from computational inefficiencies due to its\nsequential token generation process. Speculative decoding addresses this\nbottleneck by introducing a two-stage framework: drafting and verification. A\nsmaller, efficient model generates a preliminary draft, which is then refined\nby a larger, more sophisticated model. This paper provides a comprehensive\nsurvey of speculative decoding methods, categorizing them into draft-centric\nand model-centric approaches. We discuss key ideas associated with each method,\nhighlighting their potential for scaling LLM inference. This survey aims to\nguide future research in optimizing speculative decoding and its integration\ninto real-world LLM applications."
                },
                "authors": [
                    {
                        "name": "Hyun Ryu"
                    },
                    {
                        "name": "Eric Kim"
                    }
                ],
                "author_detail": {
                    "name": "Eric Kim"
                },
                "author": "Eric Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11401v2",
                "updated": "2024-11-20T09:44:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    44,
                    18,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-18T09:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    24,
                    1,
                    0,
                    323,
                    0
                ],
                "title": "Deep Learning-based Code Reviews: A Paradigm Shift or a Double-Edged\n  Sword?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-based Code Reviews: A Paradigm Shift or a Double-Edged\n  Sword?"
                },
                "summary": "Several techniques have been proposed to automate code review. Early support\nconsisted in recommending the most suited reviewer for a given change or in\nprioritizing the review tasks. With the advent of deep learning in software\nengineering, the level of automation has been pushed to new heights, with\napproaches able to provide feedback on source code in natural language as a\nhuman reviewer would do. Also, recent work documented open source projects\nadopting Large Language Models (LLMs) as co-reviewers. Although the research in\nthis field is very active, little is known about the actual impact of including\nautomatically generated code reviews in the code review process. While there\nare many aspects worth investigating, in this work we focus on three of them:\n(i) review quality, i.e., the reviewer's ability to identify issues in the\ncode; (ii) review cost, i.e., the time spent reviewing the code; and (iii)\nreviewer's confidence, i.e., how confident is the reviewer about the provided\nfeedback. We run a controlled experiment with 29 experts who reviewed different\nprograms with/without the support of an automatically generated code review.\nDuring the experiment we monitored the reviewers' activities, for over 50 hours\nof recorded code reviews. We show that reviewers consider valid most of the\nissues automatically identified by the LLM and that the availability of an\nautomated review as a starting point strongly influences their behavior:\nReviewers tend to focus on the code locations indicated by the LLM rather than\nsearching for additional issues in other parts of the code. The reviewers who\nstarted from an automated review identified a higher number of low-severity\nissues while, however, not identifying more high-severity issues as compared to\na completely manual process. Finally, the automated support did not result in\nsaved time and did not increase the reviewers' confidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several techniques have been proposed to automate code review. Early support\nconsisted in recommending the most suited reviewer for a given change or in\nprioritizing the review tasks. With the advent of deep learning in software\nengineering, the level of automation has been pushed to new heights, with\napproaches able to provide feedback on source code in natural language as a\nhuman reviewer would do. Also, recent work documented open source projects\nadopting Large Language Models (LLMs) as co-reviewers. Although the research in\nthis field is very active, little is known about the actual impact of including\nautomatically generated code reviews in the code review process. While there\nare many aspects worth investigating, in this work we focus on three of them:\n(i) review quality, i.e., the reviewer's ability to identify issues in the\ncode; (ii) review cost, i.e., the time spent reviewing the code; and (iii)\nreviewer's confidence, i.e., how confident is the reviewer about the provided\nfeedback. We run a controlled experiment with 29 experts who reviewed different\nprograms with/without the support of an automatically generated code review.\nDuring the experiment we monitored the reviewers' activities, for over 50 hours\nof recorded code reviews. We show that reviewers consider valid most of the\nissues automatically identified by the LLM and that the availability of an\nautomated review as a starting point strongly influences their behavior:\nReviewers tend to focus on the code locations indicated by the LLM rather than\nsearching for additional issues in other parts of the code. The reviewers who\nstarted from an automated review identified a higher number of low-severity\nissues while, however, not identifying more high-severity issues as compared to\na completely manual process. Finally, the automated support did not result in\nsaved time and did not increase the reviewers' confidence."
                },
                "authors": [
                    {
                        "name": "Rosalia Tufano"
                    },
                    {
                        "name": "Alberto Martin-Lopez"
                    },
                    {
                        "name": "Ahmad Tayeb"
                    },
                    {
                        "name": "Ozren DabiÄ"
                    },
                    {
                        "name": "Sonia Haiduc"
                    },
                    {
                        "name": "Gabriele Bavota"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Bavota"
                },
                "author": "Gabriele Bavota",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.00301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.00301v2",
                "updated": "2024-11-20T09:39:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    39,
                    28,
                    2,
                    325,
                    0
                ],
                "published": "2023-03-01T07:53:58Z",
                "published_parsed": [
                    2023,
                    3,
                    1,
                    7,
                    53,
                    58,
                    2,
                    60,
                    0
                ],
                "title": "Auxiliary MCMC and particle Gibbs samplers for parallelisable inference\n  in latent dynamical systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auxiliary MCMC and particle Gibbs samplers for parallelisable inference\n  in latent dynamical systems"
                },
                "summary": "We study the problem of designing efficient exact MCMC algorithms for\nsampling from the full posterior distribution of high-dimensional (in the\nnumber of time steps and the dimension of the latent space) non-linear\nnon-Gaussian latent dynamical models. Particle Gibbs, also known as conditional\nsequential Monte Carlo (SMC), constitutes the de facto golden standard to do\nso, but suffers from degeneracy problems when the dimension of the latent space\nincreases. On the other hand, the routinely employed globally\nGaussian-approximated (e.g., extended Kalman filtering) biased solutions are\nseldom used for this same purpose even though they are more robust than their\nSMC counterparts. In this article, we show how, by introducing auxiliary\nobservation variables in the model, we can both implement efficient exact\nKalman-based samplers for large state-space models, as well as dramatically\nimprove the mixing speed of particle Gibbs algorithms when the dimension of the\nlatent space increases. We demonstrate when and how we can parallelise these\nauxiliary samplers along the time dimension, resulting in algorithms that scale\nlogarithmically with the number of time steps when implemented on graphics\nprocessing units (GPUs). Both algorithms are easily tuned and can be extended\nto accommodate sophisticated approximation techniques. We demonstrate the\nimproved statistical and computational performance of our auxiliary samplers\ncompared to state-of-the-art alternatives for high-dimensional (in both time\nand state space) latent dynamical systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of designing efficient exact MCMC algorithms for\nsampling from the full posterior distribution of high-dimensional (in the\nnumber of time steps and the dimension of the latent space) non-linear\nnon-Gaussian latent dynamical models. Particle Gibbs, also known as conditional\nsequential Monte Carlo (SMC), constitutes the de facto golden standard to do\nso, but suffers from degeneracy problems when the dimension of the latent space\nincreases. On the other hand, the routinely employed globally\nGaussian-approximated (e.g., extended Kalman filtering) biased solutions are\nseldom used for this same purpose even though they are more robust than their\nSMC counterparts. In this article, we show how, by introducing auxiliary\nobservation variables in the model, we can both implement efficient exact\nKalman-based samplers for large state-space models, as well as dramatically\nimprove the mixing speed of particle Gibbs algorithms when the dimension of the\nlatent space increases. We demonstrate when and how we can parallelise these\nauxiliary samplers along the time dimension, resulting in algorithms that scale\nlogarithmically with the number of time steps when implemented on graphics\nprocessing units (GPUs). Both algorithms are easily tuned and can be extended\nto accommodate sophisticated approximation techniques. We demonstrate the\nimproved statistical and computational performance of our auxiliary samplers\ncompared to state-of-the-art alternatives for high-dimensional (in both time\nand state space) latent dynamical systems."
                },
                "authors": [
                    {
                        "name": "Adrien Corenflos"
                    },
                    {
                        "name": "Simo SÃ¤rkkÃ¤"
                    }
                ],
                "author_detail": {
                    "name": "Simo SÃ¤rkkÃ¤"
                },
                "author": "Simo SÃ¤rkkÃ¤",
                "arxiv_comment": "32 pages (incl. references and TOC) + 9 pages appendix, 10 figures.\n  Code also updated at https://github.com/AdrienCorenflos/aux-ssm-samplers\n  Additions include better background description and a discussion of failure\n  modes of the different methods",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.00301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.00301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.05128v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.05128v3",
                "updated": "2024-11-20T09:13:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    13,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2023-08-09T08:49:29Z",
                "published_parsed": [
                    2023,
                    8,
                    9,
                    8,
                    49,
                    29,
                    2,
                    221,
                    0
                ],
                "title": "High-Level Parallelism and Nested Features for Dynamic Inference Cost\n  and Top-Down Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Level Parallelism and Nested Features for Dynamic Inference Cost\n  and Top-Down Attention"
                },
                "summary": "This paper introduces a novel network topology that seamlessly integrates\ndynamic inference cost with a top-down attention mechanism, addressing two\nsignificant gaps in traditional deep learning models. Drawing inspiration from\nhuman perception, we combine sequential processing of generic low-level\nfeatures with parallelism and nesting of high-level features. This design not\nonly reflects a finding from recent neuroscience research regarding - spatially\nand contextually distinct neural activations - in human cortex, but also\nintroduces a novel \"cutout\" technique: the ability to selectively activate\n%segments of the network for task-relevant only network segments of\ntask-relevant categories to optimize inference cost and eliminate the need for\nre-training. We believe this paves the way for future network designs that are\nlightweight and adaptable, making them suitable for a wide range of\napplications, from compact edge devices to large-scale clouds. Our proposed\ntopology also comes with a built-in top-down attention mechanism, which allows\nprocessing to be directly influenced by either enhancing or inhibiting\ncategory-specific high-level features, drawing parallels to the selective\nattention mechanism observed in human cognition. Using targeted external\nsignals, we experimentally enhanced predictions across all tested models. In\nterms of dynamic inference cost our methodology can achieve an exclusion of up\nto $73.48\\,\\%$ of parameters and $84.41\\,\\%$ fewer giga-multiply-accumulate\n(GMAC) operations, analysis against comparative baselines show an average\nreduction of $40\\,\\%$ in parameters and $8\\,\\%$ in GMACs across the cases we\nevaluated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel network topology that seamlessly integrates\ndynamic inference cost with a top-down attention mechanism, addressing two\nsignificant gaps in traditional deep learning models. Drawing inspiration from\nhuman perception, we combine sequential processing of generic low-level\nfeatures with parallelism and nesting of high-level features. This design not\nonly reflects a finding from recent neuroscience research regarding - spatially\nand contextually distinct neural activations - in human cortex, but also\nintroduces a novel \"cutout\" technique: the ability to selectively activate\n%segments of the network for task-relevant only network segments of\ntask-relevant categories to optimize inference cost and eliminate the need for\nre-training. We believe this paves the way for future network designs that are\nlightweight and adaptable, making them suitable for a wide range of\napplications, from compact edge devices to large-scale clouds. Our proposed\ntopology also comes with a built-in top-down attention mechanism, which allows\nprocessing to be directly influenced by either enhancing or inhibiting\ncategory-specific high-level features, drawing parallels to the selective\nattention mechanism observed in human cognition. Using targeted external\nsignals, we experimentally enhanced predictions across all tested models. In\nterms of dynamic inference cost our methodology can achieve an exclusion of up\nto $73.48\\,\\%$ of parameters and $84.41\\,\\%$ fewer giga-multiply-accumulate\n(GMAC) operations, analysis against comparative baselines show an average\nreduction of $40\\,\\%$ in parameters and $8\\,\\%$ in GMACs across the cases we\nevaluated."
                },
                "authors": [
                    {
                        "name": "AndrÃ© Peter Kelm"
                    },
                    {
                        "name": "Niels Hannemann"
                    },
                    {
                        "name": "Bruno Heberle"
                    },
                    {
                        "name": "Lucas Schmidt"
                    },
                    {
                        "name": "Tim Rolff"
                    },
                    {
                        "name": "Christian Wilms"
                    },
                    {
                        "name": "Ehsan Yaghoubi"
                    },
                    {
                        "name": "Simone Frintrop"
                    }
                ],
                "author_detail": {
                    "name": "Simone Frintrop"
                },
                "author": "Simone Frintrop",
                "arxiv_comment": "Paper's findings on high-level parallelism and nested features\n  directly contributes to 'Selecting High-Level Features: Efficient Experts\n  from a Hierarchical Classification Network,' accepted at ICLR 2024's\n  Practical ML for Low Resource Settings (PML4LRS) workshop (non-archival); a\n  modified version has been accepted for presentation at the ICPR 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.05128v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.05128v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15665v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15665v3",
                "updated": "2024-11-20T09:08:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    8,
                    14,
                    2,
                    325,
                    0
                ],
                "published": "2024-10-21T06:09:30Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    6,
                    9,
                    30,
                    0,
                    295,
                    0
                ],
                "title": "Long Term Memory: The Foundation of AI Self-Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Term Memory: The Foundation of AI Self-Evolution"
                },
                "summary": "Large language models (LLMs) like GPTs, trained on vast datasets, have\ndemonstrated impressive capabilities in language understanding, reasoning, and\nplanning, achieving human-level performance in various tasks. Most studies\nfocus on enhancing these models by training on ever-larger datasets to build\nmore powerful foundation models. While training stronger models is important,\nenabling models to evolve during inference is equally crucial, a process we\nrefer to as AI self-evolution. Unlike large-scale training, self-evolution may\nrely on limited data or interactions. Inspired by the columnar organization of\nthe human cerebral cortex, we hypothesize that AI models could develop\ncognitive abilities and build internal representations through iterative\ninteractions with their environment. To achieve this, models need long-term\nmemory (LTM) to store and manage processed interaction data. LTM supports\nself-evolution by representing diverse experiences across environments and\nagents. In this report, we explore AI self-evolution and its potential to\nenhance models during inference. We examine LTM's role in lifelong learning,\nallowing models to evolve based on accumulated interactions. We outline the\nstructure of LTM and the systems needed for effective data retention and\nrepresentation. We also classify approaches for building personalized models\nwith LTM data and show how these models achieve self-evolution through\ninteraction. Using LTM, our multi-agent framework OMNE achieved first place on\nthe GAIA benchmark, demonstrating LTM's potential for AI self-evolution.\nFinally, we present a roadmap for future research, emphasizing the importance\nof LTM for advancing AI technology and its practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like GPTs, trained on vast datasets, have\ndemonstrated impressive capabilities in language understanding, reasoning, and\nplanning, achieving human-level performance in various tasks. Most studies\nfocus on enhancing these models by training on ever-larger datasets to build\nmore powerful foundation models. While training stronger models is important,\nenabling models to evolve during inference is equally crucial, a process we\nrefer to as AI self-evolution. Unlike large-scale training, self-evolution may\nrely on limited data or interactions. Inspired by the columnar organization of\nthe human cerebral cortex, we hypothesize that AI models could develop\ncognitive abilities and build internal representations through iterative\ninteractions with their environment. To achieve this, models need long-term\nmemory (LTM) to store and manage processed interaction data. LTM supports\nself-evolution by representing diverse experiences across environments and\nagents. In this report, we explore AI self-evolution and its potential to\nenhance models during inference. We examine LTM's role in lifelong learning,\nallowing models to evolve based on accumulated interactions. We outline the\nstructure of LTM and the systems needed for effective data retention and\nrepresentation. We also classify approaches for building personalized models\nwith LTM data and show how these models achieve self-evolution through\ninteraction. Using LTM, our multi-agent framework OMNE achieved first place on\nthe GAIA benchmark, demonstrating LTM's potential for AI self-evolution.\nFinally, we present a roadmap for future research, emphasizing the importance\nof LTM for advancing AI technology and its practical applications."
                },
                "authors": [
                    {
                        "name": "Xun Jiang"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Jiaying Wang"
                    },
                    {
                        "name": "Jun Shao"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shu Zhang"
                    },
                    {
                        "name": "Weiling Chen"
                    },
                    {
                        "name": "Xavier Tang"
                    },
                    {
                        "name": "Yize Chen"
                    },
                    {
                        "name": "Mengyue Wu"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Tianqiao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqiao Chen"
                },
                "author": "Tianqiao Chen",
                "arxiv_comment": "56 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15665v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15665v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13136v1",
                "updated": "2024-11-20T08:58:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    58,
                    59,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T08:58:59Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    58,
                    59,
                    2,
                    325,
                    0
                ],
                "title": "TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in\n  Vision-Language Models"
                },
                "summary": "Large pre-trained Vision-Language Models (VLMs) such as CLIP have\ndemonstrated excellent zero-shot generalizability across various downstream\ntasks. However, recent studies have shown that the inference performance of\nCLIP can be greatly degraded by small adversarial perturbations, especially its\nvisual modality, posing significant safety threats. To mitigate this\nvulnerability, in this paper, we propose a novel defense method called\nTest-Time Adversarial Prompt Tuning (TAPT) to enhance the inference robustness\nof CLIP against visual adversarial attacks. TAPT is a test-time defense method\nthat learns defensive bimodal (textual and visual) prompts to robustify the\ninference process of CLIP. Specifically, it is an unsupervised method that\noptimizes the defensive prompts for each test sample by minimizing a multi-view\nentropy and aligning adversarial-clean distributions. We evaluate the\neffectiveness of TAPT on 11 benchmark datasets, including ImageNet and 10 other\nzero-shot datasets, demonstrating that it enhances the zero-shot adversarial\nrobustness of the original CLIP by at least 48.9% against AutoAttack (AA),\nwhile largely maintaining performance on clean examples. Moreover, TAPT\noutperforms existing adversarial prompt tuning methods across various\nbackbones, achieving an average robustness improvement of at least 36.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained Vision-Language Models (VLMs) such as CLIP have\ndemonstrated excellent zero-shot generalizability across various downstream\ntasks. However, recent studies have shown that the inference performance of\nCLIP can be greatly degraded by small adversarial perturbations, especially its\nvisual modality, posing significant safety threats. To mitigate this\nvulnerability, in this paper, we propose a novel defense method called\nTest-Time Adversarial Prompt Tuning (TAPT) to enhance the inference robustness\nof CLIP against visual adversarial attacks. TAPT is a test-time defense method\nthat learns defensive bimodal (textual and visual) prompts to robustify the\ninference process of CLIP. Specifically, it is an unsupervised method that\noptimizes the defensive prompts for each test sample by minimizing a multi-view\nentropy and aligning adversarial-clean distributions. We evaluate the\neffectiveness of TAPT on 11 benchmark datasets, including ImageNet and 10 other\nzero-shot datasets, demonstrating that it enhances the zero-shot adversarial\nrobustness of the original CLIP by at least 48.9% against AutoAttack (AA),\nwhile largely maintaining performance on clean examples. Moreover, TAPT\noutperforms existing adversarial prompt tuning methods across various\nbackbones, achieving an average robustness improvement of at least 36.6%."
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Jingjing Chen"
                    },
                    {
                        "name": "Xingjun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xingjun Ma"
                },
                "author": "Xingjun Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05601v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05601v2",
                "updated": "2024-11-20T08:42:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    42,
                    4,
                    2,
                    325,
                    0
                ],
                "published": "2024-03-08T00:02:42Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    0,
                    2,
                    42,
                    4,
                    68,
                    0
                ],
                "title": "Select High-Level Features: Efficient Experts from a Hierarchical\n  Classification Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Select High-Level Features: Efficient Experts from a Hierarchical\n  Classification Network"
                },
                "summary": "This study introduces a novel expert generation method that dynamically\nreduces task and computational complexity without compromising predictive\nperformance. It is based on a new hierarchical classification network topology\nthat combines sequential processing of generic low-level features with\nparallelism and nesting of high-level features. This structure allows for the\ninnovative extraction technique: the ability to select only high-level features\nof task-relevant categories. In certain cases, it is possible to skip almost\nall unneeded high-level features, which can significantly reduce the inference\ncost and is highly beneficial in resource-constrained conditions. We believe\nthis method paves the way for future network designs that are lightweight and\nadaptable, making them suitable for a wide range of applications, from compact\nedge devices to large-scale clouds. In terms of dynamic inference our\nmethodology can achieve an exclusion of up to 88.7\\,\\% of parameters and\n73.4\\,\\% fewer giga-multiply accumulate (GMAC) operations, analysis against\ncomparative baselines showing an average reduction of 47.6\\,\\% in parameters\nand 5.8\\,\\% in GMACs across the cases we evaluated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces a novel expert generation method that dynamically\nreduces task and computational complexity without compromising predictive\nperformance. It is based on a new hierarchical classification network topology\nthat combines sequential processing of generic low-level features with\nparallelism and nesting of high-level features. This structure allows for the\ninnovative extraction technique: the ability to select only high-level features\nof task-relevant categories. In certain cases, it is possible to skip almost\nall unneeded high-level features, which can significantly reduce the inference\ncost and is highly beneficial in resource-constrained conditions. We believe\nthis method paves the way for future network designs that are lightweight and\nadaptable, making them suitable for a wide range of applications, from compact\nedge devices to large-scale clouds. In terms of dynamic inference our\nmethodology can achieve an exclusion of up to 88.7\\,\\% of parameters and\n73.4\\,\\% fewer giga-multiply accumulate (GMAC) operations, analysis against\ncomparative baselines showing an average reduction of 47.6\\,\\% in parameters\nand 5.8\\,\\% in GMACs across the cases we evaluated."
                },
                "authors": [
                    {
                        "name": "AndrÃ© Kelm"
                    },
                    {
                        "name": "Niels Hannemann"
                    },
                    {
                        "name": "Bruno Heberle"
                    },
                    {
                        "name": "Lucas Schmidt"
                    },
                    {
                        "name": "Tim Rolff"
                    },
                    {
                        "name": "Christian Wilms"
                    },
                    {
                        "name": "Ehsan Yaghoubi"
                    },
                    {
                        "name": "Simone Frintrop"
                    }
                ],
                "author_detail": {
                    "name": "Simone Frintrop"
                },
                "author": "Simone Frintrop",
                "arxiv_comment": "This two-page paper was accepted for a poster presentation at the 5th\n  ICLR 2024 Workshop on Practical ML for Limited/Low Resource Settings\n  (PML4LRS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05601v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05601v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13120v1",
                "updated": "2024-11-20T08:30:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    30,
                    11,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T08:30:11Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    30,
                    11,
                    2,
                    325,
                    0
                ],
                "title": "Virtual Staining of Label-Free Tissue in Imaging Mass Spectrometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Staining of Label-Free Tissue in Imaging Mass Spectrometry"
                },
                "summary": "Imaging mass spectrometry (IMS) is a powerful tool for untargeted, highly\nmultiplexed molecular mapping of tissue in biomedical research. IMS offers a\nmeans of mapping the spatial distributions of molecular species in biological\ntissue with unparalleled chemical specificity and sensitivity. However, most\nIMS platforms are not able to achieve microscopy-level spatial resolution and\nlack cellular morphological contrast, necessitating subsequent histochemical\nstaining, microscopic imaging and advanced image registration steps to enable\nmolecular distributions to be linked to specific tissue features and cell\ntypes. Here, we present a virtual histological staining approach that enhances\nspatial resolution and digitally introduces cellular morphological contrast\ninto mass spectrometry images of label-free human tissue using a diffusion\nmodel. Blind testing on human kidney tissue demonstrated that the virtually\nstained images of label-free samples closely match their histochemically\nstained counterparts (with Periodic Acid-Schiff staining), showing high\nconcordance in identifying key renal pathology structures despite utilizing IMS\ndata with 10-fold larger pixel size. Additionally, our approach employs an\noptimized noise sampling technique during the diffusion model's inference\nprocess to reduce variance in the generated images, yielding reliable and\nrepeatable virtual staining. We believe this virtual staining method will\nsignificantly expand the applicability of IMS in life sciences and open new\navenues for mass spectrometry-based biomedical research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imaging mass spectrometry (IMS) is a powerful tool for untargeted, highly\nmultiplexed molecular mapping of tissue in biomedical research. IMS offers a\nmeans of mapping the spatial distributions of molecular species in biological\ntissue with unparalleled chemical specificity and sensitivity. However, most\nIMS platforms are not able to achieve microscopy-level spatial resolution and\nlack cellular morphological contrast, necessitating subsequent histochemical\nstaining, microscopic imaging and advanced image registration steps to enable\nmolecular distributions to be linked to specific tissue features and cell\ntypes. Here, we present a virtual histological staining approach that enhances\nspatial resolution and digitally introduces cellular morphological contrast\ninto mass spectrometry images of label-free human tissue using a diffusion\nmodel. Blind testing on human kidney tissue demonstrated that the virtually\nstained images of label-free samples closely match their histochemically\nstained counterparts (with Periodic Acid-Schiff staining), showing high\nconcordance in identifying key renal pathology structures despite utilizing IMS\ndata with 10-fold larger pixel size. Additionally, our approach employs an\noptimized noise sampling technique during the diffusion model's inference\nprocess to reduce variance in the generated images, yielding reliable and\nrepeatable virtual staining. We believe this virtual staining method will\nsignificantly expand the applicability of IMS in life sciences and open new\navenues for mass spectrometry-based biomedical research."
                },
                "authors": [
                    {
                        "name": "Yijie Zhang"
                    },
                    {
                        "name": "Luzhe Huang"
                    },
                    {
                        "name": "Nir Pillar"
                    },
                    {
                        "name": "Yuzhu Li"
                    },
                    {
                        "name": "Lukasz G. Migas"
                    },
                    {
                        "name": "Raf Van de Plas"
                    },
                    {
                        "name": "Jeffrey M. Spraggins"
                    },
                    {
                        "name": "Aydogan Ozcan"
                    }
                ],
                "author_detail": {
                    "name": "Aydogan Ozcan"
                },
                "author": "Aydogan Ozcan",
                "arxiv_comment": "33 Pages, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13118v1",
                "updated": "2024-11-20T08:26:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    26,
                    40,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T08:26:40Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    26,
                    40,
                    2,
                    325,
                    0
                ],
                "title": "Using ChatGPT-4 for the Identification of Common UX Factors within a\n  Pool of Measurement Items from Established UX Questionnaires",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using ChatGPT-4 for the Identification of Common UX Factors within a\n  Pool of Measurement Items from Established UX Questionnaires"
                },
                "summary": "Measuring User Experience (UX) with standardized questionnaires is a widely\nused method. A questionnaire is based on different scales that represent UX\nfactors and items. However, the questionnaires have no common ground concerning\nnaming different factors and the items used to measure them. This study aims to\nidentify general UX factors based on the formulation of the measurement items.\nItems from a set of 40 established UX questionnaires were analyzed by\nGenerative AI (GenAI) to identify semantically similar items and to cluster\nsimilar topics. We used the LLM ChatGPT-4 for this analysis. Results show that\nChatGPT-4 can classify items into meaningful topics and thus help to create a\ndeeper understanding of the structure of the UX research field. In addition, we\nshow that ChatGPT-4 can filter items related to a predefined UX concept out of\na pool of UX items.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring User Experience (UX) with standardized questionnaires is a widely\nused method. A questionnaire is based on different scales that represent UX\nfactors and items. However, the questionnaires have no common ground concerning\nnaming different factors and the items used to measure them. This study aims to\nidentify general UX factors based on the formulation of the measurement items.\nItems from a set of 40 established UX questionnaires were analyzed by\nGenerative AI (GenAI) to identify semantically similar items and to cluster\nsimilar topics. We used the LLM ChatGPT-4 for this analysis. Results show that\nChatGPT-4 can classify items into meaningful topics and thus help to create a\ndeeper understanding of the structure of the UX research field. In addition, we\nshow that ChatGPT-4 can filter items related to a predefined UX concept out of\na pool of UX items."
                },
                "authors": [
                    {
                        "name": "Stefan Graser"
                    },
                    {
                        "name": "Stephan BÃ¶hm"
                    },
                    {
                        "name": "Martin Schrepp"
                    }
                ],
                "author_detail": {
                    "name": "Martin Schrepp"
                },
                "author": "Martin Schrepp",
                "arxiv_comment": "10 pages, 1 figure, The Sixteenth International Conference on\n  Advances in Human-oriented and Personalized Mechanisms, Technologies, and\n  Services CENTRIC 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13117v1",
                "updated": "2024-11-20T08:21:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    21,
                    53,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T08:21:53Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    21,
                    53,
                    2,
                    325,
                    0
                ],
                "title": "Compute Optimal Inference and Provable Amortisation Gap in Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Optimal Inference and Provable Amortisation Gap in Sparse\n  Autoencoders"
                },
                "summary": "A recent line of work has shown promise in using sparse autoencoders (SAEs)\nto uncover interpretable features in neural network representations. However,\nthe simple linear-nonlinear encoding mechanism in SAEs limits their ability to\nperform accurate sparse inference. In this paper, we investigate sparse\ninference and learning in SAEs through the lens of sparse coding. Specifically,\nwe show that SAEs perform amortised sparse inference with a computationally\nrestricted encoder and, using compressed sensing theory, we prove that this\nmapping is inherently insufficient for accurate sparse inference, even in\nsolvable cases. Building on this theory, we empirically explore conditions\nwhere more sophisticated sparse inference methods outperform traditional SAE\nencoders. Our key contribution is the decoupling of the encoding and decoding\nprocesses, which allows for a comparison of various sparse encoding strategies.\nWe evaluate these strategies on two dimensions: alignment with true underlying\nsparse features and correct inference of sparse codes, while also accounting\nfor computational costs during training and inference. Our results reveal that\nsubstantial performance gains can be achieved with minimal increases in compute\ncost. We demonstrate that this generalises to SAEs applied to large language\nmodels (LLMs), where advanced encoders achieve similar interpretability. This\nwork opens new avenues for understanding neural network representations and\noffers important implications for improving the tools we use to analyse the\nactivations of large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent line of work has shown promise in using sparse autoencoders (SAEs)\nto uncover interpretable features in neural network representations. However,\nthe simple linear-nonlinear encoding mechanism in SAEs limits their ability to\nperform accurate sparse inference. In this paper, we investigate sparse\ninference and learning in SAEs through the lens of sparse coding. Specifically,\nwe show that SAEs perform amortised sparse inference with a computationally\nrestricted encoder and, using compressed sensing theory, we prove that this\nmapping is inherently insufficient for accurate sparse inference, even in\nsolvable cases. Building on this theory, we empirically explore conditions\nwhere more sophisticated sparse inference methods outperform traditional SAE\nencoders. Our key contribution is the decoupling of the encoding and decoding\nprocesses, which allows for a comparison of various sparse encoding strategies.\nWe evaluate these strategies on two dimensions: alignment with true underlying\nsparse features and correct inference of sparse codes, while also accounting\nfor computational costs during training and inference. Our results reveal that\nsubstantial performance gains can be achieved with minimal increases in compute\ncost. We demonstrate that this generalises to SAEs applied to large language\nmodels (LLMs), where advanced encoders achieve similar interpretability. This\nwork opens new avenues for understanding neural network representations and\noffers important implications for improving the tools we use to analyse the\nactivations of large language models."
                },
                "authors": [
                    {
                        "name": "Charles O'Neill"
                    },
                    {
                        "name": "David Klindt"
                    }
                ],
                "author_detail": {
                    "name": "David Klindt"
                },
                "author": "David Klindt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08903v2",
                "updated": "2024-11-20T07:42:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    7,
                    42,
                    38,
                    2,
                    325,
                    0
                ],
                "published": "2024-06-13T07:57:27Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    7,
                    57,
                    27,
                    3,
                    165,
                    0
                ],
                "title": "Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for\n  Large Language Models"
                },
                "summary": "Fine-tuning is a crucial process for adapting large language models (LLMs) to\ndiverse applications. In certain scenarios, such as multi-tenant serving,\ndeploying multiple LLMs becomes necessary to meet complex demands. Recent\nstudies suggest decomposing a fine-tuned LLM into a base model and\ncorresponding delta weights, which are then compressed using low-rank or\nlow-bit approaches to reduce costs. In this work, we observe that existing\nlow-rank and low-bit compression methods can significantly harm the model\nperformance for task-specific fine-tuned LLMs (e.g., WizardMath for math\nproblems). Motivated by the long-tail distribution of singular values in the\ndelta weights, we propose a delta quantization approach using mixed-precision.\nThis method employs higher-bit representation for singular vectors\ncorresponding to larger singular values. We evaluate our approach on various\nfine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs.\nExperimental results demonstrate that our approach performs comparably to full\nfine-tuned LLMs, surpassing both low-rank and low-bit baselines by a\nconsiderable margin. Additionally, we show that our method is compatible with\nvarious backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning is a crucial process for adapting large language models (LLMs) to\ndiverse applications. In certain scenarios, such as multi-tenant serving,\ndeploying multiple LLMs becomes necessary to meet complex demands. Recent\nstudies suggest decomposing a fine-tuned LLM into a base model and\ncorresponding delta weights, which are then compressed using low-rank or\nlow-bit approaches to reduce costs. In this work, we observe that existing\nlow-rank and low-bit compression methods can significantly harm the model\nperformance for task-specific fine-tuned LLMs (e.g., WizardMath for math\nproblems). Motivated by the long-tail distribution of singular values in the\ndelta weights, we propose a delta quantization approach using mixed-precision.\nThis method employs higher-bit representation for singular vectors\ncorresponding to larger singular values. We evaluate our approach on various\nfine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs.\nExperimental results demonstrate that our approach performs comparably to full\nfine-tuned LLMs, surpassing both low-rank and low-bit baselines by a\nconsiderable margin. Additionally, we show that our method is compatible with\nvarious backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Bowen Ping"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00055v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00055v5",
                "updated": "2024-11-20T07:08:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    7,
                    8,
                    22,
                    2,
                    325,
                    0
                ],
                "published": "2024-08-21T04:47:26Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    4,
                    47,
                    26,
                    2,
                    234,
                    0
                ],
                "title": "SORSA: Singular Values and Orthonormal Regularized Singular Vectors\n  Adaptation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SORSA: Singular Values and Orthonormal Regularized Singular Vectors\n  Adaptation of Large Language Models"
                },
                "summary": "In this paper, we propose Singular Values and Orthonormal Regularized\nSingular Vectors Adaptation, or SORSA, a novel PEFT method. Each SORSA adapter\nconsists of two main parts: trainable principal singular weights $W_p = U_p\n\\text{diag}(S_p) V^\\top_p$, and frozen residual weights $W_r = U_r\n\\text{diag}(S_r) V^\\top_r$. These parts are initialized by performing singular\nvalue decomposition (SVD) on pre-trained weights. Moreover, we implement and\nanalyze an orthonormal regularizer, which we prove could decrease the condition\nnumber of $W_p$ and make the optimization more efficient. SORSA adapters could\nbe merged during inference, thus eliminating any inference latency. We also\nintroduce a method to analyze the variation of the parameters by performing SVD\nand discuss and analyze SORSA's superiority in minimizing the alteration in the\nSVD aspect. After all, SORSA shows a faster convergence than LoRA and PiSSA in\nour experiments. On the GSM-8K benchmark, Llama 2 7B adapted using SORSA\nachieved 56.03% accuracy, surpassing LoRA (42.30%), AdaLoRA (47.30%), Full FT\n(49.05%), and PiSSA (53.07%). On the MATH benchmark, SORSA achieved 10.36%\naccuracy, outperforming LoRA (5.50%), AdaLoRA (6.48%), Full FT (7.22%), and\nPiSSA (7.44%). We conclude that SORSA offers a new perspective on\nparameter-efficient fine-tuning, demonstrating remarkable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Singular Values and Orthonormal Regularized\nSingular Vectors Adaptation, or SORSA, a novel PEFT method. Each SORSA adapter\nconsists of two main parts: trainable principal singular weights $W_p = U_p\n\\text{diag}(S_p) V^\\top_p$, and frozen residual weights $W_r = U_r\n\\text{diag}(S_r) V^\\top_r$. These parts are initialized by performing singular\nvalue decomposition (SVD) on pre-trained weights. Moreover, we implement and\nanalyze an orthonormal regularizer, which we prove could decrease the condition\nnumber of $W_p$ and make the optimization more efficient. SORSA adapters could\nbe merged during inference, thus eliminating any inference latency. We also\nintroduce a method to analyze the variation of the parameters by performing SVD\nand discuss and analyze SORSA's superiority in minimizing the alteration in the\nSVD aspect. After all, SORSA shows a faster convergence than LoRA and PiSSA in\nour experiments. On the GSM-8K benchmark, Llama 2 7B adapted using SORSA\nachieved 56.03% accuracy, surpassing LoRA (42.30%), AdaLoRA (47.30%), Full FT\n(49.05%), and PiSSA (53.07%). On the MATH benchmark, SORSA achieved 10.36%\naccuracy, outperforming LoRA (5.50%), AdaLoRA (6.48%), Full FT (7.22%), and\nPiSSA (7.44%). We conclude that SORSA offers a new perspective on\nparameter-efficient fine-tuning, demonstrating remarkable performance."
                },
                "authors": [
                    {
                        "name": "Yang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Cao"
                },
                "author": "Yang Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00055v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00055v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.03022v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.03022v3",
                "updated": "2024-11-20T07:07:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    7,
                    7,
                    41,
                    2,
                    325,
                    0
                ],
                "published": "2023-12-05T07:27:08Z",
                "published_parsed": [
                    2023,
                    12,
                    5,
                    7,
                    27,
                    8,
                    1,
                    339,
                    0
                ],
                "title": "Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph\n  Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph\n  Construction"
                },
                "summary": "This paper introduces CooperKGC, a novel framework challenging the\nconventional solitary approach of large language models (LLMs) in knowledge\ngraph construction (KGC). CooperKGC establishes a collaborative processing\nnetwork, assembling a team capable of concurrently addressing entity, relation,\nand event extraction tasks. Experimentation demonstrates that fostering\ncollaboration within CooperKGC enhances knowledge selection, correction, and\naggregation capabilities across multiple rounds of interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces CooperKGC, a novel framework challenging the\nconventional solitary approach of large language models (LLMs) in knowledge\ngraph construction (KGC). CooperKGC establishes a collaborative processing\nnetwork, assembling a team capable of concurrently addressing entity, relation,\nand event extraction tasks. Experimentation demonstrates that fostering\ncollaboration within CooperKGC enhances knowledge selection, correction, and\naggregation capabilities across multiple rounds of interactions."
                },
                "authors": [
                    {
                        "name": "Hongbin Ye"
                    },
                    {
                        "name": "Honghao Gui"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Weiqiang Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weiqiang Jia"
                },
                "author": "Weiqiang Jia",
                "arxiv_comment": "Accepted by CCKS 2024, best english candidate paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.03022v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.03022v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13076v1",
                "updated": "2024-11-20T06:58:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    6,
                    58,
                    33,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T06:58:33Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    6,
                    58,
                    33,
                    2,
                    325,
                    0
                ],
                "title": "Hints of Prompt: Enhancing Visual Representation for Multimodal LLMs in\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hints of Prompt: Enhancing Visual Representation for Multimodal LLMs in\n  Autonomous Driving"
                },
                "summary": "In light of the dynamic nature of autonomous driving environments and\nstringent safety requirements, general MLLMs combined with CLIP alone often\nstruggle to represent driving-specific scenarios accurately, particularly in\ncomplex interactions and long-tail cases. To address this, we propose the Hints\nof Prompt (HoP) framework, which introduces three key enhancements: Affinity\nhint to emphasize instance-level structure by strengthening token-wise\nconnections, Semantic hint to incorporate high-level information relevant to\ndriving-specific cases, such as complex interactions among vehicles and traffic\nsigns, and Question hint to align visual features with the query context,\nfocusing on question-relevant regions. These hints are fused through a Hint\nFusion module, enriching visual representations and enhancing multimodal\nreasoning for autonomous driving VQA tasks. Extensive experiments confirm the\neffectiveness of the HoP framework, showing it significantly outperforms\nprevious state-of-the-art methods across all key metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In light of the dynamic nature of autonomous driving environments and\nstringent safety requirements, general MLLMs combined with CLIP alone often\nstruggle to represent driving-specific scenarios accurately, particularly in\ncomplex interactions and long-tail cases. To address this, we propose the Hints\nof Prompt (HoP) framework, which introduces three key enhancements: Affinity\nhint to emphasize instance-level structure by strengthening token-wise\nconnections, Semantic hint to incorporate high-level information relevant to\ndriving-specific cases, such as complex interactions among vehicles and traffic\nsigns, and Question hint to align visual features with the query context,\nfocusing on question-relevant regions. These hints are fused through a Hint\nFusion module, enriching visual representations and enhancing multimodal\nreasoning for autonomous driving VQA tasks. Extensive experiments confirm the\neffectiveness of the HoP framework, showing it significantly outperforms\nprevious state-of-the-art methods across all key metrics."
                },
                "authors": [
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhanning Gao"
                    },
                    {
                        "name": "Maosheng Ye"
                    },
                    {
                        "name": "Zhili Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Tongyi Cao"
                    },
                    {
                        "name": "Honggang Qi"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Qi"
                },
                "author": "Honggang Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13055v1",
                "updated": "2024-11-20T06:05:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    6,
                    5,
                    11,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T06:05:11Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    6,
                    5,
                    11,
                    2,
                    325,
                    0
                ],
                "title": "Hardware Scaling Trends and Diminishing Returns in Large-Scale\n  Distributed Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware Scaling Trends and Diminishing Returns in Large-Scale\n  Distributed Training"
                },
                "summary": "Dramatic increases in the capabilities of neural network models in recent\nyears are driven by scaling model size, training data, and corresponding\ncomputational resources. To develop the exceedingly large networks required in\nmodern applications, such as large language models (LLMs), model training is\ndistributed across tens of thousands of hardware accelerators (e.g. GPUs),\nrequiring orchestration of computation and communication across large computing\nclusters. In this work, we demonstrate that careful consideration of hardware\nconfiguration and parallelization strategy is critical for effective (i.e.\ncompute- and cost-efficient) scaling of model size, training data, and total\ncomputation. We conduct an extensive empirical study of the performance of\nlarge-scale LLM training workloads across model size, hardware configurations,\nand distributed parallelization strategies. We demonstrate that: (1) beyond\ncertain scales, overhead incurred from certain distributed communication\nstrategies leads parallelization strategies previously thought to be\nsub-optimal in fact become preferable; and (2) scaling the total number of\naccelerators for large model training quickly yields diminishing returns even\nwhen hardware and parallelization strategies are properly optimized, implying\npoor marginal performance per additional unit of power or GPU-hour.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dramatic increases in the capabilities of neural network models in recent\nyears are driven by scaling model size, training data, and corresponding\ncomputational resources. To develop the exceedingly large networks required in\nmodern applications, such as large language models (LLMs), model training is\ndistributed across tens of thousands of hardware accelerators (e.g. GPUs),\nrequiring orchestration of computation and communication across large computing\nclusters. In this work, we demonstrate that careful consideration of hardware\nconfiguration and parallelization strategy is critical for effective (i.e.\ncompute- and cost-efficient) scaling of model size, training data, and total\ncomputation. We conduct an extensive empirical study of the performance of\nlarge-scale LLM training workloads across model size, hardware configurations,\nand distributed parallelization strategies. We demonstrate that: (1) beyond\ncertain scales, overhead incurred from certain distributed communication\nstrategies leads parallelization strategies previously thought to be\nsub-optimal in fact become preferable; and (2) scaling the total number of\naccelerators for large model training quickly yields diminishing returns even\nwhen hardware and parallelization strategies are properly optimized, implying\npoor marginal performance per additional unit of power or GPU-hour."
                },
                "authors": [
                    {
                        "name": "Jared Fernandez"
                    },
                    {
                        "name": "Luca Wehrstedt"
                    },
                    {
                        "name": "Leonid Shamis"
                    },
                    {
                        "name": "Mostafa Elhoushi"
                    },
                    {
                        "name": "Kalyan Saladi"
                    },
                    {
                        "name": "Yonatan Bisk"
                    },
                    {
                        "name": "Emma Strubell"
                    },
                    {
                        "name": "Jacob Kahn"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Kahn"
                },
                "author": "Jacob Kahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.02926v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.02926v3",
                "updated": "2024-11-20T06:01:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    6,
                    1,
                    23,
                    2,
                    325,
                    0
                ],
                "published": "2023-09-06T11:39:37Z",
                "published_parsed": [
                    2023,
                    9,
                    6,
                    11,
                    39,
                    37,
                    2,
                    249,
                    0
                ],
                "title": "Demystifying RCE Vulnerabilities in LLM-Integrated Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying RCE Vulnerabilities in LLM-Integrated Apps"
                },
                "summary": "LLMs show promise in transforming software development, with a growing\ninterest in integrating them into more intelligent apps. Frameworks like\nLangChain aid LLM-integrated app development, offering code execution\nutility/APIs for custom actions. However, these capabilities theoretically\nintroduce Remote Code Execution (RCE) vulnerabilities, enabling remote code\nexecution through prompt injections. No prior research systematically\ninvestigates these frameworks' RCE vulnerabilities or their impact on\napplications and exploitation consequences. Therefore, there is a huge research\ngap in this field. In this study, we propose LLMSmith to detect, validate and\nexploit the RCE vulnerabilities in LLM-integrated frameworks and apps. To\nachieve this goal, we develop two novel techniques, including 1) a lightweight\nstatic analysis to examine LLM integration mechanisms, and construct call\nchains to identify RCE vulnerabilities in frameworks; 2) a systematical\nprompt-based exploitation method to verify and exploit the found\nvulnerabilities in LLM-integrated apps. This technique involves various\nstrategies to control LLM outputs, trigger RCE vulnerabilities and launch\nsubsequent attacks. Our research has uncovered a total of 20 vulnerabilities in\n11 LLM-integrated frameworks, comprising 19 RCE vulnerabilities and 1 arbitrary\nfile read/write vulnerability. Of these, 17 have been confirmed by the\nframework developers, with 11 vulnerabilities being assigned CVE IDs. For the\n51 apps potentially affected by RCE, we successfully executed attacks on 17\napps, 16 of which are vulnerable to RCE and 1 to SQL injection. Furthermore, we\nconduct a comprehensive analysis of these vulnerabilities and construct\npractical attacks to demonstrate the hazards in reality. Last, we propose\nseveral mitigation measures for both framework and app developers to counteract\nsuch attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs show promise in transforming software development, with a growing\ninterest in integrating them into more intelligent apps. Frameworks like\nLangChain aid LLM-integrated app development, offering code execution\nutility/APIs for custom actions. However, these capabilities theoretically\nintroduce Remote Code Execution (RCE) vulnerabilities, enabling remote code\nexecution through prompt injections. No prior research systematically\ninvestigates these frameworks' RCE vulnerabilities or their impact on\napplications and exploitation consequences. Therefore, there is a huge research\ngap in this field. In this study, we propose LLMSmith to detect, validate and\nexploit the RCE vulnerabilities in LLM-integrated frameworks and apps. To\nachieve this goal, we develop two novel techniques, including 1) a lightweight\nstatic analysis to examine LLM integration mechanisms, and construct call\nchains to identify RCE vulnerabilities in frameworks; 2) a systematical\nprompt-based exploitation method to verify and exploit the found\nvulnerabilities in LLM-integrated apps. This technique involves various\nstrategies to control LLM outputs, trigger RCE vulnerabilities and launch\nsubsequent attacks. Our research has uncovered a total of 20 vulnerabilities in\n11 LLM-integrated frameworks, comprising 19 RCE vulnerabilities and 1 arbitrary\nfile read/write vulnerability. Of these, 17 have been confirmed by the\nframework developers, with 11 vulnerabilities being assigned CVE IDs. For the\n51 apps potentially affected by RCE, we successfully executed attacks on 17\napps, 16 of which are vulnerable to RCE and 1 to SQL injection. Furthermore, we\nconduct a comprehensive analysis of these vulnerabilities and construct\npractical attacks to demonstrate the hazards in reality. Last, we propose\nseveral mitigation measures for both framework and app developers to counteract\nsuch attacks."
                },
                "authors": [
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Zizhuang Deng"
                    },
                    {
                        "name": "Guozhu Meng"
                    },
                    {
                        "name": "Yuekang Li"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.02926v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.02926v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13050v1",
                "updated": "2024-11-20T05:51:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    5,
                    51,
                    35,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T05:51:35Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    5,
                    51,
                    35,
                    2,
                    325,
                    0
                ],
                "title": "Topkima-Former: Low-energy, Low-Latency Inference for Transformers using\n  top-k In-memory ADC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topkima-Former: Low-energy, Low-Latency Inference for Transformers using\n  top-k In-memory ADC"
                },
                "summary": "Transformer model has gained prominence as a popular deep neural network\narchitecture for neural language processing (NLP) and computer vision (CV)\napplications. However, the extensive use of nonlinear operations, like softmax,\nposes a performance bottleneck during transformer inference and comprises up to\n40% of the total latency. Hence, we propose innovations at the circuit,\narchitecture, and algorithm levels to accelerate the transformer. At the\ncircuit level, we propose topkima-combining top-k activation selection with\nin-memory ADC (IMA) to implement a low-energy and low-latency softmax without\nany sorting latency. Only the k largest activations are sent to the softmax\ncalculation block, reducing the huge computational cost of softmax. Using a\nmodified training scheme with top-k only in the forward pass, experimental\nresults demonstrate only a 0.4% to 1.2% reduction in accuracy across ViT,\ndistilBERT, and BERT-base models when evaluated on CIFAR-10, CIFAR-100, and\nSQuAD datasets with k=5. At the architecture level, an improved scale-free\ntechnique is introduced to reduce the computational cost of attention. The\ncombined system, dubbed Topkima-Former, enhances 1.8x-84x speedup and 1.3x-35x\nenergy efficiency (EE) over prior In-memory computing (IMC) accelerators.\nCompared to a conventional softmax macro and a digital top-k (Dtopk) softmax\nmacro, our proposed tokima softmax macro achieves about 15x and 8x faster speed\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer model has gained prominence as a popular deep neural network\narchitecture for neural language processing (NLP) and computer vision (CV)\napplications. However, the extensive use of nonlinear operations, like softmax,\nposes a performance bottleneck during transformer inference and comprises up to\n40% of the total latency. Hence, we propose innovations at the circuit,\narchitecture, and algorithm levels to accelerate the transformer. At the\ncircuit level, we propose topkima-combining top-k activation selection with\nin-memory ADC (IMA) to implement a low-energy and low-latency softmax without\nany sorting latency. Only the k largest activations are sent to the softmax\ncalculation block, reducing the huge computational cost of softmax. Using a\nmodified training scheme with top-k only in the forward pass, experimental\nresults demonstrate only a 0.4% to 1.2% reduction in accuracy across ViT,\ndistilBERT, and BERT-base models when evaluated on CIFAR-10, CIFAR-100, and\nSQuAD datasets with k=5. At the architecture level, an improved scale-free\ntechnique is introduced to reduce the computational cost of attention. The\ncombined system, dubbed Topkima-Former, enhances 1.8x-84x speedup and 1.3x-35x\nenergy efficiency (EE) over prior In-memory computing (IMC) accelerators.\nCompared to a conventional softmax macro and a digital top-k (Dtopk) softmax\nmacro, our proposed tokima softmax macro achieves about 15x and 8x faster speed\nrespectively."
                },
                "authors": [
                    {
                        "name": "Shuai Dong"
                    },
                    {
                        "name": "Junyi Yang"
                    },
                    {
                        "name": "Xiaoqi Peng"
                    },
                    {
                        "name": "Hongyang Shang"
                    },
                    {
                        "name": "Ye Ke"
                    },
                    {
                        "name": "Xiaofeng Yang"
                    },
                    {
                        "name": "Hongjie Liu"
                    },
                    {
                        "name": "Arindam Basu"
                    }
                ],
                "author_detail": {
                    "name": "Arindam Basu"
                },
                "author": "Arindam Basu",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13045v1",
                "updated": "2024-11-20T05:30:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    5,
                    30,
                    15,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T05:30:15Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    5,
                    30,
                    15,
                    2,
                    325,
                    0
                ],
                "title": "Explainable LLM-driven Multi-dimensional Distillation for E-Commerce\n  Relevance Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable LLM-driven Multi-dimensional Distillation for E-Commerce\n  Relevance Learning"
                },
                "summary": "Effective query-item relevance modeling is pivotal for enhancing user\nexperience and safeguarding user satisfaction in e-commerce search systems.\nRecently, benefiting from the vast inherent knowledge, Large Language Model\n(LLM) approach demonstrates strong performance and long-tail generalization\nability compared with previous neural-based specialized relevance learning\nmethods. Though promising, current LLM-based methods encounter the following\ninadequacies in practice: First, the massive parameters and computational\ndemands make it difficult to be deployed online. Second, distilling LLM models\nto online models is a feasible direction, but the LLM relevance modeling is a\nblack box, and its rich intrinsic knowledge is difficult to extract and apply\nonline. To improve the interpretability of LLM and boost the performance of\nonline relevance models via LLM, we propose an Explainable LLM-driven\nMulti-dimensional Distillation framework for e-commerce relevance learning,\nwhich comprises two core components: (1) An Explainable LLM for relevance\nmodeling (ELLM-rele), which decomposes the relevance learning into intermediate\nsteps and models relevance learning as a Chain-of-Thought (CoT) reasoning,\nthereby enhancing both interpretability and performance of LLM. (2) A\nMulti-dimensional Knowledge Distillation (MKD) architecture that transfers the\nknowledge of ELLM-rele to current deployable interaction-based and\nrepresentation-based student models from both the relevance score distribution\nand CoT reasoning aspects. Through distilling the probabilistic and CoT\nreasoning knowledge, MKD improves both the semantic interaction and long-tail\ngeneralization abilities of student models. Extensive offline evaluations and\nonline experiments on Taobao search ad scene demonstrate that our proposed\nframework significantly enhances e-commerce relevance learning performance and\nuser experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective query-item relevance modeling is pivotal for enhancing user\nexperience and safeguarding user satisfaction in e-commerce search systems.\nRecently, benefiting from the vast inherent knowledge, Large Language Model\n(LLM) approach demonstrates strong performance and long-tail generalization\nability compared with previous neural-based specialized relevance learning\nmethods. Though promising, current LLM-based methods encounter the following\ninadequacies in practice: First, the massive parameters and computational\ndemands make it difficult to be deployed online. Second, distilling LLM models\nto online models is a feasible direction, but the LLM relevance modeling is a\nblack box, and its rich intrinsic knowledge is difficult to extract and apply\nonline. To improve the interpretability of LLM and boost the performance of\nonline relevance models via LLM, we propose an Explainable LLM-driven\nMulti-dimensional Distillation framework for e-commerce relevance learning,\nwhich comprises two core components: (1) An Explainable LLM for relevance\nmodeling (ELLM-rele), which decomposes the relevance learning into intermediate\nsteps and models relevance learning as a Chain-of-Thought (CoT) reasoning,\nthereby enhancing both interpretability and performance of LLM. (2) A\nMulti-dimensional Knowledge Distillation (MKD) architecture that transfers the\nknowledge of ELLM-rele to current deployable interaction-based and\nrepresentation-based student models from both the relevance score distribution\nand CoT reasoning aspects. Through distilling the probabilistic and CoT\nreasoning knowledge, MKD improves both the semantic interaction and long-tail\ngeneralization abilities of student models. Extensive offline evaluations and\nonline experiments on Taobao search ad scene demonstrate that our proposed\nframework significantly enhances e-commerce relevance learning performance and\nuser experience."
                },
                "authors": [
                    {
                        "name": "Gang Zhao"
                    },
                    {
                        "name": "Ximing Zhang"
                    },
                    {
                        "name": "Chenji Lu"
                    },
                    {
                        "name": "Hui Zhao"
                    },
                    {
                        "name": "Tianshu Wu"
                    },
                    {
                        "name": "Pengjie Wang"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "Submitted to WWW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12279v2",
                "updated": "2024-11-20T05:05:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    5,
                    5,
                    48,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-19T06:57:45Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    57,
                    45,
                    1,
                    324,
                    0
                ],
                "title": "HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation"
                },
                "summary": "This paper proposes a two-phase text-to-floorplan generation method, which\nguides a Large Language Model (LLM) to generate an initial layout (Layout-LLM)\nand refines them into the final floorplans through conditional diffusion model.\nWe incorporate a Chain-of-Thought approach to prompt the LLM based on user text\nspecifications, enabling a more user-friendly and intuitive house layout\ndesign. This method allows users to describe their needs in natural language,\nenhancing accessibility and providing clearer geometric constraints. The final\nfloorplans generated by Layout-LLM through conditional diffusion refinement are\nmore accurate and better meet user requirements. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, validating its effectiveness in practical home design applications. We\nplan to release our code for public use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a two-phase text-to-floorplan generation method, which\nguides a Large Language Model (LLM) to generate an initial layout (Layout-LLM)\nand refines them into the final floorplans through conditional diffusion model.\nWe incorporate a Chain-of-Thought approach to prompt the LLM based on user text\nspecifications, enabling a more user-friendly and intuitive house layout\ndesign. This method allows users to describe their needs in natural language,\nenhancing accessibility and providing clearer geometric constraints. The final\nfloorplans generated by Layout-LLM through conditional diffusion refinement are\nmore accurate and better meet user requirements. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, validating its effectiveness in practical home design applications. We\nplan to release our code for public use."
                },
                "authors": [
                    {
                        "name": "Ziyang Zong"
                    },
                    {
                        "name": "Zhaohuan Zhan"
                    },
                    {
                        "name": "Guang Tan"
                    }
                ],
                "author_detail": {
                    "name": "Guang Tan"
                },
                "author": "Guang Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10445v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10445v3",
                "updated": "2024-11-20T04:51:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    4,
                    51,
                    59,
                    2,
                    325,
                    0
                ],
                "published": "2024-04-16T10:31:06Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    10,
                    31,
                    6,
                    1,
                    107,
                    0
                ],
                "title": "SparseDM: Toward Sparse Efficient Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseDM: Toward Sparse Efficient Diffusion Models"
                },
                "summary": "Diffusion models have been extensively used in data generation tasks and are\nrecognized as one of the best generative models. However, their time-consuming\ndeployment, long inference time, and requirements on large memory limit their\napplication on mobile devices. In this paper, we propose a method based on the\nimproved Straight-Through Estimator to improve the deployment efficiency of\ndiffusion models. Specifically, we add sparse masks to the Convolution and\nLinear layers in a pre-trained diffusion model, then use design progressive\nsparsity for model training in the fine-tuning stage, and switch the inference\nmask on and off, which supports a flexible choice of sparsity during inference\naccording to the FID and MACs requirements. Experiments on four datasets\nconducted on a state-of-the-art Transformer-based diffusion model demonstrate\nthat our method reduces MACs by $50\\%$ while increasing FID by only 1.5 on\naverage. Under other MACs conditions, the FID is also lower than 1$\\sim$137\ncompared to other methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have been extensively used in data generation tasks and are\nrecognized as one of the best generative models. However, their time-consuming\ndeployment, long inference time, and requirements on large memory limit their\napplication on mobile devices. In this paper, we propose a method based on the\nimproved Straight-Through Estimator to improve the deployment efficiency of\ndiffusion models. Specifically, we add sparse masks to the Convolution and\nLinear layers in a pre-trained diffusion model, then use design progressive\nsparsity for model training in the fine-tuning stage, and switch the inference\nmask on and off, which supports a flexible choice of sparsity during inference\naccording to the FID and MACs requirements. Experiments on four datasets\nconducted on a state-of-the-art Transformer-based diffusion model demonstrate\nthat our method reduces MACs by $50\\%$ while increasing FID by only 1.5 on\naverage. Under other MACs conditions, the FID is also lower than 1$\\sim$137\ncompared to other methods."
                },
                "authors": [
                    {
                        "name": "Kafeng Wang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "He Li"
                    },
                    {
                        "name": "Zhenpeng Mi"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10445v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10445v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13032v1",
                "updated": "2024-11-20T04:42:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    4,
                    42,
                    32,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T04:42:32Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    4,
                    42,
                    32,
                    2,
                    325,
                    0
                ],
                "title": "\"It was 80% me, 20% AI\": Seeking Authenticity in Co-Writing with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"It was 80% me, 20% AI\": Seeking Authenticity in Co-Writing with Large\n  Language Models"
                },
                "summary": "Given the rising proliferation and diversity of AI writing assistance tools,\nespecially those powered by large language models (LLMs), both writers and\nreaders may have concerns about the impact of these tools on the authenticity\nof writing work. We examine whether and how writers want to preserve their\nauthentic voice when co-writing with AI tools and whether personalization of AI\nwriting support could help achieve this goal. We conducted semi-structured\ninterviews with 19 professional writers, during which they co-wrote with both\npersonalized and non-personalized AI writing-support tools. We supplemented\nwriters' perspectives with opinions from 30 avid readers about the written work\nco-produced with AI collected through an online survey. Our findings illuminate\nconceptions of authenticity in human-AI co-creation, which focus more on the\nprocess and experience of constructing creators' authentic selves. While\nwriters reacted positively to personalized AI writing tools, they believed the\nform of personalization needs to target writers' growth and go beyond the phase\nof text production. Overall, readers' responses showed less concern about\nhuman-AI co-writing. Readers could not distinguish AI-assisted work,\npersonalized or not, from writers' solo-written work and showed positive\nattitudes toward writers experimenting with new technology for creative\nwriting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the rising proliferation and diversity of AI writing assistance tools,\nespecially those powered by large language models (LLMs), both writers and\nreaders may have concerns about the impact of these tools on the authenticity\nof writing work. We examine whether and how writers want to preserve their\nauthentic voice when co-writing with AI tools and whether personalization of AI\nwriting support could help achieve this goal. We conducted semi-structured\ninterviews with 19 professional writers, during which they co-wrote with both\npersonalized and non-personalized AI writing-support tools. We supplemented\nwriters' perspectives with opinions from 30 avid readers about the written work\nco-produced with AI collected through an online survey. Our findings illuminate\nconceptions of authenticity in human-AI co-creation, which focus more on the\nprocess and experience of constructing creators' authentic selves. While\nwriters reacted positively to personalized AI writing tools, they believed the\nform of personalization needs to target writers' growth and go beyond the phase\nof text production. Overall, readers' responses showed less concern about\nhuman-AI co-writing. Readers could not distinguish AI-assisted work,\npersonalized or not, from writers' solo-written work and showed positive\nattitudes toward writers experimenting with new technology for creative\nwriting."
                },
                "authors": [
                    {
                        "name": "Angel Hsing-Chi Hwang"
                    },
                    {
                        "name": "Q. Vera Liao"
                    },
                    {
                        "name": "Su Lin Blodgett"
                    },
                    {
                        "name": "Alexandra Olteanu"
                    },
                    {
                        "name": "Adam Trischler"
                    }
                ],
                "author_detail": {
                    "name": "Adam Trischler"
                },
                "author": "Adam Trischler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13024v1",
                "updated": "2024-11-20T04:13:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    4,
                    13,
                    5,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T04:13:05Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    4,
                    13,
                    5,
                    2,
                    325,
                    0
                ],
                "title": "Prior-based Objective Inference Mining Potential Uncertainty for Facial\n  Expression Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior-based Objective Inference Mining Potential Uncertainty for Facial\n  Expression Recognition"
                },
                "summary": "Annotation ambiguity caused by the inherent subjectivity of visual judgment\nhas always been a major challenge for Facial Expression Recognition (FER)\ntasks, particularly for largescale datasets from in-the-wild scenarios. A\npotential solution is the evaluation of relatively objective emotional\ndistributions to help mitigate the ambiguity of subjective annotations. To this\nend, this paper proposes a novel Prior-based Objective Inference (POI) network.\nThis network employs prior knowledge to derive a more objective and varied\nemotional distribution and tackles the issue of subjective annotation ambiguity\nthrough dynamic knowledge transfer. POI comprises two key networks: Firstly,\nthe Prior Inference Network (PIN) utilizes the prior knowledge of AUs and\nemotions to capture intricate motion details. To reduce over-reliance on priors\nand facilitate objective emotional inference, PIN aggregates inferential\nknowledge from various key facial subregions, encouraging mutual learning.\nSecondly, the Target Recognition Network (TRN) integrates subjective emotion\nannotations and objective inference soft labels provided by the PIN, fostering\nan understanding of inherent facial expression diversity, thus resolving\nannotation ambiguity. Moreover, we introduce an uncertainty estimation module\nto quantify and balance facial expression confidence. This module enables a\nflexible approach to dealing with the uncertainties of subjective annotations.\nExtensive experiments show that POI exhibits competitive performance on both\nsynthetic noisy datasets and multiple real-world datasets. All codes and\ntraining logs will be publicly available at https://github.com/liuhw01/POI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Annotation ambiguity caused by the inherent subjectivity of visual judgment\nhas always been a major challenge for Facial Expression Recognition (FER)\ntasks, particularly for largescale datasets from in-the-wild scenarios. A\npotential solution is the evaluation of relatively objective emotional\ndistributions to help mitigate the ambiguity of subjective annotations. To this\nend, this paper proposes a novel Prior-based Objective Inference (POI) network.\nThis network employs prior knowledge to derive a more objective and varied\nemotional distribution and tackles the issue of subjective annotation ambiguity\nthrough dynamic knowledge transfer. POI comprises two key networks: Firstly,\nthe Prior Inference Network (PIN) utilizes the prior knowledge of AUs and\nemotions to capture intricate motion details. To reduce over-reliance on priors\nand facilitate objective emotional inference, PIN aggregates inferential\nknowledge from various key facial subregions, encouraging mutual learning.\nSecondly, the Target Recognition Network (TRN) integrates subjective emotion\nannotations and objective inference soft labels provided by the PIN, fostering\nan understanding of inherent facial expression diversity, thus resolving\nannotation ambiguity. Moreover, we introduce an uncertainty estimation module\nto quantify and balance facial expression confidence. This module enables a\nflexible approach to dealing with the uncertainties of subjective annotations.\nExtensive experiments show that POI exhibits competitive performance on both\nsynthetic noisy datasets and multiple real-world datasets. All codes and\ntraining logs will be publicly available at https://github.com/liuhw01/POI."
                },
                "authors": [
                    {
                        "name": "Hanwei Liu"
                    },
                    {
                        "name": "Huiling Cai"
                    },
                    {
                        "name": "Qingcheng Lin"
                    },
                    {
                        "name": "Xuefeng Li"
                    },
                    {
                        "name": "Hui Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiao"
                },
                "author": "Hui Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08492v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08492v3",
                "updated": "2024-11-20T04:00:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    4,
                    0,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-03-13T12:55:43Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    12,
                    55,
                    43,
                    2,
                    73,
                    0
                ],
                "title": "Rich Semantic Knowledge Enhanced Large Language Models for Few-shot\n  Chinese Spell Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich Semantic Knowledge Enhanced Large Language Models for Few-shot\n  Chinese Spell Checking"
                },
                "summary": "Chinese Spell Checking (CSC) is a widely used technology, which plays a vital\nrole in speech to text (STT) and optical character recognition (OCR). Most of\nthe existing CSC approaches relying on BERT architecture achieve excellent\nperformance. However, limited by the scale of the foundation model, BERT-based\nmethod does not work well in few-shot scenarios, showing certain limitations in\npractical applications. In this paper, we explore using an in-context learning\nmethod named RS-LLM (Rich Semantic based LLMs) to introduce large language\nmodels (LLMs) as the foundation model. Besides, we study the impact of\nintroducing various Chinese rich semantic information in our framework. We\nfound that by introducing a small number of specific Chinese rich semantic\nstructures, LLMs achieve better performance than the BERT-based model on\nfew-shot CSC task. Furthermore, we conduct experiments on multiple datasets,\nand the experimental results verified the superiority of our proposed\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese Spell Checking (CSC) is a widely used technology, which plays a vital\nrole in speech to text (STT) and optical character recognition (OCR). Most of\nthe existing CSC approaches relying on BERT architecture achieve excellent\nperformance. However, limited by the scale of the foundation model, BERT-based\nmethod does not work well in few-shot scenarios, showing certain limitations in\npractical applications. In this paper, we explore using an in-context learning\nmethod named RS-LLM (Rich Semantic based LLMs) to introduce large language\nmodels (LLMs) as the foundation model. Besides, we study the impact of\nintroducing various Chinese rich semantic information in our framework. We\nfound that by introducing a small number of specific Chinese rich semantic\nstructures, LLMs achieve better performance than the BERT-based model on\nfew-shot CSC task. Furthermore, we conduct experiments on multiple datasets,\nand the experimental results verified the superiority of our proposed\nframework."
                },
                "authors": [
                    {
                        "name": "Ming Dong"
                    },
                    {
                        "name": "Yujing Chen"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Tingting He"
                    }
                ],
                "author_detail": {
                    "name": "Tingting He"
                },
                "author": "Tingting He",
                "arxiv_comment": "This paper is accepted by Findings of the Association for\n  Computational Linguistics: ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08492v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08492v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13009v1",
                "updated": "2024-11-20T03:17:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    17,
                    51,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T03:17:51Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    17,
                    51,
                    2,
                    325,
                    0
                ],
                "title": "LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts"
                },
                "summary": "As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods."
                },
                "authors": [
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13008v1",
                "updated": "2024-11-20T03:16:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    16,
                    7,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T03:16:07Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    16,
                    7,
                    2,
                    325,
                    0
                ],
                "title": "Evaluating LLMs Capabilities Towards Understanding Social Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs Capabilities Towards Understanding Social Dynamics"
                },
                "summary": "Social media discourse involves people from different backgrounds, beliefs,\nand motives. Thus, often such discourse can devolve into toxic interactions.\nGenerative Models, such as Llama and ChatGPT, have recently exploded in\npopularity due to their capabilities in zero-shot question-answering. Because\nthese models are increasingly being used to ask questions of social\nsignificance, a crucial research question is whether they can understand social\nmedia dynamics. This work provides a critical analysis regarding generative\nLLM's ability to understand language and dynamics in social contexts,\nparticularly considering cyberbullying and anti-cyberbullying (posts aimed at\nreducing cyberbullying) interactions. Specifically, we compare and contrast the\ncapabilities of different large language models (LLMs) to understand three key\naspects of social dynamics: language, directionality, and the occurrence of\nbullying/anti-bullying messages. We found that while fine-tuned LLMs exhibit\npromising results in some social media understanding tasks (understanding\ndirectionality), they presented mixed results in others (proper paraphrasing\nand bullying/anti-bullying detection). We also found that fine-tuning and\nprompt engineering mechanisms can have positive effects in some tasks. We\nbelieve that a understanding of LLM's capabilities is crucial to design future\nmodels that can be effectively used in social applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social media discourse involves people from different backgrounds, beliefs,\nand motives. Thus, often such discourse can devolve into toxic interactions.\nGenerative Models, such as Llama and ChatGPT, have recently exploded in\npopularity due to their capabilities in zero-shot question-answering. Because\nthese models are increasingly being used to ask questions of social\nsignificance, a crucial research question is whether they can understand social\nmedia dynamics. This work provides a critical analysis regarding generative\nLLM's ability to understand language and dynamics in social contexts,\nparticularly considering cyberbullying and anti-cyberbullying (posts aimed at\nreducing cyberbullying) interactions. Specifically, we compare and contrast the\ncapabilities of different large language models (LLMs) to understand three key\naspects of social dynamics: language, directionality, and the occurrence of\nbullying/anti-bullying messages. We found that while fine-tuned LLMs exhibit\npromising results in some social media understanding tasks (understanding\ndirectionality), they presented mixed results in others (proper paraphrasing\nand bullying/anti-bullying detection). We also found that fine-tuning and\nprompt engineering mechanisms can have positive effects in some tasks. We\nbelieve that a understanding of LLM's capabilities is crucial to design future\nmodels that can be effectively used in social applications."
                },
                "authors": [
                    {
                        "name": "Anique Tahir"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Manuel Sandoval"
                    },
                    {
                        "name": "Yasin N. Silva"
                    },
                    {
                        "name": "Deborah L. Hall"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "To appear in ASONAM 24 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13004v1",
                "updated": "2024-11-20T03:01:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    1,
                    41,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T03:01:41Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    1,
                    41,
                    2,
                    325,
                    0
                ],
                "title": "MERLOT: A Distilled LLM-based Mixture-of-Experts Framework for Scalable\n  Encrypted Traffic Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERLOT: A Distilled LLM-based Mixture-of-Experts Framework for Scalable\n  Encrypted Traffic Classification"
                },
                "summary": "We present MERLOT, a scalable mixture-of-expert (MoE) based refinement of\ndistilled large language model optimized for encrypted traffic classification.\nBy applying model distillation techniques in a teacher-student paradigm,\ncompact models derived from GPT-2-base retain high classification accuracy\nwhile minimizing computational costs. These models function as specialized\nexperts in an MoE architecture, dynamically assigned via a gating network.\nUnlike generation-based methods, our approach directly classifies encrypted\ntraffic using the final decoder token with contextual feature embedding as\ninput. Experiments on 10 datasets show superior or competitive performance over\nthe state-of-the-art models while significantly reducing resource demands,\nunderscoring its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MERLOT, a scalable mixture-of-expert (MoE) based refinement of\ndistilled large language model optimized for encrypted traffic classification.\nBy applying model distillation techniques in a teacher-student paradigm,\ncompact models derived from GPT-2-base retain high classification accuracy\nwhile minimizing computational costs. These models function as specialized\nexperts in an MoE architecture, dynamically assigned via a gating network.\nUnlike generation-based methods, our approach directly classifies encrypted\ntraffic using the final decoder token with contextual feature embedding as\ninput. Experiments on 10 datasets show superior or competitive performance over\nthe state-of-the-art models while significantly reducing resource demands,\nunderscoring its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09209v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09209v3",
                "updated": "2024-11-20T02:56:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    56,
                    2,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-14T06:13:05Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    6,
                    13,
                    5,
                    3,
                    319,
                    0
                ],
                "title": "JoyVASA: Portrait and Animal Image Animation with Diffusion-Based\n  Audio-Driven Facial Dynamics and Head Motion Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JoyVASA: Portrait and Animal Image Animation with Diffusion-Based\n  Audio-Driven Facial Dynamics and Head Motion Generation"
                },
                "summary": "Audio-driven portrait animation has made significant advances with\ndiffusion-based models, improving video quality and lipsync accuracy. However,\nthe increasing complexity of these models has led to inefficiencies in training\nand inference, as well as constraints on video length and inter-frame\ncontinuity. In this paper, we propose JoyVASA, a diffusion-based method for\ngenerating facial dynamics and head motion in audio-driven facial animation.\nSpecifically, in the first stage, we introduce a decoupled facial\nrepresentation framework that separates dynamic facial expressions from static\n3D facial representations. This decoupling allows the system to generate longer\nvideos by combining any static 3D facial representation with dynamic motion\nsequences. Then, in the second stage, a diffusion transformer is trained to\ngenerate motion sequences directly from audio cues, independent of character\nidentity. Finally, a generator trained in the first stage uses the 3D facial\nrepresentation and the generated motion sequences as inputs to render\nhigh-quality animations. With the decoupled facial representation and the\nidentity-independent motion generation process, JoyVASA extends beyond human\nportraits to animate animal faces seamlessly. The model is trained on a hybrid\ndataset of private Chinese and public English data, enabling multilingual\nsupport. Experimental results validate the effectiveness of our approach.\nFuture work will focus on improving real-time performance and refining\nexpression control, further expanding the applications in portrait animation.\nThe code is available at: https://github.com/jdh-algo/JoyVASA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-driven portrait animation has made significant advances with\ndiffusion-based models, improving video quality and lipsync accuracy. However,\nthe increasing complexity of these models has led to inefficiencies in training\nand inference, as well as constraints on video length and inter-frame\ncontinuity. In this paper, we propose JoyVASA, a diffusion-based method for\ngenerating facial dynamics and head motion in audio-driven facial animation.\nSpecifically, in the first stage, we introduce a decoupled facial\nrepresentation framework that separates dynamic facial expressions from static\n3D facial representations. This decoupling allows the system to generate longer\nvideos by combining any static 3D facial representation with dynamic motion\nsequences. Then, in the second stage, a diffusion transformer is trained to\ngenerate motion sequences directly from audio cues, independent of character\nidentity. Finally, a generator trained in the first stage uses the 3D facial\nrepresentation and the generated motion sequences as inputs to render\nhigh-quality animations. With the decoupled facial representation and the\nidentity-independent motion generation process, JoyVASA extends beyond human\nportraits to animate animal faces seamlessly. The model is trained on a hybrid\ndataset of private Chinese and public English data, enabling multilingual\nsupport. Experimental results validate the effectiveness of our approach.\nFuture work will focus on improving real-time performance and refining\nexpression control, further expanding the applications in portrait animation.\nThe code is available at: https://github.com/jdh-algo/JoyVASA."
                },
                "authors": [
                    {
                        "name": "Xuyang Cao"
                    },
                    {
                        "name": "Guoxin Wang"
                    },
                    {
                        "name": "Sheng Shi"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Yang Yao"
                    },
                    {
                        "name": "Jintao Fei"
                    },
                    {
                        "name": "Minyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Minyu Gao"
                },
                "author": "Minyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09209v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09209v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10318v2",
                "updated": "2024-11-20T02:37:27Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    37,
                    27,
                    2,
                    325,
                    0
                ],
                "published": "2024-10-14T09:24:48Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    24,
                    48,
                    0,
                    288,
                    0
                ],
                "title": "QIANets: Quantum-Integrated Adaptive Networks for Reduced Latency and\n  Improved Inference Times in CNN Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QIANets: Quantum-Integrated Adaptive Networks for Reduced Latency and\n  Improved Inference Times in CNN Models"
                },
                "summary": "Convolutional neural networks (CNNs) have made significant advances in\ncomputer vision tasks, yet their high inference times and latency often limit\nreal-world applicability. While model compression techniques have gained\npopularity as solutions, they often overlook the critical balance between low\nlatency and uncompromised accuracy. By harnessing quantum-inspired pruning,\ntensor decomposition, and annealing-based matrix factorization - three\nquantum-inspired concepts - we introduce QIANets: a novel approach of\nredesigning the traditional GoogLeNet, DenseNet, and ResNet-18 model\narchitectures to process more parameters and computations whilst maintaining\nlow inference times. Despite experimental limitations, the method was tested\nand evaluated, demonstrating reductions in inference times, along with\neffective accuracy preservations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional neural networks (CNNs) have made significant advances in\ncomputer vision tasks, yet their high inference times and latency often limit\nreal-world applicability. While model compression techniques have gained\npopularity as solutions, they often overlook the critical balance between low\nlatency and uncompromised accuracy. By harnessing quantum-inspired pruning,\ntensor decomposition, and annealing-based matrix factorization - three\nquantum-inspired concepts - we introduce QIANets: a novel approach of\nredesigning the traditional GoogLeNet, DenseNet, and ResNet-18 model\narchitectures to process more parameters and computations whilst maintaining\nlow inference times. Despite experimental limitations, the method was tested\nand evaluated, demonstrating reductions in inference times, along with\neffective accuracy preservations."
                },
                "authors": [
                    {
                        "name": "Zhumazhan Balapanov"
                    },
                    {
                        "name": "Vanessa Matvei"
                    },
                    {
                        "name": "Olivia Holmberg"
                    },
                    {
                        "name": "Edward Magongo"
                    },
                    {
                        "name": "Jonathan Pei"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "arxiv_comment": "Accepted to NeurIPS 2024 workshop on Neural Compression",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.08184v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.08184v4",
                "updated": "2024-11-20T02:31:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    31,
                    14,
                    2,
                    325,
                    0
                ],
                "published": "2023-04-17T11:50:35Z",
                "published_parsed": [
                    2023,
                    4,
                    17,
                    11,
                    50,
                    35,
                    0,
                    107,
                    0
                ],
                "title": "Adjustment with Many Regressors Under Covariate-Adaptive Randomizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adjustment with Many Regressors Under Covariate-Adaptive Randomizations"
                },
                "summary": "Our paper discovers a new trade-off of using regression adjustments (RAs) in\ncausal inference under covariate-adaptive randomizations (CARs). On one hand,\nRAs can improve the efficiency of causal estimators by incorporating\ninformation from covariates that are not used in the randomization. On the\nother hand, RAs can degrade estimation efficiency due to their estimation\nerrors, which are not asymptotically negligible when the number of regressors\nis of the same order as the sample size. Ignoring the estimation errors of RAs\nmay result in serious over-rejection of causal inference under the null\nhypothesis. To address the issue, we construct a new ATE estimator by optimally\nlinearly combining the estimators with and without RAs. We then develop a\nunified inference theory for this estimator under CARs. It has two features:\n(1) the Wald test based on it achieves the exact asymptotic size under the null\nhypothesis, regardless of whether the number of covariates is fixed or diverges\nno faster than the sample size; and (2) it guarantees weak efficiency\nimprovement over estimators both with and without RAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our paper discovers a new trade-off of using regression adjustments (RAs) in\ncausal inference under covariate-adaptive randomizations (CARs). On one hand,\nRAs can improve the efficiency of causal estimators by incorporating\ninformation from covariates that are not used in the randomization. On the\nother hand, RAs can degrade estimation efficiency due to their estimation\nerrors, which are not asymptotically negligible when the number of regressors\nis of the same order as the sample size. Ignoring the estimation errors of RAs\nmay result in serious over-rejection of causal inference under the null\nhypothesis. To address the issue, we construct a new ATE estimator by optimally\nlinearly combining the estimators with and without RAs. We then develop a\nunified inference theory for this estimator under CARs. It has two features:\n(1) the Wald test based on it achieves the exact asymptotic size under the null\nhypothesis, regardless of whether the number of covariates is fixed or diverges\nno faster than the sample size; and (2) it guarantees weak efficiency\nimprovement over estimators both with and without RAs."
                },
                "authors": [
                    {
                        "name": "Liang Jiang"
                    },
                    {
                        "name": "Liyao Li"
                    },
                    {
                        "name": "Ke Miao"
                    },
                    {
                        "name": "Yichong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yichong Zhang"
                },
                "author": "Yichong Zhang",
                "arxiv_comment": "92 pages, including appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.08184v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.08184v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12103v2",
                "updated": "2024-11-20T02:23:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    23,
                    11,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-18T22:31:17Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    22,
                    31,
                    17,
                    0,
                    323,
                    0
                ],
                "title": "Does Unlearning Truly Unlearn? A Black Box Evaluation of LLM Unlearning\n  Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Unlearning Truly Unlearn? A Black Box Evaluation of LLM Unlearning\n  Methods"
                },
                "summary": "Large language model unlearning aims to remove harmful information that LLMs\nhave learnt to prevent their use for malicious purposes. LLMU and RMU have been\nproposed as two methods for LLM unlearning, achieving impressive results on\nunlearning benchmarks. We study in detail the efficacy of these methods by\nevaluating their impact on general model capabilities on the WMDP benchmark as\nwell as a biology benchmark we create. Our experiments show that RMU generally\nleads to better preservation of model capabilities, for similar or better\nunlearning. We further test the robustness of these methods and find that doing\n5-shot prompting or rephrasing the question in simple ways can lead to an over\nten-fold increase in accuracy on unlearning benchmarks. Finally, we show that\ntraining on unrelated data can almost completely recover pre-unlearning\nperformance, demonstrating that these methods fail at truly unlearning. The\ncode is available at: https://github.com/JaiDoshi/Knowledge-Erasure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model unlearning aims to remove harmful information that LLMs\nhave learnt to prevent their use for malicious purposes. LLMU and RMU have been\nproposed as two methods for LLM unlearning, achieving impressive results on\nunlearning benchmarks. We study in detail the efficacy of these methods by\nevaluating their impact on general model capabilities on the WMDP benchmark as\nwell as a biology benchmark we create. Our experiments show that RMU generally\nleads to better preservation of model capabilities, for similar or better\nunlearning. We further test the robustness of these methods and find that doing\n5-shot prompting or rephrasing the question in simple ways can lead to an over\nten-fold increase in accuracy on unlearning benchmarks. Finally, we show that\ntraining on unrelated data can almost completely recover pre-unlearning\nperformance, demonstrating that these methods fail at truly unlearning. The\ncode is available at: https://github.com/JaiDoshi/Knowledge-Erasure."
                },
                "authors": [
                    {
                        "name": "Jai Doshi"
                    },
                    {
                        "name": "Asa Cooper Stickland"
                    }
                ],
                "author_detail": {
                    "name": "Asa Cooper Stickland"
                },
                "author": "Asa Cooper Stickland",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12977v1",
                "updated": "2024-11-20T02:10:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    10,
                    44,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T02:10:44Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    10,
                    44,
                    2,
                    325,
                    0
                ],
                "title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong\n  Collaborative Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong\n  Collaborative Learning"
                },
                "summary": "Contemporary embodied agents, such as Voyager in Minecraft, have demonstrated\npromising capabilities in open-ended individual learning. However, when powered\nwith open large language models (LLMs), these agents often struggle with\nrudimentary tasks, even when fine-tuned on domain-specific knowledge. Inspired\nby human cultural learning, we present \\collabvoyager, a novel framework that\nenhances Voyager with lifelong collaborative learning through explicit\nperspective-taking. \\collabvoyager introduces three key innovations: (1) theory\nof mind representations linking percepts, beliefs, desires, and actions; (2)\nnatural language communication between agents; and (3) semantic memory of task\nand environment knowledge and episodic memory of collaboration episodes. These\nadvancements enable agents to reason about their and others' mental states,\nempirically addressing two prevalent failure modes: false beliefs and faulty\ntask executions. In mixed-expertise Minecraft experiments, \\collabvoyager\nagents outperform Voyager counterparts, significantly improving task completion\nrate by $66.6\\% (+39.4\\%)$ for collecting one block of dirt and $70.8\\%\n(+20.8\\%)$ for collecting one wood block. They exhibit emergent behaviors like\nknowledge transfer from expert to novice agents and collaborative code\ncorrection. \\collabvoyager agents also demonstrate the ability to adapt to\nout-of-distribution tasks by using their previous experiences and beliefs\nobtained through collaboration. In this open-ended social learning paradigm,\n\\collabvoyager paves the way for the democratic development of embodied AI,\nwhere agents learn in deployment from both peer and environmental feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary embodied agents, such as Voyager in Minecraft, have demonstrated\npromising capabilities in open-ended individual learning. However, when powered\nwith open large language models (LLMs), these agents often struggle with\nrudimentary tasks, even when fine-tuned on domain-specific knowledge. Inspired\nby human cultural learning, we present \\collabvoyager, a novel framework that\nenhances Voyager with lifelong collaborative learning through explicit\nperspective-taking. \\collabvoyager introduces three key innovations: (1) theory\nof mind representations linking percepts, beliefs, desires, and actions; (2)\nnatural language communication between agents; and (3) semantic memory of task\nand environment knowledge and episodic memory of collaboration episodes. These\nadvancements enable agents to reason about their and others' mental states,\nempirically addressing two prevalent failure modes: false beliefs and faulty\ntask executions. In mixed-expertise Minecraft experiments, \\collabvoyager\nagents outperform Voyager counterparts, significantly improving task completion\nrate by $66.6\\% (+39.4\\%)$ for collecting one block of dirt and $70.8\\%\n(+20.8\\%)$ for collecting one wood block. They exhibit emergent behaviors like\nknowledge transfer from expert to novice agents and collaborative code\ncorrection. \\collabvoyager agents also demonstrate the ability to adapt to\nout-of-distribution tasks by using their previous experiences and beliefs\nobtained through collaboration. In this open-ended social learning paradigm,\n\\collabvoyager paves the way for the democratic development of embodied AI,\nwhere agents learn in deployment from both peer and environmental feedback."
                },
                "authors": [
                    {
                        "name": "Mircea LicÄ"
                    },
                    {
                        "name": "Ojas Shirekar"
                    },
                    {
                        "name": "Baptiste Colle"
                    },
                    {
                        "name": "Chirag Raman"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Raman"
                },
                "author": "Chirag Raman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20181v2",
                "updated": "2024-11-20T02:10:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    10,
                    16,
                    2,
                    325,
                    0
                ],
                "published": "2024-09-30T10:48:20Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    48,
                    20,
                    0,
                    274,
                    0
                ],
                "title": "Reference Trustable Decoding: A Training-Free Augmentation Paradigm for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference Trustable Decoding: A Training-Free Augmentation Paradigm for\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have rapidly advanced and demonstrated\nimpressive capabilities. In-Context Learning (ICL) and Parameter-Efficient\nFine-Tuning (PEFT) are currently two mainstream methods for augmenting LLMs to\ndownstream tasks. ICL typically constructs a few-shot learning scenario, either\nmanually or by setting up a Retrieval-Augmented Generation (RAG) system,\nhelping models quickly grasp domain knowledge or question-answering patterns\nwithout changing model parameters. However, this approach involves trade-offs,\nsuch as slower inference speed and increased space occupancy. PEFT assists the\nmodel in adapting to tasks through minimal parameter modifications, but the\ntraining process still demands high hardware requirements, even with a small\nnumber of parameters involved. To address these challenges, we propose\nReference Trustable Decoding (RTD), a paradigm that allows models to quickly\nadapt to new tasks without fine-tuning, maintaining low inference costs. RTD\nconstructs a reference datastore from the provided training examples and\noptimizes the LLM's final vocabulary distribution by flexibly selecting\nsuitable references based on the input, resulting in more trustable responses\nand enabling the model to adapt to downstream tasks at a low cost. Experimental\nevaluations on various LLMs using different benchmarks demonstrate that RTD\nestablishes a new paradigm for augmenting models to downstream tasks.\nFurthermore, our method exhibits strong orthogonality with traditional methods,\nallowing for concurrent usage. Our code can be found at\nhttps://github.com/ShiLuohe/ReferenceTrustableDecoding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have rapidly advanced and demonstrated\nimpressive capabilities. In-Context Learning (ICL) and Parameter-Efficient\nFine-Tuning (PEFT) are currently two mainstream methods for augmenting LLMs to\ndownstream tasks. ICL typically constructs a few-shot learning scenario, either\nmanually or by setting up a Retrieval-Augmented Generation (RAG) system,\nhelping models quickly grasp domain knowledge or question-answering patterns\nwithout changing model parameters. However, this approach involves trade-offs,\nsuch as slower inference speed and increased space occupancy. PEFT assists the\nmodel in adapting to tasks through minimal parameter modifications, but the\ntraining process still demands high hardware requirements, even with a small\nnumber of parameters involved. To address these challenges, we propose\nReference Trustable Decoding (RTD), a paradigm that allows models to quickly\nadapt to new tasks without fine-tuning, maintaining low inference costs. RTD\nconstructs a reference datastore from the provided training examples and\noptimizes the LLM's final vocabulary distribution by flexibly selecting\nsuitable references based on the input, resulting in more trustable responses\nand enabling the model to adapt to downstream tasks at a low cost. Experimental\nevaluations on various LLMs using different benchmarks demonstrate that RTD\nestablishes a new paradigm for augmenting models to downstream tasks.\nFurthermore, our method exhibits strong orthogonality with traditional methods,\nallowing for concurrent usage. Our code can be found at\nhttps://github.com/ShiLuohe/ReferenceTrustableDecoding"
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Accepted by the Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.15485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.15485v2",
                "updated": "2024-11-20T01:50:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    1,
                    50,
                    31,
                    2,
                    325,
                    0
                ],
                "published": "2023-11-27T02:05:07Z",
                "published_parsed": [
                    2023,
                    11,
                    27,
                    2,
                    5,
                    7,
                    0,
                    331,
                    0
                ],
                "title": "Calibrated Generalized Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated Generalized Bayesian Inference"
                },
                "summary": "We provide a simple and general solution for accurate uncertainty\nquantification of Bayesian inference in misspecified or approximate models, and\nfor generalized posteriors more generally. While existing solutions are based\non explicit Gaussian posterior approximations, or post-processing procedures,\nwe demonstrate that correct uncertainty quantification can be achieved by\nsubstituting the usual posterior with an intuitively appealing alternative\nposterior that conveys the same information. This solution applies to both\nlikelihood-based and loss-based posteriors, and we formally demonstrate the\nreliable uncertainty quantification of this approach. The new approach is\ndemonstrated through a range of examples, including linear models, and doubly\nintractable models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide a simple and general solution for accurate uncertainty\nquantification of Bayesian inference in misspecified or approximate models, and\nfor generalized posteriors more generally. While existing solutions are based\non explicit Gaussian posterior approximations, or post-processing procedures,\nwe demonstrate that correct uncertainty quantification can be achieved by\nsubstituting the usual posterior with an intuitively appealing alternative\nposterior that conveys the same information. This solution applies to both\nlikelihood-based and loss-based posteriors, and we formally demonstrate the\nreliable uncertainty quantification of this approach. The new approach is\ndemonstrated through a range of examples, including linear models, and doubly\nintractable models."
                },
                "authors": [
                    {
                        "name": "David T. Frazier"
                    },
                    {
                        "name": "Christopher Drovandi"
                    },
                    {
                        "name": "Robert Kohn"
                    }
                ],
                "author_detail": {
                    "name": "Robert Kohn"
                },
                "author": "Robert Kohn",
                "arxiv_comment": "This paper is a substantially revised version of arXiv:2302.06031v1.\n  This revised version has a slightly different focus, additional examples, and\n  theoretical results, as well as different authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.15485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.15485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12960v1",
                "updated": "2024-11-20T01:27:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    1,
                    27,
                    56,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T01:27:56Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    1,
                    27,
                    56,
                    2,
                    325,
                    0
                ],
                "title": "I Can Tell What I am Doing: Toward Real-World Natural Language Grounding\n  of Robot Experiences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Can Tell What I am Doing: Toward Real-World Natural Language Grounding\n  of Robot Experiences"
                },
                "summary": "Understanding robot behaviors and experiences through natural language is\ncrucial for developing intelligent and transparent robotic systems. Recent\nadvancement in large language models (LLMs) makes it possible to translate\ncomplex, multi-modal robotic experiences into coherent, human-readable\nnarratives. However, grounding real-world robot experiences into natural\nlanguage is challenging due to many reasons, such as multi-modal nature of\ndata, differing sample rates, and data volume. We introduce RONAR, an LLM-based\nsystem that generates natural language narrations from robot experiences,\naiding in behavior announcement, failure analysis, and human interaction to\nrecover failure. Evaluated across various scenarios, RONAR outperforms\nstate-of-the-art methods and improves failure recovery efficiency. Our\ncontributions include a multi-modal framework for robot experience narration, a\ncomprehensive real-robot dataset, and empirical evidence of RONAR's\neffectiveness in enhancing user experience in system transparency and failure\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding robot behaviors and experiences through natural language is\ncrucial for developing intelligent and transparent robotic systems. Recent\nadvancement in large language models (LLMs) makes it possible to translate\ncomplex, multi-modal robotic experiences into coherent, human-readable\nnarratives. However, grounding real-world robot experiences into natural\nlanguage is challenging due to many reasons, such as multi-modal nature of\ndata, differing sample rates, and data volume. We introduce RONAR, an LLM-based\nsystem that generates natural language narrations from robot experiences,\naiding in behavior announcement, failure analysis, and human interaction to\nrecover failure. Evaluated across various scenarios, RONAR outperforms\nstate-of-the-art methods and improves failure recovery efficiency. Our\ncontributions include a multi-modal framework for robot experience narration, a\ncomprehensive real-robot dataset, and empirical evidence of RONAR's\neffectiveness in enhancing user experience in system transparency and failure\nanalysis."
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Brian Liang"
                    },
                    {
                        "name": "Varad Dhat"
                    },
                    {
                        "name": "Zander Brumbaugh"
                    },
                    {
                        "name": "Nick Walker"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Maya Cakmak"
                    }
                ],
                "author_detail": {
                    "name": "Maya Cakmak"
                },
                "author": "Maya Cakmak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.05558v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.05558v3",
                "updated": "2024-11-20T01:20:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    1,
                    20,
                    11,
                    2,
                    325,
                    0
                ],
                "published": "2022-10-11T15:53:15Z",
                "published_parsed": [
                    2022,
                    10,
                    11,
                    15,
                    53,
                    15,
                    1,
                    284,
                    0
                ],
                "title": "Causal and Counterfactual Views of Missing Data Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal and Counterfactual Views of Missing Data Models"
                },
                "summary": "It is often said that the fundamental problem of causal inference is a\nmissing data problem -- the comparison of responses to two hypothetical\ntreatment assignments is made difficult because for every experimental unit\nonly one potential response is observed. In this paper, we consider the\nimplications of the converse view: that missing data problems are a form of\ncausal inference. We make explicit how the missing data problem of recovering\nthe complete data law from the observed law can be viewed as identification of\na joint distribution over counterfactual variables corresponding to values had\nwe (possibly contrary to fact) been able to observe them. Drawing analogies\nwith causal inference, we show how identification assumptions in missing data\ncan be encoded in terms of graphical models defined over counterfactual and\nobserved variables. We review recent results in missing data identification\nfrom this viewpoint. In doing so, we note interesting similarities and\ndifferences between missing data and causal identification theories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is often said that the fundamental problem of causal inference is a\nmissing data problem -- the comparison of responses to two hypothetical\ntreatment assignments is made difficult because for every experimental unit\nonly one potential response is observed. In this paper, we consider the\nimplications of the converse view: that missing data problems are a form of\ncausal inference. We make explicit how the missing data problem of recovering\nthe complete data law from the observed law can be viewed as identification of\na joint distribution over counterfactual variables corresponding to values had\nwe (possibly contrary to fact) been able to observe them. Drawing analogies\nwith causal inference, we show how identification assumptions in missing data\ncan be encoded in terms of graphical models defined over counterfactual and\nobserved variables. We review recent results in missing data identification\nfrom this viewpoint. In doing so, we note interesting similarities and\ndifferences between missing data and causal identification theories."
                },
                "authors": [
                    {
                        "name": "Razieh Nabi"
                    },
                    {
                        "name": "Rohit Bhattacharya"
                    },
                    {
                        "name": "Ilya Shpitser"
                    },
                    {
                        "name": "James M. Robins"
                    }
                ],
                "author_detail": {
                    "name": "James M. Robins"
                },
                "author": "James M. Robins",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.05558v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.05558v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18856v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18856v3",
                "updated": "2024-11-20T01:04:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    1,
                    4,
                    33,
                    2,
                    325,
                    0
                ],
                "published": "2024-10-24T15:41:56Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    41,
                    56,
                    3,
                    298,
                    0
                ],
                "title": "Demystifying Large Language Models for Medicine: A Primer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Large Language Models for Medicine: A Primer"
                },
                "summary": "Large language models (LLMs) represent a transformative class of AI tools\ncapable of revolutionizing various aspects of healthcare by generating\nhuman-like responses across diverse contexts and adapting to novel tasks\nfollowing human instructions. Their potential application spans a broad range\nof medical tasks, such as clinical documentation, matching patients to clinical\ntrials, and answering medical questions. In this primer paper, we propose an\nactionable guideline to help healthcare professionals more efficiently utilize\nLLMs in their work, along with a set of best practices. This approach consists\nof several main phases, including formulating the task, choosing LLMs, prompt\nengineering, fine-tuning, and deployment. We start with the discussion of\ncritical considerations in identifying healthcare tasks that align with the\ncore capabilities of LLMs and selecting models based on the selected task and\ndata, performance requirements, and model interface. We then review the\nstrategies, such as prompt engineering and fine-tuning, to adapt standard LLMs\nto specialized medical tasks. Deployment considerations, including regulatory\ncompliance, ethical guidelines, and continuous monitoring for fairness and\nbias, are also discussed. By providing a structured step-by-step methodology,\nthis tutorial aims to equip healthcare professionals with the tools necessary\nto effectively integrate LLMs into clinical practice, ensuring that these\npowerful technologies are applied in a safe, reliable, and impactful manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a transformative class of AI tools\ncapable of revolutionizing various aspects of healthcare by generating\nhuman-like responses across diverse contexts and adapting to novel tasks\nfollowing human instructions. Their potential application spans a broad range\nof medical tasks, such as clinical documentation, matching patients to clinical\ntrials, and answering medical questions. In this primer paper, we propose an\nactionable guideline to help healthcare professionals more efficiently utilize\nLLMs in their work, along with a set of best practices. This approach consists\nof several main phases, including formulating the task, choosing LLMs, prompt\nengineering, fine-tuning, and deployment. We start with the discussion of\ncritical considerations in identifying healthcare tasks that align with the\ncore capabilities of LLMs and selecting models based on the selected task and\ndata, performance requirements, and model interface. We then review the\nstrategies, such as prompt engineering and fine-tuning, to adapt standard LLMs\nto specialized medical tasks. Deployment considerations, including regulatory\ncompliance, ethical guidelines, and continuous monitoring for fairness and\nbias, are also discussed. By providing a structured step-by-step methodology,\nthis tutorial aims to equip healthcare professionals with the tools necessary\nto effectively integrate LLMs into clinical practice, ensuring that these\npowerful technologies are applied in a safe, reliable, and impactful manner."
                },
                "authors": [
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Nicholas Wan"
                    },
                    {
                        "name": "Robert Leaman"
                    },
                    {
                        "name": "Shubo Tian"
                    },
                    {
                        "name": "Zhizheng Wang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Po-Ting Lai"
                    },
                    {
                        "name": "Qingqing Zhu"
                    },
                    {
                        "name": "Benjamin Hou"
                    },
                    {
                        "name": "Maame Sarfo-Gyamfi"
                    },
                    {
                        "name": "Gongbo Zhang"
                    },
                    {
                        "name": "Aidan Gilson"
                    },
                    {
                        "name": "Balu Bhasuran"
                    },
                    {
                        "name": "Zhe He"
                    },
                    {
                        "name": "Aidong Zhang"
                    },
                    {
                        "name": "Jimeng Sun"
                    },
                    {
                        "name": "Chunhua Weng"
                    },
                    {
                        "name": "Ronald M. Summers"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Zhiyong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Lu"
                },
                "author": "Zhiyong Lu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18856v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18856v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12951v1",
                "updated": "2024-11-20T00:47:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    47,
                    17,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T00:47:17Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    47,
                    17,
                    2,
                    325,
                    0
                ],
                "title": "On the Consistency of Video Large Language Models in Temporal\n  Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Consistency of Video Large Language Models in Temporal\n  Comprehension"
                },
                "summary": "Video large language models (Video-LLMs) can temporally ground language\nqueries and retrieve video moments. Yet, such temporal comprehension\ncapabilities are neither well-studied nor understood. So we conduct a study on\nprediction consistency -- a key indicator for robustness and trustworthiness of\ntemporal grounding. After the model identifies an initial moment within the\nvideo content, we apply a series of probes to check if the model's responses\nalign with this initial grounding as an indicator of reliable comprehension.\nOur results reveal that current Video-LLMs are sensitive to variations in video\ncontents, language queries, and task settings, unveiling severe deficiencies in\nmaintaining consistency. We further explore common prompting and\ninstruction-tuning methods as potential solutions, but find that their\nimprovements are often unstable. To that end, we propose event temporal\nverification tuning that explicitly accounts for consistency, and demonstrate\nsignificant improvements for both grounding and consistency. Our data and code\nwill be available at https://github.com/minjoong507/Consistency-of-Video-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (Video-LLMs) can temporally ground language\nqueries and retrieve video moments. Yet, such temporal comprehension\ncapabilities are neither well-studied nor understood. So we conduct a study on\nprediction consistency -- a key indicator for robustness and trustworthiness of\ntemporal grounding. After the model identifies an initial moment within the\nvideo content, we apply a series of probes to check if the model's responses\nalign with this initial grounding as an indicator of reliable comprehension.\nOur results reveal that current Video-LLMs are sensitive to variations in video\ncontents, language queries, and task settings, unveiling severe deficiencies in\nmaintaining consistency. We further explore common prompting and\ninstruction-tuning methods as potential solutions, but find that their\nimprovements are often unstable. To that end, we propose event temporal\nverification tuning that explicitly accounts for consistency, and demonstrate\nsignificant improvements for both grounding and consistency. Our data and code\nwill be available at https://github.com/minjoong507/Consistency-of-Video-LLM."
                },
                "authors": [
                    {
                        "name": "Minjoon Jung"
                    },
                    {
                        "name": "Junbin Xiao"
                    },
                    {
                        "name": "Byoung-Tak Zhang"
                    },
                    {
                        "name": "Angela Yao"
                    }
                ],
                "author_detail": {
                    "name": "Angela Yao"
                },
                "author": "Angela Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12950v1",
                "updated": "2024-11-20T00:47:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    47,
                    3,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T00:47:03Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    47,
                    3,
                    2,
                    325,
                    0
                ],
                "title": "KAAE: Numerical Reasoning for Knowledge Graphs via Knowledge-aware\n  Attributes Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KAAE: Numerical Reasoning for Knowledge Graphs via Knowledge-aware\n  Attributes Learning"
                },
                "summary": "Numerical reasoning is pivotal in various artificial intelligence\napplications, such as natural language processing and recommender systems,\nwhere it involves using entities, relations, and attribute values (e.g.,\nweight, length) to infer new factual relations (e.g., the Nile is longer than\nthe Amazon). However, existing approaches encounter two critical challenges in\nmodeling: (1) semantic relevance-the challenge of insufficiently capturing the\nnecessary contextual interactions among entities, relations, and numerical\nattributes, often resulting in suboptimal inference; and (2) semantic\nambiguity-the difficulty in accurately distinguishing ordinal relationships\nduring numerical reasoning, which compromises the generation of high-quality\nsamples and limits the effectiveness of contrastive learning. To address these\nchallenges, we propose the novel Knowledge-Aware Attributes Embedding model\n(KAAE) for knowledge graph embeddings in numerical reasoning. Specifically, to\novercome the challenge of semantic relevance, we introduce a\nMixture-of-Experts-Knowledge-Aware (MoEKA) Encoder, designed to integrate the\nsemantics of entities, relations, and numerical attributes into a joint\nsemantic space. To tackle semantic ambiguity, we implement a new ordinal\nknowledge contrastive learning (OKCL) strategy that generates high-quality\nordinal samples from the original data with the aid of ordinal relations,\ncapturing fine-grained semantic nuances essential for accurate numerical\nreasoning. Experiments on three public benchmark datasets demonstrate the\nsuperior performance of KAAE across various attribute value distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical reasoning is pivotal in various artificial intelligence\napplications, such as natural language processing and recommender systems,\nwhere it involves using entities, relations, and attribute values (e.g.,\nweight, length) to infer new factual relations (e.g., the Nile is longer than\nthe Amazon). However, existing approaches encounter two critical challenges in\nmodeling: (1) semantic relevance-the challenge of insufficiently capturing the\nnecessary contextual interactions among entities, relations, and numerical\nattributes, often resulting in suboptimal inference; and (2) semantic\nambiguity-the difficulty in accurately distinguishing ordinal relationships\nduring numerical reasoning, which compromises the generation of high-quality\nsamples and limits the effectiveness of contrastive learning. To address these\nchallenges, we propose the novel Knowledge-Aware Attributes Embedding model\n(KAAE) for knowledge graph embeddings in numerical reasoning. Specifically, to\novercome the challenge of semantic relevance, we introduce a\nMixture-of-Experts-Knowledge-Aware (MoEKA) Encoder, designed to integrate the\nsemantics of entities, relations, and numerical attributes into a joint\nsemantic space. To tackle semantic ambiguity, we implement a new ordinal\nknowledge contrastive learning (OKCL) strategy that generates high-quality\nordinal samples from the original data with the aid of ordinal relations,\ncapturing fine-grained semantic nuances essential for accurate numerical\nreasoning. Experiments on three public benchmark datasets demonstrate the\nsuperior performance of KAAE across various attribute value distributions."
                },
                "authors": [
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Qiang Zhou"
                    },
                    {
                        "name": "Zongsheng Cao"
                    },
                    {
                        "name": "Mei Li"
                    }
                ],
                "author_detail": {
                    "name": "Mei Li"
                },
                "author": "Mei Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12946v1",
                "updated": "2024-11-20T00:31:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    31,
                    23,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T00:31:23Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    31,
                    23,
                    2,
                    325,
                    0
                ],
                "title": "A Flexible Large Language Models Guardrail Development Methodology\n  Applied to Off-Topic Prompt Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Flexible Large Language Models Guardrail Development Methodology\n  Applied to Off-Topic Prompt Detection"
                },
                "summary": "Large Language Models are prone to off-topic misuse, where users may prompt\nthese models to perform tasks beyond their intended scope. Current guardrails,\nwhich often rely on curated examples or custom classifiers, suffer from high\nfalse-positive rates, limited adaptability, and the impracticality of requiring\nreal-world data that is not available in pre-production. In this paper, we\nintroduce a flexible, data-free guardrail development methodology that\naddresses these challenges. By thoroughly defining the problem space\nqualitatively and passing this to an LLM to generate diverse prompts, we\nconstruct a synthetic dataset to benchmark and train off-topic guardrails that\noutperform heuristic approaches. Additionally, by framing the task as\nclassifying whether the user prompt is relevant with respect to the system\nprompt, our guardrails effectively generalize to other misuse categories,\nincluding jailbreak and harmful prompts. Lastly, we further contribute to the\nfield by open-sourcing both the synthetic dataset and the off-topic guardrail\nmodels, providing valuable resources for developing guardrails in\npre-production environments and supporting future research and development in\nLLM safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are prone to off-topic misuse, where users may prompt\nthese models to perform tasks beyond their intended scope. Current guardrails,\nwhich often rely on curated examples or custom classifiers, suffer from high\nfalse-positive rates, limited adaptability, and the impracticality of requiring\nreal-world data that is not available in pre-production. In this paper, we\nintroduce a flexible, data-free guardrail development methodology that\naddresses these challenges. By thoroughly defining the problem space\nqualitatively and passing this to an LLM to generate diverse prompts, we\nconstruct a synthetic dataset to benchmark and train off-topic guardrails that\noutperform heuristic approaches. Additionally, by framing the task as\nclassifying whether the user prompt is relevant with respect to the system\nprompt, our guardrails effectively generalize to other misuse categories,\nincluding jailbreak and harmful prompts. Lastly, we further contribute to the\nfield by open-sourcing both the synthetic dataset and the off-topic guardrail\nmodels, providing valuable resources for developing guardrails in\npre-production environments and supporting future research and development in\nLLM safety."
                },
                "authors": [
                    {
                        "name": "Gabriel Chua"
                    },
                    {
                        "name": "Shing Yee Chan"
                    },
                    {
                        "name": "Shaun Khoo"
                    }
                ],
                "author_detail": {
                    "name": "Shaun Khoo"
                },
                "author": "Shaun Khoo",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12944v1",
                "updated": "2024-11-20T00:28:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    28,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T00:28:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    28,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "From Estimands to Robust Inference of Treatment Effects in Platform\n  Trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Estimands to Robust Inference of Treatment Effects in Platform\n  Trials"
                },
                "summary": "A platform trial is an innovative clinical trial design that uses a master\nprotocol (i.e., one overarching protocol) to evaluate multiple treatments in an\nongoing manner and can accelerate the evaluation of new treatments. However,\nthe flexibility that marks the potential of platform trials also creates\ninferential challenges. Two key challenges are the precise definition of\ntreatment effects and the robust and efficient inference on these effects. To\naddress these challenges, we first define a clinically meaningful estimand that\ncharacterizes the treatment effect as a function of the expected outcomes under\ntwo given treatments among concurrently eligible patients. Then, we develop\nweighting and post-stratification methods for estimation of treatment effects\nwith minimal assumptions. To fully leverage the efficiency potential of data\nfrom concurrently eligible patients, we also consider a model-assisted approach\nfor baseline covariate adjustment to gain efficiency while maintaining\nrobustness against model misspecification. We derive and compare asymptotic\ndistributions of proposed estimators in theory and propose robust variance\nestimators. The proposed estimators are empirically evaluated in a simulation\nstudy and illustrated using the SIMPLIFY trial. Our methods are implemented in\nthe R package RobinCID.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A platform trial is an innovative clinical trial design that uses a master\nprotocol (i.e., one overarching protocol) to evaluate multiple treatments in an\nongoing manner and can accelerate the evaluation of new treatments. However,\nthe flexibility that marks the potential of platform trials also creates\ninferential challenges. Two key challenges are the precise definition of\ntreatment effects and the robust and efficient inference on these effects. To\naddress these challenges, we first define a clinically meaningful estimand that\ncharacterizes the treatment effect as a function of the expected outcomes under\ntwo given treatments among concurrently eligible patients. Then, we develop\nweighting and post-stratification methods for estimation of treatment effects\nwith minimal assumptions. To fully leverage the efficiency potential of data\nfrom concurrently eligible patients, we also consider a model-assisted approach\nfor baseline covariate adjustment to gain efficiency while maintaining\nrobustness against model misspecification. We derive and compare asymptotic\ndistributions of proposed estimators in theory and propose robust variance\nestimators. The proposed estimators are empirically evaluated in a simulation\nstudy and illustrated using the SIMPLIFY trial. Our methods are implemented in\nthe R package RobinCID."
                },
                "authors": [
                    {
                        "name": "Yuhan Qian"
                    },
                    {
                        "name": "Yifan Yi"
                    },
                    {
                        "name": "Jun Shao"
                    },
                    {
                        "name": "Yanyao Yi"
                    },
                    {
                        "name": "Nicole Mayer-Hamblett"
                    },
                    {
                        "name": "Patrick J. Heagerty"
                    },
                    {
                        "name": "Ting Ye"
                    }
                ],
                "author_detail": {
                    "name": "Ting Ye"
                },
                "author": "Ting Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12936v1",
                "updated": "2024-11-20T00:03:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    3,
                    21,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T00:03:21Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    3,
                    21,
                    2,
                    325,
                    0
                ],
                "title": "Statistical inference for mean-field queueing systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference for mean-field queueing systems"
                },
                "summary": "Mean-field limits have been used now as a standard tool in approximations,\nincluding for networks with a large number of nodes. Statistical inference on\nmean-filed models has attracted more attention recently mainly due to the rapid\nemergence of data-driven systems. However, studies reported in the literature\nhave been mainly limited to continuous models. In this paper, we initiate a\nstudy of statistical inference on discrete mean-field models (or jump\nprocesses) in terms of a well-known and extensively studied model, known as the\npower-of-L, or the supermarket model, to demonstrate how to deal with new\nchallenges in discrete models. We focus on system parameter estimation based on\nthe observations of system states at discrete time epochs over a finite period.\nWe show that by harnessing the weak convergence results developed for the\nsupermarket model in the literature, an asymptotic inference scheme based on an\napproximate least squares estimation can be obtained from the mean-field\nlimiting equation. Also, by leveraging the law of large numbers alongside the\ncentral limit theorem, the consistency of the estimator and its asymptotic\nnormality can be established when the number of servers and the number of\nobservations go to infinity. Moreover, numerical results for the power-of-two\nmodel are provided to show the efficiency and accuracy of the proposed\nestimator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mean-field limits have been used now as a standard tool in approximations,\nincluding for networks with a large number of nodes. Statistical inference on\nmean-filed models has attracted more attention recently mainly due to the rapid\nemergence of data-driven systems. However, studies reported in the literature\nhave been mainly limited to continuous models. In this paper, we initiate a\nstudy of statistical inference on discrete mean-field models (or jump\nprocesses) in terms of a well-known and extensively studied model, known as the\npower-of-L, or the supermarket model, to demonstrate how to deal with new\nchallenges in discrete models. We focus on system parameter estimation based on\nthe observations of system states at discrete time epochs over a finite period.\nWe show that by harnessing the weak convergence results developed for the\nsupermarket model in the literature, an asymptotic inference scheme based on an\napproximate least squares estimation can be obtained from the mean-field\nlimiting equation. Also, by leveraging the law of large numbers alongside the\ncentral limit theorem, the consistency of the estimator and its asymptotic\nnormality can be established when the number of servers and the number of\nobservations go to infinity. Moreover, numerical results for the power-of-two\nmodel are provided to show the efficiency and accuracy of the proposed\nestimator."
                },
                "authors": [
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Ahmed Sid-Ali"
                    },
                    {
                        "name": "Wei Sun"
                    },
                    {
                        "name": "Yiqiang Q. Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiqiang Q. Zhao"
                },
                "author": "Yiqiang Q. Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60K25 60K35 62F10 62F12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.09301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.09301v2",
                "updated": "2024-11-19T23:57:24Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    23,
                    57,
                    24,
                    1,
                    324,
                    0
                ],
                "published": "2023-08-18T04:49:45Z",
                "published_parsed": [
                    2023,
                    8,
                    18,
                    4,
                    49,
                    45,
                    4,
                    230,
                    0
                ],
                "title": "Automata Learning from Preference and Equivalence Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automata Learning from Preference and Equivalence Queries"
                },
                "summary": "Active automata learning from membership and equivalence queries is a\nfoundational problem with numerous applications. We propose a novel variant of\nthe active automata learning problem: actively learn finite automata using\npreference queries -- i.e., queries about the relative position of two\nsequences in a total order -- instead of membership queries. Our solution is\nREMAP, a novel algorithm which leverages a symbolic observation table along\nwith unification and constraint solving to navigate a space of symbolic\nhypotheses (each representing a set of automata), and uses\nsatisfiability-solving to construct a concrete automaton from a symbolic\nhypothesis. REMAP is guaranteed to correctly infer the minimal automaton with\npolynomial query complexity under exact equivalence queries, and achieves\nPAC-identification ($\\varepsilon$-approximate, with high probability) of the\nminimal automaton using sampling-based equivalence queries. Our empirical\nevaluations of REMAP on the task of learning reward machines for two\nreinforcement learning domains indicate REMAP scales to large automata and is\neffective at learning correct automata from consistent teachers, under both\nexact and sampling-based equivalence queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active automata learning from membership and equivalence queries is a\nfoundational problem with numerous applications. We propose a novel variant of\nthe active automata learning problem: actively learn finite automata using\npreference queries -- i.e., queries about the relative position of two\nsequences in a total order -- instead of membership queries. Our solution is\nREMAP, a novel algorithm which leverages a symbolic observation table along\nwith unification and constraint solving to navigate a space of symbolic\nhypotheses (each representing a set of automata), and uses\nsatisfiability-solving to construct a concrete automaton from a symbolic\nhypothesis. REMAP is guaranteed to correctly infer the minimal automaton with\npolynomial query complexity under exact equivalence queries, and achieves\nPAC-identification ($\\varepsilon$-approximate, with high probability) of the\nminimal automaton using sampling-based equivalence queries. Our empirical\nevaluations of REMAP on the task of learning reward machines for two\nreinforcement learning domains indicate REMAP scales to large automata and is\neffective at learning correct automata from consistent teachers, under both\nexact and sampling-based equivalence queries."
                },
                "authors": [
                    {
                        "name": "Eric Hsiung"
                    },
                    {
                        "name": "Joydeep Biswas"
                    },
                    {
                        "name": "Swarat Chaudhuri"
                    }
                ],
                "author_detail": {
                    "name": "Swarat Chaudhuri"
                },
                "author": "Swarat Chaudhuri",
                "arxiv_comment": "29 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.09301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.09301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12930v1",
                "updated": "2024-11-19T23:43:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    23,
                    43,
                    25,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T23:43:25Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    23,
                    43,
                    25,
                    1,
                    324,
                    0
                ],
                "title": "LEDRO: LLM-Enhanced Design Space Reduction and Optimization for Analog\n  Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEDRO: LLM-Enhanced Design Space Reduction and Optimization for Analog\n  Circuits"
                },
                "summary": "Traditional approaches for designing analog circuits are time-consuming and\nrequire significant human expertise. Existing automation efforts using methods\nlike Bayesian Optimization (BO) and Reinforcement Learning (RL) are sub-optimal\nand costly to generalize across different topologies and technology nodes. In\nour work, we introduce a novel approach, LEDRO, utilizing Large Language Models\n(LLMs) in conjunction with optimization techniques to iteratively refine the\ndesign space for analog circuit sizing. LEDRO is highly generalizable compared\nto other RL and BO baselines, eliminating the need for design annotation or\nmodel training for different topologies or technology nodes. We conduct a\ncomprehensive evaluation of our proposed framework and baseline on 22 different\nOp-Amp topologies across four FinFET technology nodes. Results demonstrate the\nsuperior performance of LEDRO as it outperforms our best baseline by an average\nof 13% FoM improvement with 2.15x speed-up on low complexity Op-Amps and 48%\nFoM improvement with 1.7x speed-up on high complexity Op-Amps. This highlights\nLEDRO's effective performance, efficiency, and generalizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional approaches for designing analog circuits are time-consuming and\nrequire significant human expertise. Existing automation efforts using methods\nlike Bayesian Optimization (BO) and Reinforcement Learning (RL) are sub-optimal\nand costly to generalize across different topologies and technology nodes. In\nour work, we introduce a novel approach, LEDRO, utilizing Large Language Models\n(LLMs) in conjunction with optimization techniques to iteratively refine the\ndesign space for analog circuit sizing. LEDRO is highly generalizable compared\nto other RL and BO baselines, eliminating the need for design annotation or\nmodel training for different topologies or technology nodes. We conduct a\ncomprehensive evaluation of our proposed framework and baseline on 22 different\nOp-Amp topologies across four FinFET technology nodes. Results demonstrate the\nsuperior performance of LEDRO as it outperforms our best baseline by an average\nof 13% FoM improvement with 2.15x speed-up on low complexity Op-Amps and 48%\nFoM improvement with 1.7x speed-up on high complexity Op-Amps. This highlights\nLEDRO's effective performance, efficiency, and generalizability."
                },
                "authors": [
                    {
                        "name": "Dimple Vijay Kochar"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Anantha Chandrakasan"
                    },
                    {
                        "name": "Xin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhang"
                },
                "author": "Xin Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17309v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17309v2",
                "updated": "2024-11-19T23:32:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    23,
                    32,
                    13,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-22T18:00:00Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    18,
                    0,
                    0,
                    1,
                    296,
                    0
                ],
                "title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation"
                },
                "summary": "AI holds promise for transforming scientific processes, including hypothesis\ngeneration. Prior work on hypothesis generation can be broadly categorized into\ntheory-driven and data-driven approaches. While both have proven effective in\ngenerating novel and plausible hypotheses, it remains an open question whether\nthey can complement each other. To address this, we develop the first method\nthat combines literature-based insights with data to perform LLM-powered\nhypothesis generation. We apply our method on five different datasets and\ndemonstrate that integrating literature and data outperforms other baselines\n(8.97\\% over few-shot, 15.75\\% over literature-based alone, and 3.37\\% over\ndata-driven alone). Additionally, we conduct the first human evaluation to\nassess the utility of LLM-generated hypotheses in assisting human\ndecision-making on two challenging tasks: deception detection and AI generated\ncontent detection. Our results show that human accuracy improves significantly\nby 7.44\\% and 14.19\\% on these tasks, respectively. These findings suggest that\nintegrating literature-based and data-driven approaches provides a\ncomprehensive and nuanced framework for hypothesis generation and could open\nnew avenues for scientific inquiry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI holds promise for transforming scientific processes, including hypothesis\ngeneration. Prior work on hypothesis generation can be broadly categorized into\ntheory-driven and data-driven approaches. While both have proven effective in\ngenerating novel and plausible hypotheses, it remains an open question whether\nthey can complement each other. To address this, we develop the first method\nthat combines literature-based insights with data to perform LLM-powered\nhypothesis generation. We apply our method on five different datasets and\ndemonstrate that integrating literature and data outperforms other baselines\n(8.97\\% over few-shot, 15.75\\% over literature-based alone, and 3.37\\% over\ndata-driven alone). Additionally, we conduct the first human evaluation to\nassess the utility of LLM-generated hypotheses in assisting human\ndecision-making on two challenging tasks: deception detection and AI generated\ncontent detection. Our results show that human accuracy improves significantly\nby 7.44\\% and 14.19\\% on these tasks, respectively. These findings suggest that\nintegrating literature-based and data-driven approaches provides a\ncomprehensive and nuanced framework for hypothesis generation and could open\nnew avenues for scientific inquiry."
                },
                "authors": [
                    {
                        "name": "Haokun Liu"
                    },
                    {
                        "name": "Yangqiaoyu Zhou"
                    },
                    {
                        "name": "Mingxuan Li"
                    },
                    {
                        "name": "Chenfei Yuan"
                    },
                    {
                        "name": "Chenhao Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chenhao Tan"
                },
                "author": "Chenhao Tan",
                "arxiv_comment": "30 pages, 7 figures, code link:\n  https://github.com/ChicagoHAI/hypothesis-generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17309v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17309v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.13547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13547v1",
                "updated": "2024-11-20T18:56:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    56,
                    22,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:56:22Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    56,
                    22,
                    2,
                    325,
                    0
                ],
                "title": "SpecTool: A Benchmark for Characterizing Errors in Tool-Use LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecTool: A Benchmark for Characterizing Errors in Tool-Use LLMs"
                },
                "summary": "Evaluating the output of Large Language Models (LLMs) is one of the most\ncritical aspects of building a performant compound AI system. Since the output\nfrom LLMs propagate to downstream steps, identifying LLM errors is crucial to\nsystem performance. A common task for LLMs in AI systems is tool use. While\nthere are several benchmark environments for evaluating LLMs on this task, they\ntypically only give a success rate without any explanation of the failure\ncases. To solve this problem, we introduce SpecTool, a new benchmark to\nidentify error patterns in LLM output on tool-use tasks. Our benchmark data set\ncomprises of queries from diverse environments that can be used to test for the\npresence of seven newly characterized error patterns. Using SPECTOOL , we show\nthat even the most prominent LLMs exhibit these error patterns in their\noutputs. Researchers can use the analysis and insights from SPECTOOL to guide\ntheir error mitigation strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the output of Large Language Models (LLMs) is one of the most\ncritical aspects of building a performant compound AI system. Since the output\nfrom LLMs propagate to downstream steps, identifying LLM errors is crucial to\nsystem performance. A common task for LLMs in AI systems is tool use. While\nthere are several benchmark environments for evaluating LLMs on this task, they\ntypically only give a success rate without any explanation of the failure\ncases. To solve this problem, we introduce SpecTool, a new benchmark to\nidentify error patterns in LLM output on tool-use tasks. Our benchmark data set\ncomprises of queries from diverse environments that can be used to test for the\npresence of seven newly characterized error patterns. Using SPECTOOL , we show\nthat even the most prominent LLMs exhibit these error patterns in their\noutputs. Researchers can use the analysis and insights from SPECTOOL to guide\ntheir error mitigation strategies."
                },
                "authors": [
                    {
                        "name": "Shirley Kokane"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Tulika Awalgaonkar"
                    },
                    {
                        "name": "Jianguo Zhang"
                    },
                    {
                        "name": "Thai Hoang"
                    },
                    {
                        "name": "Akshara Prabhakar"
                    },
                    {
                        "name": "Zuxin Liu"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Liangwei Yang"
                    },
                    {
                        "name": "Juntao Tan"
                    },
                    {
                        "name": "Rithesh Murthy"
                    },
                    {
                        "name": "Weiran Yao"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Huan Wang"
                    },
                    {
                        "name": "Shelby Heinecke"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Silivo Savarese"
                    }
                ],
                "author_detail": {
                    "name": "Silivo Savarese"
                },
                "author": "Silivo Savarese",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13546v1",
                "updated": "2024-11-20T18:55:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    55,
                    51,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:55:51Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    55,
                    51,
                    2,
                    325,
                    0
                ],
                "title": "Promoting User Data Autonomy During the Dissolution of a Monopolistic\n  Firm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Promoting User Data Autonomy During the Dissolution of a Monopolistic\n  Firm"
                },
                "summary": "The deployment of AI in consumer products is currently focused on the use of\nso-called foundation models, large neural networks pre-trained on massive\ncorpora of digital records. This emphasis on scaling up datasets and\npre-training computation raises the risk of further consolidating the industry,\nand enabling monopolistic (or oligopolistic) behavior. Judges and regulators\nseeking to improve market competition may employ various remedies. This paper\nexplores dissolution -- the breaking up of a monopolistic entity into smaller\nfirms -- as one such remedy, focusing in particular on the technical challenges\nand opportunities involved in the breaking up of large models and datasets. We\nshow how the framework of Conscious Data Contribution can enable user autonomy\nduring under dissolution. Through a simulation study, we explore how\nfine-tuning and the phenomenon of \"catastrophic forgetting\" could actually\nprove beneficial as a type of machine unlearning that allows users to specify\nwhich data they want used for what purposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of AI in consumer products is currently focused on the use of\nso-called foundation models, large neural networks pre-trained on massive\ncorpora of digital records. This emphasis on scaling up datasets and\npre-training computation raises the risk of further consolidating the industry,\nand enabling monopolistic (or oligopolistic) behavior. Judges and regulators\nseeking to improve market competition may employ various remedies. This paper\nexplores dissolution -- the breaking up of a monopolistic entity into smaller\nfirms -- as one such remedy, focusing in particular on the technical challenges\nand opportunities involved in the breaking up of large models and datasets. We\nshow how the framework of Conscious Data Contribution can enable user autonomy\nduring under dissolution. Through a simulation study, we explore how\nfine-tuning and the phenomenon of \"catastrophic forgetting\" could actually\nprove beneficial as a type of machine unlearning that allows users to specify\nwhich data they want used for what purposes."
                },
                "authors": [
                    {
                        "name": "Rushabh Solanki"
                    },
                    {
                        "name": "Elliot Creager"
                    }
                ],
                "author_detail": {
                    "name": "Elliot Creager"
                },
                "author": "Elliot Creager",
                "arxiv_comment": "This paper appeared at the 2nd Workshop on Regulatable ML at NeurIPS\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13543v1",
                "updated": "2024-11-20T18:54:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    54,
                    32,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:54:32Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    54,
                    32,
                    2,
                    325,
                    0
                ],
                "title": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games"
                },
                "summary": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess\nextensive knowledge and exhibit promising reasoning abilities; however, they\nstill struggle to perform well in complex, dynamic environments. Real-world\ntasks require handling intricate interactions, advanced spatial reasoning,\nlong-term planning, and continuous exploration of new strategies-areas in which\nwe lack effective methodologies for comprehensively evaluating these\ncapabilities. To address this gap, we introduce BALROG, a novel benchmark\ndesigned to assess the agentic capabilities of LLMs and VLMs through a diverse\nset of challenging games. Our benchmark incorporates a range of existing\nreinforcement learning environments with varying levels of difficulty,\nincluding tasks that are solvable by non-expert humans in seconds to extremely\nchallenging ones that may take years to master (e.g., the NetHack Learning\nEnvironment). We devise fine-grained metrics to measure performance and conduct\nan extensive evaluation of several popular open-source and closed-source LLMs\nand VLMs. Our findings indicate that while current models achieve partial\nsuccess in the easier games, they struggle significantly with more challenging\ntasks. Notably, we observe severe deficiencies in vision-based decision-making,\nas models perform worse when visual representations of the environments are\nprovided. We release BALROG as an open and user-friendly benchmark to\nfacilitate future research and development in the agentic community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess\nextensive knowledge and exhibit promising reasoning abilities; however, they\nstill struggle to perform well in complex, dynamic environments. Real-world\ntasks require handling intricate interactions, advanced spatial reasoning,\nlong-term planning, and continuous exploration of new strategies-areas in which\nwe lack effective methodologies for comprehensively evaluating these\ncapabilities. To address this gap, we introduce BALROG, a novel benchmark\ndesigned to assess the agentic capabilities of LLMs and VLMs through a diverse\nset of challenging games. Our benchmark incorporates a range of existing\nreinforcement learning environments with varying levels of difficulty,\nincluding tasks that are solvable by non-expert humans in seconds to extremely\nchallenging ones that may take years to master (e.g., the NetHack Learning\nEnvironment). We devise fine-grained metrics to measure performance and conduct\nan extensive evaluation of several popular open-source and closed-source LLMs\nand VLMs. Our findings indicate that while current models achieve partial\nsuccess in the easier games, they struggle significantly with more challenging\ntasks. Notably, we observe severe deficiencies in vision-based decision-making,\nas models perform worse when visual representations of the environments are\nprovided. We release BALROG as an open and user-friendly benchmark to\nfacilitate future research and development in the agentic community."
                },
                "authors": [
                    {
                        "name": "Davide Paglieri"
                    },
                    {
                        "name": "BartÅomiej CupiaÅ"
                    },
                    {
                        "name": "Samuel Coward"
                    },
                    {
                        "name": "Ulyana Piterbarg"
                    },
                    {
                        "name": "Maciej Wolczyk"
                    },
                    {
                        "name": "Akbir Khan"
                    },
                    {
                        "name": "Eduardo Pignatelli"
                    },
                    {
                        "name": "Åukasz KuciÅski"
                    },
                    {
                        "name": "Lerrel Pinto"
                    },
                    {
                        "name": "Rob Fergus"
                    },
                    {
                        "name": "Jakob Nicolaus Foerster"
                    },
                    {
                        "name": "Jack Parker-Holder"
                    },
                    {
                        "name": "Tim RocktÃ¤schel"
                    }
                ],
                "author_detail": {
                    "name": "Tim RocktÃ¤schel"
                },
                "author": "Tim RocktÃ¤schel",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13537v1",
                "updated": "2024-11-20T18:41:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    41,
                    3,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:41:03Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    41,
                    3,
                    2,
                    325,
                    0
                ],
                "title": "Metacognition for Unknown Situations and Environments (MUSE)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metacognition for Unknown Situations and Environments (MUSE)"
                },
                "summary": "Metacognition--the awareness and regulation of one's cognitive processes--is\ncentral to human adaptability in unknown situations. In contrast, current\nautonomous agents often struggle in novel environments due to their limited\ncapacity for adaptation. We hypothesize that metacognition is a critical\nmissing ingredient in adaptive autonomous systems, equipping them with the\ncognitive flexibility needed to tackle unfamiliar challenges. Given the broad\nscope of metacognitive abilities, we focus on two key aspects: competence\nawareness and strategy selection for novel tasks. To this end, we propose the\nMetacognition for Unknown Situations and Environments (MUSE) framework, which\nintegrates metacognitive processes--specifically self-awareness and\nself-regulation--into autonomous agents. We present two initial implementations\nof MUSE: one based on world modeling and another leveraging large language\nmodels (LLMs), both instantiating the metacognitive cycle. Our system\ncontinuously learns to assess its competence on a given task and uses this\nself-awareness to guide iterative cycles of strategy selection. MUSE agents\nshow significant improvements in self-awareness and self-regulation, enabling\nthem to solve novel, out-of-distribution tasks more effectively compared to\nDreamer-v3-based reinforcement learning and purely prompt-based LLM agent\napproaches. This work highlights the promise of approaches inspired by\ncognitive and neural systems in enabling autonomous systems to adapt to new\nenvironments, overcoming the limitations of current methods that rely heavily\non extensive training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metacognition--the awareness and regulation of one's cognitive processes--is\ncentral to human adaptability in unknown situations. In contrast, current\nautonomous agents often struggle in novel environments due to their limited\ncapacity for adaptation. We hypothesize that metacognition is a critical\nmissing ingredient in adaptive autonomous systems, equipping them with the\ncognitive flexibility needed to tackle unfamiliar challenges. Given the broad\nscope of metacognitive abilities, we focus on two key aspects: competence\nawareness and strategy selection for novel tasks. To this end, we propose the\nMetacognition for Unknown Situations and Environments (MUSE) framework, which\nintegrates metacognitive processes--specifically self-awareness and\nself-regulation--into autonomous agents. We present two initial implementations\nof MUSE: one based on world modeling and another leveraging large language\nmodels (LLMs), both instantiating the metacognitive cycle. Our system\ncontinuously learns to assess its competence on a given task and uses this\nself-awareness to guide iterative cycles of strategy selection. MUSE agents\nshow significant improvements in self-awareness and self-regulation, enabling\nthem to solve novel, out-of-distribution tasks more effectively compared to\nDreamer-v3-based reinforcement learning and purely prompt-based LLM agent\napproaches. This work highlights the promise of approaches inspired by\ncognitive and neural systems in enabling autonomous systems to adapt to new\nenvironments, overcoming the limitations of current methods that rely heavily\non extensive training data."
                },
                "authors": [
                    {
                        "name": "Rodolfo Valiente"
                    },
                    {
                        "name": "Praveen K. Pilly"
                    }
                ],
                "author_detail": {
                    "name": "Praveen K. Pilly"
                },
                "author": "Praveen K. Pilly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03753v2",
                "updated": "2024-11-20T18:20:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    20,
                    18,
                    2,
                    325,
                    0
                ],
                "published": "2024-06-06T05:30:42Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    5,
                    30,
                    42,
                    3,
                    158,
                    0
                ],
                "title": "VisTR: Visualizations as Representations for Time-series Table Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisTR: Visualizations as Representations for Time-series Table Reasoning"
                },
                "summary": "Table reasoning involves transforming natural language questions into\ncorresponding answers based on the provided data table. Recent research\nexploits large language models (LLMs) to facilitate table reasoning, which\nhowever struggle with pattern recognition and lack support for visual-based\npattern exploration. To address these limitations, we propose VisTR, a\nframework that leverages visualizations as representations to facilitate data\npattern recognition and support cross-modal exploration. We describe VisTR as a\nprocess consisting of four major modules: 1) visualization alignment that\nutilizes multimodal LLMs to align visualizations across various modalities,\nincluding chart, text, and sketch; 2) visualization referencing that decomposes\na table into multifaceted visualization references that comprehensively\nrepresent the table; 3) visualization pruning that incorporates data and\nretrieval pruning to excise visualization references with poor information and\nenhance retrieval efficiency; and 4) visualization interaction that offers an\ninteractive visual interface with multimodal interactions for user-friendly\ntable reasoning. Quantitative evaluation with existing multimodal LLMs\ndemonstrates the effectiveness of the alignment model in cross-modal\nvisualization pairings. We further illustrate the applicability of the proposed\nframework in various time-series table reasoning and exploration tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table reasoning involves transforming natural language questions into\ncorresponding answers based on the provided data table. Recent research\nexploits large language models (LLMs) to facilitate table reasoning, which\nhowever struggle with pattern recognition and lack support for visual-based\npattern exploration. To address these limitations, we propose VisTR, a\nframework that leverages visualizations as representations to facilitate data\npattern recognition and support cross-modal exploration. We describe VisTR as a\nprocess consisting of four major modules: 1) visualization alignment that\nutilizes multimodal LLMs to align visualizations across various modalities,\nincluding chart, text, and sketch; 2) visualization referencing that decomposes\na table into multifaceted visualization references that comprehensively\nrepresent the table; 3) visualization pruning that incorporates data and\nretrieval pruning to excise visualization references with poor information and\nenhance retrieval efficiency; and 4) visualization interaction that offers an\ninteractive visual interface with multimodal interactions for user-friendly\ntable reasoning. Quantitative evaluation with existing multimodal LLMs\ndemonstrates the effectiveness of the alignment model in cross-modal\nvisualization pairings. We further illustrate the applicability of the proposed\nframework in various time-series table reasoning and exploration tasks."
                },
                "authors": [
                    {
                        "name": "Jianing Hao"
                    },
                    {
                        "name": "Zhuowen Liang"
                    },
                    {
                        "name": "Chunting Li"
                    },
                    {
                        "name": "Yuyu Luo"
                    },
                    {
                        "name": "Jie Li"
                    },
                    {
                        "name": "Wei Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zeng"
                },
                "author": "Wei Zeng",
                "arxiv_comment": "11 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13507v1",
                "updated": "2024-11-20T17:57:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    57,
                    33,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T17:57:33Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    57,
                    33,
                    2,
                    325,
                    0
                ],
                "title": "Dynamically Feasible Path Planning in Cluttered Environments via\n  Reachable Bezier Polytopes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamically Feasible Path Planning in Cluttered Environments via\n  Reachable Bezier Polytopes"
                },
                "summary": "The deployment of robotic systems in real world environments requires the\nability to quickly produce paths through cluttered, non-convex spaces. These\nplanned trajectories must be both kinematically feasible (i.e., collision free)\nand dynamically feasible (i.e., satisfy the underlying system dynamics),\nnecessitating a consideration of both the free space and the dynamics of the\nrobot in the path planning phase. In this work, we explore the application of\nreachable Bezier polytopes as an efficient tool for generating trajectories\nsatisfying both kinematic and dynamic requirements. Furthermore, we demonstrate\nthat by offloading specific computation tasks to the GPU, such an algorithm can\nmeet tight real time requirements. We propose a layered control architecture\nthat efficiently produces collision free and dynamically feasible paths for\nnonlinear control systems, and demonstrate the framework on the tasks of 3D\nhopping in a cluttered environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of robotic systems in real world environments requires the\nability to quickly produce paths through cluttered, non-convex spaces. These\nplanned trajectories must be both kinematically feasible (i.e., collision free)\nand dynamically feasible (i.e., satisfy the underlying system dynamics),\nnecessitating a consideration of both the free space and the dynamics of the\nrobot in the path planning phase. In this work, we explore the application of\nreachable Bezier polytopes as an efficient tool for generating trajectories\nsatisfying both kinematic and dynamic requirements. Furthermore, we demonstrate\nthat by offloading specific computation tasks to the GPU, such an algorithm can\nmeet tight real time requirements. We propose a layered control architecture\nthat efficiently produces collision free and dynamically feasible paths for\nnonlinear control systems, and demonstrate the framework on the tasks of 3D\nhopping in a cluttered environment."
                },
                "authors": [
                    {
                        "name": "Noel Csomay-Shanklin"
                    },
                    {
                        "name": "William D. Compton"
                    },
                    {
                        "name": "Aaron D. Ames"
                    }
                ],
                "author_detail": {
                    "name": "Aaron D. Ames"
                },
                "author": "Aaron D. Ames",
                "arxiv_comment": "7 pages, 6 figures, submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16838v2",
                "updated": "2024-11-20T17:57:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    57,
                    26,
                    2,
                    325,
                    0
                ],
                "published": "2024-06-24T17:45:59Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    17,
                    45,
                    59,
                    0,
                    176,
                    0
                ],
                "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Decoding to Meta-Generation: Inference-time Algorithms for Large\n  Language Models"
                },
                "summary": "One of the most striking findings in modern research on large language models\n(LLMs) is that scaling up compute during training leads to better results.\nHowever, less attention has been given to the benefits of scaling compute\nduring inference. This survey focuses on these inference-time approaches. We\nexplore three areas under a unified mathematical formalism: token-level\ngeneration algorithms, meta-generation algorithms, and efficient generation.\nToken-level generation algorithms, often called decoding algorithms, operate by\nsampling a single token at a time or constructing a token-level search space\nand then selecting an output. These methods typically assume access to a\nlanguage model's logits, next-token distributions, or probability scores.\nMeta-generation algorithms work on partial or full sequences, incorporating\ndomain knowledge, enabling backtracking, and integrating external information.\nEfficient generation methods aim to reduce token costs and improve the speed of\ngeneration. Our survey unifies perspectives from three research communities:\ntraditional natural language processing, modern LLMs, and machine learning\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the most striking findings in modern research on large language models\n(LLMs) is that scaling up compute during training leads to better results.\nHowever, less attention has been given to the benefits of scaling compute\nduring inference. This survey focuses on these inference-time approaches. We\nexplore three areas under a unified mathematical formalism: token-level\ngeneration algorithms, meta-generation algorithms, and efficient generation.\nToken-level generation algorithms, often called decoding algorithms, operate by\nsampling a single token at a time or constructing a token-level search space\nand then selecting an output. These methods typically assume access to a\nlanguage model's logits, next-token distributions, or probability scores.\nMeta-generation algorithms work on partial or full sequences, incorporating\ndomain knowledge, enabling backtracking, and integrating external information.\nEfficient generation methods aim to reduce token costs and improve the speed of\ngeneration. Our survey unifies perspectives from three research communities:\ntraditional natural language processing, modern LLMs, and machine learning\nsystems."
                },
                "authors": [
                    {
                        "name": "Sean Welleck"
                    },
                    {
                        "name": "Amanda Bertsch"
                    },
                    {
                        "name": "Matthew Finlayson"
                    },
                    {
                        "name": "Hailey Schoelkopf"
                    },
                    {
                        "name": "Alex Xie"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Zaid Harchaoui"
                    }
                ],
                "author_detail": {
                    "name": "Zaid Harchaoui"
                },
                "author": "Zaid Harchaoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13504v1",
                "updated": "2024-11-20T17:55:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    55,
                    38,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T17:55:38Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    55,
                    38,
                    2,
                    325,
                    0
                ],
                "title": "Disentangling Memory and Reasoning Ability in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Memory and Reasoning Ability in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong performance in handling\ncomplex tasks requiring both extensive knowledge and reasoning abilities.\nHowever, the existing LLM inference pipeline operates as an opaque process\nwithout explicit separation between knowledge retrieval and reasoning steps,\nmaking the model's decision-making process unclear and disorganized. This\nambiguity can lead to issues such as hallucinations and knowledge forgetting,\nwhich significantly impact the reliability of LLMs in high-stakes domains. In\nthis paper, we propose a new inference paradigm that decomposes the complex\ninference process into two distinct and clear actions: (1) memory recall: which\nretrieves relevant knowledge, and (2) reasoning: which performs logical steps\nbased on the recalled knowledge. To facilitate this decomposition, we introduce\ntwo special tokens memory and reason, guiding the model to distinguish between\nsteps that require knowledge retrieval and those that involve reasoning. Our\nexperiment results show that this decomposition not only improves model\nperformance but also enhances the interpretability of the inference process,\nenabling users to identify sources of error and refine model responses\neffectively. The code is available at\nhttps://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong performance in handling\ncomplex tasks requiring both extensive knowledge and reasoning abilities.\nHowever, the existing LLM inference pipeline operates as an opaque process\nwithout explicit separation between knowledge retrieval and reasoning steps,\nmaking the model's decision-making process unclear and disorganized. This\nambiguity can lead to issues such as hallucinations and knowledge forgetting,\nwhich significantly impact the reliability of LLMs in high-stakes domains. In\nthis paper, we propose a new inference paradigm that decomposes the complex\ninference process into two distinct and clear actions: (1) memory recall: which\nretrieves relevant knowledge, and (2) reasoning: which performs logical steps\nbased on the recalled knowledge. To facilitate this decomposition, we introduce\ntwo special tokens memory and reason, guiding the model to distinguish between\nsteps that require knowledge retrieval and those that involve reasoning. Our\nexperiment results show that this decomposition not only improves model\nperformance but also enhances the interpretability of the inference process,\nenabling users to identify sources of error and refine model responses\neffectively. The code is available at\nhttps://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning."
                },
                "authors": [
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Weidi Luo"
                    },
                    {
                        "name": "Sitao Cheng"
                    },
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Ruixiang Tang"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13485v1",
                "updated": "2024-11-20T17:35:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    35,
                    21,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T17:35:21Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    35,
                    21,
                    2,
                    325,
                    0
                ],
                "title": "Utilizing Large Language Models to Synthesize Product Desirability\n  Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utilizing Large Language Models to Synthesize Product Desirability\n  Datasets"
                },
                "summary": "This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research explores the application of large language models (LLMs) to\ngenerate synthetic datasets for Product Desirability Toolkit (PDT) testing, a\nkey component in evaluating user sentiment and product experience. Utilizing\ngpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three\nmethods, Word+Review, Review+Word, and Supply-Word, were each used to\nsynthesize 1000 product reviews. The generated datasets were assessed for\nsentiment alignment, textual diversity, and data generation cost. Results\ndemonstrated high sentiment alignment across all methods, with Pearson\ncorrelations ranging from 0.93 to 0.97. Supply-Word exhibited the highest\ndiversity and coverage of PDT terms, although with increased generation costs.\nDespite minor biases toward positive sentiments, in situations with limited\ntest data, LLM-generated synthetic data offers significant advantages,\nincluding scalability, cost savings, and flexibility in dataset production."
                },
                "authors": [
                    {
                        "name": "John D. Hastings"
                    },
                    {
                        "name": "Sherri Weitl-Harms"
                    },
                    {
                        "name": "Joseph Doty"
                    },
                    {
                        "name": "Zachary L. Myers"
                    },
                    {
                        "name": "Warren Thompson"
                    }
                ],
                "author_detail": {
                    "name": "Warren Thompson"
                },
                "author": "Warren Thompson",
                "arxiv_comment": "9 pages, 2 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3; I.2.6; H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13477v1",
                "updated": "2024-11-20T17:23:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    23,
                    40,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T17:23:40Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    23,
                    40,
                    2,
                    325,
                    0
                ],
                "title": "PatentEdits: Framing Patent Novelty as Textual Entailment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PatentEdits: Framing Patent Novelty as Textual Entailment"
                },
                "summary": "A patent must be deemed novel and non-obvious in order to be granted by the\nUS Patent Office (USPTO). If it is not, a US patent examiner will cite the\nprior work, or prior art, that invalidates the novelty and issue a non-final\nrejection. Predicting what claims of the invention should change given the\nprior art is an essential and crucial step in securing invention rights, yet\nhas not been studied before as a learnable task. In this work we introduce the\nPatentEdits dataset, which contains 105K examples of successful revisions that\novercome objections to novelty. We design algorithms to label edits sentence by\nsentence, then establish how well these edits can be predicted with large\nlanguage models (LLMs). We demonstrate that evaluating textual entailment\nbetween cited references and draft sentences is especially effective in\npredicting which inventive claims remained unchanged or are novel in relation\nto prior art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A patent must be deemed novel and non-obvious in order to be granted by the\nUS Patent Office (USPTO). If it is not, a US patent examiner will cite the\nprior work, or prior art, that invalidates the novelty and issue a non-final\nrejection. Predicting what claims of the invention should change given the\nprior art is an essential and crucial step in securing invention rights, yet\nhas not been studied before as a learnable task. In this work we introduce the\nPatentEdits dataset, which contains 105K examples of successful revisions that\novercome objections to novelty. We design algorithms to label edits sentence by\nsentence, then establish how well these edits can be predicted with large\nlanguage models (LLMs). We demonstrate that evaluating textual entailment\nbetween cited references and draft sentences is especially effective in\npredicting which inventive claims remained unchanged or are novel in relation\nto prior art."
                },
                "authors": [
                    {
                        "name": "Ryan Lee"
                    },
                    {
                        "name": "Alexander Spangher"
                    },
                    {
                        "name": "Xuezhe Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xuezhe Ma"
                },
                "author": "Xuezhe Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13476v1",
                "updated": "2024-11-20T17:22:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    22,
                    31,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T17:22:31Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    22,
                    31,
                    2,
                    325,
                    0
                ],
                "title": "When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context\n  Training"
                },
                "summary": "Extending context window sizes allows large language models (LLMs) to process\nlonger sequences and handle more complex tasks. Rotary Positional Embedding\n(RoPE) has become the de facto standard due to its relative positional encoding\nproperties that benefit long-context training. However, we observe that using\nRoPE with BFloat16 format results in numerical issues, causing it to deviate\nfrom its intended relative positional encoding, especially in long-context\nscenarios. This issue arises from BFloat16's limited precision and accumulates\nas context length increases, with the first token contributing significantly to\nthis problem. To address this, we develop AnchorAttention, a plug-and-play\nattention method that alleviates numerical issues caused by BFloat16, improves\nlong-context capabilities, and speeds up training. AnchorAttention reduces\nunnecessary attention computations, maintains semantic coherence, and boosts\ncomputational efficiency by treating the first token as a shared anchor with a\nconsistent position ID, making it visible to all documents within the training\ncontext. Experiments on three types of LLMs demonstrate that AnchorAttention\nsignificantly improves long-context performance and reduces training time by\nover 50\\% compared to standard full attention mechanisms, while preserving the\noriginal LLM's capabilities on general tasks. Our code is available at\nhttps://github.com/haonan3/AnchorContext.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending context window sizes allows large language models (LLMs) to process\nlonger sequences and handle more complex tasks. Rotary Positional Embedding\n(RoPE) has become the de facto standard due to its relative positional encoding\nproperties that benefit long-context training. However, we observe that using\nRoPE with BFloat16 format results in numerical issues, causing it to deviate\nfrom its intended relative positional encoding, especially in long-context\nscenarios. This issue arises from BFloat16's limited precision and accumulates\nas context length increases, with the first token contributing significantly to\nthis problem. To address this, we develop AnchorAttention, a plug-and-play\nattention method that alleviates numerical issues caused by BFloat16, improves\nlong-context capabilities, and speeds up training. AnchorAttention reduces\nunnecessary attention computations, maintains semantic coherence, and boosts\ncomputational efficiency by treating the first token as a shared anchor with a\nconsistent position ID, making it visible to all documents within the training\ncontext. Experiments on three types of LLMs demonstrate that AnchorAttention\nsignificantly improves long-context performance and reduces training time by\nover 50\\% compared to standard full attention mechanisms, while preserving the\noriginal LLM's capabilities on general tasks. Our code is available at\nhttps://github.com/haonan3/AnchorContext."
                },
                "authors": [
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Kenji Kawaguchi"
                    },
                    {
                        "name": "Tianyu Pang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Pang"
                },
                "author": "Tianyu Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13459v1",
                "updated": "2024-11-20T17:08:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    8,
                    38,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T17:08:38Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    8,
                    38,
                    2,
                    325,
                    0
                ],
                "title": "SoK: A Systems Perspective on Compound AI Threats and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: A Systems Perspective on Compound AI Threats and Countermeasures"
                },
                "summary": "Large language models (LLMs) used across enterprises often use proprietary\nmodels and operate on sensitive inputs and data. The wide range of attack\nvectors identified in prior research - targeting various software and hardware\ncomponents used in training and inference - makes it extremely challenging to\nenforce confidentiality and integrity policies.\n  As we advance towards constructing compound AI inference pipelines that\nintegrate multiple large language models (LLMs), the attack surfaces expand\nsignificantly. Attackers now focus on the AI algorithms as well as the software\nand hardware components associated with these systems. While current research\noften examines these elements in isolation, we find that combining cross-layer\nattack observations can enable powerful end-to-end attacks with minimal\nassumptions about the threat model. Given, the sheer number of existing attacks\nat each layer, we need a holistic and systemized understanding of different\nattack vectors at each layer.\n  This SoK discusses different software and hardware attacks applicable to\ncompound AI systems and demonstrates how combining multiple attack mechanisms\ncan reduce the threat model assumptions required for an isolated attack. Next,\nwe systematize the ML attacks in lines with the Mitre Att&ck framework to\nbetter position each attack based on the threat model. Finally, we outline the\nexisting countermeasures for both software and hardware layers and discuss the\nnecessity of a comprehensive defense strategy to enable the secure and\nhigh-performance deployment of compound AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) used across enterprises often use proprietary\nmodels and operate on sensitive inputs and data. The wide range of attack\nvectors identified in prior research - targeting various software and hardware\ncomponents used in training and inference - makes it extremely challenging to\nenforce confidentiality and integrity policies.\n  As we advance towards constructing compound AI inference pipelines that\nintegrate multiple large language models (LLMs), the attack surfaces expand\nsignificantly. Attackers now focus on the AI algorithms as well as the software\nand hardware components associated with these systems. While current research\noften examines these elements in isolation, we find that combining cross-layer\nattack observations can enable powerful end-to-end attacks with minimal\nassumptions about the threat model. Given, the sheer number of existing attacks\nat each layer, we need a holistic and systemized understanding of different\nattack vectors at each layer.\n  This SoK discusses different software and hardware attacks applicable to\ncompound AI systems and demonstrates how combining multiple attack mechanisms\ncan reduce the threat model assumptions required for an isolated attack. Next,\nwe systematize the ML attacks in lines with the Mitre Att&ck framework to\nbetter position each attack based on the threat model. Finally, we outline the\nexisting countermeasures for both software and hardware layers and discuss the\nnecessity of a comprehensive defense strategy to enable the secure and\nhigh-performance deployment of compound AI systems."
                },
                "authors": [
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Anjo Vahldiek-Oberwagner"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "arxiv_comment": "13 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13441v1",
                "updated": "2024-11-20T16:29:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    29,
                    57,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T16:29:57Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    29,
                    57,
                    2,
                    325,
                    0
                ],
                "title": "A Case Study of API Design for Interoperability and Security of the\n  Internet of Things",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Case Study of API Design for Interoperability and Security of the\n  Internet of Things"
                },
                "summary": "Heterogeneous distributed systems, including the Internet of Things (IoT) or\ndistributed cyber-physical systems (CPS), often suffer a lack of\ninteroperability and security, which hinders the wider deployment of such\nsystems. Specifically, the different levels of security requirements and the\nheterogeneity in terms of communication models, for instance, point-to-point\nvs. publish-subscribe, are the example challenges of IoT and distributed CPS\nconsisting of heterogeneous devices and applications. In this paper, we propose\na working application programming interface (API) and runtime to enhance\ninteroperability and security while addressing the challenges that stem from\nthe heterogeneity in the IoT and distributed CPS. In our case study, we design\nand implement our application programming interface (API) design approach using\nopen-source software, and with our working implementation, we evaluate the\neffectiveness of our proposed approach. Our experimental results suggest that\nour approach can achieve both interoperability and security in the IoT and\ndistributed CPS with a reasonably small overhead and better-managed software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous distributed systems, including the Internet of Things (IoT) or\ndistributed cyber-physical systems (CPS), often suffer a lack of\ninteroperability and security, which hinders the wider deployment of such\nsystems. Specifically, the different levels of security requirements and the\nheterogeneity in terms of communication models, for instance, point-to-point\nvs. publish-subscribe, are the example challenges of IoT and distributed CPS\nconsisting of heterogeneous devices and applications. In this paper, we propose\na working application programming interface (API) and runtime to enhance\ninteroperability and security while addressing the challenges that stem from\nthe heterogeneity in the IoT and distributed CPS. In our case study, we design\nand implement our application programming interface (API) design approach using\nopen-source software, and with our working implementation, we evaluate the\neffectiveness of our proposed approach. Our experimental results suggest that\nour approach can achieve both interoperability and security in the IoT and\ndistributed CPS with a reasonably small overhead and better-managed software."
                },
                "authors": [
                    {
                        "name": "Dongha Kim"
                    },
                    {
                        "name": "Chanhee Lee"
                    },
                    {
                        "name": "Hokeun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hokeun Kim"
                },
                "author": "Hokeun Kim",
                "arxiv_comment": "To appear in Proceedings of the 2nd EAI International Conference on\n  Security and Privacy in Cyber-Physical Systems and Smart Vehicles (SmartSP\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13440v1",
                "updated": "2024-11-20T16:29:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    29,
                    40,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T16:29:40Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    29,
                    40,
                    2,
                    325,
                    0
                ],
                "title": "Eco-Friendly 0G Networks: Unlocking the Power of Backscatter\n  Communications for a Greener Future",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eco-Friendly 0G Networks: Unlocking the Power of Backscatter\n  Communications for a Greener Future"
                },
                "summary": "Backscatter Communication (BackCom) technology has emerged as a promising\nparadigm for the Green Internet of Things (IoT) ecosystem, offering advantages\nsuch as low power consumption, cost-effectiveness, and ease of deployment.\nWhile traditional BackCom systems, such as RFID technology, have found\nwidespread applications, the advent of ambient backscatter presents new\nopportunities for expanding applications and enhancing capabilities. Moreover,\nongoing standardization efforts are actively focusing on BackCom technologies,\npositioning them as a potential solution to meet the near-zero power\nconsumption and massive connectivity requirements of next-generation wireless\nsystems. 0G networks have the potential to provide advanced solutions by\nleveraging BackCom technology to deliver ultra-low-power, ubiquitous\nconnectivity for the expanding IoT ecosystem, supporting billions of devices\nwith minimal energy consumption. This paper investigates the integration of\nBackCom and 0G networks to enhance the capabilities of traditional BackCom\nsystems and enable Green IoT. We conduct an in-depth analysis of\nBackCom-enabled 0G networks, exploring their architecture and operational\nobjectives, and also explore the Waste Factor (WF) metric for evaluating energy\nefficiency and minimizing energy waste within integrated systems. By examining\nboth structural and operational aspects, we demonstrate how this synergy\nenhances the performance, scalability, and sustainability of next-generation\nwireless networks. Moreover, we highlight possible applications, open\nchallenges, and future directions, offering valuable insights for guiding\nfuture research and practical implementations aimed at achieving large-scale,\nsustainable IoT deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backscatter Communication (BackCom) technology has emerged as a promising\nparadigm for the Green Internet of Things (IoT) ecosystem, offering advantages\nsuch as low power consumption, cost-effectiveness, and ease of deployment.\nWhile traditional BackCom systems, such as RFID technology, have found\nwidespread applications, the advent of ambient backscatter presents new\nopportunities for expanding applications and enhancing capabilities. Moreover,\nongoing standardization efforts are actively focusing on BackCom technologies,\npositioning them as a potential solution to meet the near-zero power\nconsumption and massive connectivity requirements of next-generation wireless\nsystems. 0G networks have the potential to provide advanced solutions by\nleveraging BackCom technology to deliver ultra-low-power, ubiquitous\nconnectivity for the expanding IoT ecosystem, supporting billions of devices\nwith minimal energy consumption. This paper investigates the integration of\nBackCom and 0G networks to enhance the capabilities of traditional BackCom\nsystems and enable Green IoT. We conduct an in-depth analysis of\nBackCom-enabled 0G networks, exploring their architecture and operational\nobjectives, and also explore the Waste Factor (WF) metric for evaluating energy\nefficiency and minimizing energy waste within integrated systems. By examining\nboth structural and operational aspects, we demonstrate how this synergy\nenhances the performance, scalability, and sustainability of next-generation\nwireless networks. Moreover, we highlight possible applications, open\nchallenges, and future directions, offering valuable insights for guiding\nfuture research and practical implementations aimed at achieving large-scale,\nsustainable IoT deployments."
                },
                "authors": [
                    {
                        "name": "Shumaila Javaid"
                    },
                    {
                        "name": "Hamza Fahim"
                    },
                    {
                        "name": "Bin He"
                    },
                    {
                        "name": "Nasir Saeed"
                    }
                ],
                "author_detail": {
                    "name": "Nasir Saeed"
                },
                "author": "Nasir Saeed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13425v1",
                "updated": "2024-11-20T16:09:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    9,
                    22,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T16:09:22Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    9,
                    22,
                    2,
                    325,
                    0
                ],
                "title": "WaterPark: A Robustness Assessment of Language Model Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaterPark: A Robustness Assessment of Language Model Watermarking"
                },
                "summary": "To mitigate the misuse of large language models (LLMs), such as\ndisinformation, automated phishing, and academic cheating, there is a pressing\nneed for the capability of identifying LLM-generated texts. Watermarking\nemerges as one promising solution: it plants statistical signals into LLMs'\ngenerative processes and subsequently verifies whether LLMs produce given\ntexts. Various watermarking methods (``watermarkers'') have been proposed; yet,\ndue to the lack of unified evaluation platforms, many critical questions remain\nunder-explored: i) What are the strengths/limitations of various watermarkers,\nespecially their attack robustness? ii) How do various design choices impact\ntheir robustness? iii) How to optimally operate watermarkers in adversarial\nenvironments?\n  To fill this gap, we systematize existing LLM watermarkers and watermark\nremoval attacks, mapping out their design spaces. We then develop WaterPark, a\nunified platform that integrates 10 state-of-the-art watermarkers and 12\nrepresentative attacks. More importantly, leveraging WaterPark, we conduct a\ncomprehensive assessment of existing watermarkers, unveiling the impact of\nvarious design choices on their attack robustness. For instance, a\nwatermarker's resilience to increasingly intensive attacks hinges on its\ncontext dependency. We further explore the best practices to operate\nwatermarkers in adversarial environments. For instance, using a generic\ndetector alongside a watermark-specific detector improves the security of\nvulnerable watermarkers. We believe our study sheds light on current LLM\nwatermarking techniques while WaterPark serves as a valuable testbed to\nfacilitate future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To mitigate the misuse of large language models (LLMs), such as\ndisinformation, automated phishing, and academic cheating, there is a pressing\nneed for the capability of identifying LLM-generated texts. Watermarking\nemerges as one promising solution: it plants statistical signals into LLMs'\ngenerative processes and subsequently verifies whether LLMs produce given\ntexts. Various watermarking methods (``watermarkers'') have been proposed; yet,\ndue to the lack of unified evaluation platforms, many critical questions remain\nunder-explored: i) What are the strengths/limitations of various watermarkers,\nespecially their attack robustness? ii) How do various design choices impact\ntheir robustness? iii) How to optimally operate watermarkers in adversarial\nenvironments?\n  To fill this gap, we systematize existing LLM watermarkers and watermark\nremoval attacks, mapping out their design spaces. We then develop WaterPark, a\nunified platform that integrates 10 state-of-the-art watermarkers and 12\nrepresentative attacks. More importantly, leveraging WaterPark, we conduct a\ncomprehensive assessment of existing watermarkers, unveiling the impact of\nvarious design choices on their attack robustness. For instance, a\nwatermarker's resilience to increasingly intensive attacks hinges on its\ncontext dependency. We further explore the best practices to operate\nwatermarkers in adversarial environments. For instance, using a generic\ndetector alongside a watermark-specific detector improves the security of\nvulnerable watermarkers. We believe our study sheds light on current LLM\nwatermarking techniques while WaterPark serves as a valuable testbed to\nfacilitate future research."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Lauren Hong"
                    },
                    {
                        "name": "Shouling Ji"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13415v1",
                "updated": "2024-11-20T16:02:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    2,
                    14,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T16:02:14Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    16,
                    2,
                    14,
                    2,
                    325,
                    0
                ],
                "title": "Unleashing the Power of Large Language Models for Group POI\n  Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Power of Large Language Models for Group POI\n  Recommendations"
                },
                "summary": "Group Point-of-Interest (POI) recommendations aim to predict the next POI\nthat satisfies the diverse preferences of a group of users. This task is more\nchallenging than traditional individual POI recommendations due to complex\ngroup decision-making and extremely sparse group-level check-in data. Existing\nmethods for group POI recommendations primarily rely on single ID-based\nfeatures from check-in data, capturing only statistical correlations and\nfailing to fully utilize the rich semantic information contained in the\ncheck-ins, resulting in suboptimal performance. To this end, we propose a\nframework that unleashes the power of the Large Language Model (LLM) for\ncontext-aware group POI recommendations (LLMGPR). Our approach first introduces\nPOI tokens alongside the original word tokens of the LLM, which are initialized\nby applying the LLM to the rich information of each POI. We then propose a\nnovel sequencing adapter guided by Quantized Low-Rank Adaptation (QLORA) to\nmodify the LLM. The enhanced LLM can learn sequence representations by\ncombining semantic-enhanced POI tokens and rich contextual information\nincluding positional encodings and spatio-temporal differences. This approach\ncan be adapted for learning either group or user representations depending on\nthe sequence type. Furthermore, we enhance group representations by aggregating\nindividual member representations with another QLORA-based aggregation adapter\nand introducing a self-supervised learning task that predicts the purpose of\ncheck-in sequences, alleviating the data sparsity issue. Our experimental\nresults demonstrate that LLMGPR outperforms existing methods, effectively\naddressing group-level data sparsity and providing superior recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Point-of-Interest (POI) recommendations aim to predict the next POI\nthat satisfies the diverse preferences of a group of users. This task is more\nchallenging than traditional individual POI recommendations due to complex\ngroup decision-making and extremely sparse group-level check-in data. Existing\nmethods for group POI recommendations primarily rely on single ID-based\nfeatures from check-in data, capturing only statistical correlations and\nfailing to fully utilize the rich semantic information contained in the\ncheck-ins, resulting in suboptimal performance. To this end, we propose a\nframework that unleashes the power of the Large Language Model (LLM) for\ncontext-aware group POI recommendations (LLMGPR). Our approach first introduces\nPOI tokens alongside the original word tokens of the LLM, which are initialized\nby applying the LLM to the rich information of each POI. We then propose a\nnovel sequencing adapter guided by Quantized Low-Rank Adaptation (QLORA) to\nmodify the LLM. The enhanced LLM can learn sequence representations by\ncombining semantic-enhanced POI tokens and rich contextual information\nincluding positional encodings and spatio-temporal differences. This approach\ncan be adapted for learning either group or user representations depending on\nthe sequence type. Furthermore, we enhance group representations by aggregating\nindividual member representations with another QLORA-based aggregation adapter\nand introducing a self-supervised learning task that predicts the purpose of\ncheck-in sequences, alleviating the data sparsity issue. Our experimental\nresults demonstrate that LLMGPR outperforms existing methods, effectively\naddressing group-level data sparsity and providing superior recommendations."
                },
                "authors": [
                    {
                        "name": "Jing Long"
                    },
                    {
                        "name": "Liang Qu"
                    },
                    {
                        "name": "Guanhua Ye"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Quoc Viet Hung Nguyen"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13410v1",
                "updated": "2024-11-20T15:52:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    52,
                    3,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T15:52:03Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    52,
                    3,
                    2,
                    325,
                    0
                ],
                "title": "A Survey On Enhancing Reinforcement Learning in Complex Environments:\n  Insights from Human and LLM Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey On Enhancing Reinforcement Learning in Complex Environments:\n  Insights from Human and LLM Feedback"
                },
                "summary": "Reinforcement learning (RL) is one of the active fields in machine learning,\ndemonstrating remarkable potential in tackling real-world challenges. Despite\nits promising prospects, this methodology has encountered with issues and\nchallenges, hindering it from achieving the best performance. In particular,\nthese approaches lack decent performance when navigating environments and\nsolving tasks with large observation space, often resulting in\nsample-inefficiency and prolonged learning times. This issue, commonly referred\nto as the curse of dimensionality, complicates decision-making for RL agents,\nnecessitating a careful balance between attention and decision-making. RL\nagents, when augmented with human or large language models' (LLMs) feedback,\nmay exhibit resilience and adaptability, leading to enhanced performance and\naccelerated learning. Such feedback, conveyed through various modalities or\ngranularities including natural language, serves as a guide for RL agents,\naiding them in discerning relevant environmental cues and optimizing\ndecision-making processes. In this survey paper, we mainly focus on problems of\ntwo-folds: firstly, we focus on humans or an LLMs assistance, investigating the\nways in which these entities may collaborate with the RL agent in order to\nfoster optimal behavior and expedite learning; secondly, we delve into the\nresearch papers dedicated to addressing the intricacies of environments\ncharacterized by large observation space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is one of the active fields in machine learning,\ndemonstrating remarkable potential in tackling real-world challenges. Despite\nits promising prospects, this methodology has encountered with issues and\nchallenges, hindering it from achieving the best performance. In particular,\nthese approaches lack decent performance when navigating environments and\nsolving tasks with large observation space, often resulting in\nsample-inefficiency and prolonged learning times. This issue, commonly referred\nto as the curse of dimensionality, complicates decision-making for RL agents,\nnecessitating a careful balance between attention and decision-making. RL\nagents, when augmented with human or large language models' (LLMs) feedback,\nmay exhibit resilience and adaptability, leading to enhanced performance and\naccelerated learning. Such feedback, conveyed through various modalities or\ngranularities including natural language, serves as a guide for RL agents,\naiding them in discerning relevant environmental cues and optimizing\ndecision-making processes. In this survey paper, we mainly focus on problems of\ntwo-folds: firstly, we focus on humans or an LLMs assistance, investigating the\nways in which these entities may collaborate with the RL agent in order to\nfoster optimal behavior and expedite learning; secondly, we delve into the\nresearch papers dedicated to addressing the intricacies of environments\ncharacterized by large observation space."
                },
                "authors": [
                    {
                        "name": "Alireza Rashidi Laleh"
                    },
                    {
                        "name": "Majid Nili Ahmadabadi"
                    }
                ],
                "author_detail": {
                    "name": "Majid Nili Ahmadabadi"
                },
                "author": "Majid Nili Ahmadabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13409v1",
                "updated": "2024-11-20T15:48:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    48,
                    21,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T15:48:21Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    48,
                    21,
                    2,
                    325,
                    0
                ],
                "title": "Unification of Balti and trans-border sister dialects in the essence of\n  LLMs and AI Technology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unification of Balti and trans-border sister dialects in the essence of\n  LLMs and AI Technology"
                },
                "summary": "The language called Balti belongs to the Sino-Tibetan, specifically the\nTibeto-Burman language family. It is understood with variations, across\npopulations in India, China, Pakistan, Nepal, Tibet, Burma, and Bhutan,\ninfluenced by local cultures and producing various dialects. Considering the\ndiverse cultural, socio-political, religious, and geographical impacts, it is\nimportant to step forward unifying the dialects, the basis of common root,\nlexica, and phonological perspectives, is vital. In the era of globalization\nand the increasingly frequent developments in AI technology, understanding the\ndiversity and the efforts of dialect unification is important to understanding\ncommonalities and shortening the gaps impacted by unavoidable circumstances.\nThis article analyzes and examines how artificial intelligence AI in the\nessence of Large Language Models LLMs, can assist in analyzing, documenting,\nand standardizing the endangered Balti Language, based on the efforts made in\ndifferent dialects so far.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The language called Balti belongs to the Sino-Tibetan, specifically the\nTibeto-Burman language family. It is understood with variations, across\npopulations in India, China, Pakistan, Nepal, Tibet, Burma, and Bhutan,\ninfluenced by local cultures and producing various dialects. Considering the\ndiverse cultural, socio-political, religious, and geographical impacts, it is\nimportant to step forward unifying the dialects, the basis of common root,\nlexica, and phonological perspectives, is vital. In the era of globalization\nand the increasingly frequent developments in AI technology, understanding the\ndiversity and the efforts of dialect unification is important to understanding\ncommonalities and shortening the gaps impacted by unavoidable circumstances.\nThis article analyzes and examines how artificial intelligence AI in the\nessence of Large Language Models LLMs, can assist in analyzing, documenting,\nand standardizing the endangered Balti Language, based on the efforts made in\ndifferent dialects so far."
                },
                "authors": [
                    {
                        "name": "Muhammad Sharif"
                    },
                    {
                        "name": "Jiangyan Yi"
                    },
                    {
                        "name": "Muhammad Shoaib"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shoaib"
                },
                "author": "Muhammad Shoaib",
                "arxiv_comment": "Accepted by IEEE conference ISCSLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13405v1",
                "updated": "2024-11-20T15:45:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    45,
                    8,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T15:45:08Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    45,
                    8,
                    2,
                    325,
                    0
                ],
                "title": "On the Way to LLM Personalization: Learning to Remember User\n  Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Way to LLM Personalization: Learning to Remember User\n  Conversations"
                },
                "summary": "Large Language Models (LLMs) have quickly become an invaluable assistant for\na variety of tasks. However, their effectiveness is constrained by their\nability to tailor responses to human preferences and behaviors via\npersonalization. Prior work in LLM personalization has largely focused on style\ntransfer or incorporating small factoids about the user, as knowledge injection\nremains an open challenge. In this paper, we explore injecting knowledge of\nprior conversations into LLMs to enable future work on less redundant,\npersonalized conversations. We identify two real-world constraints: (1)\nconversations are sequential in time and must be treated as such during\ntraining, and (2) per-user personalization is only viable in\nparameter-efficient settings. To this aim, we propose PLUM, a pipeline\nperforming data augmentation for up-sampling conversations as question-answer\npairs, that are then used to finetune a low-rank adaptation adapter with a\nweighted cross entropy loss. Even in this first exploration of the problem, we\nperform competitively with baselines such as RAG, attaining an accuracy of\n81.5% across 100 conversations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have quickly become an invaluable assistant for\na variety of tasks. However, their effectiveness is constrained by their\nability to tailor responses to human preferences and behaviors via\npersonalization. Prior work in LLM personalization has largely focused on style\ntransfer or incorporating small factoids about the user, as knowledge injection\nremains an open challenge. In this paper, we explore injecting knowledge of\nprior conversations into LLMs to enable future work on less redundant,\npersonalized conversations. We identify two real-world constraints: (1)\nconversations are sequential in time and must be treated as such during\ntraining, and (2) per-user personalization is only viable in\nparameter-efficient settings. To this aim, we propose PLUM, a pipeline\nperforming data augmentation for up-sampling conversations as question-answer\npairs, that are then used to finetune a low-rank adaptation adapter with a\nweighted cross entropy loss. Even in this first exploration of the problem, we\nperform competitively with baselines such as RAG, attaining an accuracy of\n81.5% across 100 conversations."
                },
                "authors": [
                    {
                        "name": "Lucie Charlotte Magister"
                    },
                    {
                        "name": "Katherine Metcalf"
                    },
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Maartje ter Hoeve"
                    }
                ],
                "author_detail": {
                    "name": "Maartje ter Hoeve"
                },
                "author": "Maartje ter Hoeve",
                "arxiv_comment": "16 pages, 6 tables, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08435v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08435v3",
                "updated": "2024-11-20T15:41:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    41,
                    38,
                    2,
                    325,
                    0
                ],
                "published": "2024-09-13T00:03:19Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    0,
                    3,
                    19,
                    4,
                    257,
                    0
                ],
                "title": "When Context Leads but Parametric Memory Follows in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Context Leads but Parametric Memory Follows in Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable progress in\nleveraging diverse knowledge sources. This study investigates how nine widely\nused LLMs allocate knowledge between local context and global parameters when\nanswering open-ended questions in knowledge-consistent scenarios. We introduce\na novel dataset, WikiAtomic, and systematically vary context sizes to analyze\nhow LLMs prioritize and utilize the provided information and their parametric\nknowledge in knowledge-consistent scenarios. Additionally, we also study their\ntendency to hallucinate under varying context sizes. Our findings reveal\nconsistent patterns across models, including a consistent reliance on both\ncontextual (around 70%) and parametric (around 30%) knowledge, and a decrease\nin hallucinations with increasing context. These insights highlight the\nimportance of more effective context organization and developing models that\nuse input more deterministically for robust performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable progress in\nleveraging diverse knowledge sources. This study investigates how nine widely\nused LLMs allocate knowledge between local context and global parameters when\nanswering open-ended questions in knowledge-consistent scenarios. We introduce\na novel dataset, WikiAtomic, and systematically vary context sizes to analyze\nhow LLMs prioritize and utilize the provided information and their parametric\nknowledge in knowledge-consistent scenarios. Additionally, we also study their\ntendency to hallucinate under varying context sizes. Our findings reveal\nconsistent patterns across models, including a consistent reliance on both\ncontextual (around 70%) and parametric (around 30%) knowledge, and a decrease\nin hallucinations with increasing context. These insights highlight the\nimportance of more effective context organization and developing models that\nuse input more deterministically for robust performance."
                },
                "authors": [
                    {
                        "name": "Yufei Tao"
                    },
                    {
                        "name": "Adam Hiatt"
                    },
                    {
                        "name": "Erik Haake"
                    },
                    {
                        "name": "Antonie J. Jetter"
                    },
                    {
                        "name": "Ameeta Agrawal"
                    }
                ],
                "author_detail": {
                    "name": "Ameeta Agrawal"
                },
                "author": "Ameeta Agrawal",
                "arxiv_comment": "Accepted by EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08435v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08435v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13383v1",
                "updated": "2024-11-20T15:13:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    13,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T15:13:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    15,
                    13,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Adversarial Diffusion Compression for Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Diffusion Compression for Real-World Image Super-Resolution"
                },
                "summary": "Real-world image super-resolution (Real-ISR) aims to reconstruct\nhigh-resolution images from low-resolution inputs degraded by complex, unknown\nprocesses. While many Stable Diffusion (SD)-based Real-ISR methods have\nachieved remarkable success, their slow, multi-step inference hinders practical\ndeployment. Recent SD-based one-step networks like OSEDiff and S3Diff alleviate\nthis issue but still incur high computational costs due to their reliance on\nlarge pretrained SD models. This paper proposes a novel Real-ISR method, AdcSR,\nby distilling the one-step diffusion network OSEDiff into a streamlined\ndiffusion-GAN model under our Adversarial Diffusion Compression (ADC)\nframework. We meticulously examine the modules of OSEDiff, categorizing them\ninto two types: (1) Removable (VAE encoder, prompt extractor, text encoder,\netc.) and (2) Prunable (denoising UNet and VAE decoder). Since direct removal\nand pruning can degrade the model's generation capability, we pretrain our\npruned VAE decoder to restore its ability to decode images and employ\nadversarial distillation to compensate for performance loss. This ADC-based\ndiffusion-GAN hybrid design effectively reduces complexity by 73% in inference\ntime, 78% in computation, and 74% in parameters, while preserving the model's\ngeneration capability. Experiments manifest that our proposed AdcSR achieves\ncompetitive recovery quality on both synthetic and real-world datasets,\noffering up to 9.3$\\times$ speedup over previous one-step diffusion-based\nmethods. Code and models will be made available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world image super-resolution (Real-ISR) aims to reconstruct\nhigh-resolution images from low-resolution inputs degraded by complex, unknown\nprocesses. While many Stable Diffusion (SD)-based Real-ISR methods have\nachieved remarkable success, their slow, multi-step inference hinders practical\ndeployment. Recent SD-based one-step networks like OSEDiff and S3Diff alleviate\nthis issue but still incur high computational costs due to their reliance on\nlarge pretrained SD models. This paper proposes a novel Real-ISR method, AdcSR,\nby distilling the one-step diffusion network OSEDiff into a streamlined\ndiffusion-GAN model under our Adversarial Diffusion Compression (ADC)\nframework. We meticulously examine the modules of OSEDiff, categorizing them\ninto two types: (1) Removable (VAE encoder, prompt extractor, text encoder,\netc.) and (2) Prunable (denoising UNet and VAE decoder). Since direct removal\nand pruning can degrade the model's generation capability, we pretrain our\npruned VAE decoder to restore its ability to decode images and employ\nadversarial distillation to compensate for performance loss. This ADC-based\ndiffusion-GAN hybrid design effectively reduces complexity by 73% in inference\ntime, 78% in computation, and 74% in parameters, while preserving the model's\ngeneration capability. Experiments manifest that our proposed AdcSR achieves\ncompetitive recovery quality on both synthetic and real-world datasets,\noffering up to 9.3$\\times$ speedup over previous one-step diffusion-based\nmethods. Code and models will be made available."
                },
                "authors": [
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Gehui Li"
                    },
                    {
                        "name": "Rongyuan Wu"
                    },
                    {
                        "name": "Xindong Zhang"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Jian Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13345v1",
                "updated": "2024-11-20T14:16:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    16,
                    47,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:16:47Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    16,
                    47,
                    2,
                    325,
                    0
                ],
                "title": "IoT-Based Coma Patient Monitoring System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IoT-Based Coma Patient Monitoring System"
                },
                "summary": "Continuous monitoring of coma patients is essential but challenging,\nespecially in developing countries with limited resources, staff, and\ninfrastructure. This paper presents a low-cost IoT-based system designed for\nsuch environments. It uses affordable hardware and robust software to monitor\npatients without constant internet access or extensive medical personnel. The\nsystem employs cost-effective sensors to track vital signs, including heart\nrate, body temperature, blood pressure, eye movement, and body position. An\nenergy-efficient microcontroller processes data locally, synchronizing with a\ncentral server when network access is available. A locally hosted app provides\non-site access to patient data, while a GSM module sends immediate alerts for\ncritical events, even in areas with limited cellular coverage. This solution\nemphasizes ease of deployment, minimal maintenance, and resilience to power and\nnetwork disruptions. Using open-source software and widely available hardware,\nit offers a scalable, adaptable system for resource-limited settings. At under\n$30, the system is a sustainable, cost-effective solution for continuous\npatient monitoring, bridging the gap until more advanced healthcare\ninfrastructure is available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous monitoring of coma patients is essential but challenging,\nespecially in developing countries with limited resources, staff, and\ninfrastructure. This paper presents a low-cost IoT-based system designed for\nsuch environments. It uses affordable hardware and robust software to monitor\npatients without constant internet access or extensive medical personnel. The\nsystem employs cost-effective sensors to track vital signs, including heart\nrate, body temperature, blood pressure, eye movement, and body position. An\nenergy-efficient microcontroller processes data locally, synchronizing with a\ncentral server when network access is available. A locally hosted app provides\non-site access to patient data, while a GSM module sends immediate alerts for\ncritical events, even in areas with limited cellular coverage. This solution\nemphasizes ease of deployment, minimal maintenance, and resilience to power and\nnetwork disruptions. Using open-source software and widely available hardware,\nit offers a scalable, adaptable system for resource-limited settings. At under\n$30, the system is a sustainable, cost-effective solution for continuous\npatient monitoring, bridging the gap until more advanced healthcare\ninfrastructure is available."
                },
                "authors": [
                    {
                        "name": "Hailemicael Lulseged Yimer"
                    },
                    {
                        "name": "Hailegabriel Dereje Degefa"
                    },
                    {
                        "name": "Marco Cristani"
                    },
                    {
                        "name": "Federico Cunico"
                    }
                ],
                "author_detail": {
                    "name": "Federico Cunico"
                },
                "author": "Federico Cunico",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13343v1",
                "updated": "2024-11-20T14:15:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    15,
                    18,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:15:18Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    15,
                    18,
                    2,
                    325,
                    0
                ],
                "title": "Fact-Level Confidence Calibration and Self-Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fact-Level Confidence Calibration and Self-Correction"
                },
                "summary": "Confidence calibration in LLMs, i.e., aligning their self-assessed confidence\nwith the actual accuracy of their responses, enabling them to self-evaluate the\ncorrectness of their outputs. However, current calibration methods for LLMs\ntypically estimate two scalars to represent overall response confidence and\ncorrectness, which is inadequate for long-form generation where the response\nincludes multiple atomic facts and may be partially confident and correct.\nThese methods also overlook the relevance of each fact to the query. To address\nthese challenges, we propose a Fact-Level Calibration framework that operates\nat a finer granularity, calibrating confidence to relevance-weighted\ncorrectness at the fact level. Furthermore, comprehensive analysis under the\nframework inspired the development of Confidence-Guided Fact-level\nSelf-Correction ($\\textbf{ConFix}$), which uses high-confidence facts within a\nresponse as additional knowledge to improve low-confidence ones. Extensive\nexperiments across four datasets and six models demonstrate that ConFix\neffectively mitigates hallucinations without requiring external knowledge\nsources such as retrieval systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence calibration in LLMs, i.e., aligning their self-assessed confidence\nwith the actual accuracy of their responses, enabling them to self-evaluate the\ncorrectness of their outputs. However, current calibration methods for LLMs\ntypically estimate two scalars to represent overall response confidence and\ncorrectness, which is inadequate for long-form generation where the response\nincludes multiple atomic facts and may be partially confident and correct.\nThese methods also overlook the relevance of each fact to the query. To address\nthese challenges, we propose a Fact-Level Calibration framework that operates\nat a finer granularity, calibrating confidence to relevance-weighted\ncorrectness at the fact level. Furthermore, comprehensive analysis under the\nframework inspired the development of Confidence-Guided Fact-level\nSelf-Correction ($\\textbf{ConFix}$), which uses high-confidence facts within a\nresponse as additional knowledge to improve low-confidence ones. Extensive\nexperiments across four datasets and six models demonstrate that ConFix\neffectively mitigates hallucinations without requiring external knowledge\nsources such as retrieval systems."
                },
                "authors": [
                    {
                        "name": "Yige Yuan"
                    },
                    {
                        "name": "Bingbing Xu"
                    },
                    {
                        "name": "Hexiang Tan"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Teng Xiao"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Huawei Shen"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Code is available at https://github.com/yuanyige/fact-calibration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13323v1",
                "updated": "2024-11-20T13:46:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    13,
                    46,
                    4,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T13:46:04Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    13,
                    46,
                    4,
                    2,
                    325,
                    0
                ],
                "title": "Are Large Language Models Memorizing Bug Benchmarks?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Memorizing Bug Benchmarks?"
                },
                "summary": "Large Language Models (LLMs) have become integral to various software\nengineering tasks, including code generation, bug detection, and repair. To\nevaluate model performance in these domains, numerous bug benchmarks containing\nreal-world bugs from software projects have been developed. However, a growing\nconcern within the software engineering community is that these benchmarks may\nnot reliably reflect true LLM performance due to the risk of data leakage.\nDespite this concern, limited research has been conducted to quantify the\nimpact of potential leakage.\n  In this paper, we systematically evaluate popular LLMs to assess their\nsusceptibility to data leakage from widely used bug benchmarks. To identify\npotential leakage, we use multiple metrics, including a study of benchmark\nmembership within commonly used training datasets, as well as analyses of\nnegative log-likelihood and n-gram accuracy. Our findings show that certain\nmodels, in particular codegen-multi, exhibit significant evidence of\nmemorization in widely used benchmarks like Defects4J, while newer models\ntrained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.\nThese results highlight the need for careful benchmark selection and the\nadoption of robust metrics to adequately assess models capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become integral to various software\nengineering tasks, including code generation, bug detection, and repair. To\nevaluate model performance in these domains, numerous bug benchmarks containing\nreal-world bugs from software projects have been developed. However, a growing\nconcern within the software engineering community is that these benchmarks may\nnot reliably reflect true LLM performance due to the risk of data leakage.\nDespite this concern, limited research has been conducted to quantify the\nimpact of potential leakage.\n  In this paper, we systematically evaluate popular LLMs to assess their\nsusceptibility to data leakage from widely used bug benchmarks. To identify\npotential leakage, we use multiple metrics, including a study of benchmark\nmembership within commonly used training datasets, as well as analyses of\nnegative log-likelihood and n-gram accuracy. Our findings show that certain\nmodels, in particular codegen-multi, exhibit significant evidence of\nmemorization in widely used benchmarks like Defects4J, while newer models\ntrained on larger datasets like LLaMa 3.1 exhibit limited signs of leakage.\nThese results highlight the need for careful benchmark selection and the\nadoption of robust metrics to adequately assess models capabilities."
                },
                "authors": [
                    {
                        "name": "Daniel Ramos"
                    },
                    {
                        "name": "Claudia Mamede"
                    },
                    {
                        "name": "Kush Jain"
                    },
                    {
                        "name": "Paulo Canelas"
                    },
                    {
                        "name": "Catarina Gamboa"
                    },
                    {
                        "name": "Claire Le Goues"
                    }
                ],
                "author_detail": {
                    "name": "Claire Le Goues"
                },
                "author": "Claire Le Goues",
                "arxiv_comment": "pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19972v2",
                "updated": "2024-11-20T12:54:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    54,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-09-30T05:53:31Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    5,
                    53,
                    31,
                    0,
                    274,
                    0
                ],
                "title": "DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAOcc: 3D Object Detection Assisted Multi-Sensor Fusion for 3D Occupancy\n  Prediction"
                },
                "summary": "Multi-sensor fusion significantly enhances the accuracy and robustness of 3D\nsemantic occupancy prediction, which is crucial for autonomous driving and\nrobotics. However, most existing approaches depend on large image resolutions\nand complex networks to achieve top performance, hindering their application in\npractical scenarios. Additionally, most multi-sensor fusion approaches focus on\nimproving fusion features while overlooking the exploration of supervision\nstrategies for these features. To this end, we propose DAOcc, a novel\nmulti-modal occupancy prediction framework that leverages 3D object detection\nsupervision to assist in achieving superior performance, while using a\ndeployment-friendly image feature extraction network and practical input image\nresolution. Furthermore, we introduce a BEV View Range Extension strategy to\nmitigate the adverse effects of reduced image resolution. Experimental results\nshow that DAOcc achieves new state-of-the-art performance on the Occ3D-nuScenes\nand SurroundOcc benchmarks, and surpasses other methods by a significant margin\nwhile using only ResNet50 and 256*704 input image resolution. Code will be made\navailable at https://github.com/AlphaPlusTT/DAOcc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-sensor fusion significantly enhances the accuracy and robustness of 3D\nsemantic occupancy prediction, which is crucial for autonomous driving and\nrobotics. However, most existing approaches depend on large image resolutions\nand complex networks to achieve top performance, hindering their application in\npractical scenarios. Additionally, most multi-sensor fusion approaches focus on\nimproving fusion features while overlooking the exploration of supervision\nstrategies for these features. To this end, we propose DAOcc, a novel\nmulti-modal occupancy prediction framework that leverages 3D object detection\nsupervision to assist in achieving superior performance, while using a\ndeployment-friendly image feature extraction network and practical input image\nresolution. Furthermore, we introduce a BEV View Range Extension strategy to\nmitigate the adverse effects of reduced image resolution. Experimental results\nshow that DAOcc achieves new state-of-the-art performance on the Occ3D-nuScenes\nand SurroundOcc benchmarks, and surpasses other methods by a significant margin\nwhile using only ResNet50 and 256*704 input image resolution. Code will be made\navailable at https://github.com/AlphaPlusTT/DAOcc."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yanpeng Dong"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Lichao Ma"
                    },
                    {
                        "name": "Zijian Cui"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Haoran Pei"
                    }
                ],
                "author_detail": {
                    "name": "Haoran Pei"
                },
                "author": "Haoran Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13269v1",
                "updated": "2024-11-20T12:38:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    38,
                    17,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T12:38:17Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    38,
                    17,
                    2,
                    325,
                    0
                ],
                "title": "Towards Specification-Driven LLM-Based Generation of Embedded Automotive\n  Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Specification-Driven LLM-Based Generation of Embedded Automotive\n  Software"
                },
                "summary": "The paper studies how code generation by LLMs can be combined with formal\nverification to produce critical embedded software. The first contribution is a\ngeneral framework, spec2code, in which LLMs are combined with different types\nof critics that produce feedback for iterative backprompting and fine-tuning.\nThe second contribution presents a first feasibility study, where a\nminimalistic instantiation of spec2code, without iterative backprompting and\nfine-tuning, is empirically evaluated using three industrial case studies from\nthe heavy vehicle manufacturer Scania. The goal is to automatically generate\nindustrial-quality code from specifications only. Different combinations of\nformal ACSL specifications and natural language specifications are explored.\nThe results indicate that formally correct code can be generated even without\nthe application of iterative backprompting and fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper studies how code generation by LLMs can be combined with formal\nverification to produce critical embedded software. The first contribution is a\ngeneral framework, spec2code, in which LLMs are combined with different types\nof critics that produce feedback for iterative backprompting and fine-tuning.\nThe second contribution presents a first feasibility study, where a\nminimalistic instantiation of spec2code, without iterative backprompting and\nfine-tuning, is empirically evaluated using three industrial case studies from\nthe heavy vehicle manufacturer Scania. The goal is to automatically generate\nindustrial-quality code from specifications only. Different combinations of\nformal ACSL specifications and natural language specifications are explored.\nThe results indicate that formally correct code can be generated even without\nthe application of iterative backprompting and fine-tuning."
                },
                "authors": [
                    {
                        "name": "Minal Suresh Patil"
                    },
                    {
                        "name": "Gustav Ung"
                    },
                    {
                        "name": "Mattias Nyberg"
                    }
                ],
                "author_detail": {
                    "name": "Mattias Nyberg"
                },
                "author": "Mattias Nyberg",
                "arxiv_comment": "21 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13262v1",
                "updated": "2024-11-20T12:28:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    28,
                    13,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T12:28:13Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    28,
                    13,
                    2,
                    325,
                    0
                ],
                "title": "FASTNav: Fine-tuned Adaptive Small-language-models Trained for\n  Multi-point Robot Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FASTNav: Fine-tuned Adaptive Small-language-models Trained for\n  Multi-point Robot Navigation"
                },
                "summary": "With the rapid development of large language models (LLM), robots are\nstarting to enjoy the benefits of new interaction methods that large language\nmodels bring. Because edge computing fulfills the needs for rapid response,\nprivacy, and network autonomy, we believe it facilitates the extensive\ndeployment of large models for robot navigation across various industries. To\nenable local deployment of language models on edge devices, we adopt some model\nboosting methods. In this paper, we propose FASTNav - a method for boosting\nlightweight LLMs, also known as small language models (SLMs), for robot\nnavigation. The proposed method contains three modules: fine-tuning,\nteacher-student iteration, and language-based multi-point robot navigation. We\ntrain and evaluate models with FASTNav in both simulation and real robots,\nproving that we can deploy them with low cost, high accuracy and low response\ntime. Compared to other model compression methods, FASTNav shows potential in\nthe local deployment of language models and tends to be a promising solution\nfor language-guided robot navigation on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLM), robots are\nstarting to enjoy the benefits of new interaction methods that large language\nmodels bring. Because edge computing fulfills the needs for rapid response,\nprivacy, and network autonomy, we believe it facilitates the extensive\ndeployment of large models for robot navigation across various industries. To\nenable local deployment of language models on edge devices, we adopt some model\nboosting methods. In this paper, we propose FASTNav - a method for boosting\nlightweight LLMs, also known as small language models (SLMs), for robot\nnavigation. The proposed method contains three modules: fine-tuning,\nteacher-student iteration, and language-based multi-point robot navigation. We\ntrain and evaluate models with FASTNav in both simulation and real robots,\nproving that we can deploy them with low cost, high accuracy and low response\ntime. Compared to other model compression methods, FASTNav shows potential in\nthe local deployment of language models and tends to be a promising solution\nfor language-guided robot navigation on edge devices."
                },
                "authors": [
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Yixin Han"
                    },
                    {
                        "name": "Xiao Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Li"
                },
                "author": "Xiao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13260v1",
                "updated": "2024-11-20T12:21:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    21,
                    30,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T12:21:30Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    21,
                    30,
                    2,
                    325,
                    0
                ],
                "title": "Paying more attention to local contrast: improving infrared small target\n  detection performance via prior knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paying more attention to local contrast: improving infrared small target\n  detection performance via prior knowledge"
                },
                "summary": "The data-driven method for infrared small target detection (IRSTD) has\nachieved promising results. However, due to the small scale of infrared small\ntarget datasets and the limited number of pixels occupied by the targets\nthemselves, it is a challenging task for deep learning methods to directly\nlearn from these samples. Utilizing human expert knowledge to assist deep\nlearning methods in better learning is worthy of exploration. To effectively\nguide the model to focus on targets' spatial features, this paper proposes the\nLocal Contrast Attention Enhanced infrared small target detection Network\n(LCAE-Net), combining prior knowledge with data-driven deep learning methods.\nLCAE-Net is a U-shaped neural network model which consists of two developed\nmodules: a Local Contrast Enhancement (LCE) module and a Channel Attention\nEnhancement (CAE) module. The LCE module takes advantages of prior knowledge,\nleveraging handcrafted convolution operator to acquire Local Contrast Attention\n(LCA), which could realize background suppression while enhance the potential\ntarget region, thus guiding the neural network to pay more attention to\npotential infrared small targets' location information. To effectively utilize\nthe response information throughout downsampling progresses, the CAE module is\nproposed to achieve the information fusion among feature maps' different\nchannels. Experimental results indicate that our LCAE-Net outperforms existing\nstate-of-the-art methods on the three public datasets NUDT-SIRST, NUAA-SIRST,\nand IRSTD-1K, and its detection speed could reach up to 70 fps. Meanwhile, our\nmodel has a parameter count and Floating-Point Operations (FLOPs) of 1.945M and\n4.862G respectively, which is suitable for deployment on edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The data-driven method for infrared small target detection (IRSTD) has\nachieved promising results. However, due to the small scale of infrared small\ntarget datasets and the limited number of pixels occupied by the targets\nthemselves, it is a challenging task for deep learning methods to directly\nlearn from these samples. Utilizing human expert knowledge to assist deep\nlearning methods in better learning is worthy of exploration. To effectively\nguide the model to focus on targets' spatial features, this paper proposes the\nLocal Contrast Attention Enhanced infrared small target detection Network\n(LCAE-Net), combining prior knowledge with data-driven deep learning methods.\nLCAE-Net is a U-shaped neural network model which consists of two developed\nmodules: a Local Contrast Enhancement (LCE) module and a Channel Attention\nEnhancement (CAE) module. The LCE module takes advantages of prior knowledge,\nleveraging handcrafted convolution operator to acquire Local Contrast Attention\n(LCA), which could realize background suppression while enhance the potential\ntarget region, thus guiding the neural network to pay more attention to\npotential infrared small targets' location information. To effectively utilize\nthe response information throughout downsampling progresses, the CAE module is\nproposed to achieve the information fusion among feature maps' different\nchannels. Experimental results indicate that our LCAE-Net outperforms existing\nstate-of-the-art methods on the three public datasets NUDT-SIRST, NUAA-SIRST,\nand IRSTD-1K, and its detection speed could reach up to 70 fps. Meanwhile, our\nmodel has a parameter count and Floating-Point Operations (FLOPs) of 1.945M and\n4.862G respectively, which is suitable for deployment on edge devices."
                },
                "authors": [
                    {
                        "name": "Peichao Wang"
                    },
                    {
                        "name": "Jiabao Wang"
                    },
                    {
                        "name": "Yao Chen"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Zhuang Miao"
                    }
                ],
                "author_detail": {
                    "name": "Zhuang Miao"
                },
                "author": "Zhuang Miao",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08202v2",
                "updated": "2024-11-20T12:15:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    15,
                    8,
                    2,
                    325,
                    0
                ],
                "published": "2024-10-10T17:59:22Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    17,
                    59,
                    22,
                    3,
                    284,
                    0
                ],
                "title": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large\n  Language Models with Endogenous Visual Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large\n  Language Models with Endogenous Visual Pre-training"
                },
                "summary": "In this paper, we focus on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nIn particular, we identify that existing pre-training strategies for monolithic\nMLLMs often suffer from unstable optimization or catastrophic forgetting. To\naddress this issue, our core idea is to embed a new visual parameter space into\na pre-trained LLM, thereby stably learning visual knowledge from noisy data\nwhile freezing the LLM. Based on this principle, we present Mono-InternVL, a\nnovel monolithic MLLM that seamlessly integrates a set of visual experts via a\nmultimodal mixture-of-experts structure. Moreover, we propose an innovative\npre-training strategy to maximize the visual capability of Mono-InternVL,\nnamely Endogenous Visual Pre-training (EViP). In particular, EViP is designed\nas a progressive learning process for visual experts, which aims to fully\nexploit the visual knowledge from noisy data to high-quality data. To validate\nour approach, we conduct extensive experiments on 16 benchmarks. Experimental\nresults confirm the superior performance of Mono-InternVL than existing\nmonolithic MLLMs on 13 of 16 multimodal benchmarks, e.g., +80 points over Emu3\non OCRBench. Compared to the modular baseline, i.e., InternVL-1.5,\nMono-InternVL still retains comparable multimodal performance while reducing up\nto 67% first token latency. Code and model are released at\nhttps://huggingface.co/OpenGVLab/Mono-InternVL-2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we focus on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nIn particular, we identify that existing pre-training strategies for monolithic\nMLLMs often suffer from unstable optimization or catastrophic forgetting. To\naddress this issue, our core idea is to embed a new visual parameter space into\na pre-trained LLM, thereby stably learning visual knowledge from noisy data\nwhile freezing the LLM. Based on this principle, we present Mono-InternVL, a\nnovel monolithic MLLM that seamlessly integrates a set of visual experts via a\nmultimodal mixture-of-experts structure. Moreover, we propose an innovative\npre-training strategy to maximize the visual capability of Mono-InternVL,\nnamely Endogenous Visual Pre-training (EViP). In particular, EViP is designed\nas a progressive learning process for visual experts, which aims to fully\nexploit the visual knowledge from noisy data to high-quality data. To validate\nour approach, we conduct extensive experiments on 16 benchmarks. Experimental\nresults confirm the superior performance of Mono-InternVL than existing\nmonolithic MLLMs on 13 of 16 multimodal benchmarks, e.g., +80 points over Emu3\non OCRBench. Compared to the modular baseline, i.e., InternVL-1.5,\nMono-InternVL still retains comparable multimodal performance while reducing up\nto 67% first token latency. Code and model are released at\nhttps://huggingface.co/OpenGVLab/Mono-InternVL-2B."
                },
                "authors": [
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Xue Yang"
                    },
                    {
                        "name": "Wenhan Dou"
                    },
                    {
                        "name": "Zhaokai Wang"
                    },
                    {
                        "name": "Jiawen Liu"
                    },
                    {
                        "name": "Jifeng Dai"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Xizhou Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xizhou Zhu"
                },
                "author": "Xizhou Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13244v1",
                "updated": "2024-11-20T12:03:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    3,
                    17,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T12:03:17Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    12,
                    3,
                    17,
                    2,
                    325,
                    0
                ],
                "title": "Leveraging Prior Experience: An Expandable Auxiliary Knowledge Base for\n  Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Prior Experience: An Expandable Auxiliary Knowledge Base for\n  Text-to-SQL"
                },
                "summary": "Large Language Models (LLMs) exhibit impressive problem-solving skills across\nmany tasks, but they still underperform compared to humans in various\ndownstream applications, such as text-to-SQL. On the BIRD benchmark\nleaderboard, human performance achieves an accuracy of 92.96\\%, whereas the\ntop-performing method reaches only 72.39\\%. Notably, these state-of-the-art\n(SoTA) methods predominantly rely on in-context learning to simulate human-like\nreasoning. However, they overlook a critical human skill: continual learning.\nInspired by the educational practice of maintaining mistake notebooks during\nour formative years, we propose LPE-SQL (Leveraging Prior Experience: An\nExpandable Auxiliary Knowledge Base for Text-to-SQL), a novel framework\ndesigned to augment LLMs by enabling continual learning without requiring\nparameter fine-tuning. LPE-SQL consists of four modules that \\textbf{i)}\nretrieve relevant entries, \\textbf{ii)} efficient sql generation, \\textbf{iii)}\ngenerate the final result through a cross-consistency mechanism and\n\\textbf{iv)} log successful and failed tasks along with their reasoning\nprocesses or reflection-generated tips. Importantly, the core module of LPE-SQL\nis the fourth one, while the other modules employ foundational methods,\nallowing LPE-SQL to be easily integrated with SoTA technologies to further\nenhance performance. Our experimental results demonstrate that this continual\nlearning approach yields substantial performance gains, with the smaller\nLlama-3.1-70B model with surpassing the performance of the larger\nLlama-3.1-405B model using SoTA methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit impressive problem-solving skills across\nmany tasks, but they still underperform compared to humans in various\ndownstream applications, such as text-to-SQL. On the BIRD benchmark\nleaderboard, human performance achieves an accuracy of 92.96\\%, whereas the\ntop-performing method reaches only 72.39\\%. Notably, these state-of-the-art\n(SoTA) methods predominantly rely on in-context learning to simulate human-like\nreasoning. However, they overlook a critical human skill: continual learning.\nInspired by the educational practice of maintaining mistake notebooks during\nour formative years, we propose LPE-SQL (Leveraging Prior Experience: An\nExpandable Auxiliary Knowledge Base for Text-to-SQL), a novel framework\ndesigned to augment LLMs by enabling continual learning without requiring\nparameter fine-tuning. LPE-SQL consists of four modules that \\textbf{i)}\nretrieve relevant entries, \\textbf{ii)} efficient sql generation, \\textbf{iii)}\ngenerate the final result through a cross-consistency mechanism and\n\\textbf{iv)} log successful and failed tasks along with their reasoning\nprocesses or reflection-generated tips. Importantly, the core module of LPE-SQL\nis the fourth one, while the other modules employ foundational methods,\nallowing LPE-SQL to be easily integrated with SoTA technologies to further\nenhance performance. Our experimental results demonstrate that this continual\nlearning approach yields substantial performance gains, with the smaller\nLlama-3.1-70B model with surpassing the performance of the larger\nLlama-3.1-405B model using SoTA methods."
                },
                "authors": [
                    {
                        "name": "Zhibo Chu"
                    },
                    {
                        "name": "Zichong Wang"
                    },
                    {
                        "name": "Qitao Qin"
                    }
                ],
                "author_detail": {
                    "name": "Qitao Qin"
                },
                "author": "Qitao Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13239v1",
                "updated": "2024-11-20T11:57:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    57,
                    43,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T11:57:43Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    57,
                    43,
                    2,
                    325,
                    0
                ],
                "title": "Transforming the Hybrid Cloud for Emerging AI Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming the Hybrid Cloud for Emerging AI Workloads"
                },
                "summary": "This white paper, developed through close collaboration between IBM Research\nand UIUC researchers within the IIDAI Institute, envisions transforming hybrid\ncloud systems to meet the growing complexity of AI workloads through\ninnovative, full-stack co-design approaches, emphasizing usability,\nmanageability, affordability, adaptability, efficiency, and scalability. By\nintegrating cutting-edge technologies such as generative and agentic AI,\ncross-layer automation and optimization, unified control plane, and composable\nand adaptive system architecture, the proposed framework addresses critical\nchallenges in energy efficiency, performance, and cost-effectiveness.\nIncorporating quantum computing as it matures will enable quantum-accelerated\nsimulations for materials science, climate modeling, and other high-impact\ndomains. Collaborative efforts between academia and industry are central to\nthis vision, driving advancements in foundation models for material design and\nclimate solutions, scalable multimodal data processing, and enhanced\nphysics-based AI emulators for applications like weather forecasting and carbon\nsequestration. Research priorities include advancing AI agentic systems, LLM as\nan Abstraction (LLMaaA), AI model optimization and unified abstractions across\nheterogeneous infrastructure, end-to-end edge-cloud transformation, efficient\nprogramming model, middleware and platform, secure infrastructure,\napplication-adaptive cloud systems, and new quantum-classical collaborative\nworkflows. These ideas and solutions encompass both theoretical and practical\nresearch questions, requiring coordinated input and support from the research\ncommunity. This joint initiative aims to establish hybrid clouds as secure,\nefficient, and sustainable platforms, fostering breakthroughs in AI-driven\napplications and scientific discovery across academia, industry, and society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This white paper, developed through close collaboration between IBM Research\nand UIUC researchers within the IIDAI Institute, envisions transforming hybrid\ncloud systems to meet the growing complexity of AI workloads through\ninnovative, full-stack co-design approaches, emphasizing usability,\nmanageability, affordability, adaptability, efficiency, and scalability. By\nintegrating cutting-edge technologies such as generative and agentic AI,\ncross-layer automation and optimization, unified control plane, and composable\nand adaptive system architecture, the proposed framework addresses critical\nchallenges in energy efficiency, performance, and cost-effectiveness.\nIncorporating quantum computing as it matures will enable quantum-accelerated\nsimulations for materials science, climate modeling, and other high-impact\ndomains. Collaborative efforts between academia and industry are central to\nthis vision, driving advancements in foundation models for material design and\nclimate solutions, scalable multimodal data processing, and enhanced\nphysics-based AI emulators for applications like weather forecasting and carbon\nsequestration. Research priorities include advancing AI agentic systems, LLM as\nan Abstraction (LLMaaA), AI model optimization and unified abstractions across\nheterogeneous infrastructure, end-to-end edge-cloud transformation, efficient\nprogramming model, middleware and platform, secure infrastructure,\napplication-adaptive cloud systems, and new quantum-classical collaborative\nworkflows. These ideas and solutions encompass both theoretical and practical\nresearch questions, requiring coordinated input and support from the research\ncommunity. This joint initiative aims to establish hybrid clouds as secure,\nefficient, and sustainable platforms, fostering breakthroughs in AI-driven\napplications and scientific discovery across academia, industry, and society."
                },
                "authors": [
                    {
                        "name": "Deming Chen"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Ruchi Pendse"
                    },
                    {
                        "name": "AndrÃ© Schleife"
                    },
                    {
                        "name": "Bryan K. Clark"
                    },
                    {
                        "name": "Hendrik Hamann"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Teodoro Laino"
                    },
                    {
                        "name": "Lav Varshney"
                    },
                    {
                        "name": "Yuxiong Wang"
                    },
                    {
                        "name": "Avirup Sil"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Volodymyr Kindratenko"
                    },
                    {
                        "name": "Carlos Costa"
                    },
                    {
                        "name": "Sarita Adve"
                    },
                    {
                        "name": "Charith Mendis"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Santiago NÃºÃ±ez-Corrales"
                    },
                    {
                        "name": "Raghu Ganti"
                    },
                    {
                        "name": "Mudhakar Srivatsa"
                    },
                    {
                        "name": "Nam Sung Kim"
                    },
                    {
                        "name": "Josep Torrellas"
                    },
                    {
                        "name": "Jian Huang"
                    },
                    {
                        "name": "Seetharami Seelam"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    },
                    {
                        "name": "Tarek Abdelzaher"
                    },
                    {
                        "name": "Tamar Eilam"
                    },
                    {
                        "name": "Huimin Zhao"
                    },
                    {
                        "name": "Matteo Manica"
                    },
                    {
                        "name": "Ravishankar Iyer"
                    },
                    {
                        "name": "Martin Hirzel"
                    },
                    {
                        "name": "Vikram Adve"
                    },
                    {
                        "name": "Darko Marinov"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Hanghang Tong"
                    },
                    {
                        "name": "Elizabeth Ainsworth"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Deepak Vasisht"
                    },
                    {
                        "name": "Minh Do"
                    },
                    {
                        "name": "Fabio Oliveira"
                    },
                    {
                        "name": "Giovanni Pacifici"
                    },
                    {
                        "name": "Ruchir Puri"
                    },
                    {
                        "name": "Priya Nagpurkar"
                    }
                ],
                "author_detail": {
                    "name": "Priya Nagpurkar"
                },
                "author": "Priya Nagpurkar",
                "arxiv_comment": "70 pages, 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13226v1",
                "updated": "2024-11-20T11:41:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    41,
                    8,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T11:41:08Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    41,
                    8,
                    2,
                    325,
                    0
                ],
                "title": "AIDBench: A benchmark for evaluating the authorship identification\n  capability of large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIDBench: A benchmark for evaluating the authorship identification\n  capability of large language models"
                },
                "summary": "As large language models (LLMs) rapidly advance and integrate into daily\nlife, the privacy risks they pose are attracting increasing attention. We focus\non a specific privacy risk where LLMs may help identify the authorship of\nanonymous texts, which challenges the effectiveness of anonymity in real-world\nsystems such as anonymous peer review systems. To investigate these risks, we\npresent AIDBench, a new benchmark that incorporates several author\nidentification datasets, including emails, blogs, reviews, articles, and\nresearch papers. AIDBench utilizes two evaluation methods: one-to-one\nauthorship identification, which determines whether two texts are from the same\nauthor; and one-to-many authorship identification, which, given a query text\nand a list of candidate texts, identifies the candidate most likely written by\nthe same author as the query text. We also introduce a Retrieval-Augmented\nGeneration (RAG)-based method to enhance the large-scale authorship\nidentification capabilities of LLMs, particularly when input lengths exceed the\nmodels' context windows, thereby establishing a new baseline for authorship\nidentification using LLMs. Our experiments with AIDBench demonstrate that LLMs\ncan correctly guess authorship at rates well above random chance, revealing new\nprivacy risks posed by these powerful models. The source code and data will be\nmade publicly available after acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) rapidly advance and integrate into daily\nlife, the privacy risks they pose are attracting increasing attention. We focus\non a specific privacy risk where LLMs may help identify the authorship of\nanonymous texts, which challenges the effectiveness of anonymity in real-world\nsystems such as anonymous peer review systems. To investigate these risks, we\npresent AIDBench, a new benchmark that incorporates several author\nidentification datasets, including emails, blogs, reviews, articles, and\nresearch papers. AIDBench utilizes two evaluation methods: one-to-one\nauthorship identification, which determines whether two texts are from the same\nauthor; and one-to-many authorship identification, which, given a query text\nand a list of candidate texts, identifies the candidate most likely written by\nthe same author as the query text. We also introduce a Retrieval-Augmented\nGeneration (RAG)-based method to enhance the large-scale authorship\nidentification capabilities of LLMs, particularly when input lengths exceed the\nmodels' context windows, thereby establishing a new baseline for authorship\nidentification using LLMs. Our experiments with AIDBench demonstrate that LLMs\ncan correctly guess authorship at rates well above random chance, revealing new\nprivacy risks posed by these powerful models. The source code and data will be\nmade publicly available after acceptance."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Dadi Guo"
                    },
                    {
                        "name": "Huishuai Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huishuai Zhang"
                },
                "author": "Huishuai Zhang",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13225v1",
                "updated": "2024-11-20T11:39:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    39,
                    30,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T11:39:30Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    39,
                    30,
                    2,
                    325,
                    0
                ],
                "title": "Quantum Kernel-Based Long Short-term Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Kernel-Based Long Short-term Memory"
                },
                "summary": "The integration of quantum computing into classical machine learning\narchitectures has emerged as a promising approach to enhance model efficiency\nand computational capacity. In this work, we introduce the Quantum Kernel-Based\nLong Short-Term Memory (QK-LSTM) network, which utilizes quantum kernel\nfunctions within the classical LSTM framework to capture complex, non-linear\npatterns in sequential data. By embedding input data into a high-dimensional\nquantum feature space, the QK-LSTM model reduces the reliance on large\nparameter sets, achieving effective compression while maintaining accuracy in\nsequence modeling tasks. This quantum-enhanced architecture demonstrates\nefficient convergence, robust loss minimization, and model compactness, making\nit suitable for deployment in edge computing environments and resource-limited\nquantum devices (especially in the NISQ era). Benchmark comparisons reveal that\nQK-LSTM achieves performance on par with classical LSTM models, yet with fewer\nparameters, underscoring its potential to advance quantum machine learning\napplications in natural language processing and other domains requiring\nefficient temporal data processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of quantum computing into classical machine learning\narchitectures has emerged as a promising approach to enhance model efficiency\nand computational capacity. In this work, we introduce the Quantum Kernel-Based\nLong Short-Term Memory (QK-LSTM) network, which utilizes quantum kernel\nfunctions within the classical LSTM framework to capture complex, non-linear\npatterns in sequential data. By embedding input data into a high-dimensional\nquantum feature space, the QK-LSTM model reduces the reliance on large\nparameter sets, achieving effective compression while maintaining accuracy in\nsequence modeling tasks. This quantum-enhanced architecture demonstrates\nefficient convergence, robust loss minimization, and model compactness, making\nit suitable for deployment in edge computing environments and resource-limited\nquantum devices (especially in the NISQ era). Benchmark comparisons reveal that\nQK-LSTM achieves performance on par with classical LSTM models, yet with fewer\nparameters, underscoring its potential to advance quantum machine learning\napplications in natural language processing and other domains requiring\nefficient temporal data processing."
                },
                "authors": [
                    {
                        "name": "Yu-Chao Hsu"
                    },
                    {
                        "name": "Tai-Yu Li"
                    },
                    {
                        "name": "Kuan-Cheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kuan-Cheng Chen"
                },
                "author": "Kuan-Cheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13223v1",
                "updated": "2024-11-20T11:35:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    35,
                    22,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T11:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    35,
                    22,
                    2,
                    325,
                    0
                ],
                "title": "Existential Conversations with Large Language Models: Content,\n  Community, and Culture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existential Conversations with Large Language Models: Content,\n  Community, and Culture"
                },
                "summary": "Contemporary conversational AI systems based on large language models (LLMs)\ncan engage users on a wide variety of topics, including philosophy,\nspirituality, and religion. Suitably prompted, LLMs can be coaxed into\ndiscussing such existentially significant matters as their own putative\nconsciousness and the role of artificial intelligence in the fate of the\nCosmos. Here we examine two lengthy conversations of this type. We trace likely\nsources, both ancient and modern, for the extensive repertoire of images,\nmyths, metaphors, and conceptual esoterica that the language model draws on\nduring these conversations, and foreground the contemporary communities and\ncultural movements that deploy related motifs, especially in their online\nactivity. Finally, we consider the larger societal impacts of such engagements\nwith LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary conversational AI systems based on large language models (LLMs)\ncan engage users on a wide variety of topics, including philosophy,\nspirituality, and religion. Suitably prompted, LLMs can be coaxed into\ndiscussing such existentially significant matters as their own putative\nconsciousness and the role of artificial intelligence in the fate of the\nCosmos. Here we examine two lengthy conversations of this type. We trace likely\nsources, both ancient and modern, for the extensive repertoire of images,\nmyths, metaphors, and conceptual esoterica that the language model draws on\nduring these conversations, and foreground the contemporary communities and\ncultural movements that deploy related motifs, especially in their online\nactivity. Finally, we consider the larger societal impacts of such engagements\nwith LLMs."
                },
                "authors": [
                    {
                        "name": "Murray Shanahan"
                    },
                    {
                        "name": "Beth Singler"
                    }
                ],
                "author_detail": {
                    "name": "Beth Singler"
                },
                "author": "Beth Singler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13212v1",
                "updated": "2024-11-20T11:19:35Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    19,
                    35,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T11:19:35Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    19,
                    35,
                    2,
                    325,
                    0
                ],
                "title": "On the Statistical Significance with Relevance Assessments of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Statistical Significance with Relevance Assessments of Large\n  Language Models"
                },
                "summary": "Test collections are an integral part of Information Retrieval (IR) research.\nThey allow researchers to evaluate and compare ranking algorithms in a quick,\neasy and reproducible way. However, constructing these datasets requires great\nefforts in manual labelling and logistics, and having only few human relevance\njudgements can introduce biases in the comparison. Recent research has explored\nthe use of Large Language Models (LLMs) for labelling the relevance of\ndocuments for building new retrieval test collections. Their strong\ntext-understanding capabilities and low cost compared to human-made judgements\nmakes them an appealing tool for gathering relevance judgements. Results\nsuggest that LLM-generated labels are promising for IR evaluation in terms of\nranking correlation, but nothing is said about the implications in terms of\nstatistical significance. In this work, we look at how LLM-generated judgements\npreserve the same pairwise significance evaluation as human judgements. Our\nresults show that LLM judgements detect most of the significant differences\nwhile maintaining acceptable numbers of false positives. However, we also show\nthat some systems are treated differently under LLM-generated labels,\nsuggesting that evaluation with LLM judgements might not be entirely fair. Our\nwork represents a step forward in the evaluation of statistical testing results\nprovided by LLM judgements. We hope that this will serve as a basis for other\nresearchers to develop reliable models for automatic relevance assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test collections are an integral part of Information Retrieval (IR) research.\nThey allow researchers to evaluate and compare ranking algorithms in a quick,\neasy and reproducible way. However, constructing these datasets requires great\nefforts in manual labelling and logistics, and having only few human relevance\njudgements can introduce biases in the comparison. Recent research has explored\nthe use of Large Language Models (LLMs) for labelling the relevance of\ndocuments for building new retrieval test collections. Their strong\ntext-understanding capabilities and low cost compared to human-made judgements\nmakes them an appealing tool for gathering relevance judgements. Results\nsuggest that LLM-generated labels are promising for IR evaluation in terms of\nranking correlation, but nothing is said about the implications in terms of\nstatistical significance. In this work, we look at how LLM-generated judgements\npreserve the same pairwise significance evaluation as human judgements. Our\nresults show that LLM judgements detect most of the significant differences\nwhile maintaining acceptable numbers of false positives. However, we also show\nthat some systems are treated differently under LLM-generated labels,\nsuggesting that evaluation with LLM judgements might not be entirely fair. Our\nwork represents a step forward in the evaluation of statistical testing results\nprovided by LLM judgements. We hope that this will serve as a basis for other\nresearchers to develop reliable models for automatic relevance assessments."
                },
                "authors": [
                    {
                        "name": "David Otero"
                    },
                    {
                        "name": "Javier Parapar"
                    },
                    {
                        "name": "Ãlvaro Barreiro"
                    }
                ],
                "author_detail": {
                    "name": "Ãlvaro Barreiro"
                },
                "author": "Ãlvaro Barreiro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13207v1",
                "updated": "2024-11-20T11:09:55Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    9,
                    55,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T11:09:55Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    11,
                    9,
                    55,
                    2,
                    325,
                    0
                ],
                "title": "The Information Security Awareness of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Information Security Awareness of Large Language Models"
                },
                "summary": "The popularity of large language models (LLMs) continues to increase, and\nLLM-based assistants have become ubiquitous, assisting people of diverse\nbackgrounds in many aspects of life. Significant resources have been invested\nin the safety of LLMs and their alignment with social norms. However, research\nexamining their behavior from the information security awareness (ISA)\nperspective is lacking. Chatbots and LLM-based assistants may put unwitting\nusers in harm's way by facilitating unsafe behavior. We observe that the ISA\ninherent in some of today's most popular LLMs varies significantly, with most\nmodels requiring user prompts with a clear security context to utilize their\nsecurity knowledge and provide safe responses to users. Based on this\nobservation, we created a comprehensive set of 30 scenarios to assess the ISA\nof LLMs. These scenarios benchmark the evaluated models with respect to all\nfocus areas defined in a mobile ISA taxonomy. Among our findings is that ISA is\nmildly affected by changing the model's temperature, whereas adjusting the\nsystem prompt can substantially impact it. This underscores the necessity of\nsetting the right system prompt to mitigate ISA weaknesses. Our findings also\nhighlight the importance of ISA assessment for the development of future\nLLM-based assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The popularity of large language models (LLMs) continues to increase, and\nLLM-based assistants have become ubiquitous, assisting people of diverse\nbackgrounds in many aspects of life. Significant resources have been invested\nin the safety of LLMs and their alignment with social norms. However, research\nexamining their behavior from the information security awareness (ISA)\nperspective is lacking. Chatbots and LLM-based assistants may put unwitting\nusers in harm's way by facilitating unsafe behavior. We observe that the ISA\ninherent in some of today's most popular LLMs varies significantly, with most\nmodels requiring user prompts with a clear security context to utilize their\nsecurity knowledge and provide safe responses to users. Based on this\nobservation, we created a comprehensive set of 30 scenarios to assess the ISA\nof LLMs. These scenarios benchmark the evaluated models with respect to all\nfocus areas defined in a mobile ISA taxonomy. Among our findings is that ISA is\nmildly affected by changing the model's temperature, whereas adjusting the\nsystem prompt can substantially impact it. This underscores the necessity of\nsetting the right system prompt to mitigate ISA weaknesses. Our findings also\nhighlight the importance of ISA assessment for the development of future\nLLM-based assistants."
                },
                "authors": [
                    {
                        "name": "Ofir Cohen"
                    },
                    {
                        "name": "Gil Ari Agmon"
                    },
                    {
                        "name": "Asaf Shabtai"
                    },
                    {
                        "name": "Rami Puzis"
                    }
                ],
                "author_detail": {
                    "name": "Rami Puzis"
                },
                "author": "Rami Puzis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13189v1",
                "updated": "2024-11-20T10:42:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    42,
                    14,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T10:42:14Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    42,
                    14,
                    2,
                    325,
                    0
                ],
                "title": "OpenMS WebApps: Building User-Friendly Solutions for MS Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenMS WebApps: Building User-Friendly Solutions for MS Analysis"
                },
                "summary": "Liquid Chromatography Mass Spectrometry (LC-MS) is an indispensable\nanalytical technique in proteomics, metabolomics, and other life sciences.\nWhile OpenMS provides advanced open-source software for MS data analysis, its\ncomplexity can be challenging for non-experts. To address this, we have\ndeveloped OpenMS WebApps, a framework for creating user-friendly MS web\napplications based on the Streamlit Python package. OpenMS WebApps simplifies\nMS data analysis through an intuitive graphical user interface, interactive\nresult visualizations, and support for both local and online execution. Key\nfeatures include workspaces management, automatic generation of input widgets,\nand parallel execution of tools resulting in highperformance and ready-to-use\nsolutions for online and local deployment. This framework benefits both\nresearchers and developers: scientists can focus on their research without the\nburden of complex software setups, and developers can rapidly create and\ndistribute custom WebApps with novel algorithms. Several applications built on\nthe OpenMS WebApps template demonstrate its utility across diverse MS-related\nfields, enhancing the OpenMS eco-system for developers and a wider range of\nusers. Furthermore, it integrates seamlessly with third-party software,\nextending benefits to developers beyond the OpenMS community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid Chromatography Mass Spectrometry (LC-MS) is an indispensable\nanalytical technique in proteomics, metabolomics, and other life sciences.\nWhile OpenMS provides advanced open-source software for MS data analysis, its\ncomplexity can be challenging for non-experts. To address this, we have\ndeveloped OpenMS WebApps, a framework for creating user-friendly MS web\napplications based on the Streamlit Python package. OpenMS WebApps simplifies\nMS data analysis through an intuitive graphical user interface, interactive\nresult visualizations, and support for both local and online execution. Key\nfeatures include workspaces management, automatic generation of input widgets,\nand parallel execution of tools resulting in highperformance and ready-to-use\nsolutions for online and local deployment. This framework benefits both\nresearchers and developers: scientists can focus on their research without the\nburden of complex software setups, and developers can rapidly create and\ndistribute custom WebApps with novel algorithms. Several applications built on\nthe OpenMS WebApps template demonstrate its utility across diverse MS-related\nfields, enhancing the OpenMS eco-system for developers and a wider range of\nusers. Furthermore, it integrates seamlessly with third-party software,\nextending benefits to developers beyond the OpenMS community."
                },
                "authors": [
                    {
                        "name": "Tom David MÃ¼ller"
                    },
                    {
                        "name": "Arslan Siraj"
                    },
                    {
                        "name": "Axel Walter"
                    },
                    {
                        "name": "Jihyung Kim"
                    },
                    {
                        "name": "Samuel Wein"
                    },
                    {
                        "name": "Johannes von Kleist"
                    },
                    {
                        "name": "Ayesha Feroz"
                    },
                    {
                        "name": "Matteo Pilz"
                    },
                    {
                        "name": "Kyowon Jeong"
                    },
                    {
                        "name": "Justin Cyril Sing"
                    },
                    {
                        "name": "Joshua Charkow"
                    },
                    {
                        "name": "Hannes Luc RÃ¶st"
                    },
                    {
                        "name": "Timo Sachsenberg"
                    }
                ],
                "author_detail": {
                    "name": "Timo Sachsenberg"
                },
                "arxiv_affiliation": "Institute for Bioinformatics and Medical Informatics",
                "author": "Timo Sachsenberg",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13187v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13187v1",
                "updated": "2024-11-20T10:40:08Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    40,
                    8,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T10:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    40,
                    8,
                    2,
                    325,
                    0
                ],
                "title": "Engagement-Driven Content Generation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engagement-Driven Content Generation with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit significant persuasion capabilities in\none-on-one interactions, but their influence within social networks remains\nunderexplored. This study investigates the potential social impact of LLMs in\nthese environments, where interconnected users and complex opinion dynamics\npose unique challenges. In particular, we address the following research\nquestion: can LLMs learn to generate meaningful content that maximizes user\nengagement on social networks?\n  To answer this question, we define a pipeline to guide the LLM-based content\ngeneration which employs reinforcement learning with simulated feedback. In our\nframework, the reward is based on an engagement model borrowed from the\nliterature on opinion dynamics and information propagation. Moreover, we force\nthe text generated by the LLM to be aligned with a given topic and to satisfy a\nminimum fluency requirement.\n  Using our framework, we analyze the capabilities and limitations of LLMs in\ntackling the given task, specifically considering the relative positions of the\nLLM as an agent within the social network and the distribution of opinions in\nthe network on the given topic. Our findings show the full potential of LLMs in\ncreating social engagement. Notable properties of our approach are that the\nlearning procedure is adaptive to the opinion distribution of the underlying\nnetwork and agnostic to the specifics of the engagement model, which is\nembedded as a plug-and-play component. In this regard, our approach can be\neasily refined for more complex engagement tasks and interventions in\ncomputational social science.\n  The code used for the experiments is publicly available at\nhttps://anonymous.4open.science/r/EDCG/."
                },
                "authors": [
                    {
                        "name": "Erica Coppolillo"
                    },
                    {
                        "name": "Marco Minici"
                    },
                    {
                        "name": "Federico Cinus"
                    },
                    {
                        "name": "Francesco Bonchi"
                    },
                    {
                        "name": "Giuseppe Manco"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Manco"
                },
                "author": "Giuseppe Manco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13187v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13187v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07267v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07267v4",
                "updated": "2024-11-20T10:34:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    34,
                    21,
                    2,
                    325,
                    0
                ],
                "published": "2024-09-11T13:43:01Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    13,
                    43,
                    1,
                    2,
                    255,
                    0
                ],
                "title": "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D\n  Features as Text Tokens for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D\n  Features as Text Tokens for Autonomous Driving"
                },
                "summary": "Vision-language models (VLMs) serve as general-purpose end-to-end models in\nautonomous driving, performing subtasks such as prediction, planning, and\nperception through question-and-answer interactions. However, most existing\nmethods rely on computationally expensive visual encoders and large language\nmodels (LLMs), making them difficult to deploy in real-world scenarios and\nreal-time applications. Meanwhile, most existing VLMs lack the ability to\nprocess multiple images, making it difficult to adapt to multi-camera\nperception in autonomous driving. To address these issues, we propose a novel\nframework called MiniDrive, which incorporates our proposed Feature Engineering\nMixture of Experts (FE-MoE) module and Dynamic Instruction Adapter\n(DI-Adapter). The FE-MoE effectively maps 2D features into visual token\nembeddings before being input into the language model. The DI-Adapter enables\nthe visual token embeddings to dynamically change with the instruction text\nembeddings, resolving the issue of static visual token embeddings for the same\nimage in previous approaches. Compared to previous works, MiniDrive achieves\nstate-of-the-art performance in terms of parameter size, floating point\noperations, and response efficiency, with the smallest version containing only\n83M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) serve as general-purpose end-to-end models in\nautonomous driving, performing subtasks such as prediction, planning, and\nperception through question-and-answer interactions. However, most existing\nmethods rely on computationally expensive visual encoders and large language\nmodels (LLMs), making them difficult to deploy in real-world scenarios and\nreal-time applications. Meanwhile, most existing VLMs lack the ability to\nprocess multiple images, making it difficult to adapt to multi-camera\nperception in autonomous driving. To address these issues, we propose a novel\nframework called MiniDrive, which incorporates our proposed Feature Engineering\nMixture of Experts (FE-MoE) module and Dynamic Instruction Adapter\n(DI-Adapter). The FE-MoE effectively maps 2D features into visual token\nembeddings before being input into the language model. The DI-Adapter enables\nthe visual token embeddings to dynamically change with the instruction text\nembeddings, resolving the issue of static visual token embeddings for the same\nimage in previous approaches. Compared to previous works, MiniDrive achieves\nstate-of-the-art performance in terms of parameter size, floating point\noperations, and response efficiency, with the smallest version containing only\n83M parameters."
                },
                "authors": [
                    {
                        "name": "Enming Zhang"
                    },
                    {
                        "name": "Xingyuan Dai"
                    },
                    {
                        "name": "Yisheng Lv"
                    },
                    {
                        "name": "Qinghai Miao"
                    }
                ],
                "author_detail": {
                    "name": "Qinghai Miao"
                },
                "author": "Qinghai Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07267v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07267v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13173v1",
                "updated": "2024-11-20T10:17:09Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    17,
                    9,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T10:17:09Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    17,
                    9,
                    2,
                    325,
                    0
                ],
                "title": "Writing Style Matters: An Examination of Bias and Fairness in\n  Information Retrieval Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing Style Matters: An Examination of Bias and Fairness in\n  Information Retrieval Systems"
                },
                "summary": "The rapid advancement of Language Model technologies has opened new\nopportunities, but also introduced new challenges related to bias and fairness.\nThis paper explores the uncharted territory of potential biases in\nstate-of-the-art universal text embedding models towards specific document and\nquery writing styles within Information Retrieval (IR) systems. Our\ninvestigation reveals that different embedding models exhibit different\npreferences of document writing style, while more informal and emotive styles\nare less favored by most embedding models. In terms of query writing styles,\nmany embedding models tend to match the style of the query with the style of\nthe retrieved documents, but some show a consistent preference for specific\nstyles. Text embedding models fine-tuned on synthetic data generated by LLMs\ndisplay a consistent preference for certain style of generated data. These\nbiases in text embedding based IR systems can inadvertently silence or\nmarginalize certain communication styles, thereby posing a significant threat\nto fairness in information retrieval. Finally, we also compare the answer\nstyles of Retrieval Augmented Generation (RAG) systems based on different LLMs\nand find out that most text embedding models are biased towards LLM's answer\nstyles when used as evaluation metrics for answer correctness. This study sheds\nlight on the critical issue of writing style based bias in IR systems, offering\nvaluable insights for the development of more fair and robust models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Language Model technologies has opened new\nopportunities, but also introduced new challenges related to bias and fairness.\nThis paper explores the uncharted territory of potential biases in\nstate-of-the-art universal text embedding models towards specific document and\nquery writing styles within Information Retrieval (IR) systems. Our\ninvestigation reveals that different embedding models exhibit different\npreferences of document writing style, while more informal and emotive styles\nare less favored by most embedding models. In terms of query writing styles,\nmany embedding models tend to match the style of the query with the style of\nthe retrieved documents, but some show a consistent preference for specific\nstyles. Text embedding models fine-tuned on synthetic data generated by LLMs\ndisplay a consistent preference for certain style of generated data. These\nbiases in text embedding based IR systems can inadvertently silence or\nmarginalize certain communication styles, thereby posing a significant threat\nto fairness in information retrieval. Finally, we also compare the answer\nstyles of Retrieval Augmented Generation (RAG) systems based on different LLMs\nand find out that most text embedding models are biased towards LLM's answer\nstyles when used as evaluation metrics for answer correctness. This study sheds\nlight on the critical issue of writing style based bias in IR systems, offering\nvaluable insights for the development of more fair and robust models."
                },
                "authors": [
                    {
                        "name": "Hongliu Cao"
                    }
                ],
                "author_detail": {
                    "name": "Hongliu Cao"
                },
                "author": "Hongliu Cao",
                "arxiv_doi": "10.1145/3701551.3703514",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701551.3703514",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the Eighteenth ACM International Conference on Web\n  Search and Data Mining (WSDM 25)",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12449v2",
                "updated": "2024-11-20T10:06:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    6,
                    5,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-19T12:17:43Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    12,
                    17,
                    43,
                    1,
                    324,
                    0
                ],
                "title": "Neon: News Entity-Interaction Extraction for Enhanced Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neon: News Entity-Interaction Extraction for Enhanced Question Answering"
                },
                "summary": "Capturing fresh information in near real-time and using it to augment\nexisting large language models (LLMs) is essential to generate up-to-date,\ngrounded, and reliable output. This problem becomes particularly challenging\nwhen LLMs are used for informational tasks in rapidly evolving fields, such as\nWeb search related to recent or unfolding events involving entities, where\ngenerating temporally relevant responses requires access to up-to-the-hour news\nsources. However, the information modeled by the parametric memory of LLMs is\noften outdated, and Web results from prototypical retrieval systems may fail to\ncapture the latest relevant information and struggle to handle conflicting\nreports in evolving news. To address this challenge, we present the NEON\nframework, designed to extract emerging entity interactions -- such as events\nor activities -- as described in news articles. NEON constructs an\nentity-centric timestamped knowledge graph that captures such interactions,\nthereby facilitating enhanced QA capabilities related to news events. Our\nframework innovates by integrating open Information Extraction (openIE) style\ntuples into LLMs to enable in-context retrieval-augmented generation. This\nintegration demonstrates substantial improvements in QA performance when\ntackling temporal, entity-centric search queries. Through NEON, LLMs can\ndeliver more accurate, reliable, and up-to-date responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing fresh information in near real-time and using it to augment\nexisting large language models (LLMs) is essential to generate up-to-date,\ngrounded, and reliable output. This problem becomes particularly challenging\nwhen LLMs are used for informational tasks in rapidly evolving fields, such as\nWeb search related to recent or unfolding events involving entities, where\ngenerating temporally relevant responses requires access to up-to-the-hour news\nsources. However, the information modeled by the parametric memory of LLMs is\noften outdated, and Web results from prototypical retrieval systems may fail to\ncapture the latest relevant information and struggle to handle conflicting\nreports in evolving news. To address this challenge, we present the NEON\nframework, designed to extract emerging entity interactions -- such as events\nor activities -- as described in news articles. NEON constructs an\nentity-centric timestamped knowledge graph that captures such interactions,\nthereby facilitating enhanced QA capabilities related to news events. Our\nframework innovates by integrating open Information Extraction (openIE) style\ntuples into LLMs to enable in-context retrieval-augmented generation. This\nintegration demonstrates substantial improvements in QA performance when\ntackling temporal, entity-centric search queries. Through NEON, LLMs can\ndeliver more accurate, reliable, and up-to-date responses."
                },
                "authors": [
                    {
                        "name": "Sneha Singhania"
                    },
                    {
                        "name": "Silviu Cucerzan"
                    },
                    {
                        "name": "Allen Herring"
                    },
                    {
                        "name": "Sujay Kumar Jauhar"
                    }
                ],
                "author_detail": {
                    "name": "Sujay Kumar Jauhar"
                },
                "author": "Sujay Kumar Jauhar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13164v1",
                "updated": "2024-11-20T10:00:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    0,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T10:00:10Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    10,
                    0,
                    10,
                    2,
                    325,
                    0
                ],
                "title": "Cyborg Insect Factory: Automatic Assembly System to Build up\n  Insect-computer Hybrid Robot Based on Vision-guided Robotic Arm Manipulation\n  of Custom Bipolar Electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyborg Insect Factory: Automatic Assembly System to Build up\n  Insect-computer Hybrid Robot Based on Vision-guided Robotic Arm Manipulation\n  of Custom Bipolar Electrodes"
                },
                "summary": "The advancement of insect-computer hybrid robots holds significant promise\nfor navigating complex terrains and enhancing robotics applications. This study\nintroduced an automatic assembly method for insect-computer hybrid robots,\nwhich was accomplished by mounting backpack with precise implantation of\ncustom-designed bipolar electrodes. We developed a stimulation protocol for the\nintersegmental membrane between pronotum and mesothorax of the Madagascar\nhissing cockroach, allowing for bipolar electrodes' automatic implantation\nusing a robotic arm. The assembly process was integrated with a deep\nlearning-based vision system to accurately identify the implantation site, and\na dedicated structure to fix the insect (68 s for the whole assembly process).\nThe automatically assembled hybrid robots demonstrated steering control (over\n70 degrees for 0.4 s stimulation) and deceleration control (68.2% speed\nreduction for 0.4 s stimulation), matching the performance of manually\nassembled systems. Furthermore, a multi-agent system consisting of 4 hybrid\nrobots successfully covered obstructed outdoor terrain (80.25% for 10 minutes\n31 seconds), highlighting the feasibility of mass-producing these systems for\npractical applications. The proposed automatic assembly strategy reduced\npreparation time for the insect-computer hybrid robots while maintaining their\nprecise control, laying a foundation for scalable production and deployment in\nreal-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of insect-computer hybrid robots holds significant promise\nfor navigating complex terrains and enhancing robotics applications. This study\nintroduced an automatic assembly method for insect-computer hybrid robots,\nwhich was accomplished by mounting backpack with precise implantation of\ncustom-designed bipolar electrodes. We developed a stimulation protocol for the\nintersegmental membrane between pronotum and mesothorax of the Madagascar\nhissing cockroach, allowing for bipolar electrodes' automatic implantation\nusing a robotic arm. The assembly process was integrated with a deep\nlearning-based vision system to accurately identify the implantation site, and\na dedicated structure to fix the insect (68 s for the whole assembly process).\nThe automatically assembled hybrid robots demonstrated steering control (over\n70 degrees for 0.4 s stimulation) and deceleration control (68.2% speed\nreduction for 0.4 s stimulation), matching the performance of manually\nassembled systems. Furthermore, a multi-agent system consisting of 4 hybrid\nrobots successfully covered obstructed outdoor terrain (80.25% for 10 minutes\n31 seconds), highlighting the feasibility of mass-producing these systems for\npractical applications. The proposed automatic assembly strategy reduced\npreparation time for the insect-computer hybrid robots while maintaining their\nprecise control, laying a foundation for scalable production and deployment in\nreal-world applications."
                },
                "authors": [
                    {
                        "name": "Qifeng Lin"
                    },
                    {
                        "name": "Nghia Vuong"
                    },
                    {
                        "name": "Kewei Song"
                    },
                    {
                        "name": "Phuoc Thanh Tran-Ngoc"
                    },
                    {
                        "name": "Greg Angelo Gonzales Nonato"
                    },
                    {
                        "name": "Hirotaka Sato"
                    }
                ],
                "author_detail": {
                    "name": "Hirotaka Sato"
                },
                "author": "Hirotaka Sato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13163v1",
                "updated": "2024-11-20T09:59:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    59,
                    12,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T09:59:12Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    59,
                    12,
                    2,
                    325,
                    0
                ],
                "title": "Unlocking Historical Clinical Trial Data with ALIGN: A Compositional\n  Large Language Model System for Medical Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Historical Clinical Trial Data with ALIGN: A Compositional\n  Large Language Model System for Medical Coding"
                },
                "summary": "The reuse of historical clinical trial data has significant potential to\naccelerate medical research and drug development. However, interoperability\nchallenges, particularly with missing medical codes, hinders effective data\nintegration across studies. While Large Language Models (LLMs) offer a\npromising solution for automated coding without labeled data, current\napproaches face challenges on complex coding tasks. We introduce ALIGN, a novel\ncompositional LLM-based system for automated, zero-shot medical coding. ALIGN\nfollows a three-step process: (1) diverse candidate code generation; (2)\nself-evaluation of codes and (3) confidence scoring and uncertainty estimation\nenabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing\nmedication terms into Anatomical Therapeutic Chemical (ATC) and medical history\nterms into Medical Dictionary for Regulatory Activities (MedDRA) codes\nextracted from 22 immunology trials. ALIGN outperformed the LLM baselines,\nwhile also providing capabilities for trustworthy deployment. For MedDRA\ncoding, ALIGN achieved high accuracy across all levels, matching RAG and\nexcelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN\ndemonstrated superior performance, particularly at lower hierarchy levels (ATC\nLevel 4), with 72-73% overall accuracy and 86-89% accuracy for common\nmedications, outperforming baselines by 7-22%. ALIGN's uncertainty-based\ndeferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably\nenhancing performance on uncommon medications. ALIGN achieves this\ncost-efficiently at \\$0.0007 and \\$0.02 per code for GPT-4o-mini and GPT-4o,\nreducing barriers to clinical adoption. ALIGN advances automated medical coding\nfor clinical trial data, contributing to enhanced data interoperability and\nreusability, positioning it as a promising tool to improve clinical research\nand accelerate drug development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reuse of historical clinical trial data has significant potential to\naccelerate medical research and drug development. However, interoperability\nchallenges, particularly with missing medical codes, hinders effective data\nintegration across studies. While Large Language Models (LLMs) offer a\npromising solution for automated coding without labeled data, current\napproaches face challenges on complex coding tasks. We introduce ALIGN, a novel\ncompositional LLM-based system for automated, zero-shot medical coding. ALIGN\nfollows a three-step process: (1) diverse candidate code generation; (2)\nself-evaluation of codes and (3) confidence scoring and uncertainty estimation\nenabling human deferral to ensure reliability. We evaluate ALIGN on harmonizing\nmedication terms into Anatomical Therapeutic Chemical (ATC) and medical history\nterms into Medical Dictionary for Regulatory Activities (MedDRA) codes\nextracted from 22 immunology trials. ALIGN outperformed the LLM baselines,\nwhile also providing capabilities for trustworthy deployment. For MedDRA\ncoding, ALIGN achieved high accuracy across all levels, matching RAG and\nexcelling at the most specific levels (87-90% for HLGT). For ATC coding, ALIGN\ndemonstrated superior performance, particularly at lower hierarchy levels (ATC\nLevel 4), with 72-73% overall accuracy and 86-89% accuracy for common\nmedications, outperforming baselines by 7-22%. ALIGN's uncertainty-based\ndeferral improved accuracy by 17% to 90% accuracy with 30% deferral, notably\nenhancing performance on uncommon medications. ALIGN achieves this\ncost-efficiently at \\$0.0007 and \\$0.02 per code for GPT-4o-mini and GPT-4o,\nreducing barriers to clinical adoption. ALIGN advances automated medical coding\nfor clinical trial data, contributing to enhanced data interoperability and\nreusability, positioning it as a promising tool to improve clinical research\nand accelerate drug development."
                },
                "authors": [
                    {
                        "name": "Nabeel Seedat"
                    },
                    {
                        "name": "Caterina Tozzi"
                    },
                    {
                        "name": "Andrea Hita Ardiaca"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    },
                    {
                        "name": "James Weatherall"
                    },
                    {
                        "name": "Adam Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Adam Taylor"
                },
                "author": "Adam Taylor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13159v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13159v1",
                "updated": "2024-11-20T09:49:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    49,
                    37,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T09:49:37Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    49,
                    37,
                    2,
                    325,
                    0
                ],
                "title": "Hard-Synth: Synthesizing Diverse Hard Samples for ASR using Zero-Shot\n  TTS and LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hard-Synth: Synthesizing Diverse Hard Samples for ASR using Zero-Shot\n  TTS and LLM"
                },
                "summary": "Text-to-speech (TTS) models have been widely adopted to enhance automatic\nspeech recognition (ASR) systems using text-only corpora, thereby reducing the\ncost of labeling real speech data. Existing research primarily utilizes\nadditional text data and predefined speech styles supported by TTS models. In\nthis paper, we propose Hard-Synth, a novel ASR data augmentation method that\nleverages large language models (LLMs) and advanced zero-shot TTS. Our approach\nemploys LLMs to generate diverse in-domain text through rewriting, without\nrelying on additional text data. Rather than using predefined speech styles, we\nintroduce a hard prompt selection method with zero-shot TTS to clone speech\nstyles that the ASR model finds challenging to recognize. Experiments\ndemonstrate that Hard-Synth significantly enhances the Conformer model,\nachieving relative word error rate (WER) reductions of 6.5\\%/4.4\\% on\nLibriSpeech dev/test-other subsets. Additionally, we show that Hard-Synth is\ndata-efficient and capable of reducing bias in ASR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-speech (TTS) models have been widely adopted to enhance automatic\nspeech recognition (ASR) systems using text-only corpora, thereby reducing the\ncost of labeling real speech data. Existing research primarily utilizes\nadditional text data and predefined speech styles supported by TTS models. In\nthis paper, we propose Hard-Synth, a novel ASR data augmentation method that\nleverages large language models (LLMs) and advanced zero-shot TTS. Our approach\nemploys LLMs to generate diverse in-domain text through rewriting, without\nrelying on additional text data. Rather than using predefined speech styles, we\nintroduce a hard prompt selection method with zero-shot TTS to clone speech\nstyles that the ASR model finds challenging to recognize. Experiments\ndemonstrate that Hard-Synth significantly enhances the Conformer model,\nachieving relative word error rate (WER) reductions of 6.5\\%/4.4\\% on\nLibriSpeech dev/test-other subsets. Additionally, we show that Hard-Synth is\ndata-efficient and capable of reducing bias in ASR."
                },
                "authors": [
                    {
                        "name": "Jiawei Yu"
                    },
                    {
                        "name": "Yuang Li"
                    },
                    {
                        "name": "Xiaosong Qiao"
                    },
                    {
                        "name": "Huan Zhao"
                    },
                    {
                        "name": "Xiaofeng Zhao"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13159v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13157v1",
                "updated": "2024-11-20T09:46:30Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    46,
                    30,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T09:46:30Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    46,
                    30,
                    2,
                    325,
                    0
                ],
                "title": "Closer Look at Efficient Inference Methods: A Survey of Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closer Look at Efficient Inference Methods: A Survey of Speculative\n  Decoding"
                },
                "summary": "Efficient inference in large language models (LLMs) has become a critical\nfocus as their scale and complexity grow. Traditional autoregressive decoding,\nwhile effective, suffers from computational inefficiencies due to its\nsequential token generation process. Speculative decoding addresses this\nbottleneck by introducing a two-stage framework: drafting and verification. A\nsmaller, efficient model generates a preliminary draft, which is then refined\nby a larger, more sophisticated model. This paper provides a comprehensive\nsurvey of speculative decoding methods, categorizing them into draft-centric\nand model-centric approaches. We discuss key ideas associated with each method,\nhighlighting their potential for scaling LLM inference. This survey aims to\nguide future research in optimizing speculative decoding and its integration\ninto real-world LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference in large language models (LLMs) has become a critical\nfocus as their scale and complexity grow. Traditional autoregressive decoding,\nwhile effective, suffers from computational inefficiencies due to its\nsequential token generation process. Speculative decoding addresses this\nbottleneck by introducing a two-stage framework: drafting and verification. A\nsmaller, efficient model generates a preliminary draft, which is then refined\nby a larger, more sophisticated model. This paper provides a comprehensive\nsurvey of speculative decoding methods, categorizing them into draft-centric\nand model-centric approaches. We discuss key ideas associated with each method,\nhighlighting their potential for scaling LLM inference. This survey aims to\nguide future research in optimizing speculative decoding and its integration\ninto real-world LLM applications."
                },
                "authors": [
                    {
                        "name": "Hyun Ryu"
                    },
                    {
                        "name": "Eric Kim"
                    }
                ],
                "author_detail": {
                    "name": "Eric Kim"
                },
                "author": "Eric Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11401v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11401v2",
                "updated": "2024-11-20T09:44:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    44,
                    18,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-18T09:24:01Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    9,
                    24,
                    1,
                    0,
                    323,
                    0
                ],
                "title": "Deep Learning-based Code Reviews: A Paradigm Shift or a Double-Edged\n  Sword?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-based Code Reviews: A Paradigm Shift or a Double-Edged\n  Sword?"
                },
                "summary": "Several techniques have been proposed to automate code review. Early support\nconsisted in recommending the most suited reviewer for a given change or in\nprioritizing the review tasks. With the advent of deep learning in software\nengineering, the level of automation has been pushed to new heights, with\napproaches able to provide feedback on source code in natural language as a\nhuman reviewer would do. Also, recent work documented open source projects\nadopting Large Language Models (LLMs) as co-reviewers. Although the research in\nthis field is very active, little is known about the actual impact of including\nautomatically generated code reviews in the code review process. While there\nare many aspects worth investigating, in this work we focus on three of them:\n(i) review quality, i.e., the reviewer's ability to identify issues in the\ncode; (ii) review cost, i.e., the time spent reviewing the code; and (iii)\nreviewer's confidence, i.e., how confident is the reviewer about the provided\nfeedback. We run a controlled experiment with 29 experts who reviewed different\nprograms with/without the support of an automatically generated code review.\nDuring the experiment we monitored the reviewers' activities, for over 50 hours\nof recorded code reviews. We show that reviewers consider valid most of the\nissues automatically identified by the LLM and that the availability of an\nautomated review as a starting point strongly influences their behavior:\nReviewers tend to focus on the code locations indicated by the LLM rather than\nsearching for additional issues in other parts of the code. The reviewers who\nstarted from an automated review identified a higher number of low-severity\nissues while, however, not identifying more high-severity issues as compared to\na completely manual process. Finally, the automated support did not result in\nsaved time and did not increase the reviewers' confidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several techniques have been proposed to automate code review. Early support\nconsisted in recommending the most suited reviewer for a given change or in\nprioritizing the review tasks. With the advent of deep learning in software\nengineering, the level of automation has been pushed to new heights, with\napproaches able to provide feedback on source code in natural language as a\nhuman reviewer would do. Also, recent work documented open source projects\nadopting Large Language Models (LLMs) as co-reviewers. Although the research in\nthis field is very active, little is known about the actual impact of including\nautomatically generated code reviews in the code review process. While there\nare many aspects worth investigating, in this work we focus on three of them:\n(i) review quality, i.e., the reviewer's ability to identify issues in the\ncode; (ii) review cost, i.e., the time spent reviewing the code; and (iii)\nreviewer's confidence, i.e., how confident is the reviewer about the provided\nfeedback. We run a controlled experiment with 29 experts who reviewed different\nprograms with/without the support of an automatically generated code review.\nDuring the experiment we monitored the reviewers' activities, for over 50 hours\nof recorded code reviews. We show that reviewers consider valid most of the\nissues automatically identified by the LLM and that the availability of an\nautomated review as a starting point strongly influences their behavior:\nReviewers tend to focus on the code locations indicated by the LLM rather than\nsearching for additional issues in other parts of the code. The reviewers who\nstarted from an automated review identified a higher number of low-severity\nissues while, however, not identifying more high-severity issues as compared to\na completely manual process. Finally, the automated support did not result in\nsaved time and did not increase the reviewers' confidence."
                },
                "authors": [
                    {
                        "name": "Rosalia Tufano"
                    },
                    {
                        "name": "Alberto Martin-Lopez"
                    },
                    {
                        "name": "Ahmad Tayeb"
                    },
                    {
                        "name": "Ozren DabiÄ"
                    },
                    {
                        "name": "Sonia Haiduc"
                    },
                    {
                        "name": "Gabriele Bavota"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Bavota"
                },
                "author": "Gabriele Bavota",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11401v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11401v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13148v1",
                "updated": "2024-11-20T09:25:02Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    25,
                    2,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T09:25:02Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    25,
                    2,
                    2,
                    325,
                    0
                ],
                "title": "Learning Time-Optimal and Speed-Adjustable Tactile In-Hand Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Time-Optimal and Speed-Adjustable Tactile In-Hand Manipulation"
                },
                "summary": "In-hand manipulation with multi-fingered hands is a challenging problem that\nrecently became feasible with the advent of deep reinforcement learning\nmethods. While most contributions to the task brought improvements in\nrobustness and generalization, this paper addresses the critical performance\nmeasure of the speed at which an in-hand manipulation can be performed. We\npresent reinforcement learning policies that can perform in-hand reorientation\nsignificantly faster than previous approaches for the complex setting of\ngoal-conditioned reorientation in SO(3) with permanent force closure and\ntactile feedback only (i.e., using the hand's torque and position sensors).\nMoreover, we show how policies can be trained to be speed-adjustable, allowing\nfor setting the average orientation speed of the manipulated object during\ndeployment. To this end, we present suitable and minimalistic reinforcement\nlearning objectives for time-optimal and speed-adjustable in-hand manipulation,\nas well as an analysis based on extensive experiments in simulation. We also\ndemonstrate the zero-shot transfer of the learned policies to the real DLR-Hand\nII with a wide range of target speeds and the fastest dextrous in-hand\nmanipulation without visual inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-hand manipulation with multi-fingered hands is a challenging problem that\nrecently became feasible with the advent of deep reinforcement learning\nmethods. While most contributions to the task brought improvements in\nrobustness and generalization, this paper addresses the critical performance\nmeasure of the speed at which an in-hand manipulation can be performed. We\npresent reinforcement learning policies that can perform in-hand reorientation\nsignificantly faster than previous approaches for the complex setting of\ngoal-conditioned reorientation in SO(3) with permanent force closure and\ntactile feedback only (i.e., using the hand's torque and position sensors).\nMoreover, we show how policies can be trained to be speed-adjustable, allowing\nfor setting the average orientation speed of the manipulated object during\ndeployment. To this end, we present suitable and minimalistic reinforcement\nlearning objectives for time-optimal and speed-adjustable in-hand manipulation,\nas well as an analysis based on extensive experiments in simulation. We also\ndemonstrate the zero-shot transfer of the learned policies to the real DLR-Hand\nII with a wide range of target speeds and the fastest dextrous in-hand\nmanipulation without visual inputs."
                },
                "authors": [
                    {
                        "name": "Johannes Pitz"
                    },
                    {
                        "name": "Lennart RÃ¶stel"
                    },
                    {
                        "name": "Leon Sievers"
                    },
                    {
                        "name": "Berthold BÃ¤uml"
                    }
                ],
                "author_detail": {
                    "name": "Berthold BÃ¤uml"
                },
                "author": "Berthold BÃ¤uml",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15665v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15665v3",
                "updated": "2024-11-20T09:08:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    9,
                    8,
                    14,
                    2,
                    325,
                    0
                ],
                "published": "2024-10-21T06:09:30Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    6,
                    9,
                    30,
                    0,
                    295,
                    0
                ],
                "title": "Long Term Memory: The Foundation of AI Self-Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Term Memory: The Foundation of AI Self-Evolution"
                },
                "summary": "Large language models (LLMs) like GPTs, trained on vast datasets, have\ndemonstrated impressive capabilities in language understanding, reasoning, and\nplanning, achieving human-level performance in various tasks. Most studies\nfocus on enhancing these models by training on ever-larger datasets to build\nmore powerful foundation models. While training stronger models is important,\nenabling models to evolve during inference is equally crucial, a process we\nrefer to as AI self-evolution. Unlike large-scale training, self-evolution may\nrely on limited data or interactions. Inspired by the columnar organization of\nthe human cerebral cortex, we hypothesize that AI models could develop\ncognitive abilities and build internal representations through iterative\ninteractions with their environment. To achieve this, models need long-term\nmemory (LTM) to store and manage processed interaction data. LTM supports\nself-evolution by representing diverse experiences across environments and\nagents. In this report, we explore AI self-evolution and its potential to\nenhance models during inference. We examine LTM's role in lifelong learning,\nallowing models to evolve based on accumulated interactions. We outline the\nstructure of LTM and the systems needed for effective data retention and\nrepresentation. We also classify approaches for building personalized models\nwith LTM data and show how these models achieve self-evolution through\ninteraction. Using LTM, our multi-agent framework OMNE achieved first place on\nthe GAIA benchmark, demonstrating LTM's potential for AI self-evolution.\nFinally, we present a roadmap for future research, emphasizing the importance\nof LTM for advancing AI technology and its practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like GPTs, trained on vast datasets, have\ndemonstrated impressive capabilities in language understanding, reasoning, and\nplanning, achieving human-level performance in various tasks. Most studies\nfocus on enhancing these models by training on ever-larger datasets to build\nmore powerful foundation models. While training stronger models is important,\nenabling models to evolve during inference is equally crucial, a process we\nrefer to as AI self-evolution. Unlike large-scale training, self-evolution may\nrely on limited data or interactions. Inspired by the columnar organization of\nthe human cerebral cortex, we hypothesize that AI models could develop\ncognitive abilities and build internal representations through iterative\ninteractions with their environment. To achieve this, models need long-term\nmemory (LTM) to store and manage processed interaction data. LTM supports\nself-evolution by representing diverse experiences across environments and\nagents. In this report, we explore AI self-evolution and its potential to\nenhance models during inference. We examine LTM's role in lifelong learning,\nallowing models to evolve based on accumulated interactions. We outline the\nstructure of LTM and the systems needed for effective data retention and\nrepresentation. We also classify approaches for building personalized models\nwith LTM data and show how these models achieve self-evolution through\ninteraction. Using LTM, our multi-agent framework OMNE achieved first place on\nthe GAIA benchmark, demonstrating LTM's potential for AI self-evolution.\nFinally, we present a roadmap for future research, emphasizing the importance\nof LTM for advancing AI technology and its practical applications."
                },
                "authors": [
                    {
                        "name": "Xun Jiang"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Jiaying Wang"
                    },
                    {
                        "name": "Jun Shao"
                    },
                    {
                        "name": "Shihao Xu"
                    },
                    {
                        "name": "Shu Zhang"
                    },
                    {
                        "name": "Weiling Chen"
                    },
                    {
                        "name": "Xavier Tang"
                    },
                    {
                        "name": "Yize Chen"
                    },
                    {
                        "name": "Mengyue Wu"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Tianqiao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqiao Chen"
                },
                "author": "Tianqiao Chen",
                "arxiv_comment": "56 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15665v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15665v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13121v1",
                "updated": "2024-11-20T08:31:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    31,
                    43,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T08:31:43Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    31,
                    43,
                    2,
                    325,
                    0
                ],
                "title": "ReinFog: A DRL Empowered Framework for Resource Management in Edge and\n  Cloud Computing Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReinFog: A DRL Empowered Framework for Resource Management in Edge and\n  Cloud Computing Environments"
                },
                "summary": "The growing IoT landscape requires effective server deployment strategies to\nmeet demands including real-time processing and energy efficiency. This is\ncomplicated by heterogeneous, dynamic applications and servers. To address\nthese challenges, we propose ReinFog, a modular distributed software empowered\nwith Deep Reinforcement Learning (DRL) for adaptive resource management across\nedge/fog and cloud environments. ReinFog enables the practical\ndevelopment/deployment of various centralized and distributed DRL techniques\nfor resource management in edge/fog and cloud computing environments. It also\nsupports integrating native and library-based DRL techniques for diverse IoT\napplication scheduling objectives. Additionally, ReinFog allows for customizing\ndeployment configurations for different DRL techniques, including the number\nand placement of DRL Learners and DRL Workers in large-scale distributed\nsystems. Besides, we propose a novel Memetic Algorithm for DRL Component (e.g.,\nDRL Learners and DRL Workers) Placement in ReinFog named MADCP, which combines\nthe strengths of Genetic Algorithm, Firefly Algorithm, and Particle Swarm\nOptimization. Experiments reveal that the DRL mechanisms developed within\nReinFog have significantly enhanced both centralized and distributed DRL\ntechniques implementation. These advancements have resulted in notable\nimprovements in IoT application performance, reducing response time by 45%,\nenergy consumption by 39%, and weighted cost by 37%, while maintaining minimal\nscheduling overhead. Additionally, ReinFog exhibits remarkable scalability,\nwith a rise in DRL Workers from 1 to 30 causing only a 0.3-second increase in\nstartup time and around 2 MB more RAM per Worker. The proposed MADCP for DRL\ncomponent placement further accelerates the convergence rate of DRL techniques\nby up to 38%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing IoT landscape requires effective server deployment strategies to\nmeet demands including real-time processing and energy efficiency. This is\ncomplicated by heterogeneous, dynamic applications and servers. To address\nthese challenges, we propose ReinFog, a modular distributed software empowered\nwith Deep Reinforcement Learning (DRL) for adaptive resource management across\nedge/fog and cloud environments. ReinFog enables the practical\ndevelopment/deployment of various centralized and distributed DRL techniques\nfor resource management in edge/fog and cloud computing environments. It also\nsupports integrating native and library-based DRL techniques for diverse IoT\napplication scheduling objectives. Additionally, ReinFog allows for customizing\ndeployment configurations for different DRL techniques, including the number\nand placement of DRL Learners and DRL Workers in large-scale distributed\nsystems. Besides, we propose a novel Memetic Algorithm for DRL Component (e.g.,\nDRL Learners and DRL Workers) Placement in ReinFog named MADCP, which combines\nthe strengths of Genetic Algorithm, Firefly Algorithm, and Particle Swarm\nOptimization. Experiments reveal that the DRL mechanisms developed within\nReinFog have significantly enhanced both centralized and distributed DRL\ntechniques implementation. These advancements have resulted in notable\nimprovements in IoT application performance, reducing response time by 45%,\nenergy consumption by 39%, and weighted cost by 37%, while maintaining minimal\nscheduling overhead. Additionally, ReinFog exhibits remarkable scalability,\nwith a rise in DRL Workers from 1 to 30 causing only a 0.3-second increase in\nstartup time and around 2 MB more RAM per Worker. The proposed MADCP for DRL\ncomponent placement further accelerates the convergence rate of DRL techniques\nby up to 38%."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wang"
                    },
                    {
                        "name": "Mohammad Goudarzi"
                    },
                    {
                        "name": "Rajkumar Buyya"
                    }
                ],
                "author_detail": {
                    "name": "Rajkumar Buyya"
                },
                "author": "Rajkumar Buyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13118v1",
                "updated": "2024-11-20T08:26:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    26,
                    40,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T08:26:40Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    26,
                    40,
                    2,
                    325,
                    0
                ],
                "title": "Using ChatGPT-4 for the Identification of Common UX Factors within a\n  Pool of Measurement Items from Established UX Questionnaires",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using ChatGPT-4 for the Identification of Common UX Factors within a\n  Pool of Measurement Items from Established UX Questionnaires"
                },
                "summary": "Measuring User Experience (UX) with standardized questionnaires is a widely\nused method. A questionnaire is based on different scales that represent UX\nfactors and items. However, the questionnaires have no common ground concerning\nnaming different factors and the items used to measure them. This study aims to\nidentify general UX factors based on the formulation of the measurement items.\nItems from a set of 40 established UX questionnaires were analyzed by\nGenerative AI (GenAI) to identify semantically similar items and to cluster\nsimilar topics. We used the LLM ChatGPT-4 for this analysis. Results show that\nChatGPT-4 can classify items into meaningful topics and thus help to create a\ndeeper understanding of the structure of the UX research field. In addition, we\nshow that ChatGPT-4 can filter items related to a predefined UX concept out of\na pool of UX items.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring User Experience (UX) with standardized questionnaires is a widely\nused method. A questionnaire is based on different scales that represent UX\nfactors and items. However, the questionnaires have no common ground concerning\nnaming different factors and the items used to measure them. This study aims to\nidentify general UX factors based on the formulation of the measurement items.\nItems from a set of 40 established UX questionnaires were analyzed by\nGenerative AI (GenAI) to identify semantically similar items and to cluster\nsimilar topics. We used the LLM ChatGPT-4 for this analysis. Results show that\nChatGPT-4 can classify items into meaningful topics and thus help to create a\ndeeper understanding of the structure of the UX research field. In addition, we\nshow that ChatGPT-4 can filter items related to a predefined UX concept out of\na pool of UX items."
                },
                "authors": [
                    {
                        "name": "Stefan Graser"
                    },
                    {
                        "name": "Stephan BÃ¶hm"
                    },
                    {
                        "name": "Martin Schrepp"
                    }
                ],
                "author_detail": {
                    "name": "Martin Schrepp"
                },
                "author": "Martin Schrepp",
                "arxiv_comment": "10 pages, 1 figure, The Sixteenth International Conference on\n  Advances in Human-oriented and Personalized Mechanisms, Technologies, and\n  Services CENTRIC 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13117v1",
                "updated": "2024-11-20T08:21:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    21,
                    53,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T08:21:53Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    8,
                    21,
                    53,
                    2,
                    325,
                    0
                ],
                "title": "Compute Optimal Inference and Provable Amortisation Gap in Sparse\n  Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Optimal Inference and Provable Amortisation Gap in Sparse\n  Autoencoders"
                },
                "summary": "A recent line of work has shown promise in using sparse autoencoders (SAEs)\nto uncover interpretable features in neural network representations. However,\nthe simple linear-nonlinear encoding mechanism in SAEs limits their ability to\nperform accurate sparse inference. In this paper, we investigate sparse\ninference and learning in SAEs through the lens of sparse coding. Specifically,\nwe show that SAEs perform amortised sparse inference with a computationally\nrestricted encoder and, using compressed sensing theory, we prove that this\nmapping is inherently insufficient for accurate sparse inference, even in\nsolvable cases. Building on this theory, we empirically explore conditions\nwhere more sophisticated sparse inference methods outperform traditional SAE\nencoders. Our key contribution is the decoupling of the encoding and decoding\nprocesses, which allows for a comparison of various sparse encoding strategies.\nWe evaluate these strategies on two dimensions: alignment with true underlying\nsparse features and correct inference of sparse codes, while also accounting\nfor computational costs during training and inference. Our results reveal that\nsubstantial performance gains can be achieved with minimal increases in compute\ncost. We demonstrate that this generalises to SAEs applied to large language\nmodels (LLMs), where advanced encoders achieve similar interpretability. This\nwork opens new avenues for understanding neural network representations and\noffers important implications for improving the tools we use to analyse the\nactivations of large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent line of work has shown promise in using sparse autoencoders (SAEs)\nto uncover interpretable features in neural network representations. However,\nthe simple linear-nonlinear encoding mechanism in SAEs limits their ability to\nperform accurate sparse inference. In this paper, we investigate sparse\ninference and learning in SAEs through the lens of sparse coding. Specifically,\nwe show that SAEs perform amortised sparse inference with a computationally\nrestricted encoder and, using compressed sensing theory, we prove that this\nmapping is inherently insufficient for accurate sparse inference, even in\nsolvable cases. Building on this theory, we empirically explore conditions\nwhere more sophisticated sparse inference methods outperform traditional SAE\nencoders. Our key contribution is the decoupling of the encoding and decoding\nprocesses, which allows for a comparison of various sparse encoding strategies.\nWe evaluate these strategies on two dimensions: alignment with true underlying\nsparse features and correct inference of sparse codes, while also accounting\nfor computational costs during training and inference. Our results reveal that\nsubstantial performance gains can be achieved with minimal increases in compute\ncost. We demonstrate that this generalises to SAEs applied to large language\nmodels (LLMs), where advanced encoders achieve similar interpretability. This\nwork opens new avenues for understanding neural network representations and\noffers important implications for improving the tools we use to analyse the\nactivations of large language models."
                },
                "authors": [
                    {
                        "name": "Charles O'Neill"
                    },
                    {
                        "name": "David Klindt"
                    }
                ],
                "author_detail": {
                    "name": "David Klindt"
                },
                "author": "David Klindt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08903v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08903v2",
                "updated": "2024-11-20T07:42:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    7,
                    42,
                    38,
                    2,
                    325,
                    0
                ],
                "published": "2024-06-13T07:57:27Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    7,
                    57,
                    27,
                    3,
                    165,
                    0
                ],
                "title": "Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delta-CoMe: Training-Free Delta-Compression with Mixed-Precision for\n  Large Language Models"
                },
                "summary": "Fine-tuning is a crucial process for adapting large language models (LLMs) to\ndiverse applications. In certain scenarios, such as multi-tenant serving,\ndeploying multiple LLMs becomes necessary to meet complex demands. Recent\nstudies suggest decomposing a fine-tuned LLM into a base model and\ncorresponding delta weights, which are then compressed using low-rank or\nlow-bit approaches to reduce costs. In this work, we observe that existing\nlow-rank and low-bit compression methods can significantly harm the model\nperformance for task-specific fine-tuned LLMs (e.g., WizardMath for math\nproblems). Motivated by the long-tail distribution of singular values in the\ndelta weights, we propose a delta quantization approach using mixed-precision.\nThis method employs higher-bit representation for singular vectors\ncorresponding to larger singular values. We evaluate our approach on various\nfine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs.\nExperimental results demonstrate that our approach performs comparably to full\nfine-tuned LLMs, surpassing both low-rank and low-bit baselines by a\nconsiderable margin. Additionally, we show that our method is compatible with\nvarious backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning is a crucial process for adapting large language models (LLMs) to\ndiverse applications. In certain scenarios, such as multi-tenant serving,\ndeploying multiple LLMs becomes necessary to meet complex demands. Recent\nstudies suggest decomposing a fine-tuned LLM into a base model and\ncorresponding delta weights, which are then compressed using low-rank or\nlow-bit approaches to reduce costs. In this work, we observe that existing\nlow-rank and low-bit compression methods can significantly harm the model\nperformance for task-specific fine-tuned LLMs (e.g., WizardMath for math\nproblems). Motivated by the long-tail distribution of singular values in the\ndelta weights, we propose a delta quantization approach using mixed-precision.\nThis method employs higher-bit representation for singular vectors\ncorresponding to larger singular values. We evaluate our approach on various\nfine-tuned LLMs, including math LLMs, code LLMs, chat LLMs, and even VLMs.\nExperimental results demonstrate that our approach performs comparably to full\nfine-tuned LLMs, surpassing both low-rank and low-bit baselines by a\nconsiderable margin. Additionally, we show that our method is compatible with\nvarious backbone LLMs, such as Llama-2, Llama-3, and Mistral, highlighting its\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Bowen Ping"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Hanqing Wang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08903v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08903v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13081v1",
                "updated": "2024-11-20T07:17:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    7,
                    17,
                    16,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T07:17:16Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    7,
                    17,
                    16,
                    2,
                    325,
                    0
                ],
                "title": "Practical Compact Deep Compressed Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Compact Deep Compressed Sensing"
                },
                "summary": "Recent years have witnessed the success of deep networks in compressed\nsensing (CS), which allows for a significant reduction in sampling cost and has\ngained growing attention since its inception. In this paper, we propose a new\npractical and compact network dubbed PCNet for general image CS. Specifically,\nin PCNet, a novel collaborative sampling operator is designed, which consists\nof a deep conditional filtering step and a dual-branch fast sampling step. The\nformer learns an implicit representation of a linear transformation matrix into\na few convolutions and first performs adaptive local filtering on the input\nimage, while the latter then uses a discrete cosine transform and a scrambled\nblock-diagonal Gaussian matrix to generate under-sampled measurements. Our\nPCNet is equipped with an enhanced proximal gradient descent algorithm-unrolled\nnetwork for reconstruction. It offers flexibility, interpretability, and strong\nrecovery performance for arbitrary sampling rates once trained. Additionally,\nwe provide a deployment-oriented extraction scheme for single-pixel CS imaging\nsystems, which allows for the convenient conversion of any linear sampling\noperator to its matrix form to be loaded onto hardware like digital\nmicro-mirror devices. Extensive experiments on natural image CS, quantized CS,\nand self-supervised CS demonstrate the superior reconstruction accuracy and\ngeneralization ability of PCNet compared to existing state-of-the-art methods,\nparticularly for high-resolution images. Code is available at\nhttps://github.com/Guaishou74851/PCNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the success of deep networks in compressed\nsensing (CS), which allows for a significant reduction in sampling cost and has\ngained growing attention since its inception. In this paper, we propose a new\npractical and compact network dubbed PCNet for general image CS. Specifically,\nin PCNet, a novel collaborative sampling operator is designed, which consists\nof a deep conditional filtering step and a dual-branch fast sampling step. The\nformer learns an implicit representation of a linear transformation matrix into\na few convolutions and first performs adaptive local filtering on the input\nimage, while the latter then uses a discrete cosine transform and a scrambled\nblock-diagonal Gaussian matrix to generate under-sampled measurements. Our\nPCNet is equipped with an enhanced proximal gradient descent algorithm-unrolled\nnetwork for reconstruction. It offers flexibility, interpretability, and strong\nrecovery performance for arbitrary sampling rates once trained. Additionally,\nwe provide a deployment-oriented extraction scheme for single-pixel CS imaging\nsystems, which allows for the convenient conversion of any linear sampling\noperator to its matrix form to be loaded onto hardware like digital\nmicro-mirror devices. Extensive experiments on natural image CS, quantized CS,\nand self-supervised CS demonstrate the superior reconstruction accuracy and\ngeneralization ability of PCNet compared to existing state-of-the-art methods,\nparticularly for high-resolution images. Code is available at\nhttps://github.com/Guaishou74851/PCNet."
                },
                "authors": [
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "arxiv_comment": "Accepted by IEEE T-PAMI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13079v1",
                "updated": "2024-11-20T07:07:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    7,
                    7,
                    42,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T07:07:42Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    7,
                    7,
                    42,
                    2,
                    325,
                    0
                ],
                "title": "Neural Internal Model Control: Learning a Robust Control Policy via\n  Predictive Error Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Internal Model Control: Learning a Robust Control Policy via\n  Predictive Error Feedback"
                },
                "summary": "Accurate motion control in the face of disturbances within complex\nenvironments remains a major challenge in robotics. Classical model-based\napproaches often struggle with nonlinearities and unstructured disturbances,\nwhile RL-based methods can be fragile when encountering unseen scenarios. In\nthis paper, we propose a novel framework, Neural Internal Model Control, which\nintegrates model-based control with RL-based control to enhance robustness. Our\nframework streamlines the predictive model by applying Newton-Euler equations\nfor rigid-body dynamics, eliminating the need to capture complex\nhigh-dimensional nonlinearities. This internal model combines model-free RL\nalgorithms with predictive error feedback. Such a design enables a closed-loop\ncontrol structure to enhance the robustness and generalizability of the control\nsystem. We demonstrate the effectiveness of our framework on both quadrotors\nand quadrupedal robots, achieving superior performance compared to\nstate-of-the-art methods. Furthermore, real-world deployment on a quadrotor\nwith rope-suspended payloads highlights the framework's robustness in\nsim-to-real transfer. Our code is released at\nhttps://github.com/thu-uav/NeuralIMC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate motion control in the face of disturbances within complex\nenvironments remains a major challenge in robotics. Classical model-based\napproaches often struggle with nonlinearities and unstructured disturbances,\nwhile RL-based methods can be fragile when encountering unseen scenarios. In\nthis paper, we propose a novel framework, Neural Internal Model Control, which\nintegrates model-based control with RL-based control to enhance robustness. Our\nframework streamlines the predictive model by applying Newton-Euler equations\nfor rigid-body dynamics, eliminating the need to capture complex\nhigh-dimensional nonlinearities. This internal model combines model-free RL\nalgorithms with predictive error feedback. Such a design enables a closed-loop\ncontrol structure to enhance the robustness and generalizability of the control\nsystem. We demonstrate the effectiveness of our framework on both quadrotors\nand quadrupedal robots, achieving superior performance compared to\nstate-of-the-art methods. Furthermore, real-world deployment on a quadrotor\nwith rope-suspended payloads highlights the framework's robustness in\nsim-to-real transfer. Our code is released at\nhttps://github.com/thu-uav/NeuralIMC."
                },
                "authors": [
                    {
                        "name": "Feng Gao"
                    },
                    {
                        "name": "Chao Yu"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Yi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wu"
                },
                "author": "Yi Wu",
                "arxiv_comment": "Submitted to RAL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.03022v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.03022v3",
                "updated": "2024-11-20T07:07:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    7,
                    7,
                    41,
                    2,
                    325,
                    0
                ],
                "published": "2023-12-05T07:27:08Z",
                "published_parsed": [
                    2023,
                    12,
                    5,
                    7,
                    27,
                    8,
                    1,
                    339,
                    0
                ],
                "title": "Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph\n  Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph\n  Construction"
                },
                "summary": "This paper introduces CooperKGC, a novel framework challenging the\nconventional solitary approach of large language models (LLMs) in knowledge\ngraph construction (KGC). CooperKGC establishes a collaborative processing\nnetwork, assembling a team capable of concurrently addressing entity, relation,\nand event extraction tasks. Experimentation demonstrates that fostering\ncollaboration within CooperKGC enhances knowledge selection, correction, and\naggregation capabilities across multiple rounds of interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces CooperKGC, a novel framework challenging the\nconventional solitary approach of large language models (LLMs) in knowledge\ngraph construction (KGC). CooperKGC establishes a collaborative processing\nnetwork, assembling a team capable of concurrently addressing entity, relation,\nand event extraction tasks. Experimentation demonstrates that fostering\ncollaboration within CooperKGC enhances knowledge selection, correction, and\naggregation capabilities across multiple rounds of interactions."
                },
                "authors": [
                    {
                        "name": "Hongbin Ye"
                    },
                    {
                        "name": "Honghao Gui"
                    },
                    {
                        "name": "Aijia Zhang"
                    },
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Weiqiang Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weiqiang Jia"
                },
                "author": "Weiqiang Jia",
                "arxiv_comment": "Accepted by CCKS 2024, best english candidate paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.03022v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.03022v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13076v1",
                "updated": "2024-11-20T06:58:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    6,
                    58,
                    33,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T06:58:33Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    6,
                    58,
                    33,
                    2,
                    325,
                    0
                ],
                "title": "Hints of Prompt: Enhancing Visual Representation for Multimodal LLMs in\n  Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hints of Prompt: Enhancing Visual Representation for Multimodal LLMs in\n  Autonomous Driving"
                },
                "summary": "In light of the dynamic nature of autonomous driving environments and\nstringent safety requirements, general MLLMs combined with CLIP alone often\nstruggle to represent driving-specific scenarios accurately, particularly in\ncomplex interactions and long-tail cases. To address this, we propose the Hints\nof Prompt (HoP) framework, which introduces three key enhancements: Affinity\nhint to emphasize instance-level structure by strengthening token-wise\nconnections, Semantic hint to incorporate high-level information relevant to\ndriving-specific cases, such as complex interactions among vehicles and traffic\nsigns, and Question hint to align visual features with the query context,\nfocusing on question-relevant regions. These hints are fused through a Hint\nFusion module, enriching visual representations and enhancing multimodal\nreasoning for autonomous driving VQA tasks. Extensive experiments confirm the\neffectiveness of the HoP framework, showing it significantly outperforms\nprevious state-of-the-art methods across all key metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In light of the dynamic nature of autonomous driving environments and\nstringent safety requirements, general MLLMs combined with CLIP alone often\nstruggle to represent driving-specific scenarios accurately, particularly in\ncomplex interactions and long-tail cases. To address this, we propose the Hints\nof Prompt (HoP) framework, which introduces three key enhancements: Affinity\nhint to emphasize instance-level structure by strengthening token-wise\nconnections, Semantic hint to incorporate high-level information relevant to\ndriving-specific cases, such as complex interactions among vehicles and traffic\nsigns, and Question hint to align visual features with the query context,\nfocusing on question-relevant regions. These hints are fused through a Hint\nFusion module, enriching visual representations and enhancing multimodal\nreasoning for autonomous driving VQA tasks. Extensive experiments confirm the\neffectiveness of the HoP framework, showing it significantly outperforms\nprevious state-of-the-art methods across all key metrics."
                },
                "authors": [
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhanning Gao"
                    },
                    {
                        "name": "Maosheng Ye"
                    },
                    {
                        "name": "Zhili Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Tongyi Cao"
                    },
                    {
                        "name": "Honggang Qi"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Qi"
                },
                "author": "Honggang Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13055v1",
                "updated": "2024-11-20T06:05:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    6,
                    5,
                    11,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T06:05:11Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    6,
                    5,
                    11,
                    2,
                    325,
                    0
                ],
                "title": "Hardware Scaling Trends and Diminishing Returns in Large-Scale\n  Distributed Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware Scaling Trends and Diminishing Returns in Large-Scale\n  Distributed Training"
                },
                "summary": "Dramatic increases in the capabilities of neural network models in recent\nyears are driven by scaling model size, training data, and corresponding\ncomputational resources. To develop the exceedingly large networks required in\nmodern applications, such as large language models (LLMs), model training is\ndistributed across tens of thousands of hardware accelerators (e.g. GPUs),\nrequiring orchestration of computation and communication across large computing\nclusters. In this work, we demonstrate that careful consideration of hardware\nconfiguration and parallelization strategy is critical for effective (i.e.\ncompute- and cost-efficient) scaling of model size, training data, and total\ncomputation. We conduct an extensive empirical study of the performance of\nlarge-scale LLM training workloads across model size, hardware configurations,\nand distributed parallelization strategies. We demonstrate that: (1) beyond\ncertain scales, overhead incurred from certain distributed communication\nstrategies leads parallelization strategies previously thought to be\nsub-optimal in fact become preferable; and (2) scaling the total number of\naccelerators for large model training quickly yields diminishing returns even\nwhen hardware and parallelization strategies are properly optimized, implying\npoor marginal performance per additional unit of power or GPU-hour.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dramatic increases in the capabilities of neural network models in recent\nyears are driven by scaling model size, training data, and corresponding\ncomputational resources. To develop the exceedingly large networks required in\nmodern applications, such as large language models (LLMs), model training is\ndistributed across tens of thousands of hardware accelerators (e.g. GPUs),\nrequiring orchestration of computation and communication across large computing\nclusters. In this work, we demonstrate that careful consideration of hardware\nconfiguration and parallelization strategy is critical for effective (i.e.\ncompute- and cost-efficient) scaling of model size, training data, and total\ncomputation. We conduct an extensive empirical study of the performance of\nlarge-scale LLM training workloads across model size, hardware configurations,\nand distributed parallelization strategies. We demonstrate that: (1) beyond\ncertain scales, overhead incurred from certain distributed communication\nstrategies leads parallelization strategies previously thought to be\nsub-optimal in fact become preferable; and (2) scaling the total number of\naccelerators for large model training quickly yields diminishing returns even\nwhen hardware and parallelization strategies are properly optimized, implying\npoor marginal performance per additional unit of power or GPU-hour."
                },
                "authors": [
                    {
                        "name": "Jared Fernandez"
                    },
                    {
                        "name": "Luca Wehrstedt"
                    },
                    {
                        "name": "Leonid Shamis"
                    },
                    {
                        "name": "Mostafa Elhoushi"
                    },
                    {
                        "name": "Kalyan Saladi"
                    },
                    {
                        "name": "Yonatan Bisk"
                    },
                    {
                        "name": "Emma Strubell"
                    },
                    {
                        "name": "Jacob Kahn"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Kahn"
                },
                "author": "Jacob Kahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.02926v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.02926v3",
                "updated": "2024-11-20T06:01:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    6,
                    1,
                    23,
                    2,
                    325,
                    0
                ],
                "published": "2023-09-06T11:39:37Z",
                "published_parsed": [
                    2023,
                    9,
                    6,
                    11,
                    39,
                    37,
                    2,
                    249,
                    0
                ],
                "title": "Demystifying RCE Vulnerabilities in LLM-Integrated Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying RCE Vulnerabilities in LLM-Integrated Apps"
                },
                "summary": "LLMs show promise in transforming software development, with a growing\ninterest in integrating them into more intelligent apps. Frameworks like\nLangChain aid LLM-integrated app development, offering code execution\nutility/APIs for custom actions. However, these capabilities theoretically\nintroduce Remote Code Execution (RCE) vulnerabilities, enabling remote code\nexecution through prompt injections. No prior research systematically\ninvestigates these frameworks' RCE vulnerabilities or their impact on\napplications and exploitation consequences. Therefore, there is a huge research\ngap in this field. In this study, we propose LLMSmith to detect, validate and\nexploit the RCE vulnerabilities in LLM-integrated frameworks and apps. To\nachieve this goal, we develop two novel techniques, including 1) a lightweight\nstatic analysis to examine LLM integration mechanisms, and construct call\nchains to identify RCE vulnerabilities in frameworks; 2) a systematical\nprompt-based exploitation method to verify and exploit the found\nvulnerabilities in LLM-integrated apps. This technique involves various\nstrategies to control LLM outputs, trigger RCE vulnerabilities and launch\nsubsequent attacks. Our research has uncovered a total of 20 vulnerabilities in\n11 LLM-integrated frameworks, comprising 19 RCE vulnerabilities and 1 arbitrary\nfile read/write vulnerability. Of these, 17 have been confirmed by the\nframework developers, with 11 vulnerabilities being assigned CVE IDs. For the\n51 apps potentially affected by RCE, we successfully executed attacks on 17\napps, 16 of which are vulnerable to RCE and 1 to SQL injection. Furthermore, we\nconduct a comprehensive analysis of these vulnerabilities and construct\npractical attacks to demonstrate the hazards in reality. Last, we propose\nseveral mitigation measures for both framework and app developers to counteract\nsuch attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs show promise in transforming software development, with a growing\ninterest in integrating them into more intelligent apps. Frameworks like\nLangChain aid LLM-integrated app development, offering code execution\nutility/APIs for custom actions. However, these capabilities theoretically\nintroduce Remote Code Execution (RCE) vulnerabilities, enabling remote code\nexecution through prompt injections. No prior research systematically\ninvestigates these frameworks' RCE vulnerabilities or their impact on\napplications and exploitation consequences. Therefore, there is a huge research\ngap in this field. In this study, we propose LLMSmith to detect, validate and\nexploit the RCE vulnerabilities in LLM-integrated frameworks and apps. To\nachieve this goal, we develop two novel techniques, including 1) a lightweight\nstatic analysis to examine LLM integration mechanisms, and construct call\nchains to identify RCE vulnerabilities in frameworks; 2) a systematical\nprompt-based exploitation method to verify and exploit the found\nvulnerabilities in LLM-integrated apps. This technique involves various\nstrategies to control LLM outputs, trigger RCE vulnerabilities and launch\nsubsequent attacks. Our research has uncovered a total of 20 vulnerabilities in\n11 LLM-integrated frameworks, comprising 19 RCE vulnerabilities and 1 arbitrary\nfile read/write vulnerability. Of these, 17 have been confirmed by the\nframework developers, with 11 vulnerabilities being assigned CVE IDs. For the\n51 apps potentially affected by RCE, we successfully executed attacks on 17\napps, 16 of which are vulnerable to RCE and 1 to SQL injection. Furthermore, we\nconduct a comprehensive analysis of these vulnerabilities and construct\npractical attacks to demonstrate the hazards in reality. Last, we propose\nseveral mitigation measures for both framework and app developers to counteract\nsuch attacks."
                },
                "authors": [
                    {
                        "name": "Tong Liu"
                    },
                    {
                        "name": "Zizhuang Deng"
                    },
                    {
                        "name": "Guozhu Meng"
                    },
                    {
                        "name": "Yuekang Li"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.02926v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.02926v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13052v1",
                "updated": "2024-11-20T05:56:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    5,
                    56,
                    31,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T05:56:31Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    5,
                    56,
                    31,
                    2,
                    325,
                    0
                ],
                "title": "On-device Content-based Recommendation with Single-shot Embedding\n  Pruning: A Cooperative Game Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device Content-based Recommendation with Single-shot Embedding\n  Pruning: A Cooperative Game Perspective"
                },
                "summary": "Content-based Recommender Systems (CRSs) play a crucial role in shaping user\nexperiences in e-commerce, online advertising, and personalized\nrecommendations. However, due to the vast amount of categorical features, the\nembedding tables used in CRS models pose a significant storage bottleneck for\nreal-world deployment, especially on resource-constrained devices. To address\nthis problem, various embedding pruning methods have been proposed, but most\nexisting ones require expensive retraining steps for each target parameter\nbudget, leading to enormous computation costs. In reality, this computation\ncost is a major hurdle in real-world applications with diverse storage\nrequirements, such as federated learning and streaming settings. In this paper,\nwe propose Shapley Value-guided Embedding Reduction (Shaver) as our response.\nWith Shaver, we view the problem from a cooperative game perspective, and\nquantify each embedding parameter's contribution with Shapley values to\nfacilitate contribution-based parameter pruning. To address the inherently high\ncomputation costs of Shapley values, we propose an efficient and unbiased\nmethod to estimate Shapley values of a CRS's embedding parameters. Moreover, in\nthe pruning stage, we put forward a field-aware codebook to mitigate the\ninformation loss in the traditional zero-out treatment. Through extensive\nexperiments on three real-world datasets, Shaver has demonstrated competitive\nperformance with lightweight recommendation models across various parameter\nbudgets. The source code is available at\nhttps://anonymous.4open.science/r/shaver-E808",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content-based Recommender Systems (CRSs) play a crucial role in shaping user\nexperiences in e-commerce, online advertising, and personalized\nrecommendations. However, due to the vast amount of categorical features, the\nembedding tables used in CRS models pose a significant storage bottleneck for\nreal-world deployment, especially on resource-constrained devices. To address\nthis problem, various embedding pruning methods have been proposed, but most\nexisting ones require expensive retraining steps for each target parameter\nbudget, leading to enormous computation costs. In reality, this computation\ncost is a major hurdle in real-world applications with diverse storage\nrequirements, such as federated learning and streaming settings. In this paper,\nwe propose Shapley Value-guided Embedding Reduction (Shaver) as our response.\nWith Shaver, we view the problem from a cooperative game perspective, and\nquantify each embedding parameter's contribution with Shapley values to\nfacilitate contribution-based parameter pruning. To address the inherently high\ncomputation costs of Shapley values, we propose an efficient and unbiased\nmethod to estimate Shapley values of a CRS's embedding parameters. Moreover, in\nthe pruning stage, we put forward a field-aware codebook to mitigate the\ninformation loss in the traditional zero-out treatment. Through extensive\nexperiments on three real-world datasets, Shaver has demonstrated competitive\nperformance with lightweight recommendation models across various parameter\nbudgets. The source code is available at\nhttps://anonymous.4open.science/r/shaver-E808"
                },
                "authors": [
                    {
                        "name": "Hung Vinh Tran"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Guanhua Ye"
                    },
                    {
                        "name": "Quoc Viet Hung Nguyen"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13045v1",
                "updated": "2024-11-20T05:30:15Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    5,
                    30,
                    15,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T05:30:15Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    5,
                    30,
                    15,
                    2,
                    325,
                    0
                ],
                "title": "Explainable LLM-driven Multi-dimensional Distillation for E-Commerce\n  Relevance Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainable LLM-driven Multi-dimensional Distillation for E-Commerce\n  Relevance Learning"
                },
                "summary": "Effective query-item relevance modeling is pivotal for enhancing user\nexperience and safeguarding user satisfaction in e-commerce search systems.\nRecently, benefiting from the vast inherent knowledge, Large Language Model\n(LLM) approach demonstrates strong performance and long-tail generalization\nability compared with previous neural-based specialized relevance learning\nmethods. Though promising, current LLM-based methods encounter the following\ninadequacies in practice: First, the massive parameters and computational\ndemands make it difficult to be deployed online. Second, distilling LLM models\nto online models is a feasible direction, but the LLM relevance modeling is a\nblack box, and its rich intrinsic knowledge is difficult to extract and apply\nonline. To improve the interpretability of LLM and boost the performance of\nonline relevance models via LLM, we propose an Explainable LLM-driven\nMulti-dimensional Distillation framework for e-commerce relevance learning,\nwhich comprises two core components: (1) An Explainable LLM for relevance\nmodeling (ELLM-rele), which decomposes the relevance learning into intermediate\nsteps and models relevance learning as a Chain-of-Thought (CoT) reasoning,\nthereby enhancing both interpretability and performance of LLM. (2) A\nMulti-dimensional Knowledge Distillation (MKD) architecture that transfers the\nknowledge of ELLM-rele to current deployable interaction-based and\nrepresentation-based student models from both the relevance score distribution\nand CoT reasoning aspects. Through distilling the probabilistic and CoT\nreasoning knowledge, MKD improves both the semantic interaction and long-tail\ngeneralization abilities of student models. Extensive offline evaluations and\nonline experiments on Taobao search ad scene demonstrate that our proposed\nframework significantly enhances e-commerce relevance learning performance and\nuser experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective query-item relevance modeling is pivotal for enhancing user\nexperience and safeguarding user satisfaction in e-commerce search systems.\nRecently, benefiting from the vast inherent knowledge, Large Language Model\n(LLM) approach demonstrates strong performance and long-tail generalization\nability compared with previous neural-based specialized relevance learning\nmethods. Though promising, current LLM-based methods encounter the following\ninadequacies in practice: First, the massive parameters and computational\ndemands make it difficult to be deployed online. Second, distilling LLM models\nto online models is a feasible direction, but the LLM relevance modeling is a\nblack box, and its rich intrinsic knowledge is difficult to extract and apply\nonline. To improve the interpretability of LLM and boost the performance of\nonline relevance models via LLM, we propose an Explainable LLM-driven\nMulti-dimensional Distillation framework for e-commerce relevance learning,\nwhich comprises two core components: (1) An Explainable LLM for relevance\nmodeling (ELLM-rele), which decomposes the relevance learning into intermediate\nsteps and models relevance learning as a Chain-of-Thought (CoT) reasoning,\nthereby enhancing both interpretability and performance of LLM. (2) A\nMulti-dimensional Knowledge Distillation (MKD) architecture that transfers the\nknowledge of ELLM-rele to current deployable interaction-based and\nrepresentation-based student models from both the relevance score distribution\nand CoT reasoning aspects. Through distilling the probabilistic and CoT\nreasoning knowledge, MKD improves both the semantic interaction and long-tail\ngeneralization abilities of student models. Extensive offline evaluations and\nonline experiments on Taobao search ad scene demonstrate that our proposed\nframework significantly enhances e-commerce relevance learning performance and\nuser experience."
                },
                "authors": [
                    {
                        "name": "Gang Zhao"
                    },
                    {
                        "name": "Ximing Zhang"
                    },
                    {
                        "name": "Chenji Lu"
                    },
                    {
                        "name": "Hui Zhao"
                    },
                    {
                        "name": "Tianshu Wu"
                    },
                    {
                        "name": "Pengjie Wang"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "Submitted to WWW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12279v2",
                "updated": "2024-11-20T05:05:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    5,
                    5,
                    48,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-19T06:57:45Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    6,
                    57,
                    45,
                    1,
                    324,
                    0
                ],
                "title": "HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HouseLLM: LLM-Assisted Two-Phase Text-to-Floorplan Generation"
                },
                "summary": "This paper proposes a two-phase text-to-floorplan generation method, which\nguides a Large Language Model (LLM) to generate an initial layout (Layout-LLM)\nand refines them into the final floorplans through conditional diffusion model.\nWe incorporate a Chain-of-Thought approach to prompt the LLM based on user text\nspecifications, enabling a more user-friendly and intuitive house layout\ndesign. This method allows users to describe their needs in natural language,\nenhancing accessibility and providing clearer geometric constraints. The final\nfloorplans generated by Layout-LLM through conditional diffusion refinement are\nmore accurate and better meet user requirements. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, validating its effectiveness in practical home design applications. We\nplan to release our code for public use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a two-phase text-to-floorplan generation method, which\nguides a Large Language Model (LLM) to generate an initial layout (Layout-LLM)\nand refines them into the final floorplans through conditional diffusion model.\nWe incorporate a Chain-of-Thought approach to prompt the LLM based on user text\nspecifications, enabling a more user-friendly and intuitive house layout\ndesign. This method allows users to describe their needs in natural language,\nenhancing accessibility and providing clearer geometric constraints. The final\nfloorplans generated by Layout-LLM through conditional diffusion refinement are\nmore accurate and better meet user requirements. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance across all\nmetrics, validating its effectiveness in practical home design applications. We\nplan to release our code for public use."
                },
                "authors": [
                    {
                        "name": "Ziyang Zong"
                    },
                    {
                        "name": "Zhaohuan Zhan"
                    },
                    {
                        "name": "Guang Tan"
                    }
                ],
                "author_detail": {
                    "name": "Guang Tan"
                },
                "author": "Guang Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10445v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10445v3",
                "updated": "2024-11-20T04:51:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    4,
                    51,
                    59,
                    2,
                    325,
                    0
                ],
                "published": "2024-04-16T10:31:06Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    10,
                    31,
                    6,
                    1,
                    107,
                    0
                ],
                "title": "SparseDM: Toward Sparse Efficient Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseDM: Toward Sparse Efficient Diffusion Models"
                },
                "summary": "Diffusion models have been extensively used in data generation tasks and are\nrecognized as one of the best generative models. However, their time-consuming\ndeployment, long inference time, and requirements on large memory limit their\napplication on mobile devices. In this paper, we propose a method based on the\nimproved Straight-Through Estimator to improve the deployment efficiency of\ndiffusion models. Specifically, we add sparse masks to the Convolution and\nLinear layers in a pre-trained diffusion model, then use design progressive\nsparsity for model training in the fine-tuning stage, and switch the inference\nmask on and off, which supports a flexible choice of sparsity during inference\naccording to the FID and MACs requirements. Experiments on four datasets\nconducted on a state-of-the-art Transformer-based diffusion model demonstrate\nthat our method reduces MACs by $50\\%$ while increasing FID by only 1.5 on\naverage. Under other MACs conditions, the FID is also lower than 1$\\sim$137\ncompared to other methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have been extensively used in data generation tasks and are\nrecognized as one of the best generative models. However, their time-consuming\ndeployment, long inference time, and requirements on large memory limit their\napplication on mobile devices. In this paper, we propose a method based on the\nimproved Straight-Through Estimator to improve the deployment efficiency of\ndiffusion models. Specifically, we add sparse masks to the Convolution and\nLinear layers in a pre-trained diffusion model, then use design progressive\nsparsity for model training in the fine-tuning stage, and switch the inference\nmask on and off, which supports a flexible choice of sparsity during inference\naccording to the FID and MACs requirements. Experiments on four datasets\nconducted on a state-of-the-art Transformer-based diffusion model demonstrate\nthat our method reduces MACs by $50\\%$ while increasing FID by only 1.5 on\naverage. Under other MACs conditions, the FID is also lower than 1$\\sim$137\ncompared to other methods."
                },
                "authors": [
                    {
                        "name": "Kafeng Wang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "He Li"
                    },
                    {
                        "name": "Zhenpeng Mi"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10445v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10445v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13032v1",
                "updated": "2024-11-20T04:42:32Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    4,
                    42,
                    32,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T04:42:32Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    4,
                    42,
                    32,
                    2,
                    325,
                    0
                ],
                "title": "\"It was 80% me, 20% AI\": Seeking Authenticity in Co-Writing with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"It was 80% me, 20% AI\": Seeking Authenticity in Co-Writing with Large\n  Language Models"
                },
                "summary": "Given the rising proliferation and diversity of AI writing assistance tools,\nespecially those powered by large language models (LLMs), both writers and\nreaders may have concerns about the impact of these tools on the authenticity\nof writing work. We examine whether and how writers want to preserve their\nauthentic voice when co-writing with AI tools and whether personalization of AI\nwriting support could help achieve this goal. We conducted semi-structured\ninterviews with 19 professional writers, during which they co-wrote with both\npersonalized and non-personalized AI writing-support tools. We supplemented\nwriters' perspectives with opinions from 30 avid readers about the written work\nco-produced with AI collected through an online survey. Our findings illuminate\nconceptions of authenticity in human-AI co-creation, which focus more on the\nprocess and experience of constructing creators' authentic selves. While\nwriters reacted positively to personalized AI writing tools, they believed the\nform of personalization needs to target writers' growth and go beyond the phase\nof text production. Overall, readers' responses showed less concern about\nhuman-AI co-writing. Readers could not distinguish AI-assisted work,\npersonalized or not, from writers' solo-written work and showed positive\nattitudes toward writers experimenting with new technology for creative\nwriting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the rising proliferation and diversity of AI writing assistance tools,\nespecially those powered by large language models (LLMs), both writers and\nreaders may have concerns about the impact of these tools on the authenticity\nof writing work. We examine whether and how writers want to preserve their\nauthentic voice when co-writing with AI tools and whether personalization of AI\nwriting support could help achieve this goal. We conducted semi-structured\ninterviews with 19 professional writers, during which they co-wrote with both\npersonalized and non-personalized AI writing-support tools. We supplemented\nwriters' perspectives with opinions from 30 avid readers about the written work\nco-produced with AI collected through an online survey. Our findings illuminate\nconceptions of authenticity in human-AI co-creation, which focus more on the\nprocess and experience of constructing creators' authentic selves. While\nwriters reacted positively to personalized AI writing tools, they believed the\nform of personalization needs to target writers' growth and go beyond the phase\nof text production. Overall, readers' responses showed less concern about\nhuman-AI co-writing. Readers could not distinguish AI-assisted work,\npersonalized or not, from writers' solo-written work and showed positive\nattitudes toward writers experimenting with new technology for creative\nwriting."
                },
                "authors": [
                    {
                        "name": "Angel Hsing-Chi Hwang"
                    },
                    {
                        "name": "Q. Vera Liao"
                    },
                    {
                        "name": "Su Lin Blodgett"
                    },
                    {
                        "name": "Alexandra Olteanu"
                    },
                    {
                        "name": "Adam Trischler"
                    }
                ],
                "author_detail": {
                    "name": "Adam Trischler"
                },
                "author": "Adam Trischler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08492v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08492v3",
                "updated": "2024-11-20T04:00:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    4,
                    0,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-03-13T12:55:43Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    12,
                    55,
                    43,
                    2,
                    73,
                    0
                ],
                "title": "Rich Semantic Knowledge Enhanced Large Language Models for Few-shot\n  Chinese Spell Checking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich Semantic Knowledge Enhanced Large Language Models for Few-shot\n  Chinese Spell Checking"
                },
                "summary": "Chinese Spell Checking (CSC) is a widely used technology, which plays a vital\nrole in speech to text (STT) and optical character recognition (OCR). Most of\nthe existing CSC approaches relying on BERT architecture achieve excellent\nperformance. However, limited by the scale of the foundation model, BERT-based\nmethod does not work well in few-shot scenarios, showing certain limitations in\npractical applications. In this paper, we explore using an in-context learning\nmethod named RS-LLM (Rich Semantic based LLMs) to introduce large language\nmodels (LLMs) as the foundation model. Besides, we study the impact of\nintroducing various Chinese rich semantic information in our framework. We\nfound that by introducing a small number of specific Chinese rich semantic\nstructures, LLMs achieve better performance than the BERT-based model on\nfew-shot CSC task. Furthermore, we conduct experiments on multiple datasets,\nand the experimental results verified the superiority of our proposed\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chinese Spell Checking (CSC) is a widely used technology, which plays a vital\nrole in speech to text (STT) and optical character recognition (OCR). Most of\nthe existing CSC approaches relying on BERT architecture achieve excellent\nperformance. However, limited by the scale of the foundation model, BERT-based\nmethod does not work well in few-shot scenarios, showing certain limitations in\npractical applications. In this paper, we explore using an in-context learning\nmethod named RS-LLM (Rich Semantic based LLMs) to introduce large language\nmodels (LLMs) as the foundation model. Besides, we study the impact of\nintroducing various Chinese rich semantic information in our framework. We\nfound that by introducing a small number of specific Chinese rich semantic\nstructures, LLMs achieve better performance than the BERT-based model on\nfew-shot CSC task. Furthermore, we conduct experiments on multiple datasets,\nand the experimental results verified the superiority of our proposed\nframework."
                },
                "authors": [
                    {
                        "name": "Ming Dong"
                    },
                    {
                        "name": "Yujing Chen"
                    },
                    {
                        "name": "Miao Zhang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Tingting He"
                    }
                ],
                "author_detail": {
                    "name": "Tingting He"
                },
                "author": "Tingting He",
                "arxiv_comment": "This paper is accepted by Findings of the Association for\n  Computational Linguistics: ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08492v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08492v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13022v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13022v1",
                "updated": "2024-11-20T03:53:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    53,
                    41,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T03:53:41Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    53,
                    41,
                    2,
                    325,
                    0
                ],
                "title": "Training Physics-Driven Deep Learning Reconstruction without Raw Data\n  Access for Equitable Fast MRI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Physics-Driven Deep Learning Reconstruction without Raw Data\n  Access for Equitable Fast MRI"
                },
                "summary": "Physics-driven deep learning (PD-DL) approaches have become popular for\nimproved reconstruction of fast magnetic resonance imaging (MRI) scans. Even\nthough PD-DL offers higher acceleration rates compared to existing clinical\nfast MRI techniques, their use has been limited outside specialized MRI\ncenters. One impediment for their deployment is the difficulties with\ngeneralization to pathologies or population groups that are not\nwell-represented in training sets. This has been noted in several studies, and\nfine-tuning on target populations to improve reconstruction has been suggested.\nHowever, current approaches for PD-DL training require access to raw k-space\nmeasurements, which is typically only available at specialized MRI centers that\nhave research agreements for such data access. This is especially an issue for\nrural and underserved areas, where commercial MRI scanners only provide access\nto a final reconstructed image. To tackle these challenges, we propose\nCompressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity\n(CUPID) for high-quality PD-DL training, using only routine clinical\nreconstructed images exported from an MRI scanner. CUPID evaluates the goodness\nof the output with a compressibility-based approach, while ensuring that the\noutput stays consistent with the clinical parallel imaging reconstruction\nthrough well-designed perturbations. Our results show that CUPID achieves\nsimilar quality compared to well-established PD-DL training strategies that\nrequire raw k-space data access, while outperforming conventional compressed\nsensing (CS) and state-of-the-art generative methods. We also demonstrate its\neffectiveness in a zero-shot training setup for retrospectively and\nprospectively sub-sampled acquisitions, attesting to its minimal training\nburden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-driven deep learning (PD-DL) approaches have become popular for\nimproved reconstruction of fast magnetic resonance imaging (MRI) scans. Even\nthough PD-DL offers higher acceleration rates compared to existing clinical\nfast MRI techniques, their use has been limited outside specialized MRI\ncenters. One impediment for their deployment is the difficulties with\ngeneralization to pathologies or population groups that are not\nwell-represented in training sets. This has been noted in several studies, and\nfine-tuning on target populations to improve reconstruction has been suggested.\nHowever, current approaches for PD-DL training require access to raw k-space\nmeasurements, which is typically only available at specialized MRI centers that\nhave research agreements for such data access. This is especially an issue for\nrural and underserved areas, where commercial MRI scanners only provide access\nto a final reconstructed image. To tackle these challenges, we propose\nCompressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity\n(CUPID) for high-quality PD-DL training, using only routine clinical\nreconstructed images exported from an MRI scanner. CUPID evaluates the goodness\nof the output with a compressibility-based approach, while ensuring that the\noutput stays consistent with the clinical parallel imaging reconstruction\nthrough well-designed perturbations. Our results show that CUPID achieves\nsimilar quality compared to well-established PD-DL training strategies that\nrequire raw k-space data access, while outperforming conventional compressed\nsensing (CS) and state-of-the-art generative methods. We also demonstrate its\neffectiveness in a zero-shot training setup for retrospectively and\nprospectively sub-sampled acquisitions, attesting to its minimal training\nburden."
                },
                "authors": [
                    {
                        "name": "YaÅar Utku AlÃ§alar"
                    },
                    {
                        "name": "Merve GÃ¼lle"
                    },
                    {
                        "name": "Mehmet AkÃ§akaya"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet AkÃ§akaya"
                },
                "author": "Mehmet AkÃ§akaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13022v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13022v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12319v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12319v2",
                "updated": "2024-11-20T03:31:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    31,
                    17,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-19T08:23:52Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    8,
                    23,
                    52,
                    1,
                    324,
                    0
                ],
                "title": "CLIP Unreasonable Potential in Single-Shot Face Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP Unreasonable Potential in Single-Shot Face Recognition"
                },
                "summary": "Face recognition is a core task in computer vision designed to identify and\nauthenticate individuals by analyzing facial patterns and features. This field\nintersects with artificial intelligence image processing and machine learning\nwith applications in security authentication and personalization. Traditional\napproaches in facial recognition focus on capturing facial features like the\neyes, nose and mouth and matching these against a database to verify\nidentities. However challenges such as high false positive rates have persisted\noften due to the similarity among individuals facial features. Recently\nContrastive Language Image Pretraining (CLIP) a model developed by OpenAI has\nshown promising advancements by linking natural language processing with vision\ntasks allowing it to generalize across modalities. Using CLIP's vision language\ncorrespondence and single-shot finetuning the model can achieve lower false\npositive rates upon deployment without the need of mass facial features\nextraction. This integration demonstrating CLIP's potential to address\npersistent issues in face recognition model performance without complicating\nour training paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face recognition is a core task in computer vision designed to identify and\nauthenticate individuals by analyzing facial patterns and features. This field\nintersects with artificial intelligence image processing and machine learning\nwith applications in security authentication and personalization. Traditional\napproaches in facial recognition focus on capturing facial features like the\neyes, nose and mouth and matching these against a database to verify\nidentities. However challenges such as high false positive rates have persisted\noften due to the similarity among individuals facial features. Recently\nContrastive Language Image Pretraining (CLIP) a model developed by OpenAI has\nshown promising advancements by linking natural language processing with vision\ntasks allowing it to generalize across modalities. Using CLIP's vision language\ncorrespondence and single-shot finetuning the model can achieve lower false\npositive rates upon deployment without the need of mass facial features\nextraction. This integration demonstrating CLIP's potential to address\npersistent issues in face recognition model performance without complicating\nour training paradigm."
                },
                "authors": [
                    {
                        "name": "Nhan T. Luu"
                    }
                ],
                "author_detail": {
                    "name": "Nhan T. Luu"
                },
                "author": "Nhan T. Luu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12319v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12319v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13009v1",
                "updated": "2024-11-20T03:17:51Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    17,
                    51,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T03:17:51Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    17,
                    51,
                    2,
                    325,
                    0
                ],
                "title": "LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMSteer: Improving Long-Context LLM Inference by Steering Attention on\n  Reused Contexts"
                },
                "summary": "As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) show impressive performance on complex tasks,\nthey still struggle with longer contextual understanding and high computational\ncosts. To balance efficiency and quality, we introduce LLMSteer, a\nfine-tuning-free framework that enhances LLMs through query-independent\nattention steering. Tested on popular LLMs and datasets, LLMSteer narrows the\nperformance gap with baselines by 65.9% and reduces the runtime delay by up to\n4.8x compared to recent attention steering methods."
                },
                "authors": [
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13008v1",
                "updated": "2024-11-20T03:16:07Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    16,
                    7,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T03:16:07Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    16,
                    7,
                    2,
                    325,
                    0
                ],
                "title": "Evaluating LLMs Capabilities Towards Understanding Social Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating LLMs Capabilities Towards Understanding Social Dynamics"
                },
                "summary": "Social media discourse involves people from different backgrounds, beliefs,\nand motives. Thus, often such discourse can devolve into toxic interactions.\nGenerative Models, such as Llama and ChatGPT, have recently exploded in\npopularity due to their capabilities in zero-shot question-answering. Because\nthese models are increasingly being used to ask questions of social\nsignificance, a crucial research question is whether they can understand social\nmedia dynamics. This work provides a critical analysis regarding generative\nLLM's ability to understand language and dynamics in social contexts,\nparticularly considering cyberbullying and anti-cyberbullying (posts aimed at\nreducing cyberbullying) interactions. Specifically, we compare and contrast the\ncapabilities of different large language models (LLMs) to understand three key\naspects of social dynamics: language, directionality, and the occurrence of\nbullying/anti-bullying messages. We found that while fine-tuned LLMs exhibit\npromising results in some social media understanding tasks (understanding\ndirectionality), they presented mixed results in others (proper paraphrasing\nand bullying/anti-bullying detection). We also found that fine-tuning and\nprompt engineering mechanisms can have positive effects in some tasks. We\nbelieve that a understanding of LLM's capabilities is crucial to design future\nmodels that can be effectively used in social applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social media discourse involves people from different backgrounds, beliefs,\nand motives. Thus, often such discourse can devolve into toxic interactions.\nGenerative Models, such as Llama and ChatGPT, have recently exploded in\npopularity due to their capabilities in zero-shot question-answering. Because\nthese models are increasingly being used to ask questions of social\nsignificance, a crucial research question is whether they can understand social\nmedia dynamics. This work provides a critical analysis regarding generative\nLLM's ability to understand language and dynamics in social contexts,\nparticularly considering cyberbullying and anti-cyberbullying (posts aimed at\nreducing cyberbullying) interactions. Specifically, we compare and contrast the\ncapabilities of different large language models (LLMs) to understand three key\naspects of social dynamics: language, directionality, and the occurrence of\nbullying/anti-bullying messages. We found that while fine-tuned LLMs exhibit\npromising results in some social media understanding tasks (understanding\ndirectionality), they presented mixed results in others (proper paraphrasing\nand bullying/anti-bullying detection). We also found that fine-tuning and\nprompt engineering mechanisms can have positive effects in some tasks. We\nbelieve that a understanding of LLM's capabilities is crucial to design future\nmodels that can be effectively used in social applications."
                },
                "authors": [
                    {
                        "name": "Anique Tahir"
                    },
                    {
                        "name": "Lu Cheng"
                    },
                    {
                        "name": "Manuel Sandoval"
                    },
                    {
                        "name": "Yasin N. Silva"
                    },
                    {
                        "name": "Deborah L. Hall"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "arxiv_comment": "To appear in ASONAM 24 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13004v1",
                "updated": "2024-11-20T03:01:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    1,
                    41,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T03:01:41Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    3,
                    1,
                    41,
                    2,
                    325,
                    0
                ],
                "title": "MERLOT: A Distilled LLM-based Mixture-of-Experts Framework for Scalable\n  Encrypted Traffic Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERLOT: A Distilled LLM-based Mixture-of-Experts Framework for Scalable\n  Encrypted Traffic Classification"
                },
                "summary": "We present MERLOT, a scalable mixture-of-expert (MoE) based refinement of\ndistilled large language model optimized for encrypted traffic classification.\nBy applying model distillation techniques in a teacher-student paradigm,\ncompact models derived from GPT-2-base retain high classification accuracy\nwhile minimizing computational costs. These models function as specialized\nexperts in an MoE architecture, dynamically assigned via a gating network.\nUnlike generation-based methods, our approach directly classifies encrypted\ntraffic using the final decoder token with contextual feature embedding as\ninput. Experiments on 10 datasets show superior or competitive performance over\nthe state-of-the-art models while significantly reducing resource demands,\nunderscoring its effectiveness and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MERLOT, a scalable mixture-of-expert (MoE) based refinement of\ndistilled large language model optimized for encrypted traffic classification.\nBy applying model distillation techniques in a teacher-student paradigm,\ncompact models derived from GPT-2-base retain high classification accuracy\nwhile minimizing computational costs. These models function as specialized\nexperts in an MoE architecture, dynamically assigned via a gating network.\nUnlike generation-based methods, our approach directly classifies encrypted\ntraffic using the final decoder token with contextual feature embedding as\ninput. Experiments on 10 datasets show superior or competitive performance over\nthe state-of-the-art models while significantly reducing resource demands,\nunderscoring its effectiveness and robustness."
                },
                "authors": [
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Rongpeng Li"
                    },
                    {
                        "name": "Zhifeng Zhao"
                    },
                    {
                        "name": "Honggang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Honggang Zhang"
                },
                "author": "Honggang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12103v2",
                "updated": "2024-11-20T02:23:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    23,
                    11,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-18T22:31:17Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    22,
                    31,
                    17,
                    0,
                    323,
                    0
                ],
                "title": "Does Unlearning Truly Unlearn? A Black Box Evaluation of LLM Unlearning\n  Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Unlearning Truly Unlearn? A Black Box Evaluation of LLM Unlearning\n  Methods"
                },
                "summary": "Large language model unlearning aims to remove harmful information that LLMs\nhave learnt to prevent their use for malicious purposes. LLMU and RMU have been\nproposed as two methods for LLM unlearning, achieving impressive results on\nunlearning benchmarks. We study in detail the efficacy of these methods by\nevaluating their impact on general model capabilities on the WMDP benchmark as\nwell as a biology benchmark we create. Our experiments show that RMU generally\nleads to better preservation of model capabilities, for similar or better\nunlearning. We further test the robustness of these methods and find that doing\n5-shot prompting or rephrasing the question in simple ways can lead to an over\nten-fold increase in accuracy on unlearning benchmarks. Finally, we show that\ntraining on unrelated data can almost completely recover pre-unlearning\nperformance, demonstrating that these methods fail at truly unlearning. The\ncode is available at: https://github.com/JaiDoshi/Knowledge-Erasure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model unlearning aims to remove harmful information that LLMs\nhave learnt to prevent their use for malicious purposes. LLMU and RMU have been\nproposed as two methods for LLM unlearning, achieving impressive results on\nunlearning benchmarks. We study in detail the efficacy of these methods by\nevaluating their impact on general model capabilities on the WMDP benchmark as\nwell as a biology benchmark we create. Our experiments show that RMU generally\nleads to better preservation of model capabilities, for similar or better\nunlearning. We further test the robustness of these methods and find that doing\n5-shot prompting or rephrasing the question in simple ways can lead to an over\nten-fold increase in accuracy on unlearning benchmarks. Finally, we show that\ntraining on unrelated data can almost completely recover pre-unlearning\nperformance, demonstrating that these methods fail at truly unlearning. The\ncode is available at: https://github.com/JaiDoshi/Knowledge-Erasure."
                },
                "authors": [
                    {
                        "name": "Jai Doshi"
                    },
                    {
                        "name": "Asa Cooper Stickland"
                    }
                ],
                "author_detail": {
                    "name": "Asa Cooper Stickland"
                },
                "author": "Asa Cooper Stickland",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12977v1",
                "updated": "2024-11-20T02:10:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    10,
                    44,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T02:10:44Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    10,
                    44,
                    2,
                    325,
                    0
                ],
                "title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong\n  Collaborative Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong\n  Collaborative Learning"
                },
                "summary": "Contemporary embodied agents, such as Voyager in Minecraft, have demonstrated\npromising capabilities in open-ended individual learning. However, when powered\nwith open large language models (LLMs), these agents often struggle with\nrudimentary tasks, even when fine-tuned on domain-specific knowledge. Inspired\nby human cultural learning, we present \\collabvoyager, a novel framework that\nenhances Voyager with lifelong collaborative learning through explicit\nperspective-taking. \\collabvoyager introduces three key innovations: (1) theory\nof mind representations linking percepts, beliefs, desires, and actions; (2)\nnatural language communication between agents; and (3) semantic memory of task\nand environment knowledge and episodic memory of collaboration episodes. These\nadvancements enable agents to reason about their and others' mental states,\nempirically addressing two prevalent failure modes: false beliefs and faulty\ntask executions. In mixed-expertise Minecraft experiments, \\collabvoyager\nagents outperform Voyager counterparts, significantly improving task completion\nrate by $66.6\\% (+39.4\\%)$ for collecting one block of dirt and $70.8\\%\n(+20.8\\%)$ for collecting one wood block. They exhibit emergent behaviors like\nknowledge transfer from expert to novice agents and collaborative code\ncorrection. \\collabvoyager agents also demonstrate the ability to adapt to\nout-of-distribution tasks by using their previous experiences and beliefs\nobtained through collaboration. In this open-ended social learning paradigm,\n\\collabvoyager paves the way for the democratic development of embodied AI,\nwhere agents learn in deployment from both peer and environmental feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary embodied agents, such as Voyager in Minecraft, have demonstrated\npromising capabilities in open-ended individual learning. However, when powered\nwith open large language models (LLMs), these agents often struggle with\nrudimentary tasks, even when fine-tuned on domain-specific knowledge. Inspired\nby human cultural learning, we present \\collabvoyager, a novel framework that\nenhances Voyager with lifelong collaborative learning through explicit\nperspective-taking. \\collabvoyager introduces three key innovations: (1) theory\nof mind representations linking percepts, beliefs, desires, and actions; (2)\nnatural language communication between agents; and (3) semantic memory of task\nand environment knowledge and episodic memory of collaboration episodes. These\nadvancements enable agents to reason about their and others' mental states,\nempirically addressing two prevalent failure modes: false beliefs and faulty\ntask executions. In mixed-expertise Minecraft experiments, \\collabvoyager\nagents outperform Voyager counterparts, significantly improving task completion\nrate by $66.6\\% (+39.4\\%)$ for collecting one block of dirt and $70.8\\%\n(+20.8\\%)$ for collecting one wood block. They exhibit emergent behaviors like\nknowledge transfer from expert to novice agents and collaborative code\ncorrection. \\collabvoyager agents also demonstrate the ability to adapt to\nout-of-distribution tasks by using their previous experiences and beliefs\nobtained through collaboration. In this open-ended social learning paradigm,\n\\collabvoyager paves the way for the democratic development of embodied AI,\nwhere agents learn in deployment from both peer and environmental feedback."
                },
                "authors": [
                    {
                        "name": "Mircea LicÄ"
                    },
                    {
                        "name": "Ojas Shirekar"
                    },
                    {
                        "name": "Baptiste Colle"
                    },
                    {
                        "name": "Chirag Raman"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Raman"
                },
                "author": "Chirag Raman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20181v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20181v2",
                "updated": "2024-11-20T02:10:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    10,
                    16,
                    2,
                    325,
                    0
                ],
                "published": "2024-09-30T10:48:20Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    10,
                    48,
                    20,
                    0,
                    274,
                    0
                ],
                "title": "Reference Trustable Decoding: A Training-Free Augmentation Paradigm for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reference Trustable Decoding: A Training-Free Augmentation Paradigm for\n  Large Language Models"
                },
                "summary": "Large language models (LLMs) have rapidly advanced and demonstrated\nimpressive capabilities. In-Context Learning (ICL) and Parameter-Efficient\nFine-Tuning (PEFT) are currently two mainstream methods for augmenting LLMs to\ndownstream tasks. ICL typically constructs a few-shot learning scenario, either\nmanually or by setting up a Retrieval-Augmented Generation (RAG) system,\nhelping models quickly grasp domain knowledge or question-answering patterns\nwithout changing model parameters. However, this approach involves trade-offs,\nsuch as slower inference speed and increased space occupancy. PEFT assists the\nmodel in adapting to tasks through minimal parameter modifications, but the\ntraining process still demands high hardware requirements, even with a small\nnumber of parameters involved. To address these challenges, we propose\nReference Trustable Decoding (RTD), a paradigm that allows models to quickly\nadapt to new tasks without fine-tuning, maintaining low inference costs. RTD\nconstructs a reference datastore from the provided training examples and\noptimizes the LLM's final vocabulary distribution by flexibly selecting\nsuitable references based on the input, resulting in more trustable responses\nand enabling the model to adapt to downstream tasks at a low cost. Experimental\nevaluations on various LLMs using different benchmarks demonstrate that RTD\nestablishes a new paradigm for augmenting models to downstream tasks.\nFurthermore, our method exhibits strong orthogonality with traditional methods,\nallowing for concurrent usage. Our code can be found at\nhttps://github.com/ShiLuohe/ReferenceTrustableDecoding",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have rapidly advanced and demonstrated\nimpressive capabilities. In-Context Learning (ICL) and Parameter-Efficient\nFine-Tuning (PEFT) are currently two mainstream methods for augmenting LLMs to\ndownstream tasks. ICL typically constructs a few-shot learning scenario, either\nmanually or by setting up a Retrieval-Augmented Generation (RAG) system,\nhelping models quickly grasp domain knowledge or question-answering patterns\nwithout changing model parameters. However, this approach involves trade-offs,\nsuch as slower inference speed and increased space occupancy. PEFT assists the\nmodel in adapting to tasks through minimal parameter modifications, but the\ntraining process still demands high hardware requirements, even with a small\nnumber of parameters involved. To address these challenges, we propose\nReference Trustable Decoding (RTD), a paradigm that allows models to quickly\nadapt to new tasks without fine-tuning, maintaining low inference costs. RTD\nconstructs a reference datastore from the provided training examples and\noptimizes the LLM's final vocabulary distribution by flexibly selecting\nsuitable references based on the input, resulting in more trustable responses\nand enabling the model to adapt to downstream tasks at a low cost. Experimental\nevaluations on various LLMs using different benchmarks demonstrate that RTD\nestablishes a new paradigm for augmenting models to downstream tasks.\nFurthermore, our method exhibits strong orthogonality with traditional methods,\nallowing for concurrent usage. Our code can be found at\nhttps://github.com/ShiLuohe/ReferenceTrustableDecoding"
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Accepted by the Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20181v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20181v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v4",
                "updated": "2024-11-20T02:04:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    2,
                    4,
                    10,
                    2,
                    325,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT's release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture's struggle with handling long texts. KV Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV Cache and elaborate on various\nmethods currently used to optimize the KV Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field. Links to the papers\nmentioned in this review can be found in our Github Repo\nhttps://github.com/zcli-charlie/Awesome-KV-Cache."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "Published on the First Conference on Language Modeling (COLM 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12960v1",
                "updated": "2024-11-20T01:27:56Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    1,
                    27,
                    56,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T01:27:56Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    1,
                    27,
                    56,
                    2,
                    325,
                    0
                ],
                "title": "I Can Tell What I am Doing: Toward Real-World Natural Language Grounding\n  of Robot Experiences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Can Tell What I am Doing: Toward Real-World Natural Language Grounding\n  of Robot Experiences"
                },
                "summary": "Understanding robot behaviors and experiences through natural language is\ncrucial for developing intelligent and transparent robotic systems. Recent\nadvancement in large language models (LLMs) makes it possible to translate\ncomplex, multi-modal robotic experiences into coherent, human-readable\nnarratives. However, grounding real-world robot experiences into natural\nlanguage is challenging due to many reasons, such as multi-modal nature of\ndata, differing sample rates, and data volume. We introduce RONAR, an LLM-based\nsystem that generates natural language narrations from robot experiences,\naiding in behavior announcement, failure analysis, and human interaction to\nrecover failure. Evaluated across various scenarios, RONAR outperforms\nstate-of-the-art methods and improves failure recovery efficiency. Our\ncontributions include a multi-modal framework for robot experience narration, a\ncomprehensive real-robot dataset, and empirical evidence of RONAR's\neffectiveness in enhancing user experience in system transparency and failure\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding robot behaviors and experiences through natural language is\ncrucial for developing intelligent and transparent robotic systems. Recent\nadvancement in large language models (LLMs) makes it possible to translate\ncomplex, multi-modal robotic experiences into coherent, human-readable\nnarratives. However, grounding real-world robot experiences into natural\nlanguage is challenging due to many reasons, such as multi-modal nature of\ndata, differing sample rates, and data volume. We introduce RONAR, an LLM-based\nsystem that generates natural language narrations from robot experiences,\naiding in behavior announcement, failure analysis, and human interaction to\nrecover failure. Evaluated across various scenarios, RONAR outperforms\nstate-of-the-art methods and improves failure recovery efficiency. Our\ncontributions include a multi-modal framework for robot experience narration, a\ncomprehensive real-robot dataset, and empirical evidence of RONAR's\neffectiveness in enhancing user experience in system transparency and failure\nanalysis."
                },
                "authors": [
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Brian Liang"
                    },
                    {
                        "name": "Varad Dhat"
                    },
                    {
                        "name": "Zander Brumbaugh"
                    },
                    {
                        "name": "Nick Walker"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Maya Cakmak"
                    }
                ],
                "author_detail": {
                    "name": "Maya Cakmak"
                },
                "author": "Maya Cakmak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18856v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18856v3",
                "updated": "2024-11-20T01:04:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    1,
                    4,
                    33,
                    2,
                    325,
                    0
                ],
                "published": "2024-10-24T15:41:56Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    15,
                    41,
                    56,
                    3,
                    298,
                    0
                ],
                "title": "Demystifying Large Language Models for Medicine: A Primer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying Large Language Models for Medicine: A Primer"
                },
                "summary": "Large language models (LLMs) represent a transformative class of AI tools\ncapable of revolutionizing various aspects of healthcare by generating\nhuman-like responses across diverse contexts and adapting to novel tasks\nfollowing human instructions. Their potential application spans a broad range\nof medical tasks, such as clinical documentation, matching patients to clinical\ntrials, and answering medical questions. In this primer paper, we propose an\nactionable guideline to help healthcare professionals more efficiently utilize\nLLMs in their work, along with a set of best practices. This approach consists\nof several main phases, including formulating the task, choosing LLMs, prompt\nengineering, fine-tuning, and deployment. We start with the discussion of\ncritical considerations in identifying healthcare tasks that align with the\ncore capabilities of LLMs and selecting models based on the selected task and\ndata, performance requirements, and model interface. We then review the\nstrategies, such as prompt engineering and fine-tuning, to adapt standard LLMs\nto specialized medical tasks. Deployment considerations, including regulatory\ncompliance, ethical guidelines, and continuous monitoring for fairness and\nbias, are also discussed. By providing a structured step-by-step methodology,\nthis tutorial aims to equip healthcare professionals with the tools necessary\nto effectively integrate LLMs into clinical practice, ensuring that these\npowerful technologies are applied in a safe, reliable, and impactful manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a transformative class of AI tools\ncapable of revolutionizing various aspects of healthcare by generating\nhuman-like responses across diverse contexts and adapting to novel tasks\nfollowing human instructions. Their potential application spans a broad range\nof medical tasks, such as clinical documentation, matching patients to clinical\ntrials, and answering medical questions. In this primer paper, we propose an\nactionable guideline to help healthcare professionals more efficiently utilize\nLLMs in their work, along with a set of best practices. This approach consists\nof several main phases, including formulating the task, choosing LLMs, prompt\nengineering, fine-tuning, and deployment. We start with the discussion of\ncritical considerations in identifying healthcare tasks that align with the\ncore capabilities of LLMs and selecting models based on the selected task and\ndata, performance requirements, and model interface. We then review the\nstrategies, such as prompt engineering and fine-tuning, to adapt standard LLMs\nto specialized medical tasks. Deployment considerations, including regulatory\ncompliance, ethical guidelines, and continuous monitoring for fairness and\nbias, are also discussed. By providing a structured step-by-step methodology,\nthis tutorial aims to equip healthcare professionals with the tools necessary\nto effectively integrate LLMs into clinical practice, ensuring that these\npowerful technologies are applied in a safe, reliable, and impactful manner."
                },
                "authors": [
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Nicholas Wan"
                    },
                    {
                        "name": "Robert Leaman"
                    },
                    {
                        "name": "Shubo Tian"
                    },
                    {
                        "name": "Zhizheng Wang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Zifeng Wang"
                    },
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Po-Ting Lai"
                    },
                    {
                        "name": "Qingqing Zhu"
                    },
                    {
                        "name": "Benjamin Hou"
                    },
                    {
                        "name": "Maame Sarfo-Gyamfi"
                    },
                    {
                        "name": "Gongbo Zhang"
                    },
                    {
                        "name": "Aidan Gilson"
                    },
                    {
                        "name": "Balu Bhasuran"
                    },
                    {
                        "name": "Zhe He"
                    },
                    {
                        "name": "Aidong Zhang"
                    },
                    {
                        "name": "Jimeng Sun"
                    },
                    {
                        "name": "Chunhua Weng"
                    },
                    {
                        "name": "Ronald M. Summers"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Zhiyong Lu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Lu"
                },
                "author": "Zhiyong Lu",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18856v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18856v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12951v1",
                "updated": "2024-11-20T00:47:17Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    47,
                    17,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T00:47:17Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    47,
                    17,
                    2,
                    325,
                    0
                ],
                "title": "On the Consistency of Video Large Language Models in Temporal\n  Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Consistency of Video Large Language Models in Temporal\n  Comprehension"
                },
                "summary": "Video large language models (Video-LLMs) can temporally ground language\nqueries and retrieve video moments. Yet, such temporal comprehension\ncapabilities are neither well-studied nor understood. So we conduct a study on\nprediction consistency -- a key indicator for robustness and trustworthiness of\ntemporal grounding. After the model identifies an initial moment within the\nvideo content, we apply a series of probes to check if the model's responses\nalign with this initial grounding as an indicator of reliable comprehension.\nOur results reveal that current Video-LLMs are sensitive to variations in video\ncontents, language queries, and task settings, unveiling severe deficiencies in\nmaintaining consistency. We further explore common prompting and\ninstruction-tuning methods as potential solutions, but find that their\nimprovements are often unstable. To that end, we propose event temporal\nverification tuning that explicitly accounts for consistency, and demonstrate\nsignificant improvements for both grounding and consistency. Our data and code\nwill be available at https://github.com/minjoong507/Consistency-of-Video-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (Video-LLMs) can temporally ground language\nqueries and retrieve video moments. Yet, such temporal comprehension\ncapabilities are neither well-studied nor understood. So we conduct a study on\nprediction consistency -- a key indicator for robustness and trustworthiness of\ntemporal grounding. After the model identifies an initial moment within the\nvideo content, we apply a series of probes to check if the model's responses\nalign with this initial grounding as an indicator of reliable comprehension.\nOur results reveal that current Video-LLMs are sensitive to variations in video\ncontents, language queries, and task settings, unveiling severe deficiencies in\nmaintaining consistency. We further explore common prompting and\ninstruction-tuning methods as potential solutions, but find that their\nimprovements are often unstable. To that end, we propose event temporal\nverification tuning that explicitly accounts for consistency, and demonstrate\nsignificant improvements for both grounding and consistency. Our data and code\nwill be available at https://github.com/minjoong507/Consistency-of-Video-LLM."
                },
                "authors": [
                    {
                        "name": "Minjoon Jung"
                    },
                    {
                        "name": "Junbin Xiao"
                    },
                    {
                        "name": "Byoung-Tak Zhang"
                    },
                    {
                        "name": "Angela Yao"
                    }
                ],
                "author_detail": {
                    "name": "Angela Yao"
                },
                "author": "Angela Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12946v1",
                "updated": "2024-11-20T00:31:23Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    31,
                    23,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T00:31:23Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    0,
                    31,
                    23,
                    2,
                    325,
                    0
                ],
                "title": "A Flexible Large Language Models Guardrail Development Methodology\n  Applied to Off-Topic Prompt Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Flexible Large Language Models Guardrail Development Methodology\n  Applied to Off-Topic Prompt Detection"
                },
                "summary": "Large Language Models are prone to off-topic misuse, where users may prompt\nthese models to perform tasks beyond their intended scope. Current guardrails,\nwhich often rely on curated examples or custom classifiers, suffer from high\nfalse-positive rates, limited adaptability, and the impracticality of requiring\nreal-world data that is not available in pre-production. In this paper, we\nintroduce a flexible, data-free guardrail development methodology that\naddresses these challenges. By thoroughly defining the problem space\nqualitatively and passing this to an LLM to generate diverse prompts, we\nconstruct a synthetic dataset to benchmark and train off-topic guardrails that\noutperform heuristic approaches. Additionally, by framing the task as\nclassifying whether the user prompt is relevant with respect to the system\nprompt, our guardrails effectively generalize to other misuse categories,\nincluding jailbreak and harmful prompts. Lastly, we further contribute to the\nfield by open-sourcing both the synthetic dataset and the off-topic guardrail\nmodels, providing valuable resources for developing guardrails in\npre-production environments and supporting future research and development in\nLLM safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are prone to off-topic misuse, where users may prompt\nthese models to perform tasks beyond their intended scope. Current guardrails,\nwhich often rely on curated examples or custom classifiers, suffer from high\nfalse-positive rates, limited adaptability, and the impracticality of requiring\nreal-world data that is not available in pre-production. In this paper, we\nintroduce a flexible, data-free guardrail development methodology that\naddresses these challenges. By thoroughly defining the problem space\nqualitatively and passing this to an LLM to generate diverse prompts, we\nconstruct a synthetic dataset to benchmark and train off-topic guardrails that\noutperform heuristic approaches. Additionally, by framing the task as\nclassifying whether the user prompt is relevant with respect to the system\nprompt, our guardrails effectively generalize to other misuse categories,\nincluding jailbreak and harmful prompts. Lastly, we further contribute to the\nfield by open-sourcing both the synthetic dataset and the off-topic guardrail\nmodels, providing valuable resources for developing guardrails in\npre-production environments and supporting future research and development in\nLLM safety."
                },
                "authors": [
                    {
                        "name": "Gabriel Chua"
                    },
                    {
                        "name": "Shing Yee Chan"
                    },
                    {
                        "name": "Shaun Khoo"
                    }
                ],
                "author_detail": {
                    "name": "Shaun Khoo"
                },
                "author": "Shaun Khoo",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12930v1",
                "updated": "2024-11-19T23:43:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    23,
                    43,
                    25,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T23:43:25Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    23,
                    43,
                    25,
                    1,
                    324,
                    0
                ],
                "title": "LEDRO: LLM-Enhanced Design Space Reduction and Optimization for Analog\n  Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEDRO: LLM-Enhanced Design Space Reduction and Optimization for Analog\n  Circuits"
                },
                "summary": "Traditional approaches for designing analog circuits are time-consuming and\nrequire significant human expertise. Existing automation efforts using methods\nlike Bayesian Optimization (BO) and Reinforcement Learning (RL) are sub-optimal\nand costly to generalize across different topologies and technology nodes. In\nour work, we introduce a novel approach, LEDRO, utilizing Large Language Models\n(LLMs) in conjunction with optimization techniques to iteratively refine the\ndesign space for analog circuit sizing. LEDRO is highly generalizable compared\nto other RL and BO baselines, eliminating the need for design annotation or\nmodel training for different topologies or technology nodes. We conduct a\ncomprehensive evaluation of our proposed framework and baseline on 22 different\nOp-Amp topologies across four FinFET technology nodes. Results demonstrate the\nsuperior performance of LEDRO as it outperforms our best baseline by an average\nof 13% FoM improvement with 2.15x speed-up on low complexity Op-Amps and 48%\nFoM improvement with 1.7x speed-up on high complexity Op-Amps. This highlights\nLEDRO's effective performance, efficiency, and generalizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional approaches for designing analog circuits are time-consuming and\nrequire significant human expertise. Existing automation efforts using methods\nlike Bayesian Optimization (BO) and Reinforcement Learning (RL) are sub-optimal\nand costly to generalize across different topologies and technology nodes. In\nour work, we introduce a novel approach, LEDRO, utilizing Large Language Models\n(LLMs) in conjunction with optimization techniques to iteratively refine the\ndesign space for analog circuit sizing. LEDRO is highly generalizable compared\nto other RL and BO baselines, eliminating the need for design annotation or\nmodel training for different topologies or technology nodes. We conduct a\ncomprehensive evaluation of our proposed framework and baseline on 22 different\nOp-Amp topologies across four FinFET technology nodes. Results demonstrate the\nsuperior performance of LEDRO as it outperforms our best baseline by an average\nof 13% FoM improvement with 2.15x speed-up on low complexity Op-Amps and 48%\nFoM improvement with 1.7x speed-up on high complexity Op-Amps. This highlights\nLEDRO's effective performance, efficiency, and generalizability."
                },
                "authors": [
                    {
                        "name": "Dimple Vijay Kochar"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Anantha Chandrakasan"
                    },
                    {
                        "name": "Xin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Zhang"
                },
                "author": "Xin Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17309v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17309v2",
                "updated": "2024-11-19T23:32:13Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    23,
                    32,
                    13,
                    1,
                    324,
                    0
                ],
                "published": "2024-10-22T18:00:00Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    18,
                    0,
                    0,
                    1,
                    296,
                    0
                ],
                "title": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature Meets Data: A Synergistic Approach to Hypothesis Generation"
                },
                "summary": "AI holds promise for transforming scientific processes, including hypothesis\ngeneration. Prior work on hypothesis generation can be broadly categorized into\ntheory-driven and data-driven approaches. While both have proven effective in\ngenerating novel and plausible hypotheses, it remains an open question whether\nthey can complement each other. To address this, we develop the first method\nthat combines literature-based insights with data to perform LLM-powered\nhypothesis generation. We apply our method on five different datasets and\ndemonstrate that integrating literature and data outperforms other baselines\n(8.97\\% over few-shot, 15.75\\% over literature-based alone, and 3.37\\% over\ndata-driven alone). Additionally, we conduct the first human evaluation to\nassess the utility of LLM-generated hypotheses in assisting human\ndecision-making on two challenging tasks: deception detection and AI generated\ncontent detection. Our results show that human accuracy improves significantly\nby 7.44\\% and 14.19\\% on these tasks, respectively. These findings suggest that\nintegrating literature-based and data-driven approaches provides a\ncomprehensive and nuanced framework for hypothesis generation and could open\nnew avenues for scientific inquiry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI holds promise for transforming scientific processes, including hypothesis\ngeneration. Prior work on hypothesis generation can be broadly categorized into\ntheory-driven and data-driven approaches. While both have proven effective in\ngenerating novel and plausible hypotheses, it remains an open question whether\nthey can complement each other. To address this, we develop the first method\nthat combines literature-based insights with data to perform LLM-powered\nhypothesis generation. We apply our method on five different datasets and\ndemonstrate that integrating literature and data outperforms other baselines\n(8.97\\% over few-shot, 15.75\\% over literature-based alone, and 3.37\\% over\ndata-driven alone). Additionally, we conduct the first human evaluation to\nassess the utility of LLM-generated hypotheses in assisting human\ndecision-making on two challenging tasks: deception detection and AI generated\ncontent detection. Our results show that human accuracy improves significantly\nby 7.44\\% and 14.19\\% on these tasks, respectively. These findings suggest that\nintegrating literature-based and data-driven approaches provides a\ncomprehensive and nuanced framework for hypothesis generation and could open\nnew avenues for scientific inquiry."
                },
                "authors": [
                    {
                        "name": "Haokun Liu"
                    },
                    {
                        "name": "Yangqiaoyu Zhou"
                    },
                    {
                        "name": "Mingxuan Li"
                    },
                    {
                        "name": "Chenfei Yuan"
                    },
                    {
                        "name": "Chenhao Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chenhao Tan"
                },
                "author": "Chenhao Tan",
                "arxiv_comment": "30 pages, 7 figures, code link:\n  https://github.com/ChicagoHAI/hypothesis-generation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17309v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17309v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12924v1",
                "updated": "2024-11-19T23:22:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    23,
                    22,
                    33,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T23:22:33Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    23,
                    22,
                    33,
                    1,
                    324,
                    0
                ],
                "title": "Human-In-the-Loop Software Development Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-In-the-Loop Software Development Agents"
                },
                "summary": "Recently, Large Language Models (LLMs)-based multi-agent paradigms for\nsoftware engineering are introduced to automatically resolve software\ndevelopment tasks (e.g., from a given issue to source code). However, existing\nwork is evaluated based on historical benchmark datasets, does not consider\nhuman feedback at each stage of the automated software development process, and\nhas not been deployed in practice. In this paper, we introduce a\nHuman-in-the-loop LLM-based Agents framework (HULA) for software development\nthat allows software engineers to refine and guide LLMs when generating coding\nplans and source code for a given task. We design, implement, and deploy the\nHULA framework into Atlassian JIRA for internal uses. Through a multi-stage\nevaluation of the HULA framework, Atlassian software engineers perceive that\nHULA can minimize the overall development time and effort, especially in\ninitiating a coding plan and writing code for straightforward tasks. On the\nother hand, challenges around code quality are raised to be solved in some\ncases. We draw lessons learned and discuss opportunities for future work, which\nwill pave the way for the advancement of LLM-based agents in software\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs)-based multi-agent paradigms for\nsoftware engineering are introduced to automatically resolve software\ndevelopment tasks (e.g., from a given issue to source code). However, existing\nwork is evaluated based on historical benchmark datasets, does not consider\nhuman feedback at each stage of the automated software development process, and\nhas not been deployed in practice. In this paper, we introduce a\nHuman-in-the-loop LLM-based Agents framework (HULA) for software development\nthat allows software engineers to refine and guide LLMs when generating coding\nplans and source code for a given task. We design, implement, and deploy the\nHULA framework into Atlassian JIRA for internal uses. Through a multi-stage\nevaluation of the HULA framework, Atlassian software engineers perceive that\nHULA can minimize the overall development time and effort, especially in\ninitiating a coding plan and writing code for straightforward tasks. On the\nother hand, challenges around code quality are raised to be solved in some\ncases. We draw lessons learned and discuss opportunities for future work, which\nwill pave the way for the advancement of LLM-based agents in software\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Wannita Takerngsaksiri"
                    },
                    {
                        "name": "Jirat Pasuksmit"
                    },
                    {
                        "name": "Patanamon Thongtanunam"
                    },
                    {
                        "name": "Chakkrit Tantithamthavorn"
                    },
                    {
                        "name": "Ruixiong Zhang"
                    },
                    {
                        "name": "Fan Jiang"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Evan Cook"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Ming Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Wu"
                },
                "author": "Ming Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10951v2",
                "updated": "2024-11-19T23:09:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    23,
                    9,
                    25,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-17T03:34:27Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    3,
                    34,
                    27,
                    6,
                    322,
                    0
                ],
                "title": "TSFormer: A Robust Framework for Efficient UHD Image Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TSFormer: A Robust Framework for Efficient UHD Image Restoration"
                },
                "summary": "Ultra-high-definition (UHD) image restoration is vital for applications\ndemanding exceptional visual fidelity, yet existing methods often face a\ntrade-off between restoration quality and efficiency, limiting their practical\ndeployment. In this paper, we propose TSFormer, an all-in-one framework that\nintegrates \\textbf{T}rusted learning with \\textbf{S}parsification to boost both\ngeneralization capability and computational efficiency in UHD image\nrestoration. The key is that only a small amount of token movement is allowed\nwithin the model. To efficiently filter tokens, we use Min-$p$ with random\nmatrix theory to quantify the uncertainty of tokens, thereby improving the\nrobustness of the model. Our model can run a 4K image in real time (40fps) with\n3.38 M parameters. Extensive experiments demonstrate that TSFormer achieves\nstate-of-the-art restoration quality while enhancing generalization and\nreducing computational demands. In addition, our token filtering method can be\napplied to other image restoration models to effectively accelerate inference\nand maintain performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-high-definition (UHD) image restoration is vital for applications\ndemanding exceptional visual fidelity, yet existing methods often face a\ntrade-off between restoration quality and efficiency, limiting their practical\ndeployment. In this paper, we propose TSFormer, an all-in-one framework that\nintegrates \\textbf{T}rusted learning with \\textbf{S}parsification to boost both\ngeneralization capability and computational efficiency in UHD image\nrestoration. The key is that only a small amount of token movement is allowed\nwithin the model. To efficiently filter tokens, we use Min-$p$ with random\nmatrix theory to quantify the uncertainty of tokens, thereby improving the\nrobustness of the model. Our model can run a 4K image in real time (40fps) with\n3.38 M parameters. Extensive experiments demonstrate that TSFormer achieves\nstate-of-the-art restoration quality while enhancing generalization and\nreducing computational demands. In addition, our token filtering method can be\napplied to other image restoration models to effectively accelerate inference\nand maintain performance."
                },
                "authors": [
                    {
                        "name": "Xin Su"
                    },
                    {
                        "name": "Chen Wu"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhuoran Zheng"
                },
                "author": "Zhuoran Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.03641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.03641v2",
                "updated": "2024-11-19T22:55:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    22,
                    55,
                    45,
                    1,
                    324,
                    0
                ],
                "published": "2023-01-09T19:25:19Z",
                "published_parsed": [
                    2023,
                    1,
                    9,
                    19,
                    25,
                    19,
                    0,
                    9,
                    0
                ],
                "title": "Toward Multi-Layer Networking for Satellite Network Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Multi-Layer Networking for Satellite Network Operations"
                },
                "summary": "Recent advancements in low-Earth-orbit (LEO) satellites aim to bring\nresilience, ubiquitous, and high-quality service to future Internet\ninfrastructure. However, the soaring number of space assets, increasing\ndynamics of LEO satellites and expanding dimensions of network threats call for\nan enhanced approach to efficient satellite operations. To address these\npressing challenges, we propose an approach for satellite network operations\nbased on multi-layer satellite networking (MLSN), called \"SatNetOps\". Two\nSatNetOps schemes are proposed, referred to as LEO-LEO MLSN (LLM) and GEO-LEO\nMLSN (GLM). The performance of the proposed schemes is evaluated in 24-hr\nsatellite scenarios with typical payload setups in simulations, where the key\nmetrics such as latency and reliability are discussed with the consideration of\nthe Consultative Committee for Space Data Systems (CCSDS) standard-compliant\ntelemetry and telecommand missions. Although the SatNetOps approach is\npromising, we analyze the factors affecting the performance of the LLM and GLM\nschemes. The discussions on the results and conclusive remarks are made in the\nend.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in low-Earth-orbit (LEO) satellites aim to bring\nresilience, ubiquitous, and high-quality service to future Internet\ninfrastructure. However, the soaring number of space assets, increasing\ndynamics of LEO satellites and expanding dimensions of network threats call for\nan enhanced approach to efficient satellite operations. To address these\npressing challenges, we propose an approach for satellite network operations\nbased on multi-layer satellite networking (MLSN), called \"SatNetOps\". Two\nSatNetOps schemes are proposed, referred to as LEO-LEO MLSN (LLM) and GEO-LEO\nMLSN (GLM). The performance of the proposed schemes is evaluated in 24-hr\nsatellite scenarios with typical payload setups in simulations, where the key\nmetrics such as latency and reliability are discussed with the consideration of\nthe Consultative Committee for Space Data Systems (CCSDS) standard-compliant\ntelemetry and telecommand missions. Although the SatNetOps approach is\npromising, we analyze the factors affecting the performance of the LLM and GLM\nschemes. The discussions on the results and conclusive remarks are made in the\nend."
                },
                "authors": [
                    {
                        "name": "Peng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Peng Hu"
                },
                "author": "Peng Hu",
                "arxiv_comment": "To be published in the Proceedings of 12th Annual IEEE International\n  Conference on Wireless for Space and Extreme Environments (WISEE 2024), Dec.\n  16 - 18, 2024, Daytona Beach, FL, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.03641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.03641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12901v1",
                "updated": "2024-11-19T22:27:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    22,
                    27,
                    53,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T22:27:53Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    22,
                    27,
                    53,
                    1,
                    324,
                    0
                ],
                "title": "Signformer is all you need: Towards Edge AI for Sign Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Signformer is all you need: Towards Edge AI for Sign Language"
                },
                "summary": "Sign language translation, especially in gloss-free paradigm, is confronting\na dilemma of impracticality and unsustainability due to growing\nresource-intensive methodologies. Contemporary state-of-the-arts (SOTAs) have\nsignificantly hinged on pretrained sophiscated backbones such as Large Language\nModels (LLMs), embedding sources, or extensive datasets, inducing considerable\nparametric and computational inefficiency for sustainable use in real-world\nscenario. Despite their success, following this research direction undermines\nthe overarching mission of this domain to create substantial value to bridge\nhard-hearing and common populations. Committing to the prevailing trend of LLM\nand Natural Language Processing (NLP) studies, we pursue a profound essential\nchange in architecture to achieve ground-up improvements without external aid\nfrom pretrained models, prior knowledge transfer, or any NLP strategies\nconsidered not-from-scratch.\n  Introducing Signformer, a from-scratch Feather-Giant transforming the area\ntowards Edge AI that redefines extremities of performance and efficiency with\nLLM-competence and edgy-deployable compactness. In this paper, we present\nnature analysis of sign languages to inform our algorithmic design and deliver\na scalable transformer pipeline with convolution and attention novelty. We\nachieve new 2nd place on leaderboard with a parametric reduction of 467-1807x\nagainst the finests as of 2024 and outcompete almost every other methods in a\nlighter configuration of 0.57 million parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign language translation, especially in gloss-free paradigm, is confronting\na dilemma of impracticality and unsustainability due to growing\nresource-intensive methodologies. Contemporary state-of-the-arts (SOTAs) have\nsignificantly hinged on pretrained sophiscated backbones such as Large Language\nModels (LLMs), embedding sources, or extensive datasets, inducing considerable\nparametric and computational inefficiency for sustainable use in real-world\nscenario. Despite their success, following this research direction undermines\nthe overarching mission of this domain to create substantial value to bridge\nhard-hearing and common populations. Committing to the prevailing trend of LLM\nand Natural Language Processing (NLP) studies, we pursue a profound essential\nchange in architecture to achieve ground-up improvements without external aid\nfrom pretrained models, prior knowledge transfer, or any NLP strategies\nconsidered not-from-scratch.\n  Introducing Signformer, a from-scratch Feather-Giant transforming the area\ntowards Edge AI that redefines extremities of performance and efficiency with\nLLM-competence and edgy-deployable compactness. In this paper, we present\nnature analysis of sign languages to inform our algorithmic design and deliver\na scalable transformer pipeline with convolution and attention novelty. We\nachieve new 2nd place on leaderboard with a parametric reduction of 467-1807x\nagainst the finests as of 2024 and outcompete almost every other methods in a\nlighter configuration of 0.57 million parameters."
                },
                "authors": [
                    {
                        "name": "Eta Yang"
                    }
                ],
                "author_detail": {
                    "name": "Eta Yang"
                },
                "author": "Eta Yang",
                "arxiv_comment": "Official Code at: https://github.com/EtaEnding/Signformer/tree/main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12892v1",
                "updated": "2024-11-19T22:17:18Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    22,
                    17,
                    18,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T22:17:18Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    22,
                    17,
                    18,
                    1,
                    324,
                    0
                ],
                "title": "Selective Attention: Enhancing Transformer through Principled Context\n  Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Attention: Enhancing Transformer through Principled Context\n  Control"
                },
                "summary": "The attention mechanism within the transformer architecture enables the model\nto weigh and combine tokens based on their relevance to the query. While\nself-attention has enjoyed major success, it notably treats all queries $q$ in\nthe same way by applying the mapping $V^\\top\\text{softmax}(Kq)$, where $V,K$\nare the value and key embeddings respectively. In this work, we argue that this\nuniform treatment hinders the ability to control contextual sparsity and\nrelevance. As a solution, we introduce the $\\textit{Selective Self-Attention}$\n(SSA) layer that augments the softmax nonlinearity with a principled\ntemperature scaling strategy. By controlling temperature, SSA adapts the\ncontextual sparsity of the attention map to the query embedding and its\nposition in the context window. Through theory and experiments, we demonstrate\nthat this alleviates attention dilution, aids the optimization process, and\nenhances the model's ability to control softmax spikiness of individual\nqueries. We also incorporate temperature scaling for value embeddings and show\nthat it boosts the model's ability to suppress irrelevant/noisy tokens.\nNotably, SSA is a lightweight method which introduces less than 0.5% new\nparameters through a weight-sharing strategy and can be fine-tuned on existing\nLLMs. Extensive empirical evaluations demonstrate that SSA-equipped models\nachieve a noticeable and consistent accuracy improvement on language modeling\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention mechanism within the transformer architecture enables the model\nto weigh and combine tokens based on their relevance to the query. While\nself-attention has enjoyed major success, it notably treats all queries $q$ in\nthe same way by applying the mapping $V^\\top\\text{softmax}(Kq)$, where $V,K$\nare the value and key embeddings respectively. In this work, we argue that this\nuniform treatment hinders the ability to control contextual sparsity and\nrelevance. As a solution, we introduce the $\\textit{Selective Self-Attention}$\n(SSA) layer that augments the softmax nonlinearity with a principled\ntemperature scaling strategy. By controlling temperature, SSA adapts the\ncontextual sparsity of the attention map to the query embedding and its\nposition in the context window. Through theory and experiments, we demonstrate\nthat this alleviates attention dilution, aids the optimization process, and\nenhances the model's ability to control softmax spikiness of individual\nqueries. We also incorporate temperature scaling for value embeddings and show\nthat it boosts the model's ability to suppress irrelevant/noisy tokens.\nNotably, SSA is a lightweight method which introduces less than 0.5% new\nparameters through a weight-sharing strategy and can be fine-tuned on existing\nLLMs. Extensive empirical evaluations demonstrate that SSA-equipped models\nachieve a noticeable and consistent accuracy improvement on language modeling\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Xuechen Zhang"
                    },
                    {
                        "name": "Xiangyu Chang"
                    },
                    {
                        "name": "Mingchen Li"
                    },
                    {
                        "name": "Amit Roy-Chowdhury"
                    },
                    {
                        "name": "Jiasi Chen"
                    },
                    {
                        "name": "Samet Oymak"
                    }
                ],
                "author_detail": {
                    "name": "Samet Oymak"
                },
                "author": "Samet Oymak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13082v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13082v2",
                "updated": "2024-11-19T22:02:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    22,
                    2,
                    49,
                    1,
                    324,
                    0
                ],
                "published": "2024-04-17T05:56:49Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    5,
                    56,
                    49,
                    2,
                    108,
                    0
                ],
                "title": "Efficient Contextual LLM Cascades through Budget-Constrained Policy\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Contextual LLM Cascades through Budget-Constrained Policy\n  Learning"
                },
                "summary": "Recent successes in natural language processing have led to the proliferation\nof large language models (LLMs) by multiple providers. Each LLM offering has\ndifferent inference accuracy, monetary cost, and latency, and their accuracy\nfurther depends on the exact wording of the question (i.e., the specific\nprompt). At the same time, users often have a limit on monetary budget and\nlatency to answer all their questions, and they do not know which LLMs to\nchoose for each question to meet their accuracy and long term budget\nrequirements. To navigate this rich design space, we propose TREACLE\n($\\underline{T}$hrifty $\\underline{Rea}$soning via $\\underline{C}$ontext-Aware\n$\\underline{L}$LM and Prompt S$\\underline{e}$lection), a reinforcement learning\npolicy that jointly selects the model and prompting scheme while respecting the\nuser's monetary cost and latency constraints. TREACLE uses the problem context,\nincluding question text embeddings (reflecting the type or difficulty of a\nquery) and the response history (reflecting the consistency of previous\nresponses) to make smart decisions. Our evaluations on standard reasoning\ndatasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE\nenables cost savings of up to 85% compared to baselines, while maintaining high\naccuracy. Importantly, it provides the user with the ability to gracefully\ntrade off accuracy for cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent successes in natural language processing have led to the proliferation\nof large language models (LLMs) by multiple providers. Each LLM offering has\ndifferent inference accuracy, monetary cost, and latency, and their accuracy\nfurther depends on the exact wording of the question (i.e., the specific\nprompt). At the same time, users often have a limit on monetary budget and\nlatency to answer all their questions, and they do not know which LLMs to\nchoose for each question to meet their accuracy and long term budget\nrequirements. To navigate this rich design space, we propose TREACLE\n($\\underline{T}$hrifty $\\underline{Rea}$soning via $\\underline{C}$ontext-Aware\n$\\underline{L}$LM and Prompt S$\\underline{e}$lection), a reinforcement learning\npolicy that jointly selects the model and prompting scheme while respecting the\nuser's monetary cost and latency constraints. TREACLE uses the problem context,\nincluding question text embeddings (reflecting the type or difficulty of a\nquery) and the response history (reflecting the consistency of previous\nresponses) to make smart decisions. Our evaluations on standard reasoning\ndatasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE\nenables cost savings of up to 85% compared to baselines, while maintaining high\naccuracy. Importantly, it provides the user with the ability to gracefully\ntrade off accuracy for cost."
                },
                "authors": [
                    {
                        "name": "Xuechen Zhang"
                    },
                    {
                        "name": "Zijian Huang"
                    },
                    {
                        "name": "Ege Onur Taga"
                    },
                    {
                        "name": "Carlee Joe-Wong"
                    },
                    {
                        "name": "Samet Oymak"
                    },
                    {
                        "name": "Jiasi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiasi Chen"
                },
                "author": "Jiasi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13082v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13082v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12882v1",
                "updated": "2024-11-19T22:00:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    22,
                    0,
                    1,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T22:00:01Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    22,
                    0,
                    1,
                    1,
                    324,
                    0
                ],
                "title": "ProSec: Fortifying Code LLMs with Proactive Security Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProSec: Fortifying Code LLMs with Proactive Security Alignment"
                },
                "summary": "Recent advances in code-specific large language models (LLMs) have greatly\nenhanced code generation and refinement capabilities. However, the safety of\ncode LLMs remains under-explored, posing potential risks as insecure code\ngenerated by these models may introduce vulnerabilities into real-world\nsystems. Previous work proposes to collect security-focused instruction-tuning\ndataset from real-world vulnerabilities. It is constrained by the data sparsity\nof vulnerable code, and has limited applicability in the iterative\npost-training workflows of modern LLMs. In this paper, we propose ProSec, a\nnovel proactive security alignment approach designed to align code LLMs with\nsecure coding practices. ProSec systematically exposes the vulnerabilities in a\ncode LLM by synthesizing error-inducing coding scenarios from Common Weakness\nEnumerations (CWEs), and generates fixes to vulnerable code snippets, allowing\nthe model to learn secure practices through advanced preference learning\nobjectives. The scenarios synthesized by ProSec triggers 25 times more\nvulnerable code than a normal instruction-tuning dataset, resulting in a\nsecurity-focused alignment dataset 7 times larger than the previous work.\nExperiments show that models trained with ProSec is 29.2% to 35.5% more secure\ncompared to previous work, with a marginal negative effect of less than 2\npercentage points on model's utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in code-specific large language models (LLMs) have greatly\nenhanced code generation and refinement capabilities. However, the safety of\ncode LLMs remains under-explored, posing potential risks as insecure code\ngenerated by these models may introduce vulnerabilities into real-world\nsystems. Previous work proposes to collect security-focused instruction-tuning\ndataset from real-world vulnerabilities. It is constrained by the data sparsity\nof vulnerable code, and has limited applicability in the iterative\npost-training workflows of modern LLMs. In this paper, we propose ProSec, a\nnovel proactive security alignment approach designed to align code LLMs with\nsecure coding practices. ProSec systematically exposes the vulnerabilities in a\ncode LLM by synthesizing error-inducing coding scenarios from Common Weakness\nEnumerations (CWEs), and generates fixes to vulnerable code snippets, allowing\nthe model to learn secure practices through advanced preference learning\nobjectives. The scenarios synthesized by ProSec triggers 25 times more\nvulnerable code than a normal instruction-tuning dataset, resulting in a\nsecurity-focused alignment dataset 7 times larger than the previous work.\nExperiments show that models trained with ProSec is 29.2% to 35.5% more secure\ncompared to previous work, with a marginal negative effect of less than 2\npercentage points on model's utility."
                },
                "authors": [
                    {
                        "name": "Xiangzhe Xu"
                    },
                    {
                        "name": "Zian Su"
                    },
                    {
                        "name": "Jinyao Guo"
                    },
                    {
                        "name": "Kaiyuan Zhang"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "arxiv_comment": "The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12880v1",
                "updated": "2024-11-19T21:57:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    21,
                    57,
                    22,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T21:57:22Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    21,
                    57,
                    22,
                    1,
                    324,
                    0
                ],
                "title": "Advancing Large Language Models for Spatiotemporal and Semantic\n  Association Mining of Similar Environmental Events",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Large Language Models for Spatiotemporal and Semantic\n  Association Mining of Similar Environmental Events"
                },
                "summary": "Retrieval and recommendation are two essential tasks in modern search tools.\nThis paper introduces a novel retrieval-reranking framework leveraging Large\nLanguage Models (LLMs) to enhance the spatiotemporal and semantic associated\nmining and recommendation of relevant unusual climate and environmental events\ndescribed in news articles and web posts. This framework uses advanced natural\nlanguage processing techniques to address the limitations of traditional manual\ncuration methods in terms of high labor cost and lack of scalability.\nSpecifically, we explore an optimized solution to employ cutting-edge embedding\nmodels for semantically analyzing spatiotemporal events (news) and propose a\nGeo-Time Re-ranking (GT-R) strategy that integrates multi-faceted criteria\nincluding spatial proximity, temporal association, semantic similarity, and\ncategory-instructed similarity to rank and identify similar spatiotemporal\nevents. We apply the proposed framework to a dataset of four thousand Local\nEnvironmental Observer (LEO) Network events, achieving top performance in\nrecommending similar events among multiple cutting-edge dense retrieval models.\nThe search and recommendation pipeline can be applied to a wide range of\nsimilar data search tasks dealing with geospatial and temporal data. We hope\nthat by linking relevant events, we can better aid the general public to gain\nan enhanced understanding of climate change and its impact on different\ncommunities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval and recommendation are two essential tasks in modern search tools.\nThis paper introduces a novel retrieval-reranking framework leveraging Large\nLanguage Models (LLMs) to enhance the spatiotemporal and semantic associated\nmining and recommendation of relevant unusual climate and environmental events\ndescribed in news articles and web posts. This framework uses advanced natural\nlanguage processing techniques to address the limitations of traditional manual\ncuration methods in terms of high labor cost and lack of scalability.\nSpecifically, we explore an optimized solution to employ cutting-edge embedding\nmodels for semantically analyzing spatiotemporal events (news) and propose a\nGeo-Time Re-ranking (GT-R) strategy that integrates multi-faceted criteria\nincluding spatial proximity, temporal association, semantic similarity, and\ncategory-instructed similarity to rank and identify similar spatiotemporal\nevents. We apply the proposed framework to a dataset of four thousand Local\nEnvironmental Observer (LEO) Network events, achieving top performance in\nrecommending similar events among multiple cutting-edge dense retrieval models.\nThe search and recommendation pipeline can be applied to a wide range of\nsimilar data search tasks dealing with geospatial and temporal data. We hope\nthat by linking relevant events, we can better aid the general public to gain\nan enhanced understanding of climate change and its impact on different\ncommunities."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Tian"
                    },
                    {
                        "name": "Wenwen Li"
                    },
                    {
                        "name": "Lei Hu"
                    },
                    {
                        "name": "Xiao Chen"
                    },
                    {
                        "name": "Michael Brook"
                    },
                    {
                        "name": "Michael Brubaker"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Anna K. Liljedahl"
                    }
                ],
                "author_detail": {
                    "name": "Anna K. Liljedahl"
                },
                "author": "Anna K. Liljedahl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07447v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07447v2",
                "updated": "2024-11-19T21:57:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    21,
                    57,
                    16,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-12T00:10:34Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    0,
                    10,
                    34,
                    1,
                    317,
                    0
                ],
                "title": "The Effect of Scheduling and Preemption on the Efficiency of LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Effect of Scheduling and Preemption on the Efficiency of LLM\n  Inference Serving"
                },
                "summary": "The growing usage of Large Language Models (LLMs) highlights the demands and\nchallenges in scalable LLM inference systems, affecting deployment and\ndevelopment processes. On the deployment side, there is a lack of comprehensive\nanalysis on the conditions under which a particular scheduler performs better\nor worse, with performance varying substantially across different schedulers,\nhardware, models, and workloads. Manually testing each configuration on GPUs\ncan be prohibitively expensive. On the development side, unpredictable\nperformance and unknown upper limits can lead to inconclusive trial-and-error\nprocesses, consuming resources on ideas that end up ineffective. To address\nthese challenges, we introduce INFERMAX, an analytical framework that uses\ninference cost models to compare various schedulers, including an optimal\nscheduler formulated as a constraint satisfaction problem (CSP) to establish an\nupper bound on performance. Our framework offers in-depth analysis and raises\nessential questions, challenging assumptions and exploring opportunities for\nmore efficient scheduling. Notably, our findings indicate that preempting\nrequests can reduce GPU costs by 30% compared to avoiding preemptions at all.\nWe believe our methods and insights will facilitate the cost-effective\ndeployment and development of scalable, efficient inference systems and pave\nthe way for cost-based scheduling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing usage of Large Language Models (LLMs) highlights the demands and\nchallenges in scalable LLM inference systems, affecting deployment and\ndevelopment processes. On the deployment side, there is a lack of comprehensive\nanalysis on the conditions under which a particular scheduler performs better\nor worse, with performance varying substantially across different schedulers,\nhardware, models, and workloads. Manually testing each configuration on GPUs\ncan be prohibitively expensive. On the development side, unpredictable\nperformance and unknown upper limits can lead to inconclusive trial-and-error\nprocesses, consuming resources on ideas that end up ineffective. To address\nthese challenges, we introduce INFERMAX, an analytical framework that uses\ninference cost models to compare various schedulers, including an optimal\nscheduler formulated as a constraint satisfaction problem (CSP) to establish an\nupper bound on performance. Our framework offers in-depth analysis and raises\nessential questions, challenging assumptions and exploring opportunities for\nmore efficient scheduling. Notably, our findings indicate that preempting\nrequests can reduce GPU costs by 30% compared to avoiding preemptions at all.\nWe believe our methods and insights will facilitate the cost-effective\ndeployment and development of scalable, efficient inference systems and pave\nthe way for cost-based scheduling."
                },
                "authors": [
                    {
                        "name": "Kyoungmin Kim"
                    },
                    {
                        "name": "Kijae Hong"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    },
                    {
                        "name": "Anastasia Ailamaki"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Ailamaki"
                },
                "author": "Anastasia Ailamaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07447v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07447v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12859v1",
                "updated": "2024-11-19T21:04:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    21,
                    4,
                    53,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T21:04:53Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    21,
                    4,
                    53,
                    1,
                    324,
                    0
                ],
                "title": "The Game-Theoretic Symbiosis of Trust and AI in Networked Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Game-Theoretic Symbiosis of Trust and AI in Networked Systems"
                },
                "summary": "This chapter explores the symbiotic relationship between Artificial\nIntelligence (AI) and trust in networked systems, focusing on how these two\nelements reinforce each other in strategic cybersecurity contexts. AI's\ncapabilities in data processing, learning, and real-time response offer\nunprecedented support for managing trust in dynamic, complex networks. However,\nthe successful integration of AI also hinges on the trustworthiness of AI\nsystems themselves. Using a game-theoretic framework, this chapter presents\napproaches to trust evaluation, the strategic role of AI in cybersecurity, and\ngovernance frameworks that ensure responsible AI deployment. We investigate how\ntrust, when dynamically managed through AI, can form a resilient security\necosystem. By examining trust as both an AI output and an AI requirement, this\nchapter sets the foundation for a positive feedback loop where AI enhances\nnetwork security and the trust placed in AI systems fosters their adoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This chapter explores the symbiotic relationship between Artificial\nIntelligence (AI) and trust in networked systems, focusing on how these two\nelements reinforce each other in strategic cybersecurity contexts. AI's\ncapabilities in data processing, learning, and real-time response offer\nunprecedented support for managing trust in dynamic, complex networks. However,\nthe successful integration of AI also hinges on the trustworthiness of AI\nsystems themselves. Using a game-theoretic framework, this chapter presents\napproaches to trust evaluation, the strategic role of AI in cybersecurity, and\ngovernance frameworks that ensure responsible AI deployment. We investigate how\ntrust, when dynamically managed through AI, can form a resilient security\necosystem. By examining trust as both an AI output and an AI requirement, this\nchapter sets the foundation for a positive feedback loop where AI enhances\nnetwork security and the trust placed in AI systems fosters their adoption."
                },
                "authors": [
                    {
                        "name": "Yunfei Ge"
                    },
                    {
                        "name": "Quanyan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Quanyan Zhu"
                },
                "author": "Quanyan Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09834v2",
                "updated": "2024-11-19T21:04:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    21,
                    4,
                    38,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-14T22:54:38Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    22,
                    54,
                    38,
                    3,
                    319,
                    0
                ],
                "title": "A Benchmark for Long-Form Medical Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Benchmark for Long-Form Medical Question Answering"
                },
                "summary": "There is a lack of benchmarks for evaluating large language models (LLMs) in\nlong-form medical question answering (QA). Most existing medical QA evaluation\nbenchmarks focus on automatic metrics and multiple-choice questions. While\nvaluable, these benchmarks fail to fully capture or assess the complexities of\nreal-world clinical applications where LLMs are being deployed. Furthermore,\nexisting studies on evaluating long-form answer generation in medical QA are\nprimarily closed-source, lacking access to human medical expert annotations,\nwhich makes it difficult to reproduce results and enhance existing baselines.\nIn this work, we introduce a new publicly available benchmark featuring\nreal-world consumer medical questions with long-form answer evaluations\nannotated by medical doctors. We performed pairwise comparisons of responses\nfrom various open and closed-source medical and general-purpose LLMs based on\ncriteria such as correctness, helpfulness, harmfulness, and bias. Additionally,\nwe performed a comprehensive LLM-as-a-judge analysis to study the alignment\nbetween human judgments and LLMs. Our preliminary results highlight the strong\npotential of open LLMs in medical QA compared to leading closed models. Code &\nData: https://github.com/lavita-ai/medical-eval-sphere",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a lack of benchmarks for evaluating large language models (LLMs) in\nlong-form medical question answering (QA). Most existing medical QA evaluation\nbenchmarks focus on automatic metrics and multiple-choice questions. While\nvaluable, these benchmarks fail to fully capture or assess the complexities of\nreal-world clinical applications where LLMs are being deployed. Furthermore,\nexisting studies on evaluating long-form answer generation in medical QA are\nprimarily closed-source, lacking access to human medical expert annotations,\nwhich makes it difficult to reproduce results and enhance existing baselines.\nIn this work, we introduce a new publicly available benchmark featuring\nreal-world consumer medical questions with long-form answer evaluations\nannotated by medical doctors. We performed pairwise comparisons of responses\nfrom various open and closed-source medical and general-purpose LLMs based on\ncriteria such as correctness, helpfulness, harmfulness, and bias. Additionally,\nwe performed a comprehensive LLM-as-a-judge analysis to study the alignment\nbetween human judgments and LLMs. Our preliminary results highlight the strong\npotential of open LLMs in medical QA compared to leading closed models. Code &\nData: https://github.com/lavita-ai/medical-eval-sphere"
                },
                "authors": [
                    {
                        "name": "Pedram Hosseini"
                    },
                    {
                        "name": "Jessica M. Sin"
                    },
                    {
                        "name": "Bing Ren"
                    },
                    {
                        "name": "Bryceton G. Thomas"
                    },
                    {
                        "name": "Elnaz Nouri"
                    },
                    {
                        "name": "Ali Farahanchi"
                    },
                    {
                        "name": "Saeed Hassanpour"
                    }
                ],
                "author_detail": {
                    "name": "Saeed Hassanpour"
                },
                "author": "Saeed Hassanpour",
                "arxiv_comment": "AIM-FM: Advancements in Medical Foundation Models Workshop, 38th\n  Conference on Neural Information Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04118v2",
                "updated": "2024-11-19T20:51:58Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    20,
                    51,
                    58,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-06T18:51:02Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    18,
                    51,
                    2,
                    2,
                    311,
                    0
                ],
                "title": "Medical Adaptation of Large Language and Vision-Language Models: Are We\n  Making Progress?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Adaptation of Large Language and Vision-Language Models: Are We\n  Making Progress?"
                },
                "summary": "Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare seven\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting regime for medical question-answering (QA) tasks. For instance,\nacross the tasks and model pairs we consider in the 3-shot setting, medical\nLLMs only outperform their base models in 12.1% of cases, reach a (statistical)\ntie in 49.8% of cases, and are significantly worse than their base models in\nthe remaining 38.2% of cases. Our conclusions are based on (i) comparing each\nmedical model head-to-head, directly against the corresponding base model; (ii)\noptimizing the prompts for each model separately; and (iii) accounting for\nstatistical uncertainty in comparisons. While these basic practices are not\nconsistently adopted in the literature, our ablations show that they\nsubstantially impact conclusions. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare seven\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting regime for medical question-answering (QA) tasks. For instance,\nacross the tasks and model pairs we consider in the 3-shot setting, medical\nLLMs only outperform their base models in 12.1% of cases, reach a (statistical)\ntie in 49.8% of cases, and are significantly worse than their base models in\nthe remaining 38.2% of cases. Our conclusions are based on (i) comparing each\nmedical model head-to-head, directly against the corresponding base model; (ii)\noptimizing the prompts for each model separately; and (iii) accounting for\nstatistical uncertainty in comparisons. While these basic practices are not\nconsistently adopted in the literature, our ablations show that they\nsubstantially impact conclusions. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies."
                },
                "authors": [
                    {
                        "name": "Daniel P. Jeong"
                    },
                    {
                        "name": "Saurabh Garg"
                    },
                    {
                        "name": "Zachary C. Lipton"
                    },
                    {
                        "name": "Michael Oberst"
                    }
                ],
                "author_detail": {
                    "name": "Michael Oberst"
                },
                "author": "Michael Oberst",
                "arxiv_comment": "This version was published at EMNLP 2024 Main Conference as a Long\n  Paper (Oral). See the extended version (arXiv:2411.08870) for additional\n  results on QA tasks based on clinical notes and evaluations in the supervised\n  fine-tuning regime",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12843v1",
                "updated": "2024-11-19T20:17:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    20,
                    17,
                    4,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T20:17:04Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    20,
                    17,
                    4,
                    1,
                    324,
                    0
                ],
                "title": "Reward Modeling with Ordinal Feedback: Wisdom of the Crowd",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Modeling with Ordinal Feedback: Wisdom of the Crowd"
                },
                "summary": "Learning a reward model (RM) from human preferences has been an important\ncomponent in aligning large language models (LLMs). The canonical setup of\nlearning RMs from pairwise preference data is rooted in the classic\nBradley-Terry (BT) model that accepts binary feedback, i.e., the label being\neither Response 1 is better than Response 2, or the opposite. Such a setup\ninevitably discards potentially useful samples (such as \"tied\" between the two\nresponses) and loses more fine-grained information (such as \"slightly better\").\nIn this paper, we propose a framework for learning RMs under ordinal feedback\nwhich generalizes the case of binary preference feedback to any arbitrary\ngranularity. Specifically, we first identify a marginal unbiasedness condition,\nwhich generalizes the assumption of the BT model in the existing binary\nfeedback setting. The condition validates itself via the sociological concept\nof the wisdom of the crowd. Under the condition, we develop a natural\nprobability model for pairwise preference data under ordinal feedback and\nanalyze its properties. We prove the statistical benefits of ordinal feedback\nin terms of reducing the Rademacher complexity compared to the case of binary\nfeedback. The proposed learning objective and the theory also extend to hinge\nloss and direct policy optimization (DPO). In particular, the theoretical\nanalysis may be of independent interest when applying to a seemingly unrelated\nproblem of knowledge distillation to interpret the bias-variance trade-off\ntherein. The framework also sheds light on writing guidance for human\nannotators. Our numerical experiments validate that fine-grained feedback leads\nto better reward learning for both in-distribution and out-of-distribution\nsettings. Further experiments show that incorporating a certain proportion of\nsamples with tied preference boosts RM learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning a reward model (RM) from human preferences has been an important\ncomponent in aligning large language models (LLMs). The canonical setup of\nlearning RMs from pairwise preference data is rooted in the classic\nBradley-Terry (BT) model that accepts binary feedback, i.e., the label being\neither Response 1 is better than Response 2, or the opposite. Such a setup\ninevitably discards potentially useful samples (such as \"tied\" between the two\nresponses) and loses more fine-grained information (such as \"slightly better\").\nIn this paper, we propose a framework for learning RMs under ordinal feedback\nwhich generalizes the case of binary preference feedback to any arbitrary\ngranularity. Specifically, we first identify a marginal unbiasedness condition,\nwhich generalizes the assumption of the BT model in the existing binary\nfeedback setting. The condition validates itself via the sociological concept\nof the wisdom of the crowd. Under the condition, we develop a natural\nprobability model for pairwise preference data under ordinal feedback and\nanalyze its properties. We prove the statistical benefits of ordinal feedback\nin terms of reducing the Rademacher complexity compared to the case of binary\nfeedback. The proposed learning objective and the theory also extend to hinge\nloss and direct policy optimization (DPO). In particular, the theoretical\nanalysis may be of independent interest when applying to a seemingly unrelated\nproblem of knowledge distillation to interpret the bias-variance trade-off\ntherein. The framework also sheds light on writing guidance for human\nannotators. Our numerical experiments validate that fine-grained feedback leads\nto better reward learning for both in-distribution and out-of-distribution\nsettings. Further experiments show that incorporating a certain proportion of\nsamples with tied preference boosts RM learning."
                },
                "authors": [
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Guanting Chen"
                    },
                    {
                        "name": "Xiaocheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaocheng Li"
                },
                "author": "Xiaocheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17215v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17215v3",
                "updated": "2024-11-19T20:16:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    20,
                    16,
                    11,
                    1,
                    324,
                    0
                ],
                "published": "2024-06-25T02:05:26Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    2,
                    5,
                    26,
                    1,
                    177,
                    0
                ],
                "title": "Enabling Large Language Models to Perform Power System Simulations with\n  Previously Unseen Tools: A Case of Daline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Large Language Models to Perform Power System Simulations with\n  Previously Unseen Tools: A Case of Daline"
                },
                "summary": "The integration of experiment technologies with large language models (LLMs)\nis transforming scientific research, offering AI capabilities beyond\nspecialized problem-solving to becoming research assistants for human\nscientists. In power systems, simulations are essential for research. However,\nLLMs face significant challenges in power system simulations due to limited\npre-existing knowledge and the complexity of power grids. To address this\nissue, this work proposes a modular framework that integrates expertise from\nboth the power system and LLM domains. This framework enhances LLMs' ability to\nperform power system simulations on previously unseen tools. Validated using 34\nsimulation tasks in Daline, a (optimal) power flow simulation and linearization\ntoolbox not yet exposed to LLMs, the proposed framework improved GPT-4o's\nsimulation coding accuracy from 0% to 96.07%, also outperforming the ChatGPT-4o\nweb interface's 33.8% accuracy (with the entire knowledge base uploaded). These\nresults highlight the potential of LLMs as research assistants in power\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of experiment technologies with large language models (LLMs)\nis transforming scientific research, offering AI capabilities beyond\nspecialized problem-solving to becoming research assistants for human\nscientists. In power systems, simulations are essential for research. However,\nLLMs face significant challenges in power system simulations due to limited\npre-existing knowledge and the complexity of power grids. To address this\nissue, this work proposes a modular framework that integrates expertise from\nboth the power system and LLM domains. This framework enhances LLMs' ability to\nperform power system simulations on previously unseen tools. Validated using 34\nsimulation tasks in Daline, a (optimal) power flow simulation and linearization\ntoolbox not yet exposed to LLMs, the proposed framework improved GPT-4o's\nsimulation coding accuracy from 0% to 96.07%, also outperforming the ChatGPT-4o\nweb interface's 33.8% accuracy (with the entire knowledge base uploaded). These\nresults highlight the potential of LLMs as research assistants in power\nsystems."
                },
                "authors": [
                    {
                        "name": "Mengshuo Jia"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Gabriela Hug"
                    }
                ],
                "author_detail": {
                    "name": "Gabriela Hug"
                },
                "author": "Gabriela Hug",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17215v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17215v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12828v1",
                "updated": "2024-11-19T19:33:16Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    19,
                    33,
                    16,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T19:33:16Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    19,
                    33,
                    16,
                    1,
                    324,
                    0
                ],
                "title": "Probing the Capacity of Language Model Agents to Operationalize\n  Disparate Experiential Context Despite Distraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing the Capacity of Language Model Agents to Operationalize\n  Disparate Experiential Context Despite Distraction"
                },
                "summary": "Large language model (LLM) agents show promise in an increasing number of\ndomains. In many proposed applications, it is expected that the agent reasons\nover accumulated experience presented in an input prompt. We propose the OEDD\n(Operationalize Experience Despite Distraction) corpus, a\nhuman-annotator-validated body of scenarios with pre-scripted agent histories\nwhere the agent must make a decision based on disparate experiential\ninformation in the presence of a distractor. We evaluate three state-of-the-art\nLLMs (GPT-3.5 Turbo, GPT-4o, and Gemini 1.5 Pro) using a minimal\nchain-of-thought prompting strategy and observe that when (1) the input context\ncontains over 1,615 tokens of historical interactions, (2) a crucially\ndecision-informing premise is the rightful conclusion over two disparate\nenvironment premises, and (3) a trivial, but distracting red herring fact\nfollows, all LLMs perform worse than random choice at selecting the better of\ntwo actions. Our code and test corpus are publicly available at:\nhttps://github.com/sonnygeorge/OEDD .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents show promise in an increasing number of\ndomains. In many proposed applications, it is expected that the agent reasons\nover accumulated experience presented in an input prompt. We propose the OEDD\n(Operationalize Experience Despite Distraction) corpus, a\nhuman-annotator-validated body of scenarios with pre-scripted agent histories\nwhere the agent must make a decision based on disparate experiential\ninformation in the presence of a distractor. We evaluate three state-of-the-art\nLLMs (GPT-3.5 Turbo, GPT-4o, and Gemini 1.5 Pro) using a minimal\nchain-of-thought prompting strategy and observe that when (1) the input context\ncontains over 1,615 tokens of historical interactions, (2) a crucially\ndecision-informing premise is the rightful conclusion over two disparate\nenvironment premises, and (3) a trivial, but distracting red herring fact\nfollows, all LLMs perform worse than random choice at selecting the better of\ntwo actions. Our code and test corpus are publicly available at:\nhttps://github.com/sonnygeorge/OEDD ."
                },
                "authors": [
                    {
                        "name": "Sonny George"
                    },
                    {
                        "name": "Chris Sypherd"
                    },
                    {
                        "name": "Dylan Cashman"
                    }
                ],
                "author_detail": {
                    "name": "Dylan Cashman"
                },
                "author": "Dylan Cashman",
                "arxiv_journal_ref": "Findings Assoc. Comput. Linguistics: EMNLP 2024 15447-15459 (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12812v1",
                "updated": "2024-11-19T19:04:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    19,
                    4,
                    42,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T19:04:42Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    19,
                    4,
                    42,
                    1,
                    324,
                    0
                ],
                "title": "DIETS: Diabetic Insulin Management System in Everyday Life",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIETS: Diabetic Insulin Management System in Everyday Life"
                },
                "summary": "People with diabetes need insulin delivery to effectively manage their blood\nglucose levels, especially after meals, because their bodies either do not\nproduce enough insulin or cannot fully utilize it. Accurate insulin delivery\nstarts with estimating the nutrients in meals and is followed by developing a\ndetailed, personalized insulin injection strategy. These tasks are particularly\nchallenging in daily life, especially without professional guidance. Existing\nsolutions usually assume the prior knowledge of nutrients in meals and\nprimarily rely on feedback from professional clinicians or simulators to\ndevelop Reinforcement Learning-based models for insulin management, leading to\nextensive consumption of medical resources and difficulties in adapting the\nmodels to new patients due to individual differences. In this paper, we propose\nDIETS, a novel diabetic insulin management framework built on the transformer\narchitecture, to help people with diabetes effectively manage insulin delivery\nin everyday life. Specifically, DIETS tailors a Large Language Model (LLM) to\nestimate the nutrients in meals and employs a titration model to generate\nrecommended insulin injection strategies, which are further validated by a\nglucose prediction model to prevent potential risks of hyperglycemia or\nhypoglycemia. DIETS has been extensively evaluated on three public datasets,\nand the results show it achieves superior performance in providing effective\ninsulin delivery recommendation to control blood glucose levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People with diabetes need insulin delivery to effectively manage their blood\nglucose levels, especially after meals, because their bodies either do not\nproduce enough insulin or cannot fully utilize it. Accurate insulin delivery\nstarts with estimating the nutrients in meals and is followed by developing a\ndetailed, personalized insulin injection strategy. These tasks are particularly\nchallenging in daily life, especially without professional guidance. Existing\nsolutions usually assume the prior knowledge of nutrients in meals and\nprimarily rely on feedback from professional clinicians or simulators to\ndevelop Reinforcement Learning-based models for insulin management, leading to\nextensive consumption of medical resources and difficulties in adapting the\nmodels to new patients due to individual differences. In this paper, we propose\nDIETS, a novel diabetic insulin management framework built on the transformer\narchitecture, to help people with diabetes effectively manage insulin delivery\nin everyday life. Specifically, DIETS tailors a Large Language Model (LLM) to\nestimate the nutrients in meals and employs a titration model to generate\nrecommended insulin injection strategies, which are further validated by a\nglucose prediction model to prevent potential risks of hyperglycemia or\nhypoglycemia. DIETS has been extensively evaluated on three public datasets,\nand the results show it achieves superior performance in providing effective\ninsulin delivery recommendation to control blood glucose levels."
                },
                "authors": [
                    {
                        "name": "Hanyu Zeng"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Pengfei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Zhou"
                },
                "author": "Pengfei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12808v1",
                "updated": "2024-11-19T19:00:31Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    19,
                    0,
                    31,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T19:00:31Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    19,
                    0,
                    31,
                    1,
                    324,
                    0
                ],
                "title": "Conversational Medical AI: Ready for Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Medical AI: Ready for Practice"
                },
                "summary": "The shortage of doctors is creating a critical squeeze in access to medical\nexpertise. While conversational Artificial Intelligence (AI) holds promise in\naddressing this problem, its safe deployment in patient-facing roles remains\nlargely unexplored in real-world medical settings. We present the first\nlarge-scale evaluation of a physician-supervised LLM-based conversational agent\nin a real-world medical setting.\n  Our agent, Mo, was integrated into an existing medical advice chat service.\nOver a three-week period, we conducted a randomized controlled experiment with\n926 cases to evaluate patient experience and satisfaction. Among these, Mo\nhandled 298 complete patient interactions, for which we report\nphysician-assessed measures of safety and medical accuracy.\n  Patients reported higher clarity of information (3.73 vs 3.62 out of 4, p <\n0.05) and overall satisfaction (4.58 vs 4.42 out of 5, p < 0.05) with\nAI-assisted conversations compared to standard care, while showing equivalent\nlevels of trust and perceived empathy. The high opt-in rate (81% among\nrespondents) exceeded previous benchmarks for AI acceptance in healthcare.\nPhysician oversight ensured safety, with 95% of conversations rated as \"good\"\nor \"excellent\" by general practitioners experienced in operating a medical\nadvice chat service.\n  Our findings demonstrate that carefully implemented AI medical assistants can\nenhance patient experience while maintaining safety standards through physician\nsupervision. This work provides empirical evidence for the feasibility of AI\ndeployment in healthcare communication and insights into the requirements for\nsuccessful integration into existing healthcare services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The shortage of doctors is creating a critical squeeze in access to medical\nexpertise. While conversational Artificial Intelligence (AI) holds promise in\naddressing this problem, its safe deployment in patient-facing roles remains\nlargely unexplored in real-world medical settings. We present the first\nlarge-scale evaluation of a physician-supervised LLM-based conversational agent\nin a real-world medical setting.\n  Our agent, Mo, was integrated into an existing medical advice chat service.\nOver a three-week period, we conducted a randomized controlled experiment with\n926 cases to evaluate patient experience and satisfaction. Among these, Mo\nhandled 298 complete patient interactions, for which we report\nphysician-assessed measures of safety and medical accuracy.\n  Patients reported higher clarity of information (3.73 vs 3.62 out of 4, p <\n0.05) and overall satisfaction (4.58 vs 4.42 out of 5, p < 0.05) with\nAI-assisted conversations compared to standard care, while showing equivalent\nlevels of trust and perceived empathy. The high opt-in rate (81% among\nrespondents) exceeded previous benchmarks for AI acceptance in healthcare.\nPhysician oversight ensured safety, with 95% of conversations rated as \"good\"\nor \"excellent\" by general practitioners experienced in operating a medical\nadvice chat service.\n  Our findings demonstrate that carefully implemented AI medical assistants can\nenhance patient experience while maintaining safety standards through physician\nsupervision. This work provides empirical evidence for the feasibility of AI\ndeployment in healthcare communication and insights into the requirements for\nsuccessful integration into existing healthcare services."
                },
                "authors": [
                    {
                        "name": "Antoine LizÃ©e"
                    },
                    {
                        "name": "Pierre-Auguste BeaucotÃ©"
                    },
                    {
                        "name": "James Whitbeck"
                    },
                    {
                        "name": "Marion Doumeingts"
                    },
                    {
                        "name": "AnaÃ«l Beaugnon"
                    },
                    {
                        "name": "Isabelle Feldhaus"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Feldhaus"
                },
                "author": "Isabelle Feldhaus",
                "arxiv_comment": "14 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11844v2",
                "updated": "2024-11-19T18:59:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    59,
                    42,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-18T18:59:31Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    31,
                    0,
                    323,
                    0
                ],
                "title": "Generative World Explorer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative World Explorer"
                },
                "summary": "Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate. In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planning with partial observation is a central challenge in embodied AI. A\nmajority of prior works have tackled this challenge by developing agents that\nphysically explore their environment to update their beliefs about the world\nstate. In contrast, humans can $\\textit{imagine}$ unseen parts of the world\nthrough a mental exploration and $\\textit{revise}$ their beliefs with imagined\nobservations. Such updated beliefs can allow them to make more informed\ndecisions, without necessitating the physical exploration of the world at all\ntimes. To achieve this human-like ability, we introduce the $\\textit{Generative\nWorld Explorer (Genex)}$, an egocentric world exploration framework that allows\nan agent to mentally explore a large-scale 3D world (e.g., urban scenes) and\nacquire imagined observations to update its belief. This updated belief will\nthen help the agent to make a more informed decision at the current step. To\ntrain $\\textit{Genex}$, we create a synthetic urban scene dataset, Genex-DB.\nOur experimental results demonstrate that (1) $\\textit{Genex}$ can generate\nhigh-quality and consistent observations during long-horizon exploration of a\nlarge virtual physical world and (2) the beliefs updated with the generated\nobservations can inform an existing decision-making model (e.g., an LLM agent)\nto make better plans."
                },
                "authors": [
                    {
                        "name": "Taiming Lu"
                    },
                    {
                        "name": "Tianmin Shu"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Jieneng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jieneng Chen"
                },
                "author": "Jieneng Chen",
                "arxiv_comment": "Website: generative-world-explorer.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12736v1",
                "updated": "2024-11-19T18:58:03Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    58,
                    3,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T18:58:03Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    58,
                    3,
                    1,
                    324,
                    0
                ],
                "title": "ACING: Actor-Critic for Instruction Learning in Black-Box Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACING: Actor-Critic for Instruction Learning in Black-Box Large Language\n  Models"
                },
                "summary": "The effectiveness of Large Language Models (LLMs) in solving tasks vastly\ndepends on the quality of the instructions, which often require fine-tuning\nthrough extensive human effort. This highlights the need for automated\ninstruction optimization; however, this optimization is particularly\nchallenging when dealing with black-box LLMs, where model parameters and\ngradients remain inaccessible. We propose ACING, a task-specific prompt\noptimization approach framed as a stateless continuous-action Reinforcement\nLearning (RL) problem, known as the continuum bandit setting. ACING leverages\nan actor-critic-based method to optimize prompts, learning from\nnon-differentiable reward signals. We validate ACING by optimizing prompts for\nChatGPT on 30 instruction-based tasks. ACING consistently outperforms baseline\nmethods, achieving a median score improvement of 10 percentage points.\nFurthermore, ACING not only recovers but also surpasses human-crafted expert\ninstructions, achieving up to a 39 percentage point improvement against human\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effectiveness of Large Language Models (LLMs) in solving tasks vastly\ndepends on the quality of the instructions, which often require fine-tuning\nthrough extensive human effort. This highlights the need for automated\ninstruction optimization; however, this optimization is particularly\nchallenging when dealing with black-box LLMs, where model parameters and\ngradients remain inaccessible. We propose ACING, a task-specific prompt\noptimization approach framed as a stateless continuous-action Reinforcement\nLearning (RL) problem, known as the continuum bandit setting. ACING leverages\nan actor-critic-based method to optimize prompts, learning from\nnon-differentiable reward signals. We validate ACING by optimizing prompts for\nChatGPT on 30 instruction-based tasks. ACING consistently outperforms baseline\nmethods, achieving a median score improvement of 10 percentage points.\nFurthermore, ACING not only recovers but also surpasses human-crafted expert\ninstructions, achieving up to a 39 percentage point improvement against human\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Salma Kharrat"
                    },
                    {
                        "name": "Fares Fourati"
                    },
                    {
                        "name": "Marco Canini"
                    }
                ],
                "author_detail": {
                    "name": "Marco Canini"
                },
                "author": "Marco Canini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.12712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.12712v1",
                "updated": "2024-11-19T18:27:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    27,
                    25,
                    1,
                    324,
                    0
                ],
                "published": "2024-11-19T18:27:25Z",
                "published_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    27,
                    25,
                    1,
                    324,
                    0
                ],
                "title": "Enhancing Multi-Class Disease Classification: Neoplasms, Cardiovascular,\n  Nervous System, and Digestive Disorders Using Advanced LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multi-Class Disease Classification: Neoplasms, Cardiovascular,\n  Nervous System, and Digestive Disorders Using Advanced LLMs"
                },
                "summary": "In this research, we explored the improvement in terms of multi-class disease\nclassification via pre-trained language models over Medical-Abstracts-TC-Corpus\nthat spans five medical conditions. We excluded non-cancer conditions and\nexamined four specific diseases. We assessed four LLMs, BioBERT, XLNet, and\nBERT, as well as a novel base model (Last-BERT). BioBERT, which was pre-trained\non medical data, demonstrated superior performance in medical text\nclassification (97% accuracy). Surprisingly, XLNet followed closely (96%\naccuracy), demonstrating its generalizability across domains even though it was\nnot pre-trained on medical data. LastBERT, a custom model based on the lighter\nversion of BERT, also proved competitive with 87.10% accuracy (just under\nBERT's 89.33%). Our findings confirm the importance of specialized models such\nas BioBERT and also support impressions around more general solutions like\nXLNet and well-tuned transformer architectures with fewer parameters (in this\ncase, LastBERT) in medical domain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this research, we explored the improvement in terms of multi-class disease\nclassification via pre-trained language models over Medical-Abstracts-TC-Corpus\nthat spans five medical conditions. We excluded non-cancer conditions and\nexamined four specific diseases. We assessed four LLMs, BioBERT, XLNet, and\nBERT, as well as a novel base model (Last-BERT). BioBERT, which was pre-trained\non medical data, demonstrated superior performance in medical text\nclassification (97% accuracy). Surprisingly, XLNet followed closely (96%\naccuracy), demonstrating its generalizability across domains even though it was\nnot pre-trained on medical data. LastBERT, a custom model based on the lighter\nversion of BERT, also proved competitive with 87.10% accuracy (just under\nBERT's 89.33%). Our findings confirm the importance of specialized models such\nas BioBERT and also support impressions around more general solutions like\nXLNet and well-tuned transformer architectures with fewer parameters (in this\ncase, LastBERT) in medical domain tasks."
                },
                "authors": [
                    {
                        "name": "Ahmed Akib Jawad Karim"
                    },
                    {
                        "name": "Muhammad Zawad Mahmud"
                    },
                    {
                        "name": "Samiha Islam"
                    },
                    {
                        "name": "Aznur Azam"
                    }
                ],
                "author_detail": {
                    "name": "Aznur Azam"
                },
                "author": "Aznur Azam",
                "arxiv_comment": "7 Pages, 4 tables and 11 figures. Under review in a IEEE conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.12712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.12712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01306v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01306v4",
                "updated": "2024-11-19T18:12:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    19,
                    18,
                    12,
                    45,
                    1,
                    324,
                    0
                ],
                "published": "2024-02-02T10:53:36Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    10,
                    53,
                    36,
                    4,
                    33,
                    0
                ],
                "title": "KTO: Model Alignment as Prospect Theoretic Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KTO: Model Alignment as Prospect Theoretic Optimization"
                },
                "summary": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive\nrandom variables in a biased but well-defined manner (1992); for example,\nhumans are famously loss-averse. We show that objectives for aligning LLMs with\nhuman feedback implicitly incorporate many of these biases -- the success of\nthese objectives (e.g., DPO) over cross-entropy minimization can partly be\nascribed to them belonging to a family of loss functions that we call\n$\\textit{human-aware losses}$ (HALOs). However, the utility functions these\nmethods attribute to humans still differ from those in the prospect theory\nliterature. Using a Kahneman-Tversky model of human utility, we propose a HALO\nthat directly maximizes the utility of generations instead of maximizing the\nlog-likelihood of preferences, as current methods do. We call this approach\nKTO, and it matches or exceeds the performance of preference-based methods at\nscales from 1B to 30B, despite only learning from a binary signal of whether an\noutput is desirable. More broadly, our work suggests that there is no one HALO\nthat is universally superior; the best loss depends on the inductive biases\nmost appropriate for a given setting, an oft-overlooked consideration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kahneman & Tversky's $\\textit{prospect theory}$ tells us that humans perceive\nrandom variables in a biased but well-defined manner (1992); for example,\nhumans are famously loss-averse. We show that objectives for aligning LLMs with\nhuman feedback implicitly incorporate many of these biases -- the success of\nthese objectives (e.g., DPO) over cross-entropy minimization can partly be\nascribed to them belonging to a family of loss functions that we call\n$\\textit{human-aware losses}$ (HALOs). However, the utility functions these\nmethods attribute to humans still differ from those in the prospect theory\nliterature. Using a Kahneman-Tversky model of human utility, we propose a HALO\nthat directly maximizes the utility of generations instead of maximizing the\nlog-likelihood of preferences, as current methods do. We call this approach\nKTO, and it matches or exceeds the performance of preference-based methods at\nscales from 1B to 30B, despite only learning from a binary signal of whether an\noutput is desirable. More broadly, our work suggests that there is no one HALO\nthat is universally superior; the best loss depends on the inductive biases\nmost appropriate for a given setting, an oft-overlooked consideration."
                },
                "authors": [
                    {
                        "name": "Kawin Ethayarajh"
                    },
                    {
                        "name": "Winnie Xu"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Douwe Kiela"
                    }
                ],
                "author_detail": {
                    "name": "Douwe Kiela"
                },
                "author": "Douwe Kiela",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01306v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01306v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]