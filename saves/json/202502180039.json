[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.10389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10389v1",
                "updated": "2025-02-14T18:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "title": "Region-Adaptive Sampling for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Region-Adaptive Sampling for Diffusion Transformers"
                },
                "summary": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yiqi Zhang"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Yuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Yang"
                },
                "author": "Yuqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v2",
                "updated": "2025-02-14T17:17:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    17,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_doi": "10.1145/3701716.3715192",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715192",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.09057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by WWW 2025 (Demo Track)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10220v1",
                "updated": "2025-02-14T15:14:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:14:53Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "title": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem"
                },
                "summary": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network."
                },
                "authors": [
                    {
                        "name": "Hugo Rodrigues de Brito"
                    },
                    {
                        "name": "Daniel Simon Baltensperger"
                    },
                    {
                        "name": "Kjetil Obstfelder Uhlen"
                    }
                ],
                "author_detail": {
                    "name": "Kjetil Obstfelder Uhlen"
                },
                "author": "Kjetil Obstfelder Uhlen",
                "arxiv_comment": "11 pages, 8 figures, CIGRE Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v1",
                "updated": "2025-02-14T13:55:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hölscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "João Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jörg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jürgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Teich"
                },
                "author": "Jürgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09921v1",
                "updated": "2025-02-14T05:19:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T05:19:46Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "title": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing"
                },
                "summary": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption."
                },
                "authors": [
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Siung Noh"
                    },
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaewon Jung"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v1",
                "updated": "2025-02-14T03:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law"
                },
                "summary": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Fangjian Li"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09726v1",
                "updated": "2025-02-13T19:16:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:16:39Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "title": "Analysis of Robust and Secure DNS Protocols for IoT Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Robust and Secure DNS Protocols for IoT Devices"
                },
                "summary": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices."
                },
                "authors": [
                    {
                        "name": "Abdullah Aydeger"
                    },
                    {
                        "name": "Sanzida Hoque"
                    },
                    {
                        "name": "Engin Zeydan"
                    },
                    {
                        "name": "Kapal Dev"
                    }
                ],
                "author_detail": {
                    "name": "Kapal Dev"
                },
                "author": "Kapal Dev",
                "arxiv_comment": "6 pages, 2 tables, 2 figures. This paper has been accepted in the\n  2025 IEEE International Conference on Communications (ICC): SAC Cloud\n  Computing, Networking, and Storage Track. The final version will be published\n  in the IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v1",
                "updated": "2025-02-13T19:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v2",
                "updated": "2025-02-13T18:07:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    7,
                    4,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09541v1",
                "updated": "2025-02-13T17:57:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:57:05Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "title": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics"
                },
                "summary": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline."
                },
                "authors": [
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Advait Iyer"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09383v1",
                "updated": "2025-02-13T14:59:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    59,
                    3,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T14:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    59,
                    3,
                    3,
                    44,
                    0
                ],
                "title": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic"
                },
                "summary": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyzis to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyzis to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality."
                },
                "authors": [
                    {
                        "name": "Naomi Muggleton"
                    },
                    {
                        "name": "Charles Rahal"
                    },
                    {
                        "name": "Aaron Reeves"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Reeves"
                },
                "author": "Aaron Reeves",
                "arxiv_doi": "10.1007/s42001-025-00360-4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s42001-025-00360-4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Computational Social Science, 8(2), 1-29 (2025)",
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v2",
                "updated": "2025-02-13T12:54:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    54,
                    36,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v1",
                "updated": "2025-02-13T06:44:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "arxiv_affiliation": "Katie",
                "author": "Mingyi Hong",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08982v1",
                "updated": "2025-02-13T05:40:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T05:40:28Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "title": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory"
                },
                "summary": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Minghao Xie"
                    },
                    {
                        "name": "Shouqian Shi"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Heiner Litz"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "arxiv_doi": "10.14778/3705829.3705849",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3705829.3705849",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.08982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "PVLDB, 18(2): 335-348, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08910v1",
                "updated": "2025-02-13T02:52:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T02:52:01Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU"
                },
                "summary": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02690v2",
                "updated": "2025-02-12T14:32:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2024-04-03T12:37:34Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    12,
                    37,
                    34,
                    2,
                    94,
                    0
                ],
                "title": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse"
                },
                "summary": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths."
                },
                "authors": [
                    {
                        "name": "Yichuan Deng"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chiwun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chiwun Yang"
                },
                "author": "Chiwun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05431v2",
                "updated": "2025-02-12T13:54:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    54,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-08T03:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding"
                },
                "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08363v1",
                "updated": "2025-02-12T12:50:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:50:15Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "title": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding"
                },
                "summary": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores."
                },
                "authors": [
                    {
                        "name": "Konstantin Berestizshevsky"
                    },
                    {
                        "name": "Renzo Andri"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "arxiv_comment": "8 pages, 11 figures, work under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v2",
                "updated": "2025-02-12T11:05:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    5,
                    5,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v3",
                "updated": "2025-02-12T07:02:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    2,
                    6,
                    2,
                    43,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07903v1",
                "updated": "2025-02-11T19:17:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T19:17:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment"
                },
                "summary": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget."
                },
                "authors": [
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v1",
                "updated": "2025-02-11T18:58:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v2",
                "updated": "2025-02-11T18:45:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    45,
                    12,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v2",
                "updated": "2025-02-11T17:48:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    48,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v2",
                "updated": "2025-02-11T17:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    36,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference"
                },
                "summary": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07861v1",
                "updated": "2025-02-11T17:18:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:18:17Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "title": "BalanceKV: KV Cache Compression through Discrepancy Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BalanceKV: KV Cache Compression through Discrepancy Theory"
                },
                "summary": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Michael Kapralov"
                    },
                    {
                        "name": "Ekaterina Kochetkova"
                    },
                    {
                        "name": "Kshiteej Sheth"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03736v3",
                "updated": "2025-02-11T15:42:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    42,
                    19,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-06T04:22:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    4,
                    22,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data"
                },
                "summary": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD."
                },
                "authors": [
                    {
                        "name": "Jingyang Ou"
                    },
                    {
                        "name": "Shen Nie"
                    },
                    {
                        "name": "Kaiwen Xue"
                    },
                    {
                        "name": "Fengqi Zhu"
                    },
                    {
                        "name": "Jiacheng Sun"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v1",
                "updated": "2025-02-11T14:25:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v2",
                "updated": "2025-02-10T18:34:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    34,
                    53,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v2",
                "updated": "2025-02-10T17:19:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    19,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v2",
                "updated": "2025-02-10T15:17:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    17,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06327v1",
                "updated": "2025-02-10T10:28:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T10:28:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Prompt-Driven Continual Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Driven Continual Graph Learning"
                },
                "summary": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Tianfei Zhou"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Rui Mao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Mao"
                },
                "author": "Rui Mao",
                "arxiv_comment": "12 pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06166v1",
                "updated": "2025-02-10T05:33:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T05:33:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators"
                },
                "summary": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices."
                },
                "authors": [
                    {
                        "name": "Qi Shao"
                    },
                    {
                        "name": "Xin-Jun Liu"
                    },
                    {
                        "name": "Huichan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Huichan Zhao"
                },
                "author": "Huichan Zhao",
                "arxiv_comment": "7 pages, 10 figures, accepted by ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v2",
                "updated": "2025-02-09T20:52:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    52,
                    26,
                    6,
                    40,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_doi": "10.1103/PhysRevD.111.032007",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.032007",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.04603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 14 figures",
                "arxiv_journal_ref": "Phys. Rev. D 111, 032007 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06901v1",
                "updated": "2025-02-09T20:02:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T20:02:05Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "title": "Enabling Autoregressive Models to Fill In Masked Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Autoregressive Models to Fill In Masked Tokens"
                },
                "summary": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Guy Van den Broeck"
                },
                "author": "Guy Van den Broeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05960v1",
                "updated": "2025-02-09T17:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T17:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "title": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4"
                },
                "summary": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Ping Liu"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05859v1",
                "updated": "2025-02-09T11:36:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T11:36:45Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "title": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion"
                },
                "summary": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image."
                },
                "authors": [
                    {
                        "name": "Qingsong Yan"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Kaiyong Zhao"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Fei Deng"
                    }
                ],
                "author_detail": {
                    "name": "Fei Deng"
                },
                "author": "Fei Deng",
                "arxiv_comment": "3DV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05763v1",
                "updated": "2025-02-09T03:49:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T03:49:52Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "title": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay"
                },
                "summary": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance."
                },
                "authors": [
                    {
                        "name": "Nicholas Kernan"
                    },
                    {
                        "name": "Joey Li"
                    },
                    {
                        "name": "Rami Al-Dalky"
                    },
                    {
                        "name": "Michael Rabinovich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Rabinovich"
                },
                "author": "Michael Rabinovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v2",
                "updated": "2025-02-08T21:44:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    21,
                    44,
                    24,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Selçuk Köse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v2",
                "updated": "2025-02-08T14:11:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    14,
                    11,
                    25,
                    5,
                    39,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v2",
                "updated": "2025-02-08T11:51:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    11,
                    51,
                    57,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05511v1",
                "updated": "2025-02-08T10:14:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T10:14:21Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "title": "New and Improved Bounds for Markov Paging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New and Improved Bounds for Markov Paging"
                },
                "summary": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown."
                },
                "authors": [
                    {
                        "name": "Chirag Pabbaraju"
                    },
                    {
                        "name": "Ali Vakilian"
                    }
                ],
                "author_detail": {
                    "name": "Ali Vakilian"
                },
                "author": "Ali Vakilian",
                "arxiv_comment": "26 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05433v1",
                "updated": "2025-02-08T03:46:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:46:28Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "title": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection"
                },
                "summary": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow."
                },
                "authors": [
                    {
                        "name": "Shuheng Zhang"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "Hongbo Zhou"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05429v1",
                "updated": "2025-02-08T03:35:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:35:55Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "title": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts"
                },
                "summary": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats."
                },
                "authors": [
                    {
                        "name": "Seonghun Son"
                    },
                    {
                        "name": "Daniel Moghimi"
                    },
                    {
                        "name": "Berk Gulmezoglu"
                    }
                ],
                "author_detail": {
                    "name": "Berk Gulmezoglu"
                },
                "author": "Berk Gulmezoglu",
                "arxiv_doi": "10.1145/3676641.3716274",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716274",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.05429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Proceedings of the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS) accepted",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12304v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12304v4",
                "updated": "2025-02-07T23:14:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    23,
                    14,
                    10,
                    4,
                    38,
                    0
                ],
                "published": "2024-05-20T18:11:45Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    18,
                    11,
                    45,
                    0,
                    141,
                    0
                ],
                "title": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach"
                },
                "summary": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3711847",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711847",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.12304v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12304v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05370v1",
                "updated": "2025-02-07T22:51:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T22:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving"
                },
                "summary": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Xingqi Cui"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v2",
                "updated": "2025-02-07T22:00:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    0,
                    48,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04923v1",
                "updated": "2025-02-07T13:41:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:41:51Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "title": "Cached Multi-Lora Composition for Multi-Concept Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Multi-Lora Composition for Multi-Concept Image Generation"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch."
                },
                "authors": [
                    {
                        "name": "Xiandong Zou"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Christos-Savvas Bouganis"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v2",
                "updated": "2025-02-07T13:09:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    9,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "arxiv_comment": "AAAI 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v1",
                "updated": "2025-02-07T08:48:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v2",
                "updated": "2025-02-06T20:26:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    20,
                    26,
                    24,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v1",
                "updated": "2025-02-06T15:26:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v1",
                "updated": "2025-02-06T13:41:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v4",
                "updated": "2025-02-06T12:32:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    32,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v1",
                "updated": "2025-02-06T12:19:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keon Vin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01449v2",
                "updated": "2025-02-06T08:36:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-03T15:38:53Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    38,
                    53,
                    0,
                    34,
                    0
                ],
                "title": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies"
                },
                "summary": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs."
                },
                "authors": [
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Benigna Bruggmann"
                    },
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03805v1",
                "updated": "2025-02-06T06:31:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T06:31:47Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "title": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective"
                },
                "summary": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v1",
                "updated": "2025-02-06T04:16:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04393v1",
                "updated": "2025-02-06T03:56:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T03:56:11Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "title": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation"
                },
                "summary": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Wenzhang Sun"
                    },
                    {
                        "name": "Qirui Hou"
                    },
                    {
                        "name": "Donglin Di"
                    },
                    {
                        "name": "Jiahui Yang"
                    },
                    {
                        "name": "Yongjia Ma"
                    },
                    {
                        "name": "Jianxun Cui"
                    }
                ],
                "author_detail": {
                    "name": "Jianxun Cui"
                },
                "author": "Jianxun Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v2",
                "updated": "2025-02-06T03:16:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    16,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v2",
                "updated": "2025-02-05T22:55:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    22,
                    55,
                    47,
                    2,
                    36,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v2",
                "updated": "2025-02-05T21:44:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    21,
                    44,
                    56,
                    2,
                    36,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache"
                },
                "summary": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03589v1",
                "updated": "2025-02-05T20:09:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T20:09:51Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "title": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference"
                },
                "summary": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Shay Vargaftik"
                    },
                    {
                        "name": "Ran Ben Basat"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v2",
                "updated": "2025-02-05T09:35:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    35,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v2",
                "updated": "2025-02-05T08:22:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    22,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "Accepted to NAACL2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v2",
                "updated": "2025-02-05T08:10:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    10,
                    45,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring"
                },
                "summary": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02818v1",
                "updated": "2025-02-05T01:36:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T01:36:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL"
                },
                "summary": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments."
                },
                "authors": [
                    {
                        "name": "Wenbo Sun"
                    },
                    {
                        "name": "Qiming Guo"
                    },
                    {
                        "name": "Wenlu Wang"
                    },
                    {
                        "name": "Rihan Hai"
                    }
                ],
                "author_detail": {
                    "name": "Rihan Hai"
                },
                "author": "Rihan Hai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02750v1",
                "updated": "2025-02-04T22:37:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T22:37:17Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "title": "Cache is King: Smart Page Eviction with eBPF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache is King: Smart Page Eviction with eBPF"
                },
                "summary": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency."
                },
                "authors": [
                    {
                        "name": "Tal Zussman"
                    },
                    {
                        "name": "Ioannis Zarkadas"
                    },
                    {
                        "name": "Jeremy Carin"
                    },
                    {
                        "name": "Andrew Cheng"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Jonas Pfefferle"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02564v1",
                "updated": "2025-02-04T18:39:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:39:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing"
                },
                "summary": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%."
                },
                "authors": [
                    {
                        "name": "Atiyeh Javaheri"
                    },
                    {
                        "name": "Ali Bohlooli"
                    },
                    {
                        "name": "Kamal Jamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Kamal Jamshidi"
                },
                "author": "Kamal Jamshidi",
                "arxiv_comment": "18 pages, 14 figures, submit to Digital Communications and Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v2",
                "updated": "2025-02-04T17:14:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    14,
                    22,
                    1,
                    35,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Oral Presentation at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02493v1",
                "updated": "2025-02-04T17:09:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T17:09:21Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    9,
                    21,
                    1,
                    35,
                    0
                ],
                "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU\n  Utilization"
                },
                "summary": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is an effective and lossless method for Large Language\nModel (LLM) inference acceleration. It employs a smaller model to generate a\ndraft token sequence, which is then verified by the original base model. In\nmulti-GPU systems, inference latency can be further reduced through tensor\nparallelism (TP), while the optimal TP size of the draft model is typically\nsmaller than that of the base model, leading to GPU idling during the drafting\nstage. To solve this problem, we propose EasySpec, a layer-parallel speculation\nstrategy that optimizes the efficiency of multi-GPU utilization.EasySpec breaks\nthe sequential execution order of layers in the drafting model, enabling\nmulti-layer parallelization across devices, albeit with some induced\napproximation errors. After each drafting-and-verification iteration, the draft\nmodel's key-value (KV) cache is calibrated in a single forward pass, preventing\nlong-term error accumulation at minimal additional latency. We evaluated\nEasySpec on several mainstream open-source LLMs, using smaller versions of\nmodels from the same series as drafters. The results demonstrate that EasySpec\ncan achieve a peak speedup of 4.17x compared to vanilla decoding, while\npreserving the original distribution of the base LLMs. Specifically, the\ndrafting stage can be accelerated by up to 1.62x with a maximum accuracy drop\nof only 7%, requiring no training or fine-tuning on the draft models."
                },
                "authors": [
                    {
                        "name": "Yize Wu"
                    },
                    {
                        "name": "Ke Gao"
                    },
                    {
                        "name": "Yanjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Wu"
                },
                "author": "Yanjun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02437v1",
                "updated": "2025-02-04T16:03:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T16:03:52Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    16,
                    3,
                    52,
                    1,
                    35,
                    0
                ],
                "title": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H-MBR: Hypervisor-level Memory Bandwidth Reservation for Mixed\n  Criticality Systems"
                },
                "summary": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in fields such as automotive and aerospace have driven a\ngrowing demand for robust computational resources. Applications that were once\ndesigned for basic MCUs are now deployed on highly heterogeneous SoC platforms.\nWhile these platforms deliver the necessary computational performance, they\nalso present challenges related to resource sharing and predictability. These\nchallenges are particularly pronounced when consolidating safety and\nnon-safety-critical systems, the so-called Mixed-Criticality Systems (MCS) to\nadhere to strict SWaP-C requirements. MCS consolidation on shared platforms\nrequires stringent spatial and temporal isolation to comply with functional\nsafety standards. Virtualization, mainly leveraged by hypervisors, is a key\ntechnology that ensures spatial isolation across multiple OSes and\napplications; however, ensuring temporal isolation remains challenging due to\ncontention on shared hardwar resources, which impacts real-time performance and\npredictability. To mitigate this problem, several strategies as cache coloring\nand memory bandwidth reservation have been proposed. Although cache coloring is\ntypically implemented on state-of-the-art hypervisors, memory bandwidth\nreservation approaches are commonly implemented at the Linux kernel level or\nrely on dedicated hardware and typically do not consider the concept of VMs\nthat can run different OSes. To fill the gap between current memory bandwidth\nreservation solutions and the deployment of MCSs that operate on a hypervisor,\nthis work introduces H-MBR, an open-source VM-centric memory bandwidth\nreservation mechanism. H-MBR features (i) VM-centric bandwidth reservation,\n(ii) OS and platform agnosticism, and (iii) reduced overhead. Empirical results\nevidenced no overhead on non-regulated workloads, and negligible overhead (<1%)\nfor regulated workloads for regulation periods of 2 us or higher."
                },
                "authors": [
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonçalo Moreira"
                    },
                    {
                        "name": "José Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02430v1",
                "updated": "2025-02-04T15:55:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T15:55:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    55,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable Crawling Algorithm Utilizing Noisy Change-Indicating Signals"
                },
                "summary": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web refresh crawling is the problem of keeping a cache of web pages fresh,\nthat is, having the most recent copy available when a page is requested, given\na limited bandwidth available to the crawler. Under the assumption that the\nchange and request events, resp., to each web page follow independent Poisson\nprocesses, the optimal scheduling policy was derived by Azar et al. 2018. In\nthis paper, we study an extension of this problem where side information\nindicating content changes, such as various types of web pings, for example,\nsignals from sitemaps, content delivery networks, etc., is available.\nIncorporating such side information into the crawling policy is challenging,\nbecause (i) the signals can be noisy with false positive events and with\nmissing change events; and (ii) the crawler should achieve a fair performance\nover web pages regardless of the quality of the side information, which might\ndiffer from web page to web page. We propose a scalable crawling algorithm\nwhich (i) uses the noisy side information in an optimal way under mild\nassumptions; (ii) can be deployed without heavy centralized computation; (iii)\nis able to crawl web pages at a constant total rate without spikes in the total\nbandwidth usage over any time interval, and automatically adapt to the new\noptimal solution when the total bandwidth changes without centralized\ncomputation. Experiments clearly demonstrate the versatility of our approach."
                },
                "authors": [
                    {
                        "name": "Róbert Busa-Fekete"
                    },
                    {
                        "name": "Julian Zimmert"
                    },
                    {
                        "name": "András György"
                    },
                    {
                        "name": "Linhai Qiu"
                    },
                    {
                        "name": "Tzu-Wei Sung"
                    },
                    {
                        "name": "Hao Shen"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Sharmila Subramaniam"
                    },
                    {
                        "name": "Li Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Li Xiao"
                },
                "author": "Li Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02349v1",
                "updated": "2025-02-04T14:33:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T14:33:44Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    14,
                    33,
                    44,
                    1,
                    35,
                    0
                ],
                "title": "Random Adaptive Cache Placement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Adaptive Cache Placement Policy"
                },
                "summary": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new hybrid cache replacement algorithm that combines\nrandom allocation with a modified V-Way cache implementation. Our RAC adapts to\ncomplex cache access patterns and optimizes cache usage by improving the\nutilization of cache sets, unlike traditional cache policies. The algorithm\nutilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic\nallocation and flexible tag management. RAC extends the V-Way cache design and\nits variants by optimizing tag and data storage for enhanced efficiency.\n  We evaluated the algorithm using the ChampSim simulator with four diverse\nbenchmark traces and observed significant improvements in cache hit rates up to\n80.82% hit rate. Although the improvements in the instructions per cycle (IPC)\nwere moderate, our findings emphasize the algorithm's potential to enhance\ncache utilization and reduce memory access times."
                },
                "authors": [
                    {
                        "name": "Vrushank Ahire"
                    },
                    {
                        "name": "Pranav Menon"
                    },
                    {
                        "name": "Aniruddh Muley"
                    },
                    {
                        "name": "Abhinandan S. Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Abhinandan S. Prasad"
                },
                "author": "Abhinandan S. Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v2",
                "updated": "2025-02-04T13:45:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    13,
                    45,
                    37,
                    1,
                    35,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightTransfer: Your Long-Context LLM is Secretly a Hybrid Model with\n  Effortless Adaptation"
                },
                "summary": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer contexts introduces substantial\nmemory challenges due to the growing cost of key-value (KV) caches. Motivated\nby the efficiency gains of hybrid models and the broad availability of\npretrained large transformer backbones, we explore transitioning transformer\nmodels into hybrid architectures for a more efficient generation. In this work,\nwe propose LightTransfer, a lightweight method that transforms models such as\nLLaMA into hybrid variants. Our approach identifies lazy layers -- those\nfocusing on recent or initial tokens -- and replaces their full attention with\nstreaming attention. This transformation can be performed without any training\nfor long-context understanding tasks or with minimal fine-tuning for o1-like\nlong reasoning generation tasks that require stronger reasoning capabilities.\nExperiments across diverse benchmarks and models (e.g., LLaMA, Mistral,\nQwQ-STILL) demonstrate that, even when half of the layers are identified as\nlazy, LightTransfer achieves up to 2.17$\\times$ throughput improvement with\nminimal performance loss ($<1.5\\%$ on LongBench) and achieves 53.3\\% on math\nbenchmark AIME24 of advanced o1-like long reasoning model QwQ-STILL."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v1",
                "updated": "2025-02-04T09:48:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive\n  Token Caching in Robotic Manipulation"
                },
                "summary": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) model can process instructions and visual\nperception to directly generate actions as output in an end-to-end fashion due\nto its strong multi-modal reasoning capabilities. While the performance of VLA\nmodels is promising, their computational cost can be substantial. This raises\nchallenge for applying them on robotics tasks, which requires real-time\ndecision-making to respond quickly to environmental changes. Since robotic\ncontrol involves sequential decision-making, the visual input often exhibits\nminimal variation between successive steps. A natural idea is to reuse the\ncomputational results of unchanged visual tokens from the last step. Motivated\nby this idea, we propose VLA-Cache, an efficient vision-language-action model.\nVLA-Cache incorporates a token-selection mechanism that compares the visual\ninput at each step with the input from the previous step, adaptively\nidentifying visual tokens with minimal changes. The computational results for\nthese unchanged tokens are then reused in subsequent steps via KV-cache,\nthereby significantly improving the efficiency of the VLA-Cache model.\nExperimental results on both simulation (e.g., LIBERO benchmark and SIMPLER)\nand real-world robot valid VLA-Cache can achieve practical acceleration with\nminimal sacrifice in success rate."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02617v1",
                "updated": "2025-02-04T08:52:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T08:52:13Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    52,
                    13,
                    1,
                    35,
                    0
                ],
                "title": "PolarQuant: Quantizing KV Caches with Polar Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Quantizing KV Caches with Polar Transformation"
                },
                "summary": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) require significant memory to store Key-Value\n(KV) embeddings in their KV cache, especially when handling long-range\ncontexts. Quantization of these KV embeddings is a common technique to reduce\nmemory consumption. This work introduces PolarQuant, a novel quantization\nmethod employing random preconditioning and polar transformation. Our method\ntransforms the KV embeddings into polar coordinates using an efficient\nrecursive algorithm and then quantizes resulting angles. Our key insight is\nthat, after random preconditioning, the angles in the polar representation\nexhibit a tightly bounded and highly concentrated distribution with an\nanalytically computable form. This nice distribution eliminates the need for\nexplicit normalization, a step required by traditional quantization methods\nwhich introduces significant memory overhead because quantization parameters\n(e.g., zero point and scale) must be stored in full precision per each data\nblock. PolarQuant bypasses this normalization step, enabling substantial memory\nsavings. The long-context evaluation demonstrates that PolarQuant compresses\nthe KV cache by over x4.2 while achieving the best quality scores compared to\nthe state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Praneeth Kacham"
                    },
                    {
                        "name": "Amin Karbasi"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v4",
                "updated": "2025-02-04T08:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    8,
                    16,
                    31,
                    1,
                    35,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02069v1",
                "updated": "2025-02-04T07:40:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T07:40:26Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    7,
                    40,
                    26,
                    1,
                    35,
                    0
                ],
                "title": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-TTT: Low-Rank Test-Time Training for Vision-Language Models"
                },
                "summary": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancements in vision-language models (VLMs), such as CLIP, have\nintensified the need to address distribution shifts between training and\ntesting datasets. Although prior Test-Time Training (TTT) techniques for VLMs\nhave demonstrated robust performance, they predominantly rely on tuning text\nprompts, a process that demands substantial computational resources and is\nheavily dependent on entropy-based loss. In this paper, we propose LoRA-TTT, a\nnovel TTT method that leverages Low-Rank Adaptation (LoRA), applied exclusively\nto the image encoder of VLMs. By introducing LoRA and updating only its\nparameters during test time, our method offers a simple yet effective TTT\napproach, retaining the model's initial generalization capability while\nachieving substantial performance gains with minimal memory and runtime\noverhead. Additionally, we introduce a highly efficient reconstruction loss\ntailored for TTT. Our method can adapt to diverse domains by combining these\ntwo losses, without increasing memory consumption or runtime. Extensive\nexperiments on two benchmarks, covering 15 datasets, demonstrate that our\nmethod improves the zero-shot top-1 accuracy of CLIP-ViT-B/16 by an average of\n5.79% on the OOD benchmark and 1.36% on the fine-grained benchmark, efficiently\nsurpassing test-time prompt tuning, without relying on any external models or\ncache."
                },
                "authors": [
                    {
                        "name": "Yuto Kojima"
                    },
                    {
                        "name": "Jiarui Xu"
                    },
                    {
                        "name": "Xueyan Zou"
                    },
                    {
                        "name": "Xiaolong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolong Wang"
                },
                "author": "Xiaolong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v1",
                "updated": "2025-02-04T03:13:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local or remote disks\nwhen receiving multimodal data, and calculates and loads the KV cache in\nparallel during inference. To mitigate accuracy degradation, we have\nincorporated integrated reuse and recompute mechanisms within the system. The\nexperimental results demonstrate that MPIC can achieve up to 54% reduction in\nresponse time compared to existing context caching systems, while maintaining\nnegligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "14 pages, 11 figures, the first version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01941v1",
                "updated": "2025-02-04T02:23:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T02:23:06Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    2,
                    23,
                    6,
                    1,
                    35,
                    0
                ],
                "title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?"
                },
                "summary": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates an under-explored challenge in large language models\n(LLMs): the impact of KV cache compression methods on LLMs' fundamental\ncapabilities. While existing methods achieve impressive compression ratios on\nlong-context benchmarks, their effects on core model capabilities remain\nunderstudied. We present a comprehensive empirical study evaluating prominent\nKV cache compression methods across diverse tasks, spanning world knowledge,\ncommonsense reasoning, arithmetic reasoning, code generation, safety, and\nlong-context understanding and generation.Our analysis reveals that KV cache\ncompression methods exhibit task-specific performance degradation. Arithmetic\nreasoning tasks prove particularly sensitive to aggressive compression, with\ndifferent methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the\nDeepSeek R1 Distill model exhibits more robust compression tolerance compared\nto instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance\ndegradation. Based on our analysis of attention patterns and cross-task\ncompression performance, we propose ShotKV, a novel compression approach that\ndistinctly handles prefill and decoding phases while maintaining shot-level\nsemantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$\nperformance improvements on long-context generation tasks under aggressive\ncompression ratios."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v2",
                "updated": "2025-02-03T21:45:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    21,
                    45,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama and Qwen2.5 families of models, we\ndemonstrate that ResQ outperforms recent uniform and mixed precision PTQ\nmethods on a variety of benchmarks, achieving up to 33\\% lower perplexity on\nWikitext than the next best method SpinQuant, and upto 3\\times speedup over\n16-bit baseline. Code is available at\nhttps://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "18 pages, 7 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v1",
                "updated": "2025-02-03T20:30:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalized kinetic ion induced electron emission (IIEE) model is developed\nto obtain the emitted electron energy spectrum for a distribution of ion\nimpacts on a metallic surface. This framework is implemented as a boundary\ncondition for the continuum kinetic Boltzmann equation. The IIEE model is used\nto study how emissions affect sheath formation near biased Z-pinch electrodes.\n1X-1V (one spatial and one velocity dimension) Boltzmann-Poisson simulations\nare performed for a proton-electron plasma doubly bounded by two biased copper\nelectrodes with and without IIEE at bias potentials from 0 kV to 9 kV. The ions\nare accelerated to higher energies by the sheath potentials at the electrodes\ninducing electron emission. The secondary electron yield (SEY), defined as the\nratio of the flux of emitted electrons to impacting ions, increases with bias\npotential at both electrodes, but more significantly at the cathode. Despite\nthe SEY crossing 1 at 7 kV, a classical sheath, rather than a space-charge\nlimited or inverse sheath, forms for all cases. The emitted electrons present\nas a beam that is accelerated by the sheath potential into the domain resulting\nin increased electron temperatures due to collisions. For bias potentials\ngreater than 2 kV, the potential difference at the cathode is sufficiently\nstrong for emissive heating to increase the plasma potential compared to\nemissionless simulations. The emitted electrons increase the current in the\ndomain from 130 kA to 199 kA closely matching the experimental value of 200 kA."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "19 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01637v1",
                "updated": "2025-02-03T18:59:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T18:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    59,
                    32,
                    0,
                    34,
                    0
                ],
                "title": "Scaling Embedding Layers in Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Embedding Layers in Language Models"
                },
                "summary": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized,\n$\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for\nextending input embedding layers to enhance language model performance as layer\nsize scales. To avoid increased decoding costs, SCONE retains the original\nvocabulary while introducing embeddings for a set of frequent $n$-grams. These\nembeddings provide contextualized representation for each input token and are\nlearned with a separate model during training. During inference, they are\nprecomputed and stored in off-accelerator memory with minimal impact on\ninference speed. SCONE enables two new scaling strategies: increasing the\nnumber of cached $n$-gram embeddings and scaling the model used to learn them,\nall while maintaining fixed inference-time FLOPS. We show that scaling both\naspects allows SCONE to outperform a 1.9B parameter baseline across diverse\ncorpora, while using only half the inference-time FLOPS."
                },
                "authors": [
                    {
                        "name": "Da Yu"
                    },
                    {
                        "name": "Edith Cohen"
                    },
                    {
                        "name": "Badih Ghazi"
                    },
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Pritish Kamath"
                    },
                    {
                        "name": "Ravi Kumar"
                    },
                    {
                        "name": "Daogao Liu"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiyuan Zhang"
                },
                "author": "Chiyuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v2",
                "updated": "2025-02-03T15:15:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    15,
                    58,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v1",
                "updated": "2025-02-03T05:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial key-value (KV) caches to store contextual information,\nwhich can heavily burden computational efficiency and memory usage. Previous\nefforts to compress these KV caches primarily focused on reducing memory\ndemands but were limited in enhancing latency. To address this issue, we\nintroduce FastKV, a KV cache compression method designed to enhance latency for\nlong-context sequences. To enhance processing speeds while maintaining\naccuracy, FastKV adopts a novel Token-Selective Propagation (TSP) approach that\nretains the full context information in the initial layers of LLMs and\nselectively propagates only a portion of this information in deeper layers even\nin the prefill stage. Additionally, FastKV incorporates grouped-query attention\n(GQA)-aware KV cache compression to exploit the advantages of GQA in both\nmemory and computational efficiency. Our experimental results show that FastKV\nachieves 2.00$\\times$ and 1.40$\\times$ improvements in time-to-first-token\n(TTFT) and throughput, respectively, compared to HeadKV, the state-of-the-art\nKV cache compression method. Moreover, FastKV successfully maintains accuracy\non long-context benchmarks at levels comparable to the baselines. Our code is\navailable at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08784v2",
                "updated": "2025-02-02T14:38:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    38,
                    15,
                    6,
                    33,
                    0
                ],
                "published": "2023-10-12T07:35:30Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    7,
                    35,
                    30,
                    3,
                    285,
                    0
                ],
                "title": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction"
                },
                "summary": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches."
                },
                "authors": [
                    {
                        "name": "Pol Caselles"
                    },
                    {
                        "name": "Eduard Ramon"
                    },
                    {
                        "name": "Jaime Garcia"
                    },
                    {
                        "name": "Gil Triginer"
                    },
                    {
                        "name": "Francesc Moreno-Noguer"
                    }
                ],
                "author_detail": {
                    "name": "Francesc Moreno-Noguer"
                },
                "author": "Francesc Moreno-Noguer",
                "arxiv_doi": "10.1109/TPAMI.2025.3540542",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPAMI.2025.3540542",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16383v2",
                "updated": "2025-02-02T03:04:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    4,
                    54,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-25T01:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations"
                },
                "summary": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00527v1",
                "updated": "2025-02-01T18:59:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T18:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    59,
                    3,
                    5,
                    32,
                    0
                ],
                "title": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolarQuant: Leveraging Polar Transformation for Efficient Key Cache\n  Quantization and Decoding Acceleration"
                },
                "summary": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV cache in large language models is a dominant factor in memory usage,\nlimiting their broader applicability. Quantizing the cache to lower bit widths\nis an effective way to reduce computational costs; however, previous methods\nstruggle with quantizing key vectors due to outliers, resulting in excessive\noverhead. We propose a novel quantization approach called PolarQuant, which\nefficiently addresses the outlier challenge. We observe that outliers typically\nappear in only one of two dimensions, which are rotated together by a specific\nangle when rotary position embeddings are applied. When represented as\ntwo-dimensional vectors, these dimensions exhibit well-structured patterns,\nwith radii and angles smoothly distributed in polar coordinates. This\nalleviates the challenge of outliers on per-channel quantization, making them\nwell-suited for quantization. Thus, PolarQuant divides key vectors into groups\nof two-dimensional sub-vectors, encoding them as the corresponding quantized\nradius and the polar angle, rather than quantizing original key vectors\ndirectly. PolarQuant achieves the superior efficiency in KV cache quantization\nand accelerates the decoding process by turning the query-key inner product\ninto a table lookup, all while maintaining the downstream performance of\nfull-precision models."
                },
                "authors": [
                    {
                        "name": "Songhao Wu"
                    },
                    {
                        "name": "Ang Lv"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v3",
                "updated": "2025-02-01T16:00:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    16,
                    0,
                    50,
                    5,
                    32,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00439v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00439v1",
                "updated": "2025-02-01T14:16:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T14:16:31Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    14,
                    16,
                    31,
                    5,
                    32,
                    0
                ],
                "title": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniAttn: Reducing Inference Costs via Softmax Unification for\n  Post-Training LLMs"
                },
                "summary": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training is essential for adapting Large Language Models (LLMs) to\nreal-world applications. Deploying post-trained models faces significant\nchallenges due to substantial memory overhead and noticeable inference latency.\nExisting work has identified significant redundancies in LLMs and proposed\nefficient architectures, namely intra-layer KV sharing and cross-layer KV\nsharing. However, intra-layer KV sharing still results in high inference costs,\nwhile cross-layer KV sharing leads to significant performance degradation. As a\nresult, both methods remain suboptimal for post-training pre-trained LLMs. In\nthis paper, we identify that the \\texttt{Softmax} operation is a primary\nbottleneck for LLM inference and discover that it is actually highly redundant\nduring post-training. We propose Softmax \\textbf{Uni}fication in\n\\textbf{Att}e\\textbf{n}tion (\\textbf{UniAttn}), a novel post-training method\nthat unifies Softmax activations across transformer blocks to reduce LLM\ninference costs. Additionally, UniAttn adopts a linear projection to compensate\nfor the errors induced by Softmax unification. Experiments show that UniAttn\nmatches the performance of standard post-training while significantly reducing\ninference costs, outperforming existing efficient architectures during\npost-training. Our code will be available at\n\\url{https://github.com/Bostoncake/UniAttn}."
                },
                "authors": [
                    {
                        "name": "Yizhe Xiong"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Xin Ye"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Haoran Lian"
                    },
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "11 pages, 4 figures. Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00439v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00439v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00433v1",
                "updated": "2025-02-01T13:46:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T13:46:02Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    46,
                    2,
                    5,
                    32,
                    0
                ],
                "title": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion\n  Models"
                },
                "summary": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized generative tasks, especially in the\ndomain of text-to-image synthesis; however, their iterative denoising process\ndemands substantial computational resources. In this paper, we present a novel\nacceleration strategy that integrates token-level pruning with caching\ntechniques to tackle this computational challenge. By employing noise relative\nmagnitude, we identify significant token changes across denoising iterations.\nAdditionally, we enhance token selection by incorporating spatial clustering\nand ensuring distributional balance. Our experiments demonstrate reveal a\n50%-60% reduction in computational costs while preserving the performance of\nthe model, thereby markedly increasing the efficiency of diffusion models. The\ncode is available at https://github.com/ada-cheng/CAT-Pruning"
                },
                "authors": [
                    {
                        "name": "Xinle Cheng"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00382v1",
                "updated": "2025-02-01T09:41:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T09:41:01Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    41,
                    1,
                    5,
                    32,
                    0
                ],
                "title": "Masked Generative Nested Transformers with Decode Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Generative Nested Transformers with Decode Time Scaling"
                },
                "summary": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in visual generation have made significant strides in\nproducing content of exceptional quality. However, most methods suffer from a\nfundamental problem - a bottleneck of inference computational efficiency. Most\nof these algorithms involve multiple passes over a transformer model to\ngenerate tokens or denoise inputs. However, the model size is kept consistent\nthroughout all iterations, which makes it computationally expensive. In this\nwork, we aim to address this issue primarily through two key ideas - (a) not\nall parts of the generation process need equal compute, and we design a decode\ntime model scaling schedule to utilize compute effectively, and (b) we can\ncache and reuse some of the computation. Combining these two ideas leads to\nusing smaller models to process more tokens while large models process fewer\ntokens. These different-sized models do not increase the parameter size, as\nthey share parameters. We rigorously experiment with ImageNet256$\\times$256 ,\nUCF101, and Kinetics600 to showcase the efficacy of the proposed method for\nimage/video generation and frame prediction. Our experiments show that with\nalmost $3\\times$ less compute than baseline, our model obtains competitive\nperformance."
                },
                "authors": [
                    {
                        "name": "Sahil Goyal"
                    },
                    {
                        "name": "Debapriya Tula"
                    },
                    {
                        "name": "Gagan Jain"
                    },
                    {
                        "name": "Pradeep Shenoy"
                    },
                    {
                        "name": "Prateek Jain"
                    },
                    {
                        "name": "Sujoy Paul"
                    }
                ],
                "author_detail": {
                    "name": "Sujoy Paul"
                },
                "author": "Sujoy Paul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v2",
                "updated": "2025-02-01T04:24:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    4,
                    24,
                    16,
                    5,
                    32,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v1",
                "updated": "2025-02-01T03:49:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce memory costs in long-context inference with Large Language Models\n(LLMs), many recent works focus on compressing the key-value (KV) cache of\ndifferent tokens. However, we identify that the previous KV cache compression\nmethods measure token importance individually, neglecting the dependency\nbetween different tokens in the real-world language characterics. In light of\nthis, we introduce ChunkKV, grouping the tokens in a chunk as a basic\ncompressing unit, and retaining the most informative semantic chunks while\ndiscarding the less important ones. Furthermore, observing that ChunkKV\nexhibits higher similarity in the preserved indices across different layers, we\npropose layer-wise index reuse to further reduce computational overhead. We\nevaluated ChunkKV on cutting-edge long-context benchmarks including LongBench\nand Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context\nlearning benchmark. Our experiments with instruction tuning and multi-step\nreasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under\naggressive compression ratios compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "35 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v2",
                "updated": "2025-02-01T03:40:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    40,
                    37,
                    5,
                    32,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v2",
                "updated": "2025-01-31T19:09:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    19,
                    9,
                    19,
                    4,
                    31,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_doi": "10.1109/IPCCC59868.2024.10850382",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IPCCC59868.2024.10850382",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.12178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "pp. 1-9, doi: 10.1109/IPCCC59868.2024.10850382. keywords:\n  {Accuracy;Prefetching;Large language models;Computational\n  modeling;Companies;Transformers;User experience;Time\n  factors;Tuning;Guidelines;Large Language Models (LLMs);AI\n  Compression;Activation Sparsity;Edge LLM},",
                "arxiv_journal_ref": "2024 IEEE International Performance, Computing, and Communications\n  Conference (IPCCC), Orlando, FL, USA, 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v1",
                "updated": "2025-01-31T16:56:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee-Joe Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v1",
                "updated": "2025-01-31T16:22:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Transformer-based sequence-to-sequence generation, beam search has proven\neffective in enhancing the quality of generated sequences compared to greedy\ndecoding. Conventional beam search methods typically adopt either a sequential\nor batch-based approach. The sequential approach, while memory-efficient,\nrequires multiple decoding passes to construct a complete search tree, leading\nto significantly slower inference. On the other hand, the batch-based approach\nenables parallel computation across beams, but at the expense of high memory\nconsumption due to the need to maintain separate key-value (KV) caches for each\nbeam. In this study, we introduce a novel trie (prefix-tree)-based parallel\ndecoding method that addresses the memory inefficiency of batch-based beam\nsearch. By sharing a single KV cache among all beams that share the same\nprefix, the proposed method not only reduces memory consumption dramatically\nbut also enables parallel decoding across all branches. This innovative use of\na prefix tree offers an efficient alternative for beam search, achieving\nsignificant memory savings while preserving inference speed, making it\nparticularly well-suited for memory-constrained environments or large-scale\nmodel deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Mao Xun Huang"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v1",
                "updated": "2025-01-31T15:58:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.10392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10392v1",
                "updated": "2025-02-14T18:59:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    59,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:59:59Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    59,
                    4,
                    45,
                    0
                ],
                "title": "Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding"
                },
                "summary": "In this paper, we propose an efficient multi-level convolution architecture\nfor 3D visual grounding. Conventional methods are difficult to meet the\nrequirements of real-time inference due to the two-stage or point-based\narchitecture. Inspired by the success of multi-level fully sparse convolutional\narchitecture in 3D object detection, we aim to build a new 3D visual grounding\nframework following this technical route. However, as in 3D visual grounding\ntask the 3D scene representation should be deeply interacted with text\nfeatures, sparse convolution-based architecture is inefficient for this\ninteraction due to the large amount of voxel features. To this end, we propose\ntext-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D\nscene representation and text features in an efficient way by gradual region\npruning and target completion. Specifically, TGP iteratively sparsifies the 3D\nscene representation and thus efficiently interacts the voxel features with\ntext features by cross-attention. To mitigate the affect of pruning on delicate\ngeometric information, CBA adaptively fixes the over-pruned region by voxel\ncompletion with negligible computational overhead. Compared with previous\nsingle-stage methods, our method achieves top inference speed and surpasses\nprevious fastest method by 100\\% FPS. Our method also achieves state-of-the-art\naccuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on\nScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code\nis available at\n\\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose an efficient multi-level convolution architecture\nfor 3D visual grounding. Conventional methods are difficult to meet the\nrequirements of real-time inference due to the two-stage or point-based\narchitecture. Inspired by the success of multi-level fully sparse convolutional\narchitecture in 3D object detection, we aim to build a new 3D visual grounding\nframework following this technical route. However, as in 3D visual grounding\ntask the 3D scene representation should be deeply interacted with text\nfeatures, sparse convolution-based architecture is inefficient for this\ninteraction due to the large amount of voxel features. To this end, we propose\ntext-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D\nscene representation and text features in an efficient way by gradual region\npruning and target completion. Specifically, TGP iteratively sparsifies the 3D\nscene representation and thus efficiently interacts the voxel features with\ntext features by cross-attention. To mitigate the affect of pruning on delicate\ngeometric information, CBA adaptively fixes the over-pruned region by voxel\ncompletion with negligible computational overhead. Compared with previous\nsingle-stage methods, our method achieves top inference speed and surpasses\nprevious fastest method by 100\\% FPS. Our method also achieves state-of-the-art\naccuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on\nScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code\nis available at\n\\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}."
                },
                "authors": [
                    {
                        "name": "Wenxuan Guo"
                    },
                    {
                        "name": "Xiuwei Xu"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Jianjiang Feng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10391v1",
                "updated": "2025-02-14T18:59:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    51,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:59:51Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    51,
                    4,
                    45,
                    0
                ],
                "title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment"
                },
                "summary": "Despite notable advancements in Multimodal Large Language Models (MLLMs),\nmost state-of-the-art models have not undergone thorough alignment with human\npreferences. This gap exists because current alignment research has primarily\nachieved progress in specific areas (e.g., hallucination reduction), while the\nbroader question of whether aligning models with human preferences can\nsystematically enhance MLLM capability remains largely unexplored. To this end,\nwe introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained,\nhuman-annotated preference comparison pairs. This dataset represents a\nsubstantial advancement over existing resources, offering superior size,\ndiversity, annotation granularity, and quality. Leveraging this dataset, we\npropose several key innovations to improve both the quality of reward models\nand the efficiency of alignment algorithms. Notably, we introduce a\nCritique-Based Reward Model, which generates critiques of model outputs before\nassigning scores, offering enhanced interpretability and more informative\nfeedback compared to traditional scalar reward mechanisms. Additionally, we\npropose Dynamic Reward Scaling, a method that adjusts the loss weight of each\nsample according to the reward signal, thereby optimizing the use of\nhigh-quality comparison pairs. Our approach is rigorously evaluated across\n$\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results\ndemonstrating significant and consistent improvements in model performance.\nSpecifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm\nleads to a $\\mathbf{19.5}$% increase in conversational abilities and a\n$\\mathbf{60}$% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and\nevaluation code, as well as reward modeling and safety benchmarks. For more\ndetails, please visit our project page: https://mm-rlhf.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite notable advancements in Multimodal Large Language Models (MLLMs),\nmost state-of-the-art models have not undergone thorough alignment with human\npreferences. This gap exists because current alignment research has primarily\nachieved progress in specific areas (e.g., hallucination reduction), while the\nbroader question of whether aligning models with human preferences can\nsystematically enhance MLLM capability remains largely unexplored. To this end,\nwe introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained,\nhuman-annotated preference comparison pairs. This dataset represents a\nsubstantial advancement over existing resources, offering superior size,\ndiversity, annotation granularity, and quality. Leveraging this dataset, we\npropose several key innovations to improve both the quality of reward models\nand the efficiency of alignment algorithms. Notably, we introduce a\nCritique-Based Reward Model, which generates critiques of model outputs before\nassigning scores, offering enhanced interpretability and more informative\nfeedback compared to traditional scalar reward mechanisms. Additionally, we\npropose Dynamic Reward Scaling, a method that adjusts the loss weight of each\nsample according to the reward signal, thereby optimizing the use of\nhigh-quality comparison pairs. Our approach is rigorously evaluated across\n$\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results\ndemonstrating significant and consistent improvements in model performance.\nSpecifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm\nleads to a $\\mathbf{19.5}$% increase in conversational abilities and a\n$\\mathbf{60}$% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and\nevaluation code, as well as reward modeling and safety benchmarks. For more\ndetails, please visit our project page: https://mm-rlhf.github.io."
                },
                "authors": [
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Haochen Tian"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Peiyan Li"
                    },
                    {
                        "name": "Jianshu Zeng"
                    },
                    {
                        "name": "Wulin Xie"
                    },
                    {
                        "name": "Yang Shi"
                    },
                    {
                        "name": "Huanyu Zhang"
                    },
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Xue Wang"
                    },
                    {
                        "name": "Yibo Hu"
                    },
                    {
                        "name": "Bin Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Rong Jin"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "Project Page: https://mm-rlhf.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10388v1",
                "updated": "2025-02-14T18:59:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    28,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:59:28Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    28,
                    4,
                    45,
                    0
                ],
                "title": "Aspect-Oriented Summarization for Psychiatric Short-Term Readmission\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-Oriented Summarization for Psychiatric Short-Term Readmission\n  Prediction"
                },
                "summary": "Recent progress in large language models (LLMs) has enabled the automated\nprocessing of lengthy documents even without supervised training on a\ntask-specific dataset. Yet, their zero-shot performance in complex tasks as\nopposed to straightforward information extraction tasks remains suboptimal. One\nfeasible approach for tasks with lengthy, complex input is to first summarize\nthe document and then apply supervised fine-tuning to the summary. However, the\nsummarization process inevitably results in some loss of information. In this\nstudy we present a method for processing the summaries of long documents aimed\nto capture different important aspects of the original document. We hypothesize\nthat LLM summaries generated with different aspect-oriented prompts contain\ndifferent \\textit{information signals}, and we propose methods to measure these\ndifferences. We introduce approaches to effectively integrate signals from\nthese different summaries for supervised training of transformer models. We\nvalidate our hypotheses on a high-impact task -- 30-day readmission prediction\nfrom a psychiatric discharge -- using real-world data from four hospitals, and\nshow that our proposed method increases the prediction performance for the\ncomplex task of predicting patient outcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has enabled the automated\nprocessing of lengthy documents even without supervised training on a\ntask-specific dataset. Yet, their zero-shot performance in complex tasks as\nopposed to straightforward information extraction tasks remains suboptimal. One\nfeasible approach for tasks with lengthy, complex input is to first summarize\nthe document and then apply supervised fine-tuning to the summary. However, the\nsummarization process inevitably results in some loss of information. In this\nstudy we present a method for processing the summaries of long documents aimed\nto capture different important aspects of the original document. We hypothesize\nthat LLM summaries generated with different aspect-oriented prompts contain\ndifferent \\textit{information signals}, and we propose methods to measure these\ndifferences. We introduce approaches to effectively integrate signals from\nthese different summaries for supervised training of transformer models. We\nvalidate our hypotheses on a high-impact task -- 30-day readmission prediction\nfrom a psychiatric discharge -- using real-world data from four hospitals, and\nshow that our proposed method increases the prediction performance for the\ncomplex task of predicting patient outcome."
                },
                "authors": [
                    {
                        "name": "WonJin Yoon"
                    },
                    {
                        "name": "Boyu Ren"
                    },
                    {
                        "name": "Spencer Thomas"
                    },
                    {
                        "name": "Chanwhi Kim"
                    },
                    {
                        "name": "Guergana Savova"
                    },
                    {
                        "name": "Mei-Hua Hall"
                    },
                    {
                        "name": "Timothy Miller"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Miller"
                },
                "author": "Timothy Miller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19812v2",
                "updated": "2025-02-14T18:55:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    55,
                    59,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-29T22:23:34Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    22,
                    23,
                    34,
                    6,
                    273,
                    0
                ],
                "title": "Asymptotic and compound e-values: multiple testing and empirical Bayes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotic and compound e-values: multiple testing and empirical Bayes"
                },
                "summary": "We explicitly define the notions of (exact, approximate or asymptotic)\ncompound p-values and e-values, which have been implicitly presented and\nextensively used in the recent multiple testing literature. While it is known\nthat the e-BH procedure with compound e-values controls the FDR, we show the\nconverse: every FDR controlling procedure can be recovered by instantiating the\ne-BH procedure with certain compound e-values. Since compound e-values are\nclosed under averaging, this allows for combination and derandomization of FDR\nprocedures. We then connect compound e-values to empirical Bayes. In\nparticular, we use the fundamental theorem of compound decision theory to\nderive the log-optimal simple separable compound e-value for testing a set of\npoint nulls against point alternatives: it is a ratio of mixture likelihoods.\nWe extend universal inference to the compound setting. As one example, we\nconstruct approximate compound e-values for multiple t-tests, where the\n(nuisance) variances may be different across hypotheses. Finally, we provide\nconnections to related notions in the literature stated in terms of p-values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explicitly define the notions of (exact, approximate or asymptotic)\ncompound p-values and e-values, which have been implicitly presented and\nextensively used in the recent multiple testing literature. While it is known\nthat the e-BH procedure with compound e-values controls the FDR, we show the\nconverse: every FDR controlling procedure can be recovered by instantiating the\ne-BH procedure with certain compound e-values. Since compound e-values are\nclosed under averaging, this allows for combination and derandomization of FDR\nprocedures. We then connect compound e-values to empirical Bayes. In\nparticular, we use the fundamental theorem of compound decision theory to\nderive the log-optimal simple separable compound e-value for testing a set of\npoint nulls against point alternatives: it is a ratio of mixture likelihoods.\nWe extend universal inference to the compound setting. As one example, we\nconstruct approximate compound e-values for multiple t-tests, where the\n(nuisance) variances may be different across hypotheses. Finally, we provide\nconnections to related notions in the literature stated in terms of p-values."
                },
                "authors": [
                    {
                        "name": "Nikolaos Ignatiadis"
                    },
                    {
                        "name": "Ruodu Wang"
                    },
                    {
                        "name": "Aaditya Ramdas"
                    }
                ],
                "author_detail": {
                    "name": "Aaditya Ramdas"
                },
                "author": "Aaditya Ramdas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08008v2",
                "updated": "2025-02-14T18:52:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    52,
                    34,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-11T23:07:14Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    23,
                    7,
                    14,
                    1,
                    42,
                    0
                ],
                "title": "An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models"
                },
                "summary": "Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage."
                },
                "authors": [
                    {
                        "name": "Kasra Ahmadi"
                    },
                    {
                        "name": "Rouzbeh Behnia"
                    },
                    {
                        "name": "Reza Ebrahimi"
                    },
                    {
                        "name": "Mehran Mozaffari Kermani"
                    },
                    {
                        "name": "Jeremiah Birrell"
                    },
                    {
                        "name": "Jason Pacheco"
                    },
                    {
                        "name": "Attila A Yavuz"
                    }
                ],
                "author_detail": {
                    "name": "Attila A Yavuz"
                },
                "author": "Attila A Yavuz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10361v1",
                "updated": "2025-02-14T18:42:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    42,
                    7,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:42:07Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    42,
                    7,
                    4,
                    45,
                    0
                ],
                "title": "Enhancing Multilingual LLM Pretraining with Model-Based Data Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multilingual LLM Pretraining with Model-Based Data Selection"
                },
                "summary": "Dataset curation has become a basis for strong large language model (LLM)\nperformance. While various rule-based filtering heuristics exist for English\nand multilingual datasets, model-based filtering techniques have primarily\nfocused on English. To address the disparity stemming from limited research on\nnon-English languages, we propose a model-based filtering framework for\nmultilingual datasets that aims to identify a diverse set of structured and\nknowledge-rich samples. Our approach emphasizes transparency, simplicity, and\nefficiency, leveraging Transformer- and FastText-based classifiers to ensure\nthe broad accessibility of our technique and data. We conduct comprehensive\nablation studies on the FineWeb-2 web crawl dataset across diverse language\nfamilies, scripts, and resource availability to demonstrate the effectiveness\nof our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our\napproach can match the baseline MMLU score with as little as 15% of the\ntraining tokens, while also improving across other benchmarks. These findings\nprovide strong evidence for the generalizability of our approach to other\nlanguages. As a result, we extend our framework to 20 languages for which we\nrelease the refined pretraining datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset curation has become a basis for strong large language model (LLM)\nperformance. While various rule-based filtering heuristics exist for English\nand multilingual datasets, model-based filtering techniques have primarily\nfocused on English. To address the disparity stemming from limited research on\nnon-English languages, we propose a model-based filtering framework for\nmultilingual datasets that aims to identify a diverse set of structured and\nknowledge-rich samples. Our approach emphasizes transparency, simplicity, and\nefficiency, leveraging Transformer- and FastText-based classifiers to ensure\nthe broad accessibility of our technique and data. We conduct comprehensive\nablation studies on the FineWeb-2 web crawl dataset across diverse language\nfamilies, scripts, and resource availability to demonstrate the effectiveness\nof our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our\napproach can match the baseline MMLU score with as little as 15% of the\ntraining tokens, while also improving across other benchmarks. These findings\nprovide strong evidence for the generalizability of our approach to other\nlanguages. As a result, we extend our framework to 20 languages for which we\nrelease the refined pretraining datasets."
                },
                "authors": [
                    {
                        "name": "Bettina Messmer"
                    },
                    {
                        "name": "Vinko Sabolčec"
                    },
                    {
                        "name": "Martin Jaggi"
                    }
                ],
                "author_detail": {
                    "name": "Martin Jaggi"
                },
                "author": "Martin Jaggi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00843v2",
                "updated": "2025-02-14T18:35:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    35,
                    3,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-30T04:20:10Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    4,
                    20,
                    10,
                    2,
                    304,
                    0
                ],
                "title": "The Graph's Apprentice: Teaching an LLM Low Level Knowledge for Circuit\n  Quality Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Graph's Apprentice: Teaching an LLM Low Level Knowledge for Circuit\n  Quality Estimation"
                },
                "summary": "Logic synthesis is a crucial phase in the circuit design process, responsible\nfor transforming hardware description language (HDL) designs into optimized\nnetlists. However, traditional logic synthesis methods are computationally\nintensive, restricting their iterative use in refining chip designs. Recent\nadvancements in large language models (LLMs), particularly those fine-tuned on\nprogramming languages, present a promising alternative. This work proposes\naugmenting LLMs with predictor networks trained to estimate circuit quality\ndirectly from HDL code. To enhance performance, the model is regularized using\nembeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT)\ngraphs, thereby incorporating lower-level circuit insights. The proposed method\ndemonstrates superior performance compared to existing graph-based RTL-level\nestimation techniques on the established benchmark OpenABCD, while providing\ninstant feedback on HDL code quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic synthesis is a crucial phase in the circuit design process, responsible\nfor transforming hardware description language (HDL) designs into optimized\nnetlists. However, traditional logic synthesis methods are computationally\nintensive, restricting their iterative use in refining chip designs. Recent\nadvancements in large language models (LLMs), particularly those fine-tuned on\nprogramming languages, present a promising alternative. This work proposes\naugmenting LLMs with predictor networks trained to estimate circuit quality\ndirectly from HDL code. To enhance performance, the model is regularized using\nembeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT)\ngraphs, thereby incorporating lower-level circuit insights. The proposed method\ndemonstrates superior performance compared to existing graph-based RTL-level\nestimation techniques on the established benchmark OpenABCD, while providing\ninstant feedback on HDL code quality."
                },
                "authors": [
                    {
                        "name": "Reza Moravej"
                    },
                    {
                        "name": "Saurabh Bodhe"
                    },
                    {
                        "name": "Zhanguang Zhang"
                    },
                    {
                        "name": "Didier Chetelat"
                    },
                    {
                        "name": "Dimitrios Tsaras"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10352v1",
                "updated": "2025-02-14T18:31:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    31,
                    39,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:31:39Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    31,
                    39,
                    4,
                    45,
                    0
                ],
                "title": "Agentic Verification for Ambiguous Query Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Verification for Ambiguous Query Disambiguation"
                },
                "summary": "In this work, we tackle the challenge of disambiguating queries in\nretrieval-augmented generation (RAG) to diverse yet answerable interpretations.\nState-of-the-arts follow a Diversify-then-Verify (DtV) pipeline, where diverse\ninterpretations are generated by an LLM, later used as search queries to\nretrieve supporting passages. Such a process may introduce noise in either\ninterpretations or retrieval, particularly in enterprise settings, where LLMs\n-- trained on static data -- may struggle with domain-specific disambiguations.\nThus, a post-hoc verification phase is introduced to prune noises. Our\ndistinction is to unify diversification with verification by incorporating\nfeedback from retriever and generator early on. This joint approach improves\nboth efficiency and robustness by reducing reliance on multiple retrieval and\ninference steps, which are susceptible to cascading errors. We validate the\nefficiency and effectiveness of our method, Verified-Diversification with\nConsolidation (VERDICT), on the widely adopted ASQA benchmark to achieve\ndiverse yet verifiable interpretations. Empirical results show that VERDICT\nimproves grounding-aware F1 score by an average of 23% over the strongest\nbaseline across different backbone LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we tackle the challenge of disambiguating queries in\nretrieval-augmented generation (RAG) to diverse yet answerable interpretations.\nState-of-the-arts follow a Diversify-then-Verify (DtV) pipeline, where diverse\ninterpretations are generated by an LLM, later used as search queries to\nretrieve supporting passages. Such a process may introduce noise in either\ninterpretations or retrieval, particularly in enterprise settings, where LLMs\n-- trained on static data -- may struggle with domain-specific disambiguations.\nThus, a post-hoc verification phase is introduced to prune noises. Our\ndistinction is to unify diversification with verification by incorporating\nfeedback from retriever and generator early on. This joint approach improves\nboth efficiency and robustness by reducing reliance on multiple retrieval and\ninference steps, which are susceptible to cascading errors. We validate the\nefficiency and effectiveness of our method, Verified-Diversification with\nConsolidation (VERDICT), on the widely adopted ASQA benchmark to achieve\ndiverse yet verifiable interpretations. Empirical results show that VERDICT\nimproves grounding-aware F1 score by an average of 23% over the strongest\nbaseline across different backbone LLMs."
                },
                "authors": [
                    {
                        "name": "Youngwon Lee"
                    },
                    {
                        "name": "Seung-won Hwang"
                    },
                    {
                        "name": "Ruofan Wu"
                    },
                    {
                        "name": "Feng Yan"
                    },
                    {
                        "name": "Danmei Xu"
                    },
                    {
                        "name": "Moutasem Akkad"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13502v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13502v3",
                "updated": "2025-02-14T18:15:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    15,
                    1,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-17T12:48:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    12,
                    48,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs"
                },
                "summary": "Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to more complex\nproblems. This is difficult to study, as (i) much of the available evaluation\ndata has already been seen by the most capable models during training, and (ii)\nexisting benchmarks do not capture how problem proofs may be arbitrarily\ncomplex in various ways. In this paper, we present a data-generation framework\nfor evaluating LLMs on problems with arbitrarily complex arithmetic proofs,\ncalled MathGAP. MathGAP generates problem statements and chain-of-thought\nreasoning traces according to specifications about their arithmetic proof\nstructure, enabling systematic studies on easy-to-hard generalization with\nrespect to complexity of proof trees. Using MathGAP, we find that LLMs show a\nsignificant decrease in performance as proofs get deeper and wider. This effect\nis more pronounced in complex, nonlinear proof structures, which are\nchallenging even for the most capable models. The models are also sensitive to\nsimple changes in sentence ordering. However, they remain capable of solving\nsome complex problems, suggesting that reasoning generalization is noisy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to more complex\nproblems. This is difficult to study, as (i) much of the available evaluation\ndata has already been seen by the most capable models during training, and (ii)\nexisting benchmarks do not capture how problem proofs may be arbitrarily\ncomplex in various ways. In this paper, we present a data-generation framework\nfor evaluating LLMs on problems with arbitrarily complex arithmetic proofs,\ncalled MathGAP. MathGAP generates problem statements and chain-of-thought\nreasoning traces according to specifications about their arithmetic proof\nstructure, enabling systematic studies on easy-to-hard generalization with\nrespect to complexity of proof trees. Using MathGAP, we find that LLMs show a\nsignificant decrease in performance as proofs get deeper and wider. This effect\nis more pronounced in complex, nonlinear proof structures, which are\nchallenging even for the most capable models. The models are also sensitive to\nsimple changes in sentence ordering. However, they remain capable of solving\nsome complex problems, suggesting that reasoning generalization is noisy."
                },
                "authors": [
                    {
                        "name": "Andreas Opedal"
                    },
                    {
                        "name": "Haruki Shirakami"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Abulhair Saparov"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13502v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13502v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17331v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17331v4",
                "updated": "2025-02-14T18:09:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    9,
                    50,
                    4,
                    45,
                    0
                ],
                "published": "2023-11-29T03:10:42Z",
                "published_parsed": [
                    2023,
                    11,
                    29,
                    3,
                    10,
                    42,
                    2,
                    333,
                    0
                ],
                "title": "Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for\n  Visual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for\n  Visual Question Answering"
                },
                "summary": "Recently, to comprehensively improve Vision Language Models (VLMs) for Visual\nQuestion Answering (VQA), several methods have been proposed to further\nreinforce the inference capabilities of VLMs to independently tackle VQA tasks\nrather than some methods that only utilize VLMs as aids to Large Language\nModels (LLMs). However, these methods ignore the rich common-sense knowledge\ninside the given VQA image sampled from the real world. Thus, they cannot fully\nuse the powerful VLM for the given VQA question to achieve optimal performance.\nAttempt to overcome this limitation and inspired by the human top-down\nreasoning process, i.e., systematically exploring relevant issues to derive a\ncomprehensive answer, this work introduces a novel, explainable multi-agent\ncollaboration framework by leveraging the expansive knowledge of Large Language\nModels (LLMs) to enhance the capabilities of VLMs themselves. Specifically, our\nframework comprises three agents, i.e., Responder, Seeker, and Integrator, to\ncollaboratively answer the given VQA question by seeking its relevant issues\nand generating the final answer in such a top-down reasoning process. The\nVLM-based Responder agent generates the answer candidates for the question and\nresponds to other relevant issues. The Seeker agent, primarily based on LLM,\nidentifies relevant issues related to the question to inform the Responder\nagent and constructs a Multi-View Knowledge Base (MVKB) for the given visual\nscene by leveraging the build-in world knowledge of LLM. The Integrator agent\ncombines knowledge from the Seeker agent and the Responder agent to produce the\nfinal VQA answer. Extensive and comprehensive evaluations on diverse VQA\ndatasets with a variety of VLMs demonstrate the superior performance and\ninterpretability of our framework over the baseline method in the zero-shot\nsetting without extra training cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, to comprehensively improve Vision Language Models (VLMs) for Visual\nQuestion Answering (VQA), several methods have been proposed to further\nreinforce the inference capabilities of VLMs to independently tackle VQA tasks\nrather than some methods that only utilize VLMs as aids to Large Language\nModels (LLMs). However, these methods ignore the rich common-sense knowledge\ninside the given VQA image sampled from the real world. Thus, they cannot fully\nuse the powerful VLM for the given VQA question to achieve optimal performance.\nAttempt to overcome this limitation and inspired by the human top-down\nreasoning process, i.e., systematically exploring relevant issues to derive a\ncomprehensive answer, this work introduces a novel, explainable multi-agent\ncollaboration framework by leveraging the expansive knowledge of Large Language\nModels (LLMs) to enhance the capabilities of VLMs themselves. Specifically, our\nframework comprises three agents, i.e., Responder, Seeker, and Integrator, to\ncollaboratively answer the given VQA question by seeking its relevant issues\nand generating the final answer in such a top-down reasoning process. The\nVLM-based Responder agent generates the answer candidates for the question and\nresponds to other relevant issues. The Seeker agent, primarily based on LLM,\nidentifies relevant issues related to the question to inform the Responder\nagent and constructs a Multi-View Knowledge Base (MVKB) for the given visual\nscene by leveraging the build-in world knowledge of LLM. The Integrator agent\ncombines knowledge from the Seeker agent and the Responder agent to produce the\nfinal VQA answer. Extensive and comprehensive evaluations on diverse VQA\ndatasets with a variety of VLMs demonstrate the superior performance and\ninterpretability of our framework over the baseline method in the zero-shot\nsetting without extra training cost."
                },
                "authors": [
                    {
                        "name": "Zeqing Wang"
                    },
                    {
                        "name": "Wentao Wan"
                    },
                    {
                        "name": "Qiqing Lao"
                    },
                    {
                        "name": "Runmeng Chen"
                    },
                    {
                        "name": "Minjie Lang"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Keze Wang"
                    },
                    {
                        "name": "Liang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liang Lin"
                },
                "author": "Liang Lin",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17331v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17331v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09195v2",
                "updated": "2025-02-14T18:02:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    2,
                    57,
                    4,
                    45,
                    0
                ],
                "published": "2024-11-14T05:34:30Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    5,
                    34,
                    30,
                    3,
                    319,
                    0
                ],
                "title": "The origin channels of hierarchical binary black hole mergers in the\n  LIGO-Virgo-KAGRA O1, O2, and O3 runs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The origin channels of hierarchical binary black hole mergers in the\n  LIGO-Virgo-KAGRA O1, O2, and O3 runs"
                },
                "summary": "We infer the origin channels of hierarchical mergers observed in the\nLIGO-Virgo-KAGRA (LVK) O1, O2, and O3 runs using a hierarchical Bayesian\nanalysis under a parametric population model. By assuming the active galactic\nnucleus (AGN) disk and nuclear star cluster (NSC) channels, we find that NSCs\nlikely dominate the hierarchical merger rate in the Universe, corresponding to\na fraction of $f_{\\rm NSC}=0.87_{-0.29}^{+0.10}$ at 90\\% credible intervals in\nour fiducial model; AGN disks may contribute up to nearly half of hierarchical\nmergers detectable with LVK, specifically $f_{\\rm\ndet,AGN}=0.34_{-0.26}^{+0.38}$. We investigate the impact of the escape speed,\nalong with other population parameters on the branching fraction, suggesting\nthat the mass, mass ratio, and spin of the sources play significant roles in\npopulation analysis. We show that hierarchical mergers constitute at least\n$\\sim$$10\\%$ of the gravitational wave events detected by LVK during the O1-O3\nruns. Furthermore, we demonstrate that it is challenging to effectively infer\ndetailed information about the host environment based solely on the\ndistribution of black hole merger parameters if multiple formation channels are\nconsidered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We infer the origin channels of hierarchical mergers observed in the\nLIGO-Virgo-KAGRA (LVK) O1, O2, and O3 runs using a hierarchical Bayesian\nanalysis under a parametric population model. By assuming the active galactic\nnucleus (AGN) disk and nuclear star cluster (NSC) channels, we find that NSCs\nlikely dominate the hierarchical merger rate in the Universe, corresponding to\na fraction of $f_{\\rm NSC}=0.87_{-0.29}^{+0.10}$ at 90\\% credible intervals in\nour fiducial model; AGN disks may contribute up to nearly half of hierarchical\nmergers detectable with LVK, specifically $f_{\\rm\ndet,AGN}=0.34_{-0.26}^{+0.38}$. We investigate the impact of the escape speed,\nalong with other population parameters on the branching fraction, suggesting\nthat the mass, mass ratio, and spin of the sources play significant roles in\npopulation analysis. We show that hierarchical mergers constitute at least\n$\\sim$$10\\%$ of the gravitational wave events detected by LVK during the O1-O3\nruns. Furthermore, we demonstrate that it is challenging to effectively infer\ndetailed information about the host environment based solely on the\ndistribution of black hole merger parameters if multiple formation channels are\nconsidered."
                },
                "authors": [
                    {
                        "name": "Guo-Peng Li"
                    },
                    {
                        "name": "Xi-Long Fan"
                    }
                ],
                "author_detail": {
                    "name": "Xi-Long Fan"
                },
                "author": "Xi-Long Fan",
                "arxiv_comment": "13 pages, 2 figures, 2 tables; accepted for publication in The\n  Astrophysical Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07701v2",
                "updated": "2025-02-14T18:02:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    2,
                    38,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-11T16:58:15Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    16,
                    58,
                    15,
                    1,
                    42,
                    0
                ],
                "title": "Magic 1-For-1: Generating One Minute Video Clips within One Minute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magic 1-For-1: Generating One Minute Video Clips within One Minute"
                },
                "summary": "In this technical report, we present Magic 1-For-1 (Magic141), an efficient\nvideo generation model with optimized memory consumption and inference latency.\nThe key idea is simple: factorize the text-to-video generation task into two\nseparate easier tasks for diffusion step distillation, namely text-to-image\ngeneration and image-to-video generation. We verify that with the same\noptimization algorithm, the image-to-video task is indeed easier to converge\nover the text-to-video task. We also explore a bag of optimization tricks to\nreduce the computational cost of training the image-to-video (I2V) models from\nthree aspects: 1) model convergence speedup by using a multi-modal prior\ncondition injection; 2) inference latency speed up by applying an adversarial\nstep distillation, and 3) inference memory cost optimization with parameter\nsparsification. With those techniques, we are able to generate 5-second video\nclips within 3 seconds. By applying a test time sliding window, we are able to\ngenerate a minute-long video within one minute with significantly improved\nvisual quality and motion dynamics, spending less than 1 second for generating\n1 second video clips on average. We conduct a series of preliminary\nexplorations to find out the optimal tradeoff between computational cost and\nvideo quality during diffusion step distillation and hope this could be a good\nfoundation model for open-source explorations. The code and the model weights\nare available at https://github.com/DA-Group-PKU/Magic-1-For-1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this technical report, we present Magic 1-For-1 (Magic141), an efficient\nvideo generation model with optimized memory consumption and inference latency.\nThe key idea is simple: factorize the text-to-video generation task into two\nseparate easier tasks for diffusion step distillation, namely text-to-image\ngeneration and image-to-video generation. We verify that with the same\noptimization algorithm, the image-to-video task is indeed easier to converge\nover the text-to-video task. We also explore a bag of optimization tricks to\nreduce the computational cost of training the image-to-video (I2V) models from\nthree aspects: 1) model convergence speedup by using a multi-modal prior\ncondition injection; 2) inference latency speed up by applying an adversarial\nstep distillation, and 3) inference memory cost optimization with parameter\nsparsification. With those techniques, we are able to generate 5-second video\nclips within 3 seconds. By applying a test time sliding window, we are able to\ngenerate a minute-long video within one minute with significantly improved\nvisual quality and motion dynamics, spending less than 1 second for generating\n1 second video clips on average. We conduct a series of preliminary\nexplorations to find out the optimal tradeoff between computational cost and\nvideo quality during diffusion step distillation and hope this could be a good\nfoundation model for open-source explorations. The code and the model weights\nare available at https://github.com/DA-Group-PKU/Magic-1-For-1."
                },
                "authors": [
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Shitong Shao"
                    },
                    {
                        "name": "Tian Ye"
                    },
                    {
                        "name": "Jiantong Zhao"
                    },
                    {
                        "name": "Qingyu Yin"
                    },
                    {
                        "name": "Michael Lingelbach"
                    },
                    {
                        "name": "Li Yuan"
                    },
                    {
                        "name": "Yonghong Tian"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Daquan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Daquan Zhou"
                },
                "author": "Daquan Zhou",
                "arxiv_comment": "Serious modification needed.",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10339v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10339v1",
                "updated": "2025-02-14T17:59:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    59,
                    58,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T17:59:58Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    59,
                    58,
                    4,
                    45,
                    0
                ],
                "title": "STAR: Spectral Truncation and Rescale for Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR: Spectral Truncation and Rescale for Model Merging"
                },
                "summary": "Model merging is an efficient way of obtaining a multi-task model from\nseveral pretrained models without further fine-tuning, and it has gained\nattention in various domains, including natural language processing (NLP).\nDespite the efficiency, a key challenge in model merging is the seemingly\ninevitable decrease in task performance as the number of models increases. In\nthis paper, we propose $\\mathbf{S}$pectral $\\mathbf{T}$runcation $\\mathbf{A}$nd\n$\\mathbf{R}$escale (STAR) that aims at mitigating ``merging conflicts'' by\ntruncating small components in the respective spectral spaces, which is\nfollowed by an automatic parameter rescaling scheme to retain the nuclear norm\nof the original matrix. STAR requires no additional inference on original\ntraining data and is robust to hyperparamater choice. We demonstrate the\neffectiveness of STAR through extensive model merging cases on diverse NLP\ntasks. Specifically, STAR works robustly across varying model sizes, and can\noutperform baselines by 4.2$\\%$ when merging 12 models on Flan-T5. Our code is\npublicly available at https://github.com/IBM/STAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model merging is an efficient way of obtaining a multi-task model from\nseveral pretrained models without further fine-tuning, and it has gained\nattention in various domains, including natural language processing (NLP).\nDespite the efficiency, a key challenge in model merging is the seemingly\ninevitable decrease in task performance as the number of models increases. In\nthis paper, we propose $\\mathbf{S}$pectral $\\mathbf{T}$runcation $\\mathbf{A}$nd\n$\\mathbf{R}$escale (STAR) that aims at mitigating ``merging conflicts'' by\ntruncating small components in the respective spectral spaces, which is\nfollowed by an automatic parameter rescaling scheme to retain the nuclear norm\nof the original matrix. STAR requires no additional inference on original\ntraining data and is robust to hyperparamater choice. We demonstrate the\neffectiveness of STAR through extensive model merging cases on diverse NLP\ntasks. Specifically, STAR works robustly across varying model sizes, and can\noutperform baselines by 4.2$\\%$ when merging 12 models on Flan-T5. Our code is\npublicly available at https://github.com/IBM/STAR."
                },
                "authors": [
                    {
                        "name": "Yu-Ang Lee"
                    },
                    {
                        "name": "Ching-Yun Ko"
                    },
                    {
                        "name": "Tejaswini Pedapati"
                    },
                    {
                        "name": "I-Hsin Chung"
                    },
                    {
                        "name": "Mi-Yen Yeh"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Pin-Yu Chen"
                },
                "author": "Pin-Yu Chen",
                "arxiv_comment": "Accepted to NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10339v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10339v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10338v1",
                "updated": "2025-02-14T17:55:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    55,
                    43,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T17:55:43Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    55,
                    43,
                    4,
                    45,
                    0
                ],
                "title": "Evaluating the Meta- and Object-Level Reasoning of Large Language Models\n  for Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Meta- and Object-Level Reasoning of Large Language Models\n  for Question Answering"
                },
                "summary": "Large Language Models (LLMs) excel in natural language tasks but still face\nchallenges in Question Answering (QA) tasks requiring complex, multi-step\nreasoning. We outline the types of reasoning required in some of these tasks,\nand reframe them in terms of meta-level reasoning (akin to high-level strategic\nreasoning or planning) and object-level reasoning (embodied in lower-level\ntasks such as mathematical reasoning). Franklin, a novel dataset with\nrequirements of meta- and object-level reasoning, is introduced and used along\nwith three other datasets to evaluate four LLMs at question answering tasks\nrequiring multiple steps of reasoning. Results from human annotation studies\nsuggest LLMs demonstrate meta-level reasoning with high frequency, but struggle\nwith object-level reasoning tasks in some of the datasets used. Additionally,\nevidence suggests that LLMs find the object-level reasoning required for the\nquestions in the Franklin dataset challenging, yet they do exhibit strong\nperformance with respect to the meta-level reasoning requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language tasks but still face\nchallenges in Question Answering (QA) tasks requiring complex, multi-step\nreasoning. We outline the types of reasoning required in some of these tasks,\nand reframe them in terms of meta-level reasoning (akin to high-level strategic\nreasoning or planning) and object-level reasoning (embodied in lower-level\ntasks such as mathematical reasoning). Franklin, a novel dataset with\nrequirements of meta- and object-level reasoning, is introduced and used along\nwith three other datasets to evaluate four LLMs at question answering tasks\nrequiring multiple steps of reasoning. Results from human annotation studies\nsuggest LLMs demonstrate meta-level reasoning with high frequency, but struggle\nwith object-level reasoning tasks in some of the datasets used. Additionally,\nevidence suggests that LLMs find the object-level reasoning required for the\nquestions in the Franklin dataset challenging, yet they do exhibit strong\nperformance with respect to the meta-level reasoning requirements."
                },
                "authors": [
                    {
                        "name": "Nick Ferguson"
                    },
                    {
                        "name": "Liane Guillou"
                    },
                    {
                        "name": "Alan Bundy"
                    },
                    {
                        "name": "Kwabena Nuamah"
                    }
                ],
                "author_detail": {
                    "name": "Kwabena Nuamah"
                },
                "author": "Kwabena Nuamah",
                "arxiv_comment": "8 pages. Accepted to the Workshop on Planning in the Era of LLMs\n  (LM4Plan @ AAAI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10327v1",
                "updated": "2025-02-14T17:41:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    41,
                    33,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T17:41:33Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    41,
                    33,
                    4,
                    45,
                    0
                ],
                "title": "Many-body theory calculations of positron binding to parabenzoquinone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many-body theory calculations of positron binding to parabenzoquinone"
                },
                "summary": "Positron binding in parabenzoquinone is studied using \\textit{ab initio}\nmany-body theory. The effects of electron-positron correlations including\npolarization, virtual positronium formation and positron-hole repulsion, as\nwell as those of $\\pi$ bonds, aromaticity, and lone electron pairs, are\nconsidered. The binding energy is calculated as 60$\\pm$16 meV, considerably\nlarger than the 0.0925 meV value inferred from recent scattering calculations\nof [G. Moreira and M. Bettega, {\\emph{Eur.~Phys.~J.~D}} {\\bf 78} (2024)], but\nsubstantially smaller than we find in benzene (148$\\pm$26 meV). The positron\ncontact density (lifetime) is calculated as 8.0$\\times10^{-3}$ a.u. (2.48 ns),\nvs.~1.61$\\times 10^{-2}$ a.u. (0.81 ns) in benzene. The decrease (increase) in\nbinding (annihilation rate) in parabenzoquinone compared to benzene is ascribed\nto the loss of aromaticity: the electron density on the positive oxygen nuclei\nbeing relatively harder for the positron to probe compared to the aromatic\nrings in benzene.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positron binding in parabenzoquinone is studied using \\textit{ab initio}\nmany-body theory. The effects of electron-positron correlations including\npolarization, virtual positronium formation and positron-hole repulsion, as\nwell as those of $\\pi$ bonds, aromaticity, and lone electron pairs, are\nconsidered. The binding energy is calculated as 60$\\pm$16 meV, considerably\nlarger than the 0.0925 meV value inferred from recent scattering calculations\nof [G. Moreira and M. Bettega, {\\emph{Eur.~Phys.~J.~D}} {\\bf 78} (2024)], but\nsubstantially smaller than we find in benzene (148$\\pm$26 meV). The positron\ncontact density (lifetime) is calculated as 8.0$\\times10^{-3}$ a.u. (2.48 ns),\nvs.~1.61$\\times 10^{-2}$ a.u. (0.81 ns) in benzene. The decrease (increase) in\nbinding (annihilation rate) in parabenzoquinone compared to benzene is ascribed\nto the loss of aromaticity: the electron density on the positive oxygen nuclei\nbeing relatively harder for the positron to probe compared to the aromatic\nrings in benzene."
                },
                "authors": [
                    {
                        "name": "S. K. Gregg"
                    },
                    {
                        "name": "J. Hofierka"
                    },
                    {
                        "name": "B. Cunningham"
                    },
                    {
                        "name": "D. G. Green"
                    }
                ],
                "author_detail": {
                    "name": "D. G. Green"
                },
                "author": "D. G. Green",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10291v2",
                "updated": "2025-02-14T17:37:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    37,
                    35,
                    4,
                    45,
                    0
                ],
                "published": "2024-06-13T03:26:30Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    3,
                    26,
                    30,
                    3,
                    165,
                    0
                ],
                "title": "ResearchArena: Benchmarking Large Language Models' Ability to Collect\n  and Organize Information as Research Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResearchArena: Benchmarking Large Language Models' Ability to Collect\n  and Organize Information as Research Agents"
                },
                "summary": "Large language models (LLMs) excel across many natural language processing\ntasks but face challenges in domain-specific, analytical tasks such as\nconducting research surveys. This study introduces ResearchArena, a benchmark\ndesigned to evaluate LLMs' capabilities in conducting academic\nsurveys$\\unicode{x2013}$a foundational step in academic research. ResearchArena\nmodels the process in three stages: (1) information discovery, identifying\nrelevant literature; (2) information selection, evaluating papers' relevance\nand impact; and (3) information organization, structuring knowledge into\nhierarchical frameworks such as mind-maps. Notably, mind-map construction is\ntreated as a bonus task, reflecting its supplementary role in survey-writing.\nTo support these evaluations, we construct an offline environment of 12M\nfull-text academic papers and 7.9K survey papers. To ensure ethical compliance,\nwe do not redistribute copyrighted materials; instead, we provide code to\nconstruct the environment from the Semantic Scholar Open Research Corpus\n(S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform\ncompared to simpler keyword-based retrieval methods, underscoring significant\nopportunities for advancing LLMs in autonomous research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel across many natural language processing\ntasks but face challenges in domain-specific, analytical tasks such as\nconducting research surveys. This study introduces ResearchArena, a benchmark\ndesigned to evaluate LLMs' capabilities in conducting academic\nsurveys$\\unicode{x2013}$a foundational step in academic research. ResearchArena\nmodels the process in three stages: (1) information discovery, identifying\nrelevant literature; (2) information selection, evaluating papers' relevance\nand impact; and (3) information organization, structuring knowledge into\nhierarchical frameworks such as mind-maps. Notably, mind-map construction is\ntreated as a bonus task, reflecting its supplementary role in survey-writing.\nTo support these evaluations, we construct an offline environment of 12M\nfull-text academic papers and 7.9K survey papers. To ensure ethical compliance,\nwe do not redistribute copyrighted materials; instead, we provide code to\nconstruct the environment from the Semantic Scholar Open Research Corpus\n(S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform\ncompared to simpler keyword-based retrieval methods, underscoring significant\nopportunities for advancing LLMs in autonomous research."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Chenyan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Chenyan Xiong"
                },
                "author": "Chenyan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10325v1",
                "updated": "2025-02-14T17:34:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    34,
                    28,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T17:34:28Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    34,
                    28,
                    4,
                    45,
                    0
                ],
                "title": "Process Reward Models for LLM Agents: Practical Framework and Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models for LLM Agents: Practical Framework and Directions"
                },
                "summary": "We introduce Agent Process Reward Models (AgentPRM), a simple and scalable\nframework for training LLM agents to continually improve through interactions.\nAgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo\nrollouts to compute reward targets and optimize policies. It requires minimal\nmodifications to existing RLHF pipelines, making it easy to integrate at scale.\nBeyond AgentPRM, we propose InversePRM, which learns process rewards directly\nfrom demonstrations without explicit outcome supervision. We also explore key\nchallenges and opportunities, including exploration, process reward shaping,\nand model-predictive reasoning. We evaluate on ALFWorld benchmark, show that\nsmall 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o\nbaselines, and analyze test-time scaling, reward hacking, and more. Our code is\navailable at: https://github.com/sanjibanc/agent_prm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Agent Process Reward Models (AgentPRM), a simple and scalable\nframework for training LLM agents to continually improve through interactions.\nAgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo\nrollouts to compute reward targets and optimize policies. It requires minimal\nmodifications to existing RLHF pipelines, making it easy to integrate at scale.\nBeyond AgentPRM, we propose InversePRM, which learns process rewards directly\nfrom demonstrations without explicit outcome supervision. We also explore key\nchallenges and opportunities, including exploration, process reward shaping,\nand model-predictive reasoning. We evaluate on ALFWorld benchmark, show that\nsmall 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o\nbaselines, and analyze test-time scaling, reward hacking, and more. Our code is\navailable at: https://github.com/sanjibanc/agent_prm."
                },
                "authors": [
                    {
                        "name": "Sanjiban Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Sanjiban Choudhury"
                },
                "author": "Sanjiban Choudhury",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07516v2",
                "updated": "2025-02-14T17:24:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    24,
                    56,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-11T12:36:00Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    36,
                    0,
                    1,
                    42,
                    0
                ],
                "title": "The Devil is in the Prompts: De-Identification Traces Enhance\n  Memorization Risks in Synthetic Chest X-Ray Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Devil is in the Prompts: De-Identification Traces Enhance\n  Memorization Risks in Synthetic Chest X-Ray Generation"
                },
                "summary": "Generative models, particularly text-to-image (T2I) diffusion models, play a\ncrucial role in medical image analysis. However, these models are prone to\ntraining data memorization, posing significant risks to patient privacy.\nSynthetic chest X-ray generation is one of the most common applications in\nmedical image analysis with the MIMIC-CXR dataset serving as the primary data\nrepository for this task. This study presents the first systematic attempt to\nidentify prompts and text tokens in MIMIC-CXR that contribute the most to\ntraining data memorization. Our analysis reveals two unexpected findings: (1)\nprompts containing traces of de-identification procedures (markers introduced\nto hide Protected Health Information) are the most memorized, and (2) among all\ntokens, de-identification markers contribute the most towards memorization.\nThis highlights a broader issue with the standard anonymization practices and\nT2I synthesis with MIMIC-CXR. To exacerbate, existing inference-time\nmemorization mitigation strategies are ineffective and fail to sufficiently\nreduce the model's reliance on memorized text tokens. On this front, we propose\nactionable strategies for different stakeholders to enhance privacy and improve\nthe reliability of generative models in medical imaging. Finally, our results\nprovide a foundation for future work on developing and benchmarking\nmemorization mitigation techniques for synthetic chest X-ray generation using\nthe MIMIC-CXR dataset. The anonymized code is available at\nhttps://anonymous.4open.science/r/diffusion_memorization-8011/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models, particularly text-to-image (T2I) diffusion models, play a\ncrucial role in medical image analysis. However, these models are prone to\ntraining data memorization, posing significant risks to patient privacy.\nSynthetic chest X-ray generation is one of the most common applications in\nmedical image analysis with the MIMIC-CXR dataset serving as the primary data\nrepository for this task. This study presents the first systematic attempt to\nidentify prompts and text tokens in MIMIC-CXR that contribute the most to\ntraining data memorization. Our analysis reveals two unexpected findings: (1)\nprompts containing traces of de-identification procedures (markers introduced\nto hide Protected Health Information) are the most memorized, and (2) among all\ntokens, de-identification markers contribute the most towards memorization.\nThis highlights a broader issue with the standard anonymization practices and\nT2I synthesis with MIMIC-CXR. To exacerbate, existing inference-time\nmemorization mitigation strategies are ineffective and fail to sufficiently\nreduce the model's reliance on memorized text tokens. On this front, we propose\nactionable strategies for different stakeholders to enhance privacy and improve\nthe reliability of generative models in medical imaging. Finally, our results\nprovide a foundation for future work on developing and benchmarking\nmemorization mitigation techniques for synthetic chest X-ray generation using\nthe MIMIC-CXR dataset. The anonymized code is available at\nhttps://anonymous.4open.science/r/diffusion_memorization-8011/"
                },
                "authors": [
                    {
                        "name": "Raman Dutt"
                    }
                ],
                "author_detail": {
                    "name": "Raman Dutt"
                },
                "author": "Raman Dutt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10317v1",
                "updated": "2025-02-14T17:20:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    20,
                    34,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T17:20:34Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    20,
                    34,
                    4,
                    45,
                    0
                ],
                "title": "A Mechanistic Framework for Collider Detection in Observational Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Mechanistic Framework for Collider Detection in Observational Data"
                },
                "summary": "Understanding directionality is crucial for identifying causal structures\nfrom observational data. A key challenge lies in detecting collider structures,\nwhere a $V$--structure is formed between a child node $Z$ receiving directed\nedges from parents $X$ and $Y$, denoted by $X \\rightarrow Z \\leftarrow Y$.\nTraditional causal discovery approaches, such as constraint-based and\nscore-based structure learning algorithms, do not provide statistical inference\non estimated pathways and are often sensitive to latent confounding. To\novercome these issues, we introduce methodology to quantify directionality in\ncollider structures using a pair of conditional asymmetry coefficients to\nsimultaneously examine validity of the pathways $Y \\rightarrow Z$ and $X\n\\rightarrow Z$ in the collider structure. These coefficients are based on\nShannon's differential entropy. Leveraging kernel-based conditional density\nestimation and a nonparametric smoothing technique, we utilise our proposed\nmethod to estimate collider structures and provide uncertainty quantification.\n  Simulation studies demonstrate that our method outperforms existing structure\nlearning algorithms in accurately identifying collider structures. We further\napply our approach to investigate the role of blood pressure as a collider in\nepigenetic DNA methylation, uncovering novel insights into the genetic\nregulation of blood pressure. This framework represents a significant\nadvancement in causal structure learning, offering a robust, nonparametric\nmethod for collider detection with practical applications in biostatistics and\nepidemiology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding directionality is crucial for identifying causal structures\nfrom observational data. A key challenge lies in detecting collider structures,\nwhere a $V$--structure is formed between a child node $Z$ receiving directed\nedges from parents $X$ and $Y$, denoted by $X \\rightarrow Z \\leftarrow Y$.\nTraditional causal discovery approaches, such as constraint-based and\nscore-based structure learning algorithms, do not provide statistical inference\non estimated pathways and are often sensitive to latent confounding. To\novercome these issues, we introduce methodology to quantify directionality in\ncollider structures using a pair of conditional asymmetry coefficients to\nsimultaneously examine validity of the pathways $Y \\rightarrow Z$ and $X\n\\rightarrow Z$ in the collider structure. These coefficients are based on\nShannon's differential entropy. Leveraging kernel-based conditional density\nestimation and a nonparametric smoothing technique, we utilise our proposed\nmethod to estimate collider structures and provide uncertainty quantification.\n  Simulation studies demonstrate that our method outperforms existing structure\nlearning algorithms in accurately identifying collider structures. We further\napply our approach to investigate the role of blood pressure as a collider in\nepigenetic DNA methylation, uncovering novel insights into the genetic\nregulation of blood pressure. This framework represents a significant\nadvancement in causal structure learning, offering a robust, nonparametric\nmethod for collider detection with practical applications in biostatistics and\nepidemiology."
                },
                "authors": [
                    {
                        "name": "Soumik Purkayastha"
                    },
                    {
                        "name": "Peter X. -K. Song"
                    }
                ],
                "author_detail": {
                    "name": "Peter X. -K. Song"
                },
                "author": "Peter X. -K. Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10308v1",
                "updated": "2025-02-14T17:12:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    12,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T17:12:20Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    12,
                    20,
                    4,
                    45,
                    0
                ],
                "title": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered Preference Elicitation in Combinatorial Assignment"
                },
                "summary": "We study the potential of large language models (LLMs) as proxies for humans\nto simplify preference elicitation (PE) in combinatorial assignment. While\ntraditional PE methods rely on iterative queries to capture preferences, LLMs\noffer a one-shot alternative with reduced human effort. We propose a framework\nfor LLM proxies that can work in tandem with SOTA ML-powered preference\nelicitation schemes. Our framework handles the novel challenges introduced by\nLLMs, such as response variability and increased computational costs. We\nexperimentally evaluate the efficiency of LLM proxies against human queries in\nthe well-studied course allocation domain, and we investigate the model\ncapabilities required for success. We find that our approach improves\nallocative efficiency by up to 20%, and these results are robust across\ndifferent LLMs and to differences in quality and accuracy of reporting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the potential of large language models (LLMs) as proxies for humans\nto simplify preference elicitation (PE) in combinatorial assignment. While\ntraditional PE methods rely on iterative queries to capture preferences, LLMs\noffer a one-shot alternative with reduced human effort. We propose a framework\nfor LLM proxies that can work in tandem with SOTA ML-powered preference\nelicitation schemes. Our framework handles the novel challenges introduced by\nLLMs, such as response variability and increased computational costs. We\nexperimentally evaluate the efficiency of LLM proxies against human queries in\nthe well-studied course allocation domain, and we investigate the model\ncapabilities required for success. We find that our approach improves\nallocative efficiency by up to 20%, and these results are robust across\ndifferent LLMs and to differences in quality and accuracy of reporting."
                },
                "authors": [
                    {
                        "name": "Ermis Soumalias"
                    },
                    {
                        "name": "Yanchen Jiang"
                    },
                    {
                        "name": "Kehang Zhu"
                    },
                    {
                        "name": "Michael Curry"
                    },
                    {
                        "name": "Sven Seuken"
                    },
                    {
                        "name": "David C. Parkes"
                    }
                ],
                "author_detail": {
                    "name": "David C. Parkes"
                },
                "author": "David C. Parkes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.01706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.01706v2",
                "updated": "2025-02-14T17:05:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    5,
                    36,
                    4,
                    45,
                    0
                ],
                "published": "2023-02-03T13:04:12Z",
                "published_parsed": [
                    2023,
                    2,
                    3,
                    13,
                    4,
                    12,
                    4,
                    34,
                    0
                ],
                "title": "VT-GAN: Cooperative Tabular Data Synthesis using Vertical Federated\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VT-GAN: Cooperative Tabular Data Synthesis using Vertical Federated\n  Learning"
                },
                "summary": "This paper presents the application of Vertical Federated Learning (VFL) to\ngenerate synthetic tabular data using Generative Adversarial Networks (GANs).\nVFL is a collaborative approach to train machine learning models among distinct\ntabular data holders, such as financial institutions, who possess disjoint\nfeatures for the same group of customers. In this paper we introduce the VT-GAN\nframework, Vertical federated Tabular GAN, and demonstrate that VFL can be\nsuccessfully used to implement GANs for distributed tabular data in\nprivacy-preserving manner, with performance close to centralized GANs that\nassume shared data. We make design choices with respect to the distribution of\nGAN generator and discriminator models and introduce a training-with-shuffling\ntechnique so that no party can reconstruct training data from the GAN\nconditional vector. The paper presents (1) an implementation of VT-GAN, (2) a\ndetailed quality evaluation of the VT-GAN-generated synthetic data, (3) an\noverall scalability examination of VT-GAN framework, (4) a security analysis on\nVT-GAN's robustness against Membership Inference Attack with different settings\nof Differential Privacy, for a range of datasets with diverse distribution\ncharacteristics. Our results demonstrate that VT-GAN can consistently generate\nhigh-fidelity synthetic tabular data of comparable quality to that generated by\na centralized GAN algorithm. The difference in machine learning utility can be\nas low as 2.7%, even under extremely imbalanced data distributions across\nclients or with different numbers of clients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the application of Vertical Federated Learning (VFL) to\ngenerate synthetic tabular data using Generative Adversarial Networks (GANs).\nVFL is a collaborative approach to train machine learning models among distinct\ntabular data holders, such as financial institutions, who possess disjoint\nfeatures for the same group of customers. In this paper we introduce the VT-GAN\nframework, Vertical federated Tabular GAN, and demonstrate that VFL can be\nsuccessfully used to implement GANs for distributed tabular data in\nprivacy-preserving manner, with performance close to centralized GANs that\nassume shared data. We make design choices with respect to the distribution of\nGAN generator and discriminator models and introduce a training-with-shuffling\ntechnique so that no party can reconstruct training data from the GAN\nconditional vector. The paper presents (1) an implementation of VT-GAN, (2) a\ndetailed quality evaluation of the VT-GAN-generated synthetic data, (3) an\noverall scalability examination of VT-GAN framework, (4) a security analysis on\nVT-GAN's robustness against Membership Inference Attack with different settings\nof Differential Privacy, for a range of datasets with diverse distribution\ncharacteristics. Our results demonstrate that VT-GAN can consistently generate\nhigh-fidelity synthetic tabular data of comparable quality to that generated by\na centralized GAN algorithm. The difference in machine learning utility can be\nas low as 2.7%, even under extremely imbalanced data distributions across\nclients or with different numbers of clients."
                },
                "authors": [
                    {
                        "name": "Zilong Zhao"
                    },
                    {
                        "name": "Han Wu"
                    },
                    {
                        "name": "Aad Van Moorsel"
                    },
                    {
                        "name": "Lydia Y. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lydia Y. Chen"
                },
                "author": "Lydia Y. Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.01706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.01706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10299v1",
                "updated": "2025-02-14T17:01:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    1,
                    6,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T17:01:06Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    1,
                    6,
                    4,
                    45,
                    0
                ],
                "title": "Open-Source AI-Powered Optimization in Scalene: Advancing Python\n  Performance Profiling with DeepSeek-R1 and LLaMA 3.2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Source AI-Powered Optimization in Scalene: Advancing Python\n  Performance Profiling with DeepSeek-R1 and LLaMA 3.2"
                },
                "summary": "Python's flexibility and ease of use come at the cost of performance\ninefficiencies, requiring developers to rely on profilers to optimize\nexecution. SCALENE, a high-performance CPU, GPU, and memory profiler, provides\nfine-grained insights into Python applications while running significantly\nfaster than traditional profilers. Originally, SCALENE integrated OpenAI's API\nto generate AI-powered optimization suggestions, but its reliance on a\nproprietary API limited accessibility. This study explores the feasibility of\nusing opensource large language models (LLMs), such as DeepSeek-R1 and Llama\n3.2, to generate optimization recommendations within SCALENE. Our evaluation\nreveals that DeepSeek-R1 provides effective code optimizations comparable to\nproprietary models. We integrate DeepSeek-R1 into SCALENE to automatically\nanalyze performance bottlenecks and suggest improvements, enhancing SCALENE's\nutility while maintaining its open-source nature. This study demonstrates that\nopen-source LLMs can be viable alternatives for AI-driven code optimization,\npaving the way for more accessible and cost-effective performance analysis\ntools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python's flexibility and ease of use come at the cost of performance\ninefficiencies, requiring developers to rely on profilers to optimize\nexecution. SCALENE, a high-performance CPU, GPU, and memory profiler, provides\nfine-grained insights into Python applications while running significantly\nfaster than traditional profilers. Originally, SCALENE integrated OpenAI's API\nto generate AI-powered optimization suggestions, but its reliance on a\nproprietary API limited accessibility. This study explores the feasibility of\nusing opensource large language models (LLMs), such as DeepSeek-R1 and Llama\n3.2, to generate optimization recommendations within SCALENE. Our evaluation\nreveals that DeepSeek-R1 provides effective code optimizations comparable to\nproprietary models. We integrate DeepSeek-R1 into SCALENE to automatically\nanalyze performance bottlenecks and suggest improvements, enhancing SCALENE's\nutility while maintaining its open-source nature. This study demonstrates that\nopen-source LLMs can be viable alternatives for AI-driven code optimization,\npaving the way for more accessible and cost-effective performance analysis\ntools."
                },
                "authors": [
                    {
                        "name": "Saem Hasan"
                    },
                    {
                        "name": "Sanju Basak"
                    }
                ],
                "author_detail": {
                    "name": "Sanju Basak"
                },
                "author": "Sanju Basak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10297v1",
                "updated": "2025-02-14T16:59:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    59,
                    5,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T16:59:05Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    59,
                    5,
                    4,
                    45,
                    0
                ],
                "title": "DeltaProduct: Increasing the Expressivity of DeltaNet Through Products\n  of Householders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeltaProduct: Increasing the Expressivity of DeltaNet Through Products\n  of Householders"
                },
                "summary": "Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive\nalternatives to Transformers for sequence modeling, offering efficient training\nand linear-time inference. However, existing architectures face a fundamental\ntrade-off between expressivity and efficiency, dictated by the structure of\ntheir state-transition matrices. While diagonal matrices used in architectures\nlike Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited\nexpressivity. To address this, recent architectures such as (Gated) DeltaNet\nand RWKVv7 adopted a diagonal plus rank-1 structure, allowing simultaneous\ntoken-channel mixing, which overcomes some expressivity limitations with only a\nslight decrease in training efficiency. Building on the interpretation of\nDeltaNet's recurrence as performing one step of online gradient descent per\ntoken on an associative recall loss, we introduce DeltaProduct, which instead\ntakes multiple ($n_h$) steps per token. This naturally leads to diagonal plus\nrank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized\nHouseholder transformations, providing a tunable mechanism to balance\nexpressivity and efficiency and a stable recurrence. Through extensive\nexperiments, we demonstrate that DeltaProduct achieves superior state-tracking\nand language modeling capabilities while exhibiting significantly improved\nlength extrapolation compared to DeltaNet. Additionally, we also strengthen the\ntheoretical foundation of DeltaNet's expressivity by proving that it can solve\ndihedral group word problems in just two layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive\nalternatives to Transformers for sequence modeling, offering efficient training\nand linear-time inference. However, existing architectures face a fundamental\ntrade-off between expressivity and efficiency, dictated by the structure of\ntheir state-transition matrices. While diagonal matrices used in architectures\nlike Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited\nexpressivity. To address this, recent architectures such as (Gated) DeltaNet\nand RWKVv7 adopted a diagonal plus rank-1 structure, allowing simultaneous\ntoken-channel mixing, which overcomes some expressivity limitations with only a\nslight decrease in training efficiency. Building on the interpretation of\nDeltaNet's recurrence as performing one step of online gradient descent per\ntoken on an associative recall loss, we introduce DeltaProduct, which instead\ntakes multiple ($n_h$) steps per token. This naturally leads to diagonal plus\nrank-$n_h$ state-transition matrices, formed as products of $n_h$ generalized\nHouseholder transformations, providing a tunable mechanism to balance\nexpressivity and efficiency and a stable recurrence. Through extensive\nexperiments, we demonstrate that DeltaProduct achieves superior state-tracking\nand language modeling capabilities while exhibiting significantly improved\nlength extrapolation compared to DeltaNet. Additionally, we also strengthen the\ntheoretical foundation of DeltaNet's expressivity by proving that it can solve\ndihedral group word problems in just two layers."
                },
                "authors": [
                    {
                        "name": "Julien Siems"
                    },
                    {
                        "name": "Timur Carstensen"
                    },
                    {
                        "name": "Arber Zela"
                    },
                    {
                        "name": "Frank Hutter"
                    },
                    {
                        "name": "Massimiliano Pontil"
                    },
                    {
                        "name": "Riccardo Grazzi"
                    }
                ],
                "author_detail": {
                    "name": "Riccardo Grazzi"
                },
                "author": "Riccardo Grazzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17115v2",
                "updated": "2025-02-14T16:44:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    44,
                    8,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-25T17:28:13Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    28,
                    13,
                    2,
                    269,
                    0
                ],
                "title": "Programming Every Example: Lifting Pre-training Data Quality Like\n  Experts at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming Every Example: Lifting Pre-training Data Quality Like\n  Experts at Scale"
                },
                "summary": "Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, FineWeb,\nFineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in\ndomain-specific continual pre-training: without domain specific design, models\ntrained on OpenWebMath refined by ProX outperform human-crafted rule-based\nmethods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for\nLlama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable\nto models like Llemma-7B trained on 200B tokens. Further analysis highlights\nthat ProX significantly saves training FLOPs, offering a promising path for\nefficient LLM pre-training. We are open-sourcing ProX with >500B corpus,\nmodels, and sharing all training and implementation details for reproducible\nresearch and future innovation. Code: https://github.com/GAIR-NLP/ProX",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, FineWeb,\nFineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in\ndomain-specific continual pre-training: without domain specific design, models\ntrained on OpenWebMath refined by ProX outperform human-crafted rule-based\nmethods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for\nLlama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable\nto models like Llemma-7B trained on 200B tokens. Further analysis highlights\nthat ProX significantly saves training FLOPs, offering a promising path for\nefficient LLM pre-training. We are open-sourcing ProX with >500B corpus,\nmodels, and sharing all training and implementation details for reproducible\nresearch and future innovation. Code: https://github.com/GAIR-NLP/ProX"
                },
                "authors": [
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Zengzhi Wang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Junlong Li"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "arxiv_comment": "47 pages, 13 figures, 34 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01980v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01980v3",
                "updated": "2025-02-14T16:35:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    35,
                    51,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-03T15:22:41Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    22,
                    41,
                    1,
                    247,
                    0
                ],
                "title": "Large Language Models for Anomaly and Out-of-Distribution Detection: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Anomaly and Out-of-Distribution Detection: A\n  Survey"
                },
                "summary": "Detecting anomalies or out-of-distribution (OOD) samples is critical for\nmaintaining the reliability and trustworthiness of machine learning systems.\nRecently, Large Language Models (LLMs) have demonstrated their effectiveness\nnot only in natural language processing but also in broader applications due to\ntheir advanced comprehension and generative capabilities. The integration of\nLLMs into anomaly and OOD detection marks a significant shift from the\ntraditional paradigm in the field. This survey focuses on the problem of\nanomaly and OOD detection under the context of LLMs. We propose a new taxonomy\nto categorize existing approaches into two classes based on the role played by\nLLMs. Following our proposed taxonomy, we further discuss the related work\nunder each of the categories and finally discuss potential challenges and\ndirections for future research in this field. We also provide an up-to-date\nreading list of relevant papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting anomalies or out-of-distribution (OOD) samples is critical for\nmaintaining the reliability and trustworthiness of machine learning systems.\nRecently, Large Language Models (LLMs) have demonstrated their effectiveness\nnot only in natural language processing but also in broader applications due to\ntheir advanced comprehension and generative capabilities. The integration of\nLLMs into anomaly and OOD detection marks a significant shift from the\ntraditional paradigm in the field. This survey focuses on the problem of\nanomaly and OOD detection under the context of LLMs. We propose a new taxonomy\nto categorize existing approaches into two classes based on the role played by\nLLMs. Following our proposed taxonomy, we further discuss the related work\nunder each of the categories and finally discuss potential challenges and\ndirections for future research in this field. We also provide an up-to-date\nreading list of relevant papers."
                },
                "authors": [
                    {
                        "name": "Ruiyao Xu"
                    },
                    {
                        "name": "Kaize Ding"
                    }
                ],
                "author_detail": {
                    "name": "Kaize Ding"
                },
                "author": "Kaize Ding",
                "arxiv_comment": "Accepted to NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01980v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01980v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10276v1",
                "updated": "2025-02-14T16:33:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    33,
                    21,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T16:33:21Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    33,
                    21,
                    4,
                    45,
                    0
                ],
                "title": "A Latent Causal Inference Framework for Ordinal Variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Latent Causal Inference Framework for Ordinal Variables"
                },
                "summary": "Ordinal variables, such as on the Likert scale, are common in applied\nresearch. Yet, existing methods for causal inference tend to target nominal or\ncontinuous data. When applied to ordinal data, this fails to account for the\ninherent ordering or imposes well-defined relative magnitudes. Hence, there is\na need for specialised methods to compute interventional effects between\nordinal variables while accounting for their ordinality. One potential\nframework is to presume a latent Gaussian Directed Acyclic Graph (DAG) model:\nthat the ordinal variables originate from marginally discretizing a set of\nGaussian variables whose latent covariance matrix is constrained to satisfy the\nconditional independencies inherent in a DAG. Conditioned on a given latent\ncovariance matrix and discretisation thresholds, we derive a closed-form\nfunction for ordinal causal effects in terms of interventional distributions in\nthe latent space. Our causal estimation combines naturally with algorithms to\nlearn the latent DAG and its parameters, like the Ordinal Structural EM\nalgorithm. Simulations demonstrate the applicability of the proposed approach\nin estimating ordinal causal effects both for known and unknown structures of\nthe latent graph. As an illustration of a real-world use case, the method is\napplied to survey data of 408 patients from a study on the functional\nrelationships between symptoms of obsessive-compulsive disorder and depression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ordinal variables, such as on the Likert scale, are common in applied\nresearch. Yet, existing methods for causal inference tend to target nominal or\ncontinuous data. When applied to ordinal data, this fails to account for the\ninherent ordering or imposes well-defined relative magnitudes. Hence, there is\na need for specialised methods to compute interventional effects between\nordinal variables while accounting for their ordinality. One potential\nframework is to presume a latent Gaussian Directed Acyclic Graph (DAG) model:\nthat the ordinal variables originate from marginally discretizing a set of\nGaussian variables whose latent covariance matrix is constrained to satisfy the\nconditional independencies inherent in a DAG. Conditioned on a given latent\ncovariance matrix and discretisation thresholds, we derive a closed-form\nfunction for ordinal causal effects in terms of interventional distributions in\nthe latent space. Our causal estimation combines naturally with algorithms to\nlearn the latent DAG and its parameters, like the Ordinal Structural EM\nalgorithm. Simulations demonstrate the applicability of the proposed approach\nin estimating ordinal causal effects both for known and unknown structures of\nthe latent graph. As an illustration of a real-world use case, the method is\napplied to survey data of 408 patients from a study on the functional\nrelationships between symptoms of obsessive-compulsive disorder and depression."
                },
                "authors": [
                    {
                        "name": "Martina Scauda"
                    },
                    {
                        "name": "Jack Kuipers"
                    },
                    {
                        "name": "Giusi Moffa"
                    }
                ],
                "author_detail": {
                    "name": "Giusi Moffa"
                },
                "author": "Giusi Moffa",
                "arxiv_comment": "22 pages, 4 figures (plus 16 pages of appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06426v2",
                "updated": "2025-02-14T16:32:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    32,
                    54,
                    4,
                    45,
                    0
                ],
                "published": "2024-11-10T11:08:28Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    11,
                    8,
                    28,
                    6,
                    315,
                    0
                ],
                "title": "SequentialBreak: Large Language Models Can be Fooled by Embedding\n  Jailbreak Prompts into Sequential Prompt Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SequentialBreak: Large Language Models Can be Fooled by Embedding\n  Jailbreak Prompts into Sequential Prompt Chains"
                },
                "summary": "As the integration of the Large Language Models (LLMs) into various\napplications increases, so does their susceptibility to misuse, raising\nsignificant security concerns. Numerous jailbreak attacks have been proposed to\nassess the security defense of LLMs. Current jailbreak attacks mainly rely on\nscenario camouflage, prompt obfuscation, prompt optimization, and prompt\niterative optimization to conceal malicious prompts. In particular, sequential\nprompt chains in a single query can lead LLMs to focus on certain prompts while\nignoring others, facilitating context manipulation. This paper introduces\nSequentialBreak, a novel jailbreak attack that exploits this vulnerability. We\ndiscuss several scenarios, not limited to examples like Question Bank, Dialog\nCompletion, and Game Environment, where the harmful prompt is embedded within\nbenign ones that can fool LLMs into generating harmful responses. The distinct\nnarrative structures of these scenarios show that SequentialBreak is flexible\nenough to adapt to various prompt formats beyond those discussed. Extensive\nexperiments demonstrate that SequentialBreak uses only a single query to\nachieve a substantial gain of attack success rate over existing baselines\nagainst both open-source and closed-source models. Through our research, we\nhighlight the urgent need for more robust and resilient safeguards to enhance\nLLM security and prevent potential misuse. All the result files and website\nassociated with this research are available in this GitHub repository:\nhttps://anonymous.4open.science/r/JailBreakAttack-4F3B/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the integration of the Large Language Models (LLMs) into various\napplications increases, so does their susceptibility to misuse, raising\nsignificant security concerns. Numerous jailbreak attacks have been proposed to\nassess the security defense of LLMs. Current jailbreak attacks mainly rely on\nscenario camouflage, prompt obfuscation, prompt optimization, and prompt\niterative optimization to conceal malicious prompts. In particular, sequential\nprompt chains in a single query can lead LLMs to focus on certain prompts while\nignoring others, facilitating context manipulation. This paper introduces\nSequentialBreak, a novel jailbreak attack that exploits this vulnerability. We\ndiscuss several scenarios, not limited to examples like Question Bank, Dialog\nCompletion, and Game Environment, where the harmful prompt is embedded within\nbenign ones that can fool LLMs into generating harmful responses. The distinct\nnarrative structures of these scenarios show that SequentialBreak is flexible\nenough to adapt to various prompt formats beyond those discussed. Extensive\nexperiments demonstrate that SequentialBreak uses only a single query to\nachieve a substantial gain of attack success rate over existing baselines\nagainst both open-source and closed-source models. Through our research, we\nhighlight the urgent need for more robust and resilient safeguards to enhance\nLLM security and prevent potential misuse. All the result files and website\nassociated with this research are available in this GitHub repository:\nhttps://anonymous.4open.science/r/JailBreakAttack-4F3B/."
                },
                "authors": [
                    {
                        "name": "Bijoy Ahmed Saiem"
                    },
                    {
                        "name": "MD Sadik Hossain Shanto"
                    },
                    {
                        "name": "Rakib Ahsan"
                    },
                    {
                        "name": "Md Rafi ur Rashid"
                    }
                ],
                "author_detail": {
                    "name": "Md Rafi ur Rashid"
                },
                "author": "Md Rafi ur Rashid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13610v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13610v2",
                "updated": "2025-02-14T16:27:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    27,
                    25,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-17T14:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    14,
                    46,
                    22,
                    3,
                    291,
                    0
                ],
                "title": "MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool\n  Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool\n  Calling"
                },
                "summary": "Integrating tools into Large Language Models (LLMs) has facilitated the\nwidespread application. Despite this, in specialized downstream task contexts,\nreliance solely on tools is insufficient to fully address the complexities of\nthe real world. This particularly restricts the effective deployment of LLMs in\nfields such as medicine. In this paper, we focus on the downstream tasks of\nmedical calculators, which use standardized tests to assess an individual's\nhealth status. We introduce MeNTi, a universal agent architecture for LLMs.\nMeNTi integrates a specialized medical toolkit and employs meta-tool and nested\ncalling mechanisms to enhance LLM tool utilization. Specifically, it achieves\nflexible tool selection and nested tool calling to address practical issues\nfaced in intricate medical scenarios, including calculator selection, slot\nfilling, and unit conversion. To assess the capabilities of LLMs for\nquantitative assessment throughout the clinical process of calculator\nscenarios, we introduce CalcQA. This benchmark requires LLMs to use medical\ncalculators to perform calculations and assess patient health status. CalcQA is\nconstructed by professional physicians and includes 100 case-calculator pairs,\ncomplemented by a toolkit of 281 medical tools. The experimental results\ndemonstrate significant performance improvements with our framework. This\nresearch paves new directions for applying LLMs in demanding scenarios of\nmedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating tools into Large Language Models (LLMs) has facilitated the\nwidespread application. Despite this, in specialized downstream task contexts,\nreliance solely on tools is insufficient to fully address the complexities of\nthe real world. This particularly restricts the effective deployment of LLMs in\nfields such as medicine. In this paper, we focus on the downstream tasks of\nmedical calculators, which use standardized tests to assess an individual's\nhealth status. We introduce MeNTi, a universal agent architecture for LLMs.\nMeNTi integrates a specialized medical toolkit and employs meta-tool and nested\ncalling mechanisms to enhance LLM tool utilization. Specifically, it achieves\nflexible tool selection and nested tool calling to address practical issues\nfaced in intricate medical scenarios, including calculator selection, slot\nfilling, and unit conversion. To assess the capabilities of LLMs for\nquantitative assessment throughout the clinical process of calculator\nscenarios, we introduce CalcQA. This benchmark requires LLMs to use medical\ncalculators to perform calculations and assess patient health status. CalcQA is\nconstructed by professional physicians and includes 100 case-calculator pairs,\ncomplemented by a toolkit of 281 medical tools. The experimental results\ndemonstrate significant performance improvements with our framework. This\nresearch paves new directions for applying LLMs in demanding scenarios of\nmedicine."
                },
                "authors": [
                    {
                        "name": "Yakun Zhu"
                    },
                    {
                        "name": "Shaohang Wei"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Kui Xue"
                    },
                    {
                        "name": "Xiaofan Zhang"
                    },
                    {
                        "name": "Shaoting Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shaoting Zhang"
                },
                "author": "Shaoting Zhang",
                "arxiv_comment": "NAACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13610v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13610v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10266v1",
                "updated": "2025-02-14T16:23:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    23,
                    39,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T16:23:39Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    23,
                    39,
                    4,
                    45,
                    0
                ],
                "title": "Are Large Language Models the future crowd workers of Linguistics?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models the future crowd workers of Linguistics?"
                },
                "summary": "Data elicitation from human participants is one of the core data collection\nstrategies used in empirical linguistic research. The amount of participants in\nsuch studies may vary considerably, ranging from a handful to crowdsourcing\ndimensions. Even if they provide resourceful extensive data, both of these\nsettings come alongside many disadvantages, such as low control of\nparticipants' attention during task completion, precarious working conditions\nin crowdsourcing environments, and time-consuming experimental designs. For\nthese reasons, this research aims to answer the question of whether Large\nLanguage Models (LLMs) may overcome those obstacles if included in empirical\nlinguistic pipelines. Two reproduction case studies are conducted to gain\nclarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced\nelicitation tasks, originally designed for human participants, are reproduced\nin the proposed framework with the help of OpenAI's GPT-4o-mini model. Its\nperformance with our zero-shot prompting baseline shows the effectiveness and\nhigh versatility of LLMs, that tend to outperform human informants in\nlinguistic tasks. The findings of the second replication further highlight the\nneed to explore additional prompting techniques, such as Chain-of-Thought (CoT)\nprompting, which, in a second follow-up experiment, demonstrates higher\nalignment to human performance on both critical and filler items. Given the\nlimited scale of this study, it is worthwhile to further explore the\nperformance of LLMs in empirical Linguistics and in other future applications\nin the humanities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data elicitation from human participants is one of the core data collection\nstrategies used in empirical linguistic research. The amount of participants in\nsuch studies may vary considerably, ranging from a handful to crowdsourcing\ndimensions. Even if they provide resourceful extensive data, both of these\nsettings come alongside many disadvantages, such as low control of\nparticipants' attention during task completion, precarious working conditions\nin crowdsourcing environments, and time-consuming experimental designs. For\nthese reasons, this research aims to answer the question of whether Large\nLanguage Models (LLMs) may overcome those obstacles if included in empirical\nlinguistic pipelines. Two reproduction case studies are conducted to gain\nclarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced\nelicitation tasks, originally designed for human participants, are reproduced\nin the proposed framework with the help of OpenAI's GPT-4o-mini model. Its\nperformance with our zero-shot prompting baseline shows the effectiveness and\nhigh versatility of LLMs, that tend to outperform human informants in\nlinguistic tasks. The findings of the second replication further highlight the\nneed to explore additional prompting techniques, such as Chain-of-Thought (CoT)\nprompting, which, in a second follow-up experiment, demonstrates higher\nalignment to human performance on both critical and filler items. Given the\nlimited scale of this study, it is worthwhile to further explore the\nperformance of LLMs in empirical Linguistics and in other future applications\nin the humanities."
                },
                "authors": [
                    {
                        "name": "Iris Ferrazzo"
                    }
                ],
                "author_detail": {
                    "name": "Iris Ferrazzo"
                },
                "author": "Iris Ferrazzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10264v1",
                "updated": "2025-02-14T16:17:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    17,
                    23,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T16:17:23Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    17,
                    23,
                    4,
                    45,
                    0
                ],
                "title": "An overview of what current data can (and cannot yet) say about evolving\n  dark energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An overview of what current data can (and cannot yet) say about evolving\n  dark energy"
                },
                "summary": "Recent measurements of Baryon Acoustic Oscillations (BAO) and distance moduli\nfrom Type Ia supernovae suggest a preference for Dynamical Dark Energy (DDE)\nscenarios characterized by a time-varying equation of state (EoS). This focused\nreview assesses its robustness across independent measurements and surveys.\nUsing the Chevallier-Polarski-Linder (CPL) parameterization to describe the\nevolution of the DE EoS, we analyze over 35 dataset combinations, incorporating\nPlanck Cosmic Microwave Background (CMB) anisotropies, three independent Type\nIa supernova (SN) catalogs (PantheonPlus, Union3, DESY5), BAO measurements from\nDESI and SDSS, and expansion rate measurements $H(z)$ inferred from the\nrelative ages of massive, passively evolving galaxies at early cosmic times\nknown as Cosmic Chronometers (CC). This review has two main objectives: first,\nto evaluate the statistical significance of the DDE preference across different\ndataset combinations, which incorporate varying sources of information.\nSpecifically, we consider cases where only low-redshift probes are used in\ndifferent combinations, others where individual low-redshift probes are\nanalyzed together with CMB data, and finally, scenarios where high- and\nlow-redshift probes are included in all possible independent combinations.\nSecond, we provide a reader-friendly synthesis of what the latest cosmological\nand astrophysical probes can (and cannot yet) reveal about DDE. Overall, our\nfindings highlight that combinations that \\textit{simultaneously} include\nPantheonPlus SN and SDSS BAO significantly weaken the preference for DDE.\nHowever, intriguing hints supporting DDE emerge in combinations that do not\ninclude DESI-BAO measurements: SDSS-BAO combined with SN from Union3 and DESY5\n(with and without CMB) support the preference for DDE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent measurements of Baryon Acoustic Oscillations (BAO) and distance moduli\nfrom Type Ia supernovae suggest a preference for Dynamical Dark Energy (DDE)\nscenarios characterized by a time-varying equation of state (EoS). This focused\nreview assesses its robustness across independent measurements and surveys.\nUsing the Chevallier-Polarski-Linder (CPL) parameterization to describe the\nevolution of the DE EoS, we analyze over 35 dataset combinations, incorporating\nPlanck Cosmic Microwave Background (CMB) anisotropies, three independent Type\nIa supernova (SN) catalogs (PantheonPlus, Union3, DESY5), BAO measurements from\nDESI and SDSS, and expansion rate measurements $H(z)$ inferred from the\nrelative ages of massive, passively evolving galaxies at early cosmic times\nknown as Cosmic Chronometers (CC). This review has two main objectives: first,\nto evaluate the statistical significance of the DDE preference across different\ndataset combinations, which incorporate varying sources of information.\nSpecifically, we consider cases where only low-redshift probes are used in\ndifferent combinations, others where individual low-redshift probes are\nanalyzed together with CMB data, and finally, scenarios where high- and\nlow-redshift probes are included in all possible independent combinations.\nSecond, we provide a reader-friendly synthesis of what the latest cosmological\nand astrophysical probes can (and cannot yet) reveal about DDE. Overall, our\nfindings highlight that combinations that \\textit{simultaneously} include\nPantheonPlus SN and SDSS BAO significantly weaken the preference for DDE.\nHowever, intriguing hints supporting DDE emerge in combinations that do not\ninclude DESI-BAO measurements: SDSS-BAO combined with SN from Union3 and DESY5\n(with and without CMB) support the preference for DDE."
                },
                "authors": [
                    {
                        "name": "William Giarè"
                    },
                    {
                        "name": "Tariq Mahassen"
                    },
                    {
                        "name": "Eleonora Di Valentino"
                    },
                    {
                        "name": "Supriya Pan"
                    }
                ],
                "author_detail": {
                    "name": "Supriya Pan"
                },
                "author": "Supriya Pan",
                "arxiv_comment": "A short review; 20 pages including references; 2 tables, 5 figures;\n  comments are welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20299v2",
                "updated": "2025-02-14T16:16:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    16,
                    15,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-27T00:42:21Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    0,
                    42,
                    21,
                    6,
                    301,
                    0
                ],
                "title": "EACO-RAG: Towards Distributed Tiered LLM Deployment using Edge-Assisted\n  and Collaborative RAG with Adaptive Knowledge Update",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EACO-RAG: Towards Distributed Tiered LLM Deployment using Edge-Assisted\n  and Collaborative RAG with Adaptive Knowledge Update"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nlanguage tasks, but they require high computing power and rely on static\nknowledge. To overcome these limitations, Retrieval-Augmented Generation (RAG)\nincorporates up-to-date external information into LLMs without extensive\nfine-tuning. Meanwhile, small language models (SLMs) deployed on edge devices\noffer efficiency and low latency but often struggle with complex reasoning\ntasks. Unfortunately, current RAG approaches are predominantly based on\ncentralized databases and have not been adapted to address the distinct\nconstraints associated with deploying SLMs in edge environments. To bridge this\ngap, we propose Edge-Assisted and Collaborative RAG (EACO-RAG), a lightweight\nframework that leverages distributed edge nodes for adaptive knowledge updates\nand retrieval. EACO-RAG also employs a hierarchical collaborative gating\nmechanism to dynamically select among local, edge-assisted, and cloud-based\nstrategies, with a carefully designed algorithm based on Safe Online Bayesian\nOptimization to maximize the potential performance enhancements. Experimental\nresults demonstrate that EACO-RAG matches the accuracy of cloud-based knowledge\ngraph RAG systems while reducing total costs by up to 84.6% under relaxed delay\nconstraints and by 65.3% under stricter delay requirements. This work\nrepresents our initial effort toward achieving a distributed and scalable\ntiered LLM deployments, with EACO-RAG serving as a promising first step in\nunlocking the full potential of hybrid edge-cloud intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in\nlanguage tasks, but they require high computing power and rely on static\nknowledge. To overcome these limitations, Retrieval-Augmented Generation (RAG)\nincorporates up-to-date external information into LLMs without extensive\nfine-tuning. Meanwhile, small language models (SLMs) deployed on edge devices\noffer efficiency and low latency but often struggle with complex reasoning\ntasks. Unfortunately, current RAG approaches are predominantly based on\ncentralized databases and have not been adapted to address the distinct\nconstraints associated with deploying SLMs in edge environments. To bridge this\ngap, we propose Edge-Assisted and Collaborative RAG (EACO-RAG), a lightweight\nframework that leverages distributed edge nodes for adaptive knowledge updates\nand retrieval. EACO-RAG also employs a hierarchical collaborative gating\nmechanism to dynamically select among local, edge-assisted, and cloud-based\nstrategies, with a carefully designed algorithm based on Safe Online Bayesian\nOptimization to maximize the potential performance enhancements. Experimental\nresults demonstrate that EACO-RAG matches the accuracy of cloud-based knowledge\ngraph RAG systems while reducing total costs by up to 84.6% under relaxed delay\nconstraints and by 65.3% under stricter delay requirements. This work\nrepresents our initial effort toward achieving a distributed and scalable\ntiered LLM deployments, with EACO-RAG serving as a promising first step in\nunlocking the full potential of hybrid edge-cloud intelligence."
                },
                "authors": [
                    {
                        "name": "Jiaxing Li"
                    },
                    {
                        "name": "Chi Xu"
                    },
                    {
                        "name": "Lianchen Jia"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Cong Zhang"
                    },
                    {
                        "name": "Jiangchuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangchuan Liu"
                },
                "author": "Jiangchuan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10263v1",
                "updated": "2025-02-14T16:16:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    16,
                    2,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T16:16:02Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    16,
                    2,
                    4,
                    45,
                    0
                ],
                "title": "Large Language Models and Synthetic Data for Monitoring Dataset Mentions\n  in Research Papers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Synthetic Data for Monitoring Dataset Mentions\n  in Research Papers"
                },
                "summary": "Tracking how data is mentioned and used in research papers provides critical\ninsights for improving data discoverability, quality, and production. However,\nmanually identifying and classifying dataset mentions across vast academic\nliterature is resource-intensive and not scalable. This paper presents a\nmachine learning framework that automates dataset mention detection across\nresearch domains by leveraging large language models (LLMs), synthetic data,\nand a two-stage fine-tuning process. We employ zero-shot extraction from\nresearch papers, an LLM-as-a-Judge for quality assessment, and a reasoning\nagent for refinement to generate a weakly supervised synthetic dataset. The\nPhi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by\nfine-tuning on a manually annotated subset. At inference, a ModernBERT-based\nclassifier efficiently filters dataset mentions, reducing computational\noverhead while maintaining high recall. Evaluated on a held-out manually\nannotated sample, our fine-tuned model outperforms NuExtract-v1.5 and\nGLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how\nLLM-generated synthetic data can effectively address training data scarcity,\nimproving generalization in low-resource settings. This framework offers a\npathway toward scalable monitoring of dataset usage, enhancing transparency,\nand supporting researchers, funders, and policymakers in identifying data gaps\nand strengthening data accessibility for informed decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracking how data is mentioned and used in research papers provides critical\ninsights for improving data discoverability, quality, and production. However,\nmanually identifying and classifying dataset mentions across vast academic\nliterature is resource-intensive and not scalable. This paper presents a\nmachine learning framework that automates dataset mention detection across\nresearch domains by leveraging large language models (LLMs), synthetic data,\nand a two-stage fine-tuning process. We employ zero-shot extraction from\nresearch papers, an LLM-as-a-Judge for quality assessment, and a reasoning\nagent for refinement to generate a weakly supervised synthetic dataset. The\nPhi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by\nfine-tuning on a manually annotated subset. At inference, a ModernBERT-based\nclassifier efficiently filters dataset mentions, reducing computational\noverhead while maintaining high recall. Evaluated on a held-out manually\nannotated sample, our fine-tuned model outperforms NuExtract-v1.5 and\nGLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how\nLLM-generated synthetic data can effectively address training data scarcity,\nimproving generalization in low-resource settings. This framework offers a\npathway toward scalable monitoring of dataset usage, enhancing transparency,\nand supporting researchers, funders, and policymakers in identifying data gaps\nand strengthening data accessibility for informed decision-making."
                },
                "authors": [
                    {
                        "name": "Aivin V. Solatorio"
                    },
                    {
                        "name": "Rafael Macalaba"
                    },
                    {
                        "name": "James Liounis"
                    }
                ],
                "author_detail": {
                    "name": "James Liounis"
                },
                "author": "James Liounis",
                "arxiv_comment": "Project GitHub repository at https://github.com/worldbank/ai4data-use",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20856v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20856v2",
                "updated": "2025-02-14T16:09:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    9,
                    49,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-28T09:19:29Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    19,
                    29,
                    0,
                    302,
                    0
                ],
                "title": "Strada-LLM: Graph LLM for traffic prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strada-LLM: Graph LLM for traffic prediction"
                },
                "summary": "Traffic prediction is a vital component of intelligent transportation\nsystems. By reasoning about traffic patterns in both the spatial and temporal\ndimensions, accurate and interpretable predictions can be provided. A\nconsiderable challenge in traffic prediction lies in handling the diverse data\ndistributions caused by vastly different traffic conditions occurring at\ndifferent locations. LLMs have been a dominant solution due to their remarkable\ncapacity to adapt to new datasets with very few labeled data samples, i.e.,\nfew-shot adaptability. However, existing forecasting techniques mainly focus on\nextracting local graph information and forming a text-like prompt, leaving LLM-\nbased traffic prediction an open problem. This work presents a probabilistic\nLLM for traffic forecasting with three highlights. We propose a graph-aware LLM\nfor traffic prediction that considers proximal traffic information.\nSpecifically, by considering the traffic of neighboring nodes as covariates,\nour model outperforms the corresponding time-series LLM. Furthermore, we adopt\na lightweight approach for efficient domain adaptation when facing new data\ndistributions in few-shot fashion. The comparative experiment demonstrates the\nproposed method outperforms the state-of-the-art LLM-based methods and the\ntraditional GNN- based supervised approaches. Furthermore, Strada-LLM can be\neasily adapted to different LLM backbones without a noticeable performance\ndrop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic prediction is a vital component of intelligent transportation\nsystems. By reasoning about traffic patterns in both the spatial and temporal\ndimensions, accurate and interpretable predictions can be provided. A\nconsiderable challenge in traffic prediction lies in handling the diverse data\ndistributions caused by vastly different traffic conditions occurring at\ndifferent locations. LLMs have been a dominant solution due to their remarkable\ncapacity to adapt to new datasets with very few labeled data samples, i.e.,\nfew-shot adaptability. However, existing forecasting techniques mainly focus on\nextracting local graph information and forming a text-like prompt, leaving LLM-\nbased traffic prediction an open problem. This work presents a probabilistic\nLLM for traffic forecasting with three highlights. We propose a graph-aware LLM\nfor traffic prediction that considers proximal traffic information.\nSpecifically, by considering the traffic of neighboring nodes as covariates,\nour model outperforms the corresponding time-series LLM. Furthermore, we adopt\na lightweight approach for efficient domain adaptation when facing new data\ndistributions in few-shot fashion. The comparative experiment demonstrates the\nproposed method outperforms the state-of-the-art LLM-based methods and the\ntraditional GNN- based supervised approaches. Furthermore, Strada-LLM can be\neasily adapted to different LLM backbones without a noticeable performance\ndrop."
                },
                "authors": [
                    {
                        "name": "Seyed Mohamad Moghadas"
                    },
                    {
                        "name": "Yangxintong Lyu"
                    },
                    {
                        "name": "Bruno Cornelis"
                    },
                    {
                        "name": "Alexandre Alahi"
                    },
                    {
                        "name": "Adrian Munteanu"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Munteanu"
                },
                "author": "Adrian Munteanu",
                "arxiv_comment": "The reviewers decided to reject it. After getting the reviews, we\n  wanted to study more.",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20856v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20856v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10250v1",
                "updated": "2025-02-14T15:59:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    59,
                    33,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:59:33Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    59,
                    33,
                    4,
                    45,
                    0
                ],
                "title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision\n  Language Models"
                },
                "summary": "Vision-language models (VLMs) excel in various visual benchmarks but are\noften constrained by the lack of high-quality visual fine-tuning data. To\naddress this challenge, we introduce VisCon-100K, a novel dataset derived from\ninterleaved image-text web documents. Our approach transforms 45K web documents\nfrom the OBELICS dataset into 100K image conversation samples. We utilize\nGPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert\nthese captions into diverse free-form and multiple-choice question-answer\npairs. Integrating this dataset for fine-tuning considerably enhances VLM\nperformance across multiple benchmarks. Unlike methods that focus solely on\nfine-grained visual content, our approach leverages accompanying web context,\nyielding superior results. We also discover that a `leaky modality mix,' where\nconversation samples contain questions answerable from both the image and its\ncontextual caption, outperforms non-leaky combinations of captions and Q\\&A\npairs. VisCon-100k dataset shows strong performance with two popular VLM\napproaches: text-only large language model (LLM) aligned with a vision encoder\nusing image captions data (ShareGPT4V-7b) and multimodally pretrained LLM\n(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the\nVisCon-100K dataset, we provide a contextual captioner trained on this dataset,\nfacilitating scalable fine-tuning data generation for future research and\nopen-source applications. Using the same pipeline, but substituting our trained\ncontextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) excel in various visual benchmarks but are\noften constrained by the lack of high-quality visual fine-tuning data. To\naddress this challenge, we introduce VisCon-100K, a novel dataset derived from\ninterleaved image-text web documents. Our approach transforms 45K web documents\nfrom the OBELICS dataset into 100K image conversation samples. We utilize\nGPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert\nthese captions into diverse free-form and multiple-choice question-answer\npairs. Integrating this dataset for fine-tuning considerably enhances VLM\nperformance across multiple benchmarks. Unlike methods that focus solely on\nfine-grained visual content, our approach leverages accompanying web context,\nyielding superior results. We also discover that a `leaky modality mix,' where\nconversation samples contain questions answerable from both the image and its\ncontextual caption, outperforms non-leaky combinations of captions and Q\\&A\npairs. VisCon-100k dataset shows strong performance with two popular VLM\napproaches: text-only large language model (LLM) aligned with a vision encoder\nusing image captions data (ShareGPT4V-7b) and multimodally pretrained LLM\n(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the\nVisCon-100K dataset, we provide a contextual captioner trained on this dataset,\nfacilitating scalable fine-tuning data generation for future research and\nopen-source applications. Using the same pipeline, but substituting our trained\ncontextual captioner for GPT-4V, we also release the larger VisCon-1M dataset."
                },
                "authors": [
                    {
                        "name": "Gokul Karthik Kumar"
                    },
                    {
                        "name": "Iheb Chaabane"
                    },
                    {
                        "name": "Kebin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kebin Wu"
                },
                "author": "Kebin Wu",
                "arxiv_comment": "Accepted at PAKDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04796v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04796v2",
                "updated": "2025-02-14T15:58:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    58,
                    39,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-07T11:24:52Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    11,
                    24,
                    52,
                    5,
                    251,
                    0
                ],
                "title": "Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution\n  Detection"
                },
                "summary": "Out-of-Distribution (OOD) detection, aiming to distinguish outliers from\nknown categories, has gained prominence in practical scenarios. Recently, the\nadvent of vision-language models (VLM) has heightened interest in enhancing OOD\ndetection for VLM through few-shot tuning. However, existing methods mainly\nfocus on optimizing global prompts, ignoring refined utilization of local\ninformation with regard to outliers. Motivated by this, we freeze global\nprompts and introduce Local-Prompt, a novel coarse-to-fine tuning paradigm to\nemphasize regional enhancement with local prompts. Our method comprises two\nintegral components: global prompt guided negative augmentation and local\nprompt enhanced regional regularization. The former utilizes frozen, coarse\nglobal prompts as guiding cues to incorporate negative augmentation, thereby\nleveraging local outlier knowledge. The latter employs trainable local prompts\nand a regional regularization to capture local information effectively, aiding\nin outlier identification. We also propose regional-related metric to empower\nthe enrichment of OOD detection. Moreover, since our approach explores\nenhancing local prompts only, it can be seamlessly integrated with trained\nglobal prompts during inference to boost the performance. Comprehensive\nexperiments demonstrate the effectiveness and potential of our method. Notably,\nour method reduces average FPR95 by 5.17% against state-of-the-art method in\n4-shot tuning on challenging ImageNet-1k dataset, even outperforming 16-shot\nresults of previous methods. Code is released at\nhttps://github.com/AuroraZengfh/Local-Prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-Distribution (OOD) detection, aiming to distinguish outliers from\nknown categories, has gained prominence in practical scenarios. Recently, the\nadvent of vision-language models (VLM) has heightened interest in enhancing OOD\ndetection for VLM through few-shot tuning. However, existing methods mainly\nfocus on optimizing global prompts, ignoring refined utilization of local\ninformation with regard to outliers. Motivated by this, we freeze global\nprompts and introduce Local-Prompt, a novel coarse-to-fine tuning paradigm to\nemphasize regional enhancement with local prompts. Our method comprises two\nintegral components: global prompt guided negative augmentation and local\nprompt enhanced regional regularization. The former utilizes frozen, coarse\nglobal prompts as guiding cues to incorporate negative augmentation, thereby\nleveraging local outlier knowledge. The latter employs trainable local prompts\nand a regional regularization to capture local information effectively, aiding\nin outlier identification. We also propose regional-related metric to empower\nthe enrichment of OOD detection. Moreover, since our approach explores\nenhancing local prompts only, it can be seamlessly integrated with trained\nglobal prompts during inference to boost the performance. Comprehensive\nexperiments demonstrate the effectiveness and potential of our method. Notably,\nour method reduces average FPR95 by 5.17% against state-of-the-art method in\n4-shot tuning on challenging ImageNet-1k dataset, even outperforming 16-shot\nresults of previous methods. Code is released at\nhttps://github.com/AuroraZengfh/Local-Prompt."
                },
                "authors": [
                    {
                        "name": "Fanhu Zeng"
                    },
                    {
                        "name": "Zhen Cheng"
                    },
                    {
                        "name": "Fei Zhu"
                    },
                    {
                        "name": "Hongxin Wei"
                    },
                    {
                        "name": "Xu-Yao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xu-Yao Zhang"
                },
                "author": "Xu-Yao Zhang",
                "arxiv_comment": "Accepted by The Thirteenth International Conference on Learning\n  Representations (ICLR 2025). Code is available at\n  https://github.com/AuroraZengfh/Local-Prompt",
                "arxiv_journal_ref": "The Thirteenth International Conference on Learning\n  Representations (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04796v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04796v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10239v1",
                "updated": "2025-02-14T15:49:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    49,
                    2,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:49:02Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    49,
                    2,
                    4,
                    45,
                    0
                ],
                "title": "Efficient Zero-Order Federated Finetuning of Language Models for\n  Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Zero-Order Federated Finetuning of Language Models for\n  Resource-Constrained Devices"
                },
                "summary": "Federated fine-tuning offers a promising approach for tuning Large Language\nModels (LLMs) on edge devices while preserving data privacy. However,\nfine-tuning these models on edge devices remains challenging due to high\nmemory, communication, and computational demands. Zero-order optimization with\ntask alignment provides a potential solution, enabling fine-tuning with\ninference-level memory requirements but requires a longer convergence time. In\nthis paper, we propose Federated Split-Perturbation Zero-order Optimization\n(FedSPZO) that divides the network into two blocks, applying a different number\nof perturbations per block in a computationally effective way, achieving faster\nconvergence. Our evaluation shows a $2.5 - 7\\times $ reduction in computation\noverhead compared to zero-order state of the art techniques in federated\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated fine-tuning offers a promising approach for tuning Large Language\nModels (LLMs) on edge devices while preserving data privacy. However,\nfine-tuning these models on edge devices remains challenging due to high\nmemory, communication, and computational demands. Zero-order optimization with\ntask alignment provides a potential solution, enabling fine-tuning with\ninference-level memory requirements but requires a longer convergence time. In\nthis paper, we propose Federated Split-Perturbation Zero-order Optimization\n(FedSPZO) that divides the network into two blocks, applying a different number\nof perturbations per block in a computationally effective way, achieving faster\nconvergence. Our evaluation shows a $2.5 - 7\\times $ reduction in computation\noverhead compared to zero-order state of the art techniques in federated\nlearning."
                },
                "authors": [
                    {
                        "name": "Mohamed Aboelenien Ahmed"
                    },
                    {
                        "name": "Kilian Pfeiffer"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Heba Khdr"
                    },
                    {
                        "name": "Jörg Henkel"
                    }
                ],
                "author_detail": {
                    "name": "Jörg Henkel"
                },
                "author": "Jörg Henkel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07780v2",
                "updated": "2025-02-14T15:46:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    46,
                    51,
                    4,
                    45,
                    0
                ],
                "published": "2024-06-12T00:19:40Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    0,
                    19,
                    40,
                    2,
                    164,
                    0
                ],
                "title": "A Critical Look At Tokenwise Reward-Guided Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Critical Look At Tokenwise Reward-Guided Text Generation"
                },
                "summary": "Large language models (LLMs) can be improved by aligning with human\npreferences through fine-tuning -- the so-called reinforcement learning from\nhuman feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive\nfor many users. Due to their ability to bypass LLM fine-tuning, prediction-time\ntokenwise reward-guided text generation (RGTG) methods have recently been\nproposed. They use a reward model trained on full sequences to score partial\nsequences during decoding in a bid to steer the generation towards sequences\nwith high rewards. However, these methods have so far been only heuristically\nmotivated and poorly analyzed. In this work, we show that reward models trained\non full sequences are not compatible with scoring partial sequences. To\nalleviate this issue, we propose to train a Bradley-Terry reward model on\npartial sequences explicitly, and autoregressively sample from the implied\ntokenwise policy during decoding time. We study the properties of this reward\nmodel and the resulting policy: we show that this policy is proportional to the\nratio of two distinct RLHF policies. Our simple approach outperforms previous\nRGTG methods and performs similarly to strong offline baselines without\nlarge-scale LLM finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can be improved by aligning with human\npreferences through fine-tuning -- the so-called reinforcement learning from\nhuman feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive\nfor many users. Due to their ability to bypass LLM fine-tuning, prediction-time\ntokenwise reward-guided text generation (RGTG) methods have recently been\nproposed. They use a reward model trained on full sequences to score partial\nsequences during decoding in a bid to steer the generation towards sequences\nwith high rewards. However, these methods have so far been only heuristically\nmotivated and poorly analyzed. In this work, we show that reward models trained\non full sequences are not compatible with scoring partial sequences. To\nalleviate this issue, we propose to train a Bradley-Terry reward model on\npartial sequences explicitly, and autoregressively sample from the implied\ntokenwise policy during decoding time. We study the properties of this reward\nmodel and the resulting policy: we show that this policy is proportional to the\nratio of two distinct RLHF policies. Our simple approach outperforms previous\nRGTG methods and performs similarly to strong offline baselines without\nlarge-scale LLM finetuning."
                },
                "authors": [
                    {
                        "name": "Ahmad Rashid"
                    },
                    {
                        "name": "Ruotian Wu"
                    },
                    {
                        "name": "Julia Grosse"
                    },
                    {
                        "name": "Agustinus Kristiadi"
                    },
                    {
                        "name": "Pascal Poupart"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Poupart"
                },
                "author": "Pascal Poupart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10235v1",
                "updated": "2025-02-14T15:46:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    46,
                    19,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:46:19Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    46,
                    19,
                    4,
                    45,
                    0
                ],
                "title": "AdaPTS: Adapting Univariate Foundation Models to Probabilistic\n  Multivariate Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaPTS: Adapting Univariate Foundation Models to Probabilistic\n  Multivariate Time Series Forecasting"
                },
                "summary": "Pre-trained foundation models (FMs) have shown exceptional performance in\nunivariate time series forecasting tasks. However, several practical challenges\npersist, including managing intricate dependencies among features and\nquantifying uncertainty in predictions. This study aims to tackle these\ncritical limitations by introducing adapters; feature-space transformations\nthat facilitate the effective use of pre-trained univariate time series FMs for\nmultivariate tasks. Adapters operate by projecting multivariate inputs into a\nsuitable latent space and applying the FM independently to each dimension.\nInspired by the literature on representation learning and partially stochastic\nBayesian neural networks, we present a range of adapters and\noptimization/inference strategies. Experiments conducted on both synthetic and\nreal-world datasets confirm the efficacy of adapters, demonstrating substantial\nenhancements in forecasting accuracy and uncertainty quantification compared to\nbaseline methods. Our framework, AdaPTS, positions adapters as a modular,\nscalable, and effective solution for leveraging time series FMs in multivariate\ncontexts, thereby promoting their wider adoption in real-world applications. We\nrelease the code at https://github.com/abenechehab/AdaPTS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained foundation models (FMs) have shown exceptional performance in\nunivariate time series forecasting tasks. However, several practical challenges\npersist, including managing intricate dependencies among features and\nquantifying uncertainty in predictions. This study aims to tackle these\ncritical limitations by introducing adapters; feature-space transformations\nthat facilitate the effective use of pre-trained univariate time series FMs for\nmultivariate tasks. Adapters operate by projecting multivariate inputs into a\nsuitable latent space and applying the FM independently to each dimension.\nInspired by the literature on representation learning and partially stochastic\nBayesian neural networks, we present a range of adapters and\noptimization/inference strategies. Experiments conducted on both synthetic and\nreal-world datasets confirm the efficacy of adapters, demonstrating substantial\nenhancements in forecasting accuracy and uncertainty quantification compared to\nbaseline methods. Our framework, AdaPTS, positions adapters as a modular,\nscalable, and effective solution for leveraging time series FMs in multivariate\ncontexts, thereby promoting their wider adoption in real-world applications. We\nrelease the code at https://github.com/abenechehab/AdaPTS."
                },
                "authors": [
                    {
                        "name": "Abdelhakim Benechehab"
                    },
                    {
                        "name": "Vasilii Feofanov"
                    },
                    {
                        "name": "Giuseppe Paolo"
                    },
                    {
                        "name": "Albert Thomas"
                    },
                    {
                        "name": "Maurizio Filippone"
                    },
                    {
                        "name": "Balázs Kégl"
                    }
                ],
                "author_detail": {
                    "name": "Balázs Kégl"
                },
                "author": "Balázs Kégl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10233v1",
                "updated": "2025-02-14T15:42:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    42,
                    30,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:42:30Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    42,
                    30,
                    4,
                    45,
                    0
                ],
                "title": "Learning to Solve the Min-Max Mixed-Shelves Picker-Routing Problem via\n  Hierarchical and Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Solve the Min-Max Mixed-Shelves Picker-Routing Problem via\n  Hierarchical and Parallel Decoding"
                },
                "summary": "The Mixed-Shelves Picker Routing Problem (MSPRP) is a fundamental challenge\nin warehouse logistics, where pickers must navigate a mixed-shelves environment\nto retrieve SKUs efficiently. Traditional heuristics and optimization-based\napproaches struggle with scalability, while recent machine learning methods\noften rely on sequential decision-making, leading to high solution latency and\nsuboptimal agent coordination. In this work, we propose a novel hierarchical\nand parallel decoding approach for solving the min-max variant of the MSPRP via\nmulti-agent reinforcement learning. While our approach generates a joint\ndistribution over agent actions, allowing for fast decoding and effective\npicker coordination, our method introduces a sequential action selection to\navoid conflicts in the multi-dimensional action space. Experiments show\nstate-of-the-art performance in both solution quality and inference speed,\nparticularly for large-scale and out-of-distribution instances. Our code is\npublicly available at http://github.com/LTluttmann/marl4msprp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixed-Shelves Picker Routing Problem (MSPRP) is a fundamental challenge\nin warehouse logistics, where pickers must navigate a mixed-shelves environment\nto retrieve SKUs efficiently. Traditional heuristics and optimization-based\napproaches struggle with scalability, while recent machine learning methods\noften rely on sequential decision-making, leading to high solution latency and\nsuboptimal agent coordination. In this work, we propose a novel hierarchical\nand parallel decoding approach for solving the min-max variant of the MSPRP via\nmulti-agent reinforcement learning. While our approach generates a joint\ndistribution over agent actions, allowing for fast decoding and effective\npicker coordination, our method introduces a sequential action selection to\navoid conflicts in the multi-dimensional action space. Experiments show\nstate-of-the-art performance in both solution quality and inference speed,\nparticularly for large-scale and out-of-distribution instances. Our code is\npublicly available at http://github.com/LTluttmann/marl4msprp."
                },
                "authors": [
                    {
                        "name": "Laurin Luttmann"
                    },
                    {
                        "name": "Lin Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lin Xie"
                },
                "author": "Lin Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14050v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14050v3",
                "updated": "2025-02-14T15:39:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    39,
                    29,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-18T17:05:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    5,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation"
                },
                "summary": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. We reduce biases by\nfinetuning on curated non-harmful text, but find only direct preference\noptimization to be effective for mitigating toxicity. The mitigation caused by\napplying these methods in English also transfers to non-English languages. We\nfind evidence that the extent to which transfer takes place can be predicted by\nthe amount of data in a given language present in the model's pretraining data.\nHowever, this transfer of bias and toxicity mitigation often comes at the\nexpense of decreased language generation ability in non-English languages,\nhighlighting the importance of developing language-specific bias and toxicity\nmitigation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. We reduce biases by\nfinetuning on curated non-harmful text, but find only direct preference\noptimization to be effective for mitigating toxicity. The mitigation caused by\napplying these methods in English also transfers to non-English languages. We\nfind evidence that the extent to which transfer takes place can be predicted by\nthe amount of data in a given language present in the model's pretraining data.\nHowever, this transfer of bias and toxicity mitigation often comes at the\nexpense of decreased language generation ability in non-English languages,\nhighlighting the importance of developing language-specific bias and toxicity\nmitigation methods."
                },
                "authors": [
                    {
                        "name": "Vera Neplenbroek"
                    },
                    {
                        "name": "Arianna Bisazza"
                    },
                    {
                        "name": "Raquel Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Fernández"
                },
                "author": "Raquel Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14050v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14050v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08277v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08277v2",
                "updated": "2025-02-14T15:33:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    33,
                    27,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-12T10:31:45Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    10,
                    31,
                    45,
                    2,
                    43,
                    0
                ],
                "title": "ChorusCVR: Chorus Supervision for Entire Space Post-Click Conversion\n  Rate Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChorusCVR: Chorus Supervision for Entire Space Post-Click Conversion\n  Rate Modeling"
                },
                "summary": "Post-click conversion rate (CVR) estimation is a vital task in many\nrecommender systems of revenue businesses, e.g., e-commerce and advertising. In\na perspective of sample, a typical CVR positive sample usually goes through a\nfunnel of exposure to click to conversion. For lack of post-event labels for\nun-clicked samples, CVR learning task commonly only utilizes clicked samples,\nrather than all exposed samples as for click-through rate (CTR) learning task.\nHowever, during online inference, CVR and CTR are estimated on the same assumed\nexposure space, which leads to a inconsistency of sample space between training\nand inference, i.e., sample selection bias (SSB). To alleviate SSB, previous\nwisdom proposes to design novel auxiliary tasks to enable the CVR learning on\nun-click training samples, such as CTCVR and counterfactual CVR, etc. Although\nalleviating SSB to some extent, none of them pay attention to the\ndiscrimination between ambiguous negative samples (un-clicked) and factual\nnegative samples (clicked but un-converted) during modelling, which makes CVR\nmodel lacks robustness. To full this gap, we propose a novel ChorusCVR model to\nrealize debiased CVR learning in entire-space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-click conversion rate (CVR) estimation is a vital task in many\nrecommender systems of revenue businesses, e.g., e-commerce and advertising. In\na perspective of sample, a typical CVR positive sample usually goes through a\nfunnel of exposure to click to conversion. For lack of post-event labels for\nun-clicked samples, CVR learning task commonly only utilizes clicked samples,\nrather than all exposed samples as for click-through rate (CTR) learning task.\nHowever, during online inference, CVR and CTR are estimated on the same assumed\nexposure space, which leads to a inconsistency of sample space between training\nand inference, i.e., sample selection bias (SSB). To alleviate SSB, previous\nwisdom proposes to design novel auxiliary tasks to enable the CVR learning on\nun-click training samples, such as CTCVR and counterfactual CVR, etc. Although\nalleviating SSB to some extent, none of them pay attention to the\ndiscrimination between ambiguous negative samples (un-clicked) and factual\nnegative samples (clicked but un-converted) during modelling, which makes CVR\nmodel lacks robustness. To full this gap, we propose a novel ChorusCVR model to\nrealize debiased CVR learning in entire-space."
                },
                "authors": [
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Yucheng Lu"
                    },
                    {
                        "name": "Boyang Xia"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Kuan Xu"
                    },
                    {
                        "name": "Mingxing Wen"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Jiaming Zhang"
                    },
                    {
                        "name": "Zhaojie Liu"
                    },
                    {
                        "name": "Liyin Hong"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08277v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08277v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02844v3",
                "updated": "2025-02-14T15:32:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    32,
                    0,
                    4,
                    45,
                    0
                ],
                "published": "2025-01-06T08:43:31Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    8,
                    43,
                    31,
                    0,
                    6,
                    0
                ],
                "title": "Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification"
                },
                "summary": "Text classification is a fundamental task in data mining, pivotal to various\napplications such as tabular understanding and recommendation. Although neural\nnetwork-based models, such as CNN and BERT, have demonstrated remarkable\nperformance in text classification, their effectiveness heavily relies on\nabundant labeled training data. This dependency makes these models less\neffective in dynamic few-shot text classification, where labeled data is\nscarce, and new target labels frequently appear based on application needs.\nRecently, large language models (LLMs) have shown promise due to their\nextensive pretraining and contextual understanding ability. Current approaches\nprovide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to classify texts. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. Rather than treating each input\nindependently, GORAG constructs and maintains a weighted graph by extracting\nside information across all target texts. In this graph, text keywords and\nlabels are represented as nodes, with edges indicating the correlations between\nthem. To model these correlations, GORAG employs an edge weighting mechanism to\nprioritize the importance and reliability of extracted information and\ndynamically retrieves relevant context using a minimum-cost spanning tree\ntailored for each text input. Empirical evaluations demonstrate that GORAG\noutperforms existing approaches by providing more comprehensive and precise\ncontextual information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text classification is a fundamental task in data mining, pivotal to various\napplications such as tabular understanding and recommendation. Although neural\nnetwork-based models, such as CNN and BERT, have demonstrated remarkable\nperformance in text classification, their effectiveness heavily relies on\nabundant labeled training data. This dependency makes these models less\neffective in dynamic few-shot text classification, where labeled data is\nscarce, and new target labels frequently appear based on application needs.\nRecently, large language models (LLMs) have shown promise due to their\nextensive pretraining and contextual understanding ability. Current approaches\nprovide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to classify texts. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. Rather than treating each input\nindependently, GORAG constructs and maintains a weighted graph by extracting\nside information across all target texts. In this graph, text keywords and\nlabels are represented as nodes, with edges indicating the correlations between\nthem. To model these correlations, GORAG employs an edge weighting mechanism to\nprioritize the importance and reliability of extracted information and\ndynamically retrieves relevant context using a minimum-cost spanning tree\ntailored for each text input. Empirical evaluations demonstrate that GORAG\noutperforms existing approaches by providing more comprehensive and precise\ncontextual information."
                },
                "authors": [
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Fei Teng"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08346v2",
                "updated": "2025-02-14T15:25:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    25,
                    51,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-12T12:13:51Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    13,
                    51,
                    2,
                    43,
                    0
                ],
                "title": "Graph Foundation Models for Recommendation: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Foundation Models for Recommendation: A Comprehensive Survey"
                },
                "summary": "Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems."
                },
                "authors": [
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Yihang Wang"
                    },
                    {
                        "name": "Yuanhao Zeng"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jiashu Zhao"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yawen Li"
                    },
                    {
                        "name": "Long Xia"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07902v2",
                "updated": "2025-02-14T15:23:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    23,
                    32,
                    4,
                    45,
                    0
                ],
                "published": "2024-11-12T16:21:22Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    16,
                    21,
                    22,
                    1,
                    317,
                    0
                ],
                "title": "Bayes2IMC: In-Memory Computing for Bayesian Binary Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayes2IMC: In-Memory Computing for Bayesian Binary Neural Networks"
                },
                "summary": "Bayesian Neural Networks (BNNs) provide superior estimates of uncertainty by\ngenerating an ensemble of predictive distributions. However, inference via\nensembling is resource-intensive, requiring additional entropy sources to\ngenerate stochasticity which increases resource consumption. We introduce\nBayes2IMC, an in-memory computing (IMC) architecture designed for binary\nBayesian neural networks that leverage nanoscale device stochasticity to\ngenerate desired distributions. Our novel approach utilizes Phase-Change Memory\n(PCM) to harness inherent noise characteristics, enabling the creation of a\nbinary neural network. This design eliminates the necessity for a pre-neuron\nAnalog-to-Digital Converter (ADC), significantly improving power and area\nefficiency. We also develop a hardware-software co-optimized correction method\napplied solely on the logits in the final layer to reduce device-induced\naccuracy variations across deployments on hardware. Additionally, we devise a\nsimple compensation technique that ensures no drop in classification accuracy\ndespite conductance drift of PCM. We validate the effectiveness of our approach\non the CIFAR-10 dataset with a VGGBinaryConnect model, achieving accuracy\nmetrics comparable to ideal software implementations as well as results\nreported in the literature using other technologies. Finally, we present a\ncomplete core architecture and compare its projected power, performance, and\narea efficiency against an equivalent SRAM baseline, showing a $3.8$ to $9.6\n\\times$ improvement in total efficiency (in GOPS/W/mm$^2$) and a $2.2 $ to $5.6\n\\times$ improvement in power efficiency (in GOPS/W). In addition, the projected\nhardware performance of Bayes2IMC surpasses that of most of the BNN\narchitectures based on memristive devices reported in the literature, and\nachieves up to $20\\%$ higher power efficiency compared to the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Neural Networks (BNNs) provide superior estimates of uncertainty by\ngenerating an ensemble of predictive distributions. However, inference via\nensembling is resource-intensive, requiring additional entropy sources to\ngenerate stochasticity which increases resource consumption. We introduce\nBayes2IMC, an in-memory computing (IMC) architecture designed for binary\nBayesian neural networks that leverage nanoscale device stochasticity to\ngenerate desired distributions. Our novel approach utilizes Phase-Change Memory\n(PCM) to harness inherent noise characteristics, enabling the creation of a\nbinary neural network. This design eliminates the necessity for a pre-neuron\nAnalog-to-Digital Converter (ADC), significantly improving power and area\nefficiency. We also develop a hardware-software co-optimized correction method\napplied solely on the logits in the final layer to reduce device-induced\naccuracy variations across deployments on hardware. Additionally, we devise a\nsimple compensation technique that ensures no drop in classification accuracy\ndespite conductance drift of PCM. We validate the effectiveness of our approach\non the CIFAR-10 dataset with a VGGBinaryConnect model, achieving accuracy\nmetrics comparable to ideal software implementations as well as results\nreported in the literature using other technologies. Finally, we present a\ncomplete core architecture and compare its projected power, performance, and\narea efficiency against an equivalent SRAM baseline, showing a $3.8$ to $9.6\n\\times$ improvement in total efficiency (in GOPS/W/mm$^2$) and a $2.2 $ to $5.6\n\\times$ improvement in power efficiency (in GOPS/W). In addition, the projected\nhardware performance of Bayes2IMC surpasses that of most of the BNN\narchitectures based on memristive devices reported in the literature, and\nachieves up to $20\\%$ higher power efficiency compared to the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Prabodh Katti"
                    },
                    {
                        "name": "Clement Ruah"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    },
                    {
                        "name": "Bashir M. Al-Hashimi"
                    },
                    {
                        "name": "Bipin Rajendran"
                    }
                ],
                "author_detail": {
                    "name": "Bipin Rajendran"
                },
                "author": "Bipin Rajendran",
                "arxiv_comment": "Accepted for publication in IEEE Transactions On Circuits and Systems\n  I: Regular Papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10053v2",
                "updated": "2025-02-14T15:20:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    20,
                    47,
                    4,
                    45,
                    0
                ],
                "published": "2025-01-17T09:16:13Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    16,
                    13,
                    4,
                    17,
                    0
                ],
                "title": "AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented\n  Generation using Tree-based Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented\n  Generation using Tree-based Search"
                },
                "summary": "Leveraging the autonomous decision-making capabilities of large language\nmodels (LLMs) has demonstrated superior performance in reasoning tasks.\nHowever, despite the success of iterative or recursive retrieval-augmented\ngeneration (RAG) techniques, these methods are often constrained to a single\nsolution space when confronted with complex problems. In this paper, we propose\na novel thinking pattern in RAG that integrates system analysis with efficient\nreasoning actions, significantly activating intrinsic reasoning capabilities\nand expanding the solution space of specific tasks via Monte Carlo Tree Search\n(MCTS), which we refer to as AirRAG. Specifically, our approach designs five\nfundamental reasoning actions, which are expanded to a broad tree-based\nreasoning space using MCTS. The approach also incorporates self-consistency\nverification to explore potential reasoning paths and inference scaling law.\nAdditionally, computationally optimal strategies are employed to allocate more\ninference resources to key actions, thereby enhancing overall performance.\nExperimental results demonstrate the effectiveness of AirRAG, showing\nsignificant performance gains on complex question-answering datasets.\nFurthermore, AirRAG is flexible and lightweight, making it easy to integrate\nwith other advanced technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the autonomous decision-making capabilities of large language\nmodels (LLMs) has demonstrated superior performance in reasoning tasks.\nHowever, despite the success of iterative or recursive retrieval-augmented\ngeneration (RAG) techniques, these methods are often constrained to a single\nsolution space when confronted with complex problems. In this paper, we propose\na novel thinking pattern in RAG that integrates system analysis with efficient\nreasoning actions, significantly activating intrinsic reasoning capabilities\nand expanding the solution space of specific tasks via Monte Carlo Tree Search\n(MCTS), which we refer to as AirRAG. Specifically, our approach designs five\nfundamental reasoning actions, which are expanded to a broad tree-based\nreasoning space using MCTS. The approach also incorporates self-consistency\nverification to explore potential reasoning paths and inference scaling law.\nAdditionally, computationally optimal strategies are employed to allocate more\ninference resources to key actions, thereby enhancing overall performance.\nExperimental results demonstrate the effectiveness of AirRAG, showing\nsignificant performance gains on complex question-answering datasets.\nFurthermore, AirRAG is flexible and lightweight, making it easy to integrate\nwith other advanced technologies."
                },
                "authors": [
                    {
                        "name": "Wenfeng Feng"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Jingyi Song"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "17 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10215v1",
                "updated": "2025-02-14T15:09:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    9,
                    15,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:09:15Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    9,
                    15,
                    4,
                    45,
                    0
                ],
                "title": "Do Large Language Models Reason Causally Like Us? Even Better?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Reason Causally Like Us? Even Better?"
                },
                "summary": "Causal reasoning is a core component of intelligence. Large language models\n(LLMs) have shown impressive capabilities in generating human-like text,\nraising questions about whether their responses reflect true understanding or\nstatistical patterns. We compared causal reasoning in humans and four LLMs\nusing tasks based on collider graphs, rating the likelihood of a query variable\noccurring given evidence from other variables. We find that LLMs reason\ncausally along a spectrum from human-like to normative inference, with\nalignment shifting based on model, context, and task. Overall, GPT-4o and\nClaude showed the most normative behavior, including \"explaining away\", whereas\nGemini-Pro and GPT-3.5 did not. Although all agents deviated from the expected\nindependence of causes - Claude the least - they exhibited strong associative\nreasoning and predictive inference when assessing the likelihood of the effect\ngiven its causes. These findings underscore the need to assess AI biases as\nthey increasingly assist human decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal reasoning is a core component of intelligence. Large language models\n(LLMs) have shown impressive capabilities in generating human-like text,\nraising questions about whether their responses reflect true understanding or\nstatistical patterns. We compared causal reasoning in humans and four LLMs\nusing tasks based on collider graphs, rating the likelihood of a query variable\noccurring given evidence from other variables. We find that LLMs reason\ncausally along a spectrum from human-like to normative inference, with\nalignment shifting based on model, context, and task. Overall, GPT-4o and\nClaude showed the most normative behavior, including \"explaining away\", whereas\nGemini-Pro and GPT-3.5 did not. Although all agents deviated from the expected\nindependence of causes - Claude the least - they exhibited strong associative\nreasoning and predictive inference when assessing the likelihood of the effect\ngiven its causes. These findings underscore the need to assess AI biases as\nthey increasingly assist human decision-making."
                },
                "authors": [
                    {
                        "name": "Hanna M. Dettki"
                    },
                    {
                        "name": "Brenden M. Lake"
                    },
                    {
                        "name": "Charley M. Wu"
                    },
                    {
                        "name": "Bob Rehder"
                    }
                ],
                "author_detail": {
                    "name": "Bob Rehder"
                },
                "author": "Bob Rehder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10208v1",
                "updated": "2025-02-14T15:03:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    3,
                    43,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:03:43Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    3,
                    43,
                    4,
                    45,
                    0
                ],
                "title": "SGS-GNN: A Supervised Graph Sparsification method for Graph Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGS-GNN: A Supervised Graph Sparsification method for Graph Neural\n  Networks"
                },
                "summary": "We propose SGS-GNN, a novel supervised graph sparsifier that learns the\nsampling probability distribution of edges and samples sparse subgraphs of a\nuser-specified size to reduce the computational costs required by GNNs for\ninference tasks on large graphs. SGS-GNN employs regularizers in the loss\nfunction to enhance homophily in sparse subgraphs, boosting the accuracy of\nGNNs on heterophilic graphs, where a significant number of the neighbors of a\nnode have dissimilar labels. SGS-GNN also supports conditional updates of the\nprobability distribution learning module based on a prior, which helps narrow\nthe search space for sparse graphs. SGS-GNN requires fewer epochs to obtain\nhigh accuracies since it learns the search space of subgraphs more effectively\nthan methods using fixed distributions such as random sampling. Extensive\nexperiments using 33 homophilic and heterophilic graphs demonstrate the\nfollowing: (i) with only 20% of edges retained in the sparse subgraphs, SGS-GNN\nimproves the F1-scores by a geometric mean of 4% relative to the original\ngraph; on heterophilic graphs, the prediction accuracy is better up to 30%.\n(ii) SGS-GNN outperforms state-of-the-art methods with improvement in F1-scores\nof 4-7% in geometric mean with similar sparsities in the sampled subgraphs, and\n(iii) compared to sparsifiers that employ fixed distributions, SGS-GNN requires\nabout half the number of epochs to converge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose SGS-GNN, a novel supervised graph sparsifier that learns the\nsampling probability distribution of edges and samples sparse subgraphs of a\nuser-specified size to reduce the computational costs required by GNNs for\ninference tasks on large graphs. SGS-GNN employs regularizers in the loss\nfunction to enhance homophily in sparse subgraphs, boosting the accuracy of\nGNNs on heterophilic graphs, where a significant number of the neighbors of a\nnode have dissimilar labels. SGS-GNN also supports conditional updates of the\nprobability distribution learning module based on a prior, which helps narrow\nthe search space for sparse graphs. SGS-GNN requires fewer epochs to obtain\nhigh accuracies since it learns the search space of subgraphs more effectively\nthan methods using fixed distributions such as random sampling. Extensive\nexperiments using 33 homophilic and heterophilic graphs demonstrate the\nfollowing: (i) with only 20% of edges retained in the sparse subgraphs, SGS-GNN\nimproves the F1-scores by a geometric mean of 4% relative to the original\ngraph; on heterophilic graphs, the prediction accuracy is better up to 30%.\n(ii) SGS-GNN outperforms state-of-the-art methods with improvement in F1-scores\nof 4-7% in geometric mean with similar sparsities in the sampled subgraphs, and\n(iii) compared to sparsifiers that employ fixed distributions, SGS-GNN requires\nabout half the number of epochs to converge."
                },
                "authors": [
                    {
                        "name": "Siddhartha Shankar Das"
                    },
                    {
                        "name": "Naheed Anjum Arafat"
                    },
                    {
                        "name": "Muftiqur Rahman"
                    },
                    {
                        "name": "S M Ferdous"
                    },
                    {
                        "name": "Alex Pothen"
                    },
                    {
                        "name": "Mahantesh M Halappanavar"
                    }
                ],
                "author_detail": {
                    "name": "Mahantesh M Halappanavar"
                },
                "author": "Mahantesh M Halappanavar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10202v1",
                "updated": "2025-02-14T14:56:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    56,
                    19,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T14:56:19Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    56,
                    19,
                    4,
                    45,
                    0
                ],
                "title": "Can Post-Training Quantization Benefit from an Additional QLoRA\n  Integration?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Post-Training Quantization Benefit from an Additional QLoRA\n  Integration?"
                },
                "summary": "Large language models (LLMs) have transformed natural language processing but\npose significant challenges for real-world deployment. These models necessitate\nconsiderable computing resources, which can be costly and frequently\nunavailable. Model compression techniques such as quantization are often\nleveraged to alleviate resource demand, but they may have a negative impact on\nthe generation quality. In this study, we explore the integration of 4-bit\nPost-training Quantization (PTQ) with QLoRA to address these issues. We\ndemonstrate through extensive experiments that this integration outperforms\nstandard PTQ, and in some cases even 16-bit full-parameter fine-tuning on LLMs,\nvalidated across proprietary and public datasets with different quantization\nalgorithms. The results demonstrate the efficacy of PTQ-QLoRA integration,\noffering a viable solution for deploying powerful LLMs in resource-constrained\nenvironments without compromising on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed natural language processing but\npose significant challenges for real-world deployment. These models necessitate\nconsiderable computing resources, which can be costly and frequently\nunavailable. Model compression techniques such as quantization are often\nleveraged to alleviate resource demand, but they may have a negative impact on\nthe generation quality. In this study, we explore the integration of 4-bit\nPost-training Quantization (PTQ) with QLoRA to address these issues. We\ndemonstrate through extensive experiments that this integration outperforms\nstandard PTQ, and in some cases even 16-bit full-parameter fine-tuning on LLMs,\nvalidated across proprietary and public datasets with different quantization\nalgorithms. The results demonstrate the efficacy of PTQ-QLoRA integration,\noffering a viable solution for deploying powerful LLMs in resource-constrained\nenvironments without compromising on performance."
                },
                "authors": [
                    {
                        "name": "Xiliang Zhu"
                    },
                    {
                        "name": "Elena Khasanova"
                    },
                    {
                        "name": "Cheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Chen"
                },
                "author": "Cheng Chen",
                "arxiv_comment": "Accepted to NAACL 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14679v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14679v4",
                "updated": "2025-02-14T14:55:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    55,
                    40,
                    4,
                    45,
                    0
                ],
                "published": "2025-01-24T17:57:06Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    17,
                    57,
                    6,
                    4,
                    24,
                    0
                ],
                "title": "Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surface Vision Mamba: Leveraging Bidirectional State Space Model for\n  Efficient Spherical Manifold Representation"
                },
                "summary": "Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-based methods have demonstrated exceptional performance in\nmodelling long-range dependencies on spherical cortical surfaces, surpassing\ntraditional Geometric Deep Learning (GDL) models. However, their extensive\ninference time and high memory demands pose challenges for application to large\ndatasets with limited computing resources. Inspired by the state space model in\ncomputer vision, we introduce the attention-free Vision Mamba (Vim) to\nspherical surfaces, presenting a domain-agnostic architecture for analyzing\ndata on spherical manifolds. Our method achieves surface patching by\nrepresenting spherical data as a sequence of triangular patches derived from a\nsubdivided icosphere. The proposed Surface Vision Mamba (SiM) is evaluated on\nmultiple neurodevelopmental phenotype regression tasks using cortical surface\nmetrics from neonatal brains. Experimental results demonstrate that SiM\noutperforms both attention- and GDL-based methods, delivering 4.8 times faster\ninference and achieving 91.7% lower memory consumption compared to the Surface\nVision Transformer (SiT) under the Ico-4 grid partitioning. Sensitivity\nanalysis further underscores the potential of SiM to identify subtle cognitive\ndevelopmental patterns. The code is available at\nhttps://github.com/Rongzhao-He/surface-vision-mamba."
                },
                "authors": [
                    {
                        "name": "Rongzhao He"
                    },
                    {
                        "name": "Weihao Zheng"
                    },
                    {
                        "name": "Leilei Zhao"
                    },
                    {
                        "name": "Ying Wang"
                    },
                    {
                        "name": "Dalin Zhu"
                    },
                    {
                        "name": "Dan Wu"
                    },
                    {
                        "name": "Bin Hu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Hu"
                },
                "author": "Bin Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14679v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14679v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08416v2",
                "updated": "2025-02-14T14:55:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    55,
                    2,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-12T13:59:22Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    59,
                    22,
                    2,
                    43,
                    0
                ],
                "title": "Multifidelity Simulation-based Inference for Computationally Expensive\n  Simulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multifidelity Simulation-based Inference for Computationally Expensive\n  Simulators"
                },
                "summary": "Across many domains of science, stochastic models are an essential tool to\nunderstand the mechanisms underlying empirically observed data. Models can be\nof different levels of detail and accuracy, with models of high-fidelity (i.e.,\nhigh accuracy) to the phenomena under study being often preferable. However,\ninferring parameters of high-fidelity models via simulation-based inference is\nchallenging, especially when the simulator is computationally expensive. We\nintroduce MF-NPE, a multifidelity approach to neural posterior estimation that\nleverages inexpensive low-fidelity simulations to infer parameters of\nhigh-fidelity simulators within a limited simulation budget. MF-NPE performs\nneural posterior estimation with limited high-fidelity resources by virtue of\ntransfer learning, with the ability to prioritize individual observations using\nactive learning. On one statistical task with analytical ground-truth and two\nreal-world tasks, MF-NPE shows comparable performance to current approaches\nwhile requiring up to two orders of magnitude fewer high-fidelity simulations.\nOverall, MF-NPE opens new opportunities to perform efficient Bayesian inference\non computationally expensive simulators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Across many domains of science, stochastic models are an essential tool to\nunderstand the mechanisms underlying empirically observed data. Models can be\nof different levels of detail and accuracy, with models of high-fidelity (i.e.,\nhigh accuracy) to the phenomena under study being often preferable. However,\ninferring parameters of high-fidelity models via simulation-based inference is\nchallenging, especially when the simulator is computationally expensive. We\nintroduce MF-NPE, a multifidelity approach to neural posterior estimation that\nleverages inexpensive low-fidelity simulations to infer parameters of\nhigh-fidelity simulators within a limited simulation budget. MF-NPE performs\nneural posterior estimation with limited high-fidelity resources by virtue of\ntransfer learning, with the ability to prioritize individual observations using\nactive learning. On one statistical task with analytical ground-truth and two\nreal-world tasks, MF-NPE shows comparable performance to current approaches\nwhile requiring up to two orders of magnitude fewer high-fidelity simulations.\nOverall, MF-NPE opens new opportunities to perform efficient Bayesian inference\non computationally expensive simulators."
                },
                "authors": [
                    {
                        "name": "Anastasia N. Krouglova"
                    },
                    {
                        "name": "Hayden R. Johnson"
                    },
                    {
                        "name": "Basile Confavreux"
                    },
                    {
                        "name": "Michael Deistler"
                    },
                    {
                        "name": "Pedro J. Gonçalves"
                    }
                ],
                "author_detail": {
                    "name": "Pedro J. Gonçalves"
                },
                "author": "Pedro J. Gonçalves",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10201v1",
                "updated": "2025-02-14T14:52:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    52,
                    41,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T14:52:41Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    52,
                    41,
                    4,
                    45,
                    0
                ],
                "title": "Prediction hubs are context-informed frequent tokens in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction hubs are context-informed frequent tokens in LLMs"
                },
                "summary": "Hubness, the tendency for few points to be among the nearest neighbours of a\ndisproportionate number of other points, commonly arises when applying standard\ndistance measures to high-dimensional data, often negatively impacting\ndistance-based analysis. As autoregressive large language models (LLMs) operate\non high-dimensional representations, we ask whether they are also affected by\nhubness. We first show, theoretically, that the only representation comparison\noperation performed by LLMs, namely that between context and unembedding\nvectors to determine continuation probabilities, is not characterized by the\nconcentration of distances phenomenon that typically causes the appeareance of\nnuisance hubness. We then empirically show that this comparison still leads to\na high degree of hubness, but the hubs in this case do not constitute a\ndisturbance. They are rather the result of context-modulated frequent tokens\noften appearing in the pool of likely candidates for next token prediction. On\nthe other hand, when other distance computations involving LLM representations\nare performed, we do not have the same theoretical guarantees, and, indeed, we\nsee nuisance hubs appear. In summary, our work highlights, on the one hand, how\nhubness, while omnipresent in high-dimensional spaces, is not always a negative\nproperty that needs to be mitigated, and, on the other hand, it shows that\nvarious widely-used LLMs have developed a guessing strategy that consists in\nconstantly assigning a high probability to frequent tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hubness, the tendency for few points to be among the nearest neighbours of a\ndisproportionate number of other points, commonly arises when applying standard\ndistance measures to high-dimensional data, often negatively impacting\ndistance-based analysis. As autoregressive large language models (LLMs) operate\non high-dimensional representations, we ask whether they are also affected by\nhubness. We first show, theoretically, that the only representation comparison\noperation performed by LLMs, namely that between context and unembedding\nvectors to determine continuation probabilities, is not characterized by the\nconcentration of distances phenomenon that typically causes the appeareance of\nnuisance hubness. We then empirically show that this comparison still leads to\na high degree of hubness, but the hubs in this case do not constitute a\ndisturbance. They are rather the result of context-modulated frequent tokens\noften appearing in the pool of likely candidates for next token prediction. On\nthe other hand, when other distance computations involving LLM representations\nare performed, we do not have the same theoretical guarantees, and, indeed, we\nsee nuisance hubs appear. In summary, our work highlights, on the one hand, how\nhubness, while omnipresent in high-dimensional spaces, is not always a negative\nproperty that needs to be mitigated, and, on the other hand, it shows that\nvarious widely-used LLMs have developed a guessing strategy that consists in\nconstantly assigning a high probability to frequent tokens."
                },
                "authors": [
                    {
                        "name": "Beatrix M. G. Nielsen"
                    },
                    {
                        "name": "Iuri Macocco"
                    },
                    {
                        "name": "Marco Baroni"
                    }
                ],
                "author_detail": {
                    "name": "Marco Baroni"
                },
                "author": "Marco Baroni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01237v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01237v2",
                "updated": "2025-02-14T14:47:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    47,
                    26,
                    4,
                    45,
                    0
                ],
                "published": "2025-01-02T12:55:27Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    12,
                    55,
                    27,
                    3,
                    2,
                    0
                ],
                "title": "Self-Refinement Strategies for LLM-based Product Attribute Value\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Refinement Strategies for LLM-based Product Attribute Value\n  Extraction"
                },
                "summary": "Structured product data, in the form of attribute-value pairs, is essential\nfor e-commerce platforms to support features such as faceted product search and\nattribute-based product comparison. However, vendors often provide unstructured\nproduct descriptions, making attribute value extraction necessary to ensure\ndata consistency and usability. Large language models (LLMs) have demonstrated\ntheir potential for product attribute value extraction in few-shot scenarios.\nRecent research has shown that self-refinement techniques can improve the\nperformance of LLMs on tasks such as code generation and text-to-SQL\ntranslation. For other tasks, the application of these techniques has resulted\nin increased costs due to processing additional tokens, without achieving any\nimprovement in performance. This paper investigates applying two\nself-refinement techniques (error-based prompt rewriting and self-correction)\nto the product attribute value extraction task. The self-refinement techniques\nare evaluated across zero-shot, few-shot in-context learning, and fine-tuning\nscenarios using GPT-4o. The experiments show that both self-refinement\ntechniques fail to significantly improve the extraction performance while\nsubstantially increasing processing costs. For scenarios with development data,\nfine-tuning yields the highest performance, while the ramp-up costs of\nfine-tuning are balanced out as the amount of product descriptions increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured product data, in the form of attribute-value pairs, is essential\nfor e-commerce platforms to support features such as faceted product search and\nattribute-based product comparison. However, vendors often provide unstructured\nproduct descriptions, making attribute value extraction necessary to ensure\ndata consistency and usability. Large language models (LLMs) have demonstrated\ntheir potential for product attribute value extraction in few-shot scenarios.\nRecent research has shown that self-refinement techniques can improve the\nperformance of LLMs on tasks such as code generation and text-to-SQL\ntranslation. For other tasks, the application of these techniques has resulted\nin increased costs due to processing additional tokens, without achieving any\nimprovement in performance. This paper investigates applying two\nself-refinement techniques (error-based prompt rewriting and self-correction)\nto the product attribute value extraction task. The self-refinement techniques\nare evaluated across zero-shot, few-shot in-context learning, and fine-tuning\nscenarios using GPT-4o. The experiments show that both self-refinement\ntechniques fail to significantly improve the extraction performance while\nsubstantially increasing processing costs. For scenarios with development data,\nfine-tuning yields the highest performance, while the ramp-up costs of\nfine-tuning are balanced out as the amount of product descriptions increases."
                },
                "authors": [
                    {
                        "name": "Alexander Brinkmann"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01237v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22099v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22099v4",
                "updated": "2025-02-14T14:46:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    46,
                    3,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-29T14:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    53,
                    10,
                    1,
                    303,
                    0
                ],
                "title": "TractShapeNet: Efficient Multi-Shape Learning with 3D Tractography Point\n  Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TractShapeNet: Efficient Multi-Shape Learning with 3D Tractography Point\n  Clouds"
                },
                "summary": "Brain imaging studies have demonstrated that diffusion MRI tractography\ngeometric shape descriptors can inform the study of the brain's white matter\npathways and their relationship to brain function. In this work, we investigate\nthe possibility of utilizing a deep learning model to compute shape measures of\nthe brain's white matter connections. We introduce a novel framework,\nTractShapeNet, that leverages a point cloud representation of tractography to\ncompute five shape measures: length, span, volume, total surface area, and\nirregularity. We assess the performance of the method on a large dataset\nincluding 1065 healthy young adults. Experiments for shape measure computation\ndemonstrate that our proposed TractShapeNet outperforms other point cloud-based\nneural network models in both the Pearson correlation coefficient and\nnormalized error metrics. We compare the inference runtime results with the\nconventional shape computation tool DSI-Studio. Our results demonstrate that a\ndeep learning approach enables faster and more efficient shape measure\ncomputation. We also conduct experiments on two downstream language cognition\nprediction tasks, showing that shape measures from TractShapeNet perform\nsimilarly to those computed by DSI-Studio. Our code will be available at:\nhttps://github.com/SlicerDMRI/TractShapeNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain imaging studies have demonstrated that diffusion MRI tractography\ngeometric shape descriptors can inform the study of the brain's white matter\npathways and their relationship to brain function. In this work, we investigate\nthe possibility of utilizing a deep learning model to compute shape measures of\nthe brain's white matter connections. We introduce a novel framework,\nTractShapeNet, that leverages a point cloud representation of tractography to\ncompute five shape measures: length, span, volume, total surface area, and\nirregularity. We assess the performance of the method on a large dataset\nincluding 1065 healthy young adults. Experiments for shape measure computation\ndemonstrate that our proposed TractShapeNet outperforms other point cloud-based\nneural network models in both the Pearson correlation coefficient and\nnormalized error metrics. We compare the inference runtime results with the\nconventional shape computation tool DSI-Studio. Our results demonstrate that a\ndeep learning approach enables faster and more efficient shape measure\ncomputation. We also conduct experiments on two downstream language cognition\nprediction tasks, showing that shape measures from TractShapeNet perform\nsimilarly to those computed by DSI-Studio. Our code will be available at:\nhttps://github.com/SlicerDMRI/TractShapeNet."
                },
                "authors": [
                    {
                        "name": "Yui Lo"
                    },
                    {
                        "name": "Yuqian Chen"
                    },
                    {
                        "name": "Dongnan Liu"
                    },
                    {
                        "name": "Jon Haitz Legarreta"
                    },
                    {
                        "name": "Leo Zekelman"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Jarrett Rushmore"
                    },
                    {
                        "name": "Yogesh Rathi"
                    },
                    {
                        "name": "Nikos Makris"
                    },
                    {
                        "name": "Alexandra J. Golby"
                    },
                    {
                        "name": "Weidong Cai"
                    },
                    {
                        "name": "Lauren J. O'Donnell"
                    }
                ],
                "author_detail": {
                    "name": "Lauren J. O'Donnell"
                },
                "author": "Lauren J. O'Donnell",
                "arxiv_comment": "10 pages, 2 figures, 4 tables. This work has been accepted to 2025\n  IEEE 22nd International Symposium on Biomedical Imaging (ISBI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22099v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22099v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10197v1",
                "updated": "2025-02-14T14:44:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    44,
                    22,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T14:44:22Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    44,
                    22,
                    4,
                    45,
                    0
                ],
                "title": "MathConstruct: Challenging LLM Reasoning with Constructive Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathConstruct: Challenging LLM Reasoning with Constructive Proofs"
                },
                "summary": "While Large Language Models (LLMs) demonstrate impressive performance in\nmathematics, existing math benchmarks come with significant limitations. Many\nfocus on problems with fixed ground-truth answers, and are often saturated due\nto problem simplicity or the viability of guessing or memorization. Crucially,\nthey capture only a narrow subset of relevant math problems. To address this\nresearch gap, we introduce \\mc, a new benchmark of 126 challenging problems\nsourced from various math competitions, which targets constructive proofs, a\nwidely encountered problem type requiring the construction of mathematical\nobjects with specific properties. These proofs are particularly suitable for\nLLM evaluation, as solution correctness can be easily verified. Our automated\nverifiers also enable MathConstruct to generate problem variations, used to\nevaluate robustness. State-of-the-art LLMs solve only 54% of MathConstruct\nproblems, highlighting its complexity and importance for LLM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) demonstrate impressive performance in\nmathematics, existing math benchmarks come with significant limitations. Many\nfocus on problems with fixed ground-truth answers, and are often saturated due\nto problem simplicity or the viability of guessing or memorization. Crucially,\nthey capture only a narrow subset of relevant math problems. To address this\nresearch gap, we introduce \\mc, a new benchmark of 126 challenging problems\nsourced from various math competitions, which targets constructive proofs, a\nwidely encountered problem type requiring the construction of mathematical\nobjects with specific properties. These proofs are particularly suitable for\nLLM evaluation, as solution correctness can be easily verified. Our automated\nverifiers also enable MathConstruct to generate problem variations, used to\nevaluate robustness. State-of-the-art LLMs solve only 54% of MathConstruct\nproblems, highlighting its complexity and importance for LLM evaluation."
                },
                "authors": [
                    {
                        "name": "Mislav Balunović"
                    },
                    {
                        "name": "Jasper Dekoninck"
                    },
                    {
                        "name": "Nikola Jovanović"
                    },
                    {
                        "name": "Ivo Petrov"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07640v2",
                "updated": "2025-02-14T14:40:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    40,
                    12,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-11T15:27:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    27,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving"
                },
                "summary": "We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. Despite using only supervised\nfine-tuning, our final prover significantly outperforms the previous best\nopen-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning.\nOn the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32),\nsurpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover\nsuccessfully solves 7 problems (Pass@512), ranking first on the leaderboard.\nFurthermore, it generates 29.7K formal proofs for Lean Workbook problems,\nnearly doubling the 15.7K produced by earlier works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. Despite using only supervised\nfine-tuning, our final prover significantly outperforms the previous best\nopen-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning.\nOn the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32),\nsurpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover\nsuccessfully solves 7 problems (Pass@512), ranking first on the leaderboard.\nFurthermore, it generates 29.7K formal proofs for Lean Workbook problems,\nnearly doubling the 15.7K produced by earlier works."
                },
                "authors": [
                    {
                        "name": "Yong Lin"
                    },
                    {
                        "name": "Shange Tang"
                    },
                    {
                        "name": "Bohan Lyu"
                    },
                    {
                        "name": "Jiayun Wu"
                    },
                    {
                        "name": "Hongzhou Lin"
                    },
                    {
                        "name": "Kaiyu Yang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Mengzhou Xia"
                    },
                    {
                        "name": "Danqi Chen"
                    },
                    {
                        "name": "Sanjeev Arora"
                    },
                    {
                        "name": "Chi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chi Jin"
                },
                "author": "Chi Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09500v2",
                "updated": "2025-02-14T14:39:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    39,
                    22,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-13T17:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    10,
                    43,
                    3,
                    44,
                    0
                ],
                "title": "Eidetic Learning: an Efficient and Provable Solution to Catastrophic\n  Forgetting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eidetic Learning: an Efficient and Provable Solution to Catastrophic\n  Forgetting"
                },
                "summary": "Catastrophic forgetting -- the phenomenon of a neural network learning a task\nt1 and losing the ability to perform it after being trained on some other task\nt2 -- is a long-standing problem for neural networks [McCloskey and Cohen,\n1989]. We present a method, Eidetic Learning, that provably solves catastrophic\nforgetting. A network trained with Eidetic Learning -- here, an EideticNet --\nrequires no rehearsal or replay. We consider successive discrete tasks and show\nhow at inference time an EideticNet automatically routes new instances without\nauxiliary task information. An EideticNet bears a family resemblance to the\nsparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network\ncapacity is partitioned across tasks and the network itself performs\ndata-conditional routing. An EideticNet is easy to implement and train, is\nefficient, and has time and space complexity linear in the number of\nparameters. The guarantee of our method holds for normalization layers of\nmodern neural networks during both pre-training and fine-tuning. We show with a\nvariety of network architectures and sets of tasks that EideticNets are immune\nto forgetting. While the practical benefits of EideticNets are substantial, we\nbelieve they can be benefit practitioners and theorists alike. The code for\ntraining EideticNets is available at\nhttps://github.com/amazon-science/eideticnet-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catastrophic forgetting -- the phenomenon of a neural network learning a task\nt1 and losing the ability to perform it after being trained on some other task\nt2 -- is a long-standing problem for neural networks [McCloskey and Cohen,\n1989]. We present a method, Eidetic Learning, that provably solves catastrophic\nforgetting. A network trained with Eidetic Learning -- here, an EideticNet --\nrequires no rehearsal or replay. We consider successive discrete tasks and show\nhow at inference time an EideticNet automatically routes new instances without\nauxiliary task information. An EideticNet bears a family resemblance to the\nsparsely-gated Mixture-of-Experts layer Shazeer et al. [2016] in that network\ncapacity is partitioned across tasks and the network itself performs\ndata-conditional routing. An EideticNet is easy to implement and train, is\nefficient, and has time and space complexity linear in the number of\nparameters. The guarantee of our method holds for normalization layers of\nmodern neural networks during both pre-training and fine-tuning. We show with a\nvariety of network architectures and sets of tasks that EideticNets are immune\nto forgetting. While the practical benefits of EideticNets are substantial, we\nbelieve they can be benefit practitioners and theorists alike. The code for\ntraining EideticNets is available at\nhttps://github.com/amazon-science/eideticnet-training."
                },
                "authors": [
                    {
                        "name": "Nicholas Dronen"
                    },
                    {
                        "name": "Randall Balestriero"
                    }
                ],
                "author_detail": {
                    "name": "Randall Balestriero"
                },
                "author": "Randall Balestriero",
                "arxiv_comment": "16 pages, 6 figures; code is available at\n  https://github.com/amazon-science/eideticnet-training",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03970v2",
                "updated": "2025-02-14T14:37:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    37,
                    7,
                    4,
                    45,
                    0
                ],
                "published": "2024-02-06T12:59:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    12,
                    59,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Is Deep Learning finally better than Decision Trees on Tabular Data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Deep Learning finally better than Decision Trees on Tabular Data?"
                },
                "summary": "Tabular data is a ubiquitous data modality due to its versatility and ease of\nuse in many real-world applications. The predominant heuristics for handling\nclassification tasks on tabular data rely on classical machine learning\ntechniques, as the superiority of deep learning models has not yet been\ndemonstrated. This raises the question of whether new deep learning paradigms\ncan surpass classical approaches. Recent studies on tabular data offer a unique\nperspective on the limitations of neural networks in this domain and highlight\nthe superiority of gradient boosted decision trees (GBDTs) in terms of\nscalability and robustness across various datasets. However, novel foundation\nmodels have not been thoroughly assessed regarding quality or fairly compared\nto existing methods for tabular classification. Our study categorizes ten\nstate-of-the-art neural models based on their underlying learning paradigm,\ndemonstrating specifically that meta-learned foundation models outperform GBDTs\nin small data regimes. Although dataset-specific neural networks generally\noutperform LLM-based tabular classifiers, they are surpassed by an AutoML\nlibrary which exhibits the best performance but at the cost of higher\ncomputational demands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data is a ubiquitous data modality due to its versatility and ease of\nuse in many real-world applications. The predominant heuristics for handling\nclassification tasks on tabular data rely on classical machine learning\ntechniques, as the superiority of deep learning models has not yet been\ndemonstrated. This raises the question of whether new deep learning paradigms\ncan surpass classical approaches. Recent studies on tabular data offer a unique\nperspective on the limitations of neural networks in this domain and highlight\nthe superiority of gradient boosted decision trees (GBDTs) in terms of\nscalability and robustness across various datasets. However, novel foundation\nmodels have not been thoroughly assessed regarding quality or fairly compared\nto existing methods for tabular classification. Our study categorizes ten\nstate-of-the-art neural models based on their underlying learning paradigm,\ndemonstrating specifically that meta-learned foundation models outperform GBDTs\nin small data regimes. Although dataset-specific neural networks generally\noutperform LLM-based tabular classifiers, they are surpassed by an AutoML\nlibrary which exhibits the best performance but at the cost of higher\ncomputational demands."
                },
                "authors": [
                    {
                        "name": "Guri Zabërgja"
                    },
                    {
                        "name": "Arlind Kadra"
                    },
                    {
                        "name": "Christian M. M. Frey"
                    },
                    {
                        "name": "Josif Grabocka"
                    }
                ],
                "author_detail": {
                    "name": "Josif Grabocka"
                },
                "author": "Josif Grabocka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10194v1",
                "updated": "2025-02-14T14:36:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    36,
                    47,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T14:36:47Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    36,
                    47,
                    4,
                    45,
                    0
                ],
                "title": "Translating Common Security Assertions Across Processor Designs: A\n  RISC-V Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating Common Security Assertions Across Processor Designs: A\n  RISC-V Case Study"
                },
                "summary": "RISC-V is gaining popularity for its adaptability and cost-effectiveness in\nprocessor design. With the increasing adoption of RISC-V, the importance of\nimplementing robust security verification has grown significantly. In the state\nof the art, various approaches have been developed to strengthen the security\nverification process. Among these methods, assertion-based security\nverification has proven to be a promising approach for ensuring that security\nfeatures are effectively met. To this end, some approaches manually define\nsecurity assertions for processor designs; however, these manual methods\nrequire significant time, cost, and human expertise. Consequently, recent\napproaches focus on translating pre-defined security assertions from one design\nto another. Nonetheless, these methods are not primarily centered on processor\nsecurity, particularly RISC-V. Furthermore, many of these approaches have not\nbeen validated against real-world attacks, such as hardware Trojans. In this\nwork, we introduce a methodology for translating security assertions across\nprocessors with different architectures, using RISC-V as a case study. Our\napproach reduces time and cost compared to developing security assertions\nmanually from the outset. Our methodology was applied to five critical security\nmodules with assertion translation achieving nearly 100% success across all\nmodules. These results validate the efficacy of our approach and highlight its\npotential for enhancing security verification in modern processor designs. The\neffectiveness of the translated assertions was rigorously tested against\nhardware Trojans defined by large language models (LLMs), demonstrating their\nreliability in detecting security breaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RISC-V is gaining popularity for its adaptability and cost-effectiveness in\nprocessor design. With the increasing adoption of RISC-V, the importance of\nimplementing robust security verification has grown significantly. In the state\nof the art, various approaches have been developed to strengthen the security\nverification process. Among these methods, assertion-based security\nverification has proven to be a promising approach for ensuring that security\nfeatures are effectively met. To this end, some approaches manually define\nsecurity assertions for processor designs; however, these manual methods\nrequire significant time, cost, and human expertise. Consequently, recent\napproaches focus on translating pre-defined security assertions from one design\nto another. Nonetheless, these methods are not primarily centered on processor\nsecurity, particularly RISC-V. Furthermore, many of these approaches have not\nbeen validated against real-world attacks, such as hardware Trojans. In this\nwork, we introduce a methodology for translating security assertions across\nprocessors with different architectures, using RISC-V as a case study. Our\napproach reduces time and cost compared to developing security assertions\nmanually from the outset. Our methodology was applied to five critical security\nmodules with assertion translation achieving nearly 100% success across all\nmodules. These results validate the efficacy of our approach and highlight its\npotential for enhancing security verification in modern processor designs. The\neffectiveness of the translated assertions was rigorously tested against\nhardware Trojans defined by large language models (LLMs), demonstrating their\nreliability in detecting security breaches."
                },
                "authors": [
                    {
                        "name": "Sharjeel Imtiaz"
                    },
                    {
                        "name": "Uljana Reinsalu"
                    },
                    {
                        "name": "Tara Ghasempouri"
                    }
                ],
                "author_detail": {
                    "name": "Tara Ghasempouri"
                },
                "author": "Tara Ghasempouri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18940v2",
                "updated": "2025-02-14T14:22:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    22,
                    55,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-25T16:23:32Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    16,
                    23,
                    32,
                    2,
                    360,
                    0
                ],
                "title": "Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations"
                },
                "summary": "Songwriting is often driven by multimodal inspirations, such as imagery,\nnarratives, or existing music, yet songwriters remain unsupported by current\nmusic AI systems in incorporating these multimodal inputs into their creative\nprocesses. We introduce Amuse, a songwriting assistant that transforms\nmultimodal (image, text, or audio) inputs into chord progressions that can be\nseamlessly incorporated into songwriters' creative processes. A key feature of\nAmuse is its novel method for generating coherent chords that are relevant to\nmusic keywords in the absence of datasets with paired examples of multimodal\ninputs and chords. Specifically, we propose a method that leverages multimodal\nlarge language models (LLMs) to convert multimodal inputs into noisy chord\nsuggestions and uses a unimodal chord model to filter the suggestions. A user\nstudy with songwriters shows that Amuse effectively supports transforming\nmultimodal ideas into coherent musical suggestions, enhancing users' agency and\ncreativity throughout the songwriting process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Songwriting is often driven by multimodal inspirations, such as imagery,\nnarratives, or existing music, yet songwriters remain unsupported by current\nmusic AI systems in incorporating these multimodal inputs into their creative\nprocesses. We introduce Amuse, a songwriting assistant that transforms\nmultimodal (image, text, or audio) inputs into chord progressions that can be\nseamlessly incorporated into songwriters' creative processes. A key feature of\nAmuse is its novel method for generating coherent chords that are relevant to\nmusic keywords in the absence of datasets with paired examples of multimodal\ninputs and chords. Specifically, we propose a method that leverages multimodal\nlarge language models (LLMs) to convert multimodal inputs into noisy chord\nsuggestions and uses a unimodal chord model to filter the suggestions. A user\nstudy with songwriters shows that Amuse effectively supports transforming\nmultimodal ideas into coherent musical suggestions, enhancing users' agency and\ncreativity throughout the songwriting process."
                },
                "authors": [
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Sung-Ju Lee"
                    },
                    {
                        "name": "Chris Donahue"
                    }
                ],
                "author_detail": {
                    "name": "Chris Donahue"
                },
                "author": "Chris Donahue",
                "arxiv_comment": "Published as a conference paper at CHI 2025. Project page:\n  https://yewon-kim.com/amuse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10178v1",
                "updated": "2025-02-14T14:13:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    13,
                    55,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T14:13:55Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    13,
                    55,
                    4,
                    45,
                    0
                ],
                "title": "From Markov to Laplace: How Mamba In-Context Learns Markov Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Markov to Laplace: How Mamba In-Context Learns Markov Chains"
                },
                "summary": "While transformer-based language models have driven the AI revolution thus\nfar, their computational complexity has spurred growing interest in viable\nalternatives, such as structured state space sequence models (SSMs) and\nSelective SSMs. Among these, Mamba (S6) and its variant Mamba-2 have shown\nremarkable inference speed ups over transformers while achieving comparable or\nsuperior performance on complex language modeling tasks. However, despite these\narchitectural innovations and empirical successes, the fundamental learning\ncapabilities of Mamba remain poorly understood. In this paper, we address this\ngap by studying in-context learning (ICL) on Markov chains and uncovering a\nsurprising phenomenon: unlike transformers, even a single-layer Mamba\nefficiently learns the in-context Laplacian smoothing estimator, which is both\nBayes and minimax optimal, for all Markovian orders. To explain this, we\ntheoretically characterize the representation capacity of Mamba and reveal the\nfundamental role of convolution in enabling it to represent the optimal\nLaplacian smoothing. These theoretical insights align strongly with empirical\nresults and, to the best of our knowledge, represent the first formal\nconnection between Mamba and optimal statistical estimators. Finally, we\noutline promising research directions inspired by these findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While transformer-based language models have driven the AI revolution thus\nfar, their computational complexity has spurred growing interest in viable\nalternatives, such as structured state space sequence models (SSMs) and\nSelective SSMs. Among these, Mamba (S6) and its variant Mamba-2 have shown\nremarkable inference speed ups over transformers while achieving comparable or\nsuperior performance on complex language modeling tasks. However, despite these\narchitectural innovations and empirical successes, the fundamental learning\ncapabilities of Mamba remain poorly understood. In this paper, we address this\ngap by studying in-context learning (ICL) on Markov chains and uncovering a\nsurprising phenomenon: unlike transformers, even a single-layer Mamba\nefficiently learns the in-context Laplacian smoothing estimator, which is both\nBayes and minimax optimal, for all Markovian orders. To explain this, we\ntheoretically characterize the representation capacity of Mamba and reveal the\nfundamental role of convolution in enabling it to represent the optimal\nLaplacian smoothing. These theoretical insights align strongly with empirical\nresults and, to the best of our knowledge, represent the first formal\nconnection between Mamba and optimal statistical estimators. Finally, we\noutline promising research directions inspired by these findings."
                },
                "authors": [
                    {
                        "name": "Marco Bondaschi"
                    },
                    {
                        "name": "Nived Rajaraman"
                    },
                    {
                        "name": "Xiuying Wei"
                    },
                    {
                        "name": "Kannan Ramchandran"
                    },
                    {
                        "name": "Razvan Pascanu"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    },
                    {
                        "name": "Michael Gastpar"
                    },
                    {
                        "name": "Ashok Vardhan Makkuva"
                    }
                ],
                "author_detail": {
                    "name": "Ashok Vardhan Makkuva"
                },
                "author": "Ashok Vardhan Makkuva",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15451v2",
                "updated": "2025-02-14T14:03:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    3,
                    43,
                    4,
                    45,
                    0
                ],
                "published": "2025-01-26T08:45:37Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    8,
                    45,
                    37,
                    6,
                    26,
                    0
                ],
                "title": "STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity\n  Extraction in Chinese Hate Speech Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity\n  Extraction in Chinese Hate Speech Detection"
                },
                "summary": "The proliferation of hate speech has caused significant harm to society. The\nintensity and directionality of hate are closely tied to the target and\nargument it is associated with. However, research on hate speech detection in\nChinese has lagged behind, and existing datasets lack span-level fine-grained\nannotations. Furthermore, the lack of research on Chinese hateful slang poses a\nsignificant challenge. In this paper, we provide a solution for fine-grained\ndetection of Chinese hate speech. First, we construct a dataset containing\nTarget-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first\nspan-level Chinese hate speech dataset. Secondly, we evaluate the span-level\nhate speech detection performance of existing models using STATE ToxiCN.\nFinally, we conduct the first study on Chinese hateful slang and evaluate the\nability of LLMs to detect such expressions. Our work contributes valuable\nresources and insights to advance span-level hate speech detection in Chinese.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of hate speech has caused significant harm to society. The\nintensity and directionality of hate are closely tied to the target and\nargument it is associated with. However, research on hate speech detection in\nChinese has lagged behind, and existing datasets lack span-level fine-grained\nannotations. Furthermore, the lack of research on Chinese hateful slang poses a\nsignificant challenge. In this paper, we provide a solution for fine-grained\ndetection of Chinese hate speech. First, we construct a dataset containing\nTarget-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first\nspan-level Chinese hate speech dataset. Secondly, we evaluate the span-level\nhate speech detection performance of existing models using STATE ToxiCN.\nFinally, we conduct the first study on Chinese hateful slang and evaluate the\nability of LLMs to detect such expressions. Our work contributes valuable\nresources and insights to advance span-level hate speech detection in Chinese."
                },
                "authors": [
                    {
                        "name": "Zewen Bai"
                    },
                    {
                        "name": "Yuanyuan Sun"
                    },
                    {
                        "name": "Shengdi Yin"
                    },
                    {
                        "name": "Junyu Lu"
                    },
                    {
                        "name": "Jingjie Zeng"
                    },
                    {
                        "name": "Haohao Zhu"
                    },
                    {
                        "name": "Liang Yang"
                    },
                    {
                        "name": "Hongfei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Hongfei Lin"
                },
                "author": "Hongfei Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09078v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09078v3",
                "updated": "2025-02-14T13:46:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    46,
                    53,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-12T09:01:18Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    1,
                    18,
                    3,
                    347,
                    0
                ],
                "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency.Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency.Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought."
                },
                "authors": [
                    {
                        "name": "Zhenni Bi"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Chuanjian Liu"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "arxiv_comment": "Code will be available at\n  https://github.com/iamhankai/Forest-of-Thought",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09078v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09078v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10157v1",
                "updated": "2025-02-14T13:36:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    36,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:36:20Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    36,
                    20,
                    4,
                    45,
                    0
                ],
                "title": "SessionRec: Next Session Prediction Paradigm For Generative Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SessionRec: Next Session Prediction Paradigm For Generative Sequential\n  Recommendation"
                },
                "summary": "We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for\ngenerative sequential recommendation, addressing the fundamental misalignment\nbetween conventional next-item prediction paradigm (NIPP) and real-world\nrecommendation scenarios. Unlike NIPP's item-level autoregressive generation\nthat contradicts actual session-based user interactions, our framework\nintroduces a session-aware representation learning through hierarchical\nsequence aggregation (intra/inter-session), reducing attention computation\ncomplexity while enabling implicit modeling of massive negative interactions,\nand a session-based prediction objective that better captures users' diverse\ninterests through multi-item recommendation in next sessions. Moreover, we\nfound that incorporating a rank loss for items within the session under the\nnext session prediction paradigm can significantly improve the ranking\neffectiveness of generative sequence recommendation models. We also verified\nthat SessionRec exhibits clear power-law scaling laws similar to those observed\nin LLMs. Extensive experiments conducted on public datasets and online A/B test\nin Meituan App demonstrate the effectiveness of SessionRec. The proposed\nparadigm establishes new foundations for developing industrial-scale generative\nrecommendation systems through its model-agnostic architecture and\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for\ngenerative sequential recommendation, addressing the fundamental misalignment\nbetween conventional next-item prediction paradigm (NIPP) and real-world\nrecommendation scenarios. Unlike NIPP's item-level autoregressive generation\nthat contradicts actual session-based user interactions, our framework\nintroduces a session-aware representation learning through hierarchical\nsequence aggregation (intra/inter-session), reducing attention computation\ncomplexity while enabling implicit modeling of massive negative interactions,\nand a session-based prediction objective that better captures users' diverse\ninterests through multi-item recommendation in next sessions. Moreover, we\nfound that incorporating a rank loss for items within the session under the\nnext session prediction paradigm can significantly improve the ranking\neffectiveness of generative sequence recommendation models. We also verified\nthat SessionRec exhibits clear power-law scaling laws similar to those observed\nin LLMs. Extensive experiments conducted on public datasets and online A/B test\nin Meituan App demonstrate the effectiveness of SessionRec. The proposed\nparadigm establishes new foundations for developing industrial-scale generative\nrecommendation systems through its model-agnostic architecture and\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Hao Guo"
                    },
                    {
                        "name": "Linzhi Peng"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Xiaoteng Wang"
                    },
                    {
                        "name": "Daoyuan Wang"
                    },
                    {
                        "name": "Shichao Wang"
                    },
                    {
                        "name": "Jinpeng Wang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Sheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Chen"
                },
                "author": "Sheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10151v1",
                "updated": "2025-02-14T13:25:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    25,
                    29,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:25:29Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    25,
                    29,
                    4,
                    45,
                    0
                ],
                "title": "Semantica: Decentralized Search using a LLM-Guided Semantic Tree Overlay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantica: Decentralized Search using a LLM-Guided Semantic Tree Overlay"
                },
                "summary": "Centralized search engines are key for the Internet, but lead to undesirable\nconcentration of power. Decentralized alternatives fail to offer equal document\nretrieval accuracy and speed. Nevertheless, Semantic Overlay Networks can come\nclose to the performance of centralized solutions when the semantics of\ndocuments are properly captured. This work uses embeddings from Large Language\nModels to capture semantics and fulfill the promise of Semantic Overlay\nNetworks. Our proposed algorithm, called Semantica, constructs a prefix tree\n(trie) utilizing document embeddings calculated by a language model. Users\nconnect to each other based on the embeddings of their documents, ensuring that\nsemantically similar users are directly linked. Thereby, this construction\nmakes it more likely for user searches to be answered by the users that they\nare directly connected to, or by the users they are close to in the network\nconnection graph. The implementation of our algorithm also accommodates the\nsemantic diversity of individual users by spawning \"clone\" user identifiers in\nthe tree. Our experiments use emulation with a real-world workload to show\nSemantica's ability to identify and connect to similar users quickly. Semantica\nfinds up to ten times more semantically similar users than current\nstate-of-the-art approaches. At the same time, Semantica can retrieve more than\ntwo times the number of relevant documents given the same network load. We also\nmake our code publicly available to facilitate further research in the area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralized search engines are key for the Internet, but lead to undesirable\nconcentration of power. Decentralized alternatives fail to offer equal document\nretrieval accuracy and speed. Nevertheless, Semantic Overlay Networks can come\nclose to the performance of centralized solutions when the semantics of\ndocuments are properly captured. This work uses embeddings from Large Language\nModels to capture semantics and fulfill the promise of Semantic Overlay\nNetworks. Our proposed algorithm, called Semantica, constructs a prefix tree\n(trie) utilizing document embeddings calculated by a language model. Users\nconnect to each other based on the embeddings of their documents, ensuring that\nsemantically similar users are directly linked. Thereby, this construction\nmakes it more likely for user searches to be answered by the users that they\nare directly connected to, or by the users they are close to in the network\nconnection graph. The implementation of our algorithm also accommodates the\nsemantic diversity of individual users by spawning \"clone\" user identifiers in\nthe tree. Our experiments use emulation with a real-world workload to show\nSemantica's ability to identify and connect to similar users quickly. Semantica\nfinds up to ten times more semantically similar users than current\nstate-of-the-art approaches. At the same time, Semantica can retrieve more than\ntwo times the number of relevant documents given the same network load. We also\nmake our code publicly available to facilitate further research in the area."
                },
                "authors": [
                    {
                        "name": "Petru Neague"
                    },
                    {
                        "name": "Quinten Stokkink"
                    },
                    {
                        "name": "Naman Goel"
                    },
                    {
                        "name": "Johan Pouwelse"
                    }
                ],
                "author_detail": {
                    "name": "Johan Pouwelse"
                },
                "author": "Johan Pouwelse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12915v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12915v3",
                "updated": "2025-02-14T13:24:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    24,
                    8,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-19T17:11:27Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    17,
                    11,
                    27,
                    3,
                    263,
                    0
                ],
                "title": "Exploring Representations and Interventions in Time Series Foundation\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Representations and Interventions in Time Series Foundation\n  Models"
                },
                "summary": "Time series foundation models (TSFMs) promise to be powerful tools for a wide\nrange of applications. However, their internal representations and learned\nconcepts are still not well understood. In this study, we investigate the\nstructure and redundancy of representations across various TSFMs, examining the\nself-similarity of model layers within and across different model sizes. This\nanalysis reveals block-like redundancy in the representations, which can be\nutilized for informed pruning to improve inference speed and efficiency.\nAdditionally, we explore the concepts learned by these models - such as\nperiodicity and trends - and how these can be manipulated through latent space\nsteering to influence model behavior. Our experiments show that steering\ninterventions can introduce new features, e.g., adding periodicity or trends to\nsignals that initially lacked them. These findings underscore the value of\nrepresentational analysis for optimizing models and demonstrate how conceptual\nsteering offers new possibilities for more controlled and efficient time series\nanalysis with TSFMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series foundation models (TSFMs) promise to be powerful tools for a wide\nrange of applications. However, their internal representations and learned\nconcepts are still not well understood. In this study, we investigate the\nstructure and redundancy of representations across various TSFMs, examining the\nself-similarity of model layers within and across different model sizes. This\nanalysis reveals block-like redundancy in the representations, which can be\nutilized for informed pruning to improve inference speed and efficiency.\nAdditionally, we explore the concepts learned by these models - such as\nperiodicity and trends - and how these can be manipulated through latent space\nsteering to influence model behavior. Our experiments show that steering\ninterventions can introduce new features, e.g., adding periodicity or trends to\nsignals that initially lacked them. These findings underscore the value of\nrepresentational analysis for optimizing models and demonstrate how conceptual\nsteering offers new possibilities for more controlled and efficient time series\nanalysis with TSFMs."
                },
                "authors": [
                    {
                        "name": "Michał Wiliński"
                    },
                    {
                        "name": "Mononito Goswami"
                    },
                    {
                        "name": "Nina Żukowska"
                    },
                    {
                        "name": "Willa Potosnak"
                    },
                    {
                        "name": "Artur Dubrawski"
                    }
                ],
                "author_detail": {
                    "name": "Artur Dubrawski"
                },
                "author": "Artur Dubrawski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12915v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12915v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10148v1",
                "updated": "2025-02-14T13:23:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    23,
                    18,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:23:18Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    23,
                    18,
                    4,
                    45,
                    0
                ],
                "title": "Cooperative Multi-Agent Planning with Adaptive Skill Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Multi-Agent Planning with Adaptive Skill Synthesis"
                },
                "summary": "Despite much progress in training distributed artificial intelligence (AI),\nbuilding cooperative multi-agent systems with multi-agent reinforcement\nlearning (MARL) faces challenges in sample efficiency, interpretability, and\ntransferability. Unlike traditional learning-based methods that require\nextensive interaction with the environment, large language models (LLMs)\ndemonstrate remarkable capabilities in zero-shot planning and complex\nreasoning. However, existing LLM-based approaches heavily rely on text-based\nobservations and struggle with the non-Markovian nature of multi-agent\ninteractions under partial observability. We present COMPASS, a novel\nmulti-agent architecture that integrates vision-language models (VLMs) with a\ndynamic skill library and structured communication for decentralized\nclosed-loop decision-making. The skill library, bootstrapped from\ndemonstrations, evolves via planner-guided tasks to enable adaptive strategies.\nCOMPASS propagates entity information through multi-hop communication under\npartial observability. Evaluations on the improved StarCraft Multi-Agent\nChallenge (SMACv2) demonstrate COMPASS achieves up to 30\\% higher win rates\nthan state-of-the-art MARL algorithms in symmetric scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite much progress in training distributed artificial intelligence (AI),\nbuilding cooperative multi-agent systems with multi-agent reinforcement\nlearning (MARL) faces challenges in sample efficiency, interpretability, and\ntransferability. Unlike traditional learning-based methods that require\nextensive interaction with the environment, large language models (LLMs)\ndemonstrate remarkable capabilities in zero-shot planning and complex\nreasoning. However, existing LLM-based approaches heavily rely on text-based\nobservations and struggle with the non-Markovian nature of multi-agent\ninteractions under partial observability. We present COMPASS, a novel\nmulti-agent architecture that integrates vision-language models (VLMs) with a\ndynamic skill library and structured communication for decentralized\nclosed-loop decision-making. The skill library, bootstrapped from\ndemonstrations, evolves via planner-guided tasks to enable adaptive strategies.\nCOMPASS propagates entity information through multi-hop communication under\npartial observability. Evaluations on the improved StarCraft Multi-Agent\nChallenge (SMACv2) demonstrate COMPASS achieves up to 30\\% higher win rates\nthan state-of-the-art MARL algorithms in symmetric scenarios."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Wenshuai Zhao"
                    },
                    {
                        "name": "Joni Pajarinen"
                    }
                ],
                "author_detail": {
                    "name": "Joni Pajarinen"
                },
                "author": "Joni Pajarinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14391v2",
                "updated": "2025-02-14T13:15:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    15,
                    13,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-18T11:52:10Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    52,
                    10,
                    4,
                    292,
                    0
                ],
                "title": "Context-Aware or Context-Insensitive? Assessing LLMs' Performance in\n  Document-Level Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware or Context-Insensitive? Assessing LLMs' Performance in\n  Document-Level Translation"
                },
                "summary": "Large language models (LLMs) are increasingly strong contenders in machine\ntranslation. In this work, we focus on document-level translation, where some\nwords cannot be translated without context from outside the sentence.\nSpecifically, we investigate the ability of prominent LLMs to utilize the\ndocument context during translation through a perturbation analysis (analyzing\nmodels' robustness to perturbed and randomized document context) and an\nattribution analysis (examining the contribution of relevant context to the\ntranslation). We conduct an extensive evaluation across nine LLMs from diverse\nmodel families and training paradigms, including translation-specialized LLMs,\nalongside two encoder-decoder transformer baselines. We find that LLMs'\nimproved document-translation performance compared to encoder-decoder models is\nnot reflected in pronoun translation performance. Our analysis highlight the\nneed for context-aware finetuning of LLMs with a focus on relevant parts of the\ncontext to improve their reliability for document-level translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly strong contenders in machine\ntranslation. In this work, we focus on document-level translation, where some\nwords cannot be translated without context from outside the sentence.\nSpecifically, we investigate the ability of prominent LLMs to utilize the\ndocument context during translation through a perturbation analysis (analyzing\nmodels' robustness to perturbed and randomized document context) and an\nattribution analysis (examining the contribution of relevant context to the\ntranslation). We conduct an extensive evaluation across nine LLMs from diverse\nmodel families and training paradigms, including translation-specialized LLMs,\nalongside two encoder-decoder transformer baselines. We find that LLMs'\nimproved document-translation performance compared to encoder-decoder models is\nnot reflected in pronoun translation performance. Our analysis highlight the\nneed for context-aware finetuning of LLMs with a focus on relevant parts of the\ncontext to improve their reliability for document-level translation."
                },
                "authors": [
                    {
                        "name": "Wafaa Mohammed"
                    },
                    {
                        "name": "Vlad Niculae"
                    }
                ],
                "author_detail": {
                    "name": "Vlad Niculae"
                },
                "author": "Vlad Niculae",
                "arxiv_comment": "9 pages, 3 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10347v2",
                "updated": "2025-02-14T13:13:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    13,
                    33,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-14T10:00:49Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    0,
                    49,
                    0,
                    288,
                    0
                ],
                "title": "A Unified Approach to Routing and Cascading for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Approach to Routing and Cascading for LLMs"
                },
                "summary": "The availability of a wide range of large language models (LLMs) embedded in\nvarious agentic systems has significantly increased the potential of model\nselection strategies to improve the cost-performance tradeoff. Existing\nstrategies involve either routing, where a single model is chosen per query, or\ncascading, which sequentially runs increasingly larger models until a\nsatisfactory answer is found. However, current approaches face three key\nlimitations: they (1) lack formal proofs of optimality, (2) fail to identify\nthe conditions under which these strategies are most effective to improve the\ncost-performance tradeoff, and (3) are unable to combine both paradigms for\nfurther improvements. To address these issues, we first derive a novel optimal\nstrategy for cascading and prove the optimality of an existing routing\nstrategy. Further, we propose cascade routing, a unified framework that\nintegrates routing and cascading into a theoretically optimal strategy. Through\nour analysis, we identify good quality estimators as the critical factor for\nthe success of model selection paradigms. Finally, in our experiments, we show\nthat cascade routing consistently outperforms the individual approaches by a\nlarge margin and we analyze quality estimators to determine when routing and/or\ncascading are useful paradigms for model selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of a wide range of large language models (LLMs) embedded in\nvarious agentic systems has significantly increased the potential of model\nselection strategies to improve the cost-performance tradeoff. Existing\nstrategies involve either routing, where a single model is chosen per query, or\ncascading, which sequentially runs increasingly larger models until a\nsatisfactory answer is found. However, current approaches face three key\nlimitations: they (1) lack formal proofs of optimality, (2) fail to identify\nthe conditions under which these strategies are most effective to improve the\ncost-performance tradeoff, and (3) are unable to combine both paradigms for\nfurther improvements. To address these issues, we first derive a novel optimal\nstrategy for cascading and prove the optimality of an existing routing\nstrategy. Further, we propose cascade routing, a unified framework that\nintegrates routing and cascading into a theoretically optimal strategy. Through\nour analysis, we identify good quality estimators as the critical factor for\nthe success of model selection paradigms. Finally, in our experiments, we show\nthat cascade routing consistently outperforms the individual approaches by a\nlarge margin and we analyze quality estimators to determine when routing and/or\ncascading are useful paradigms for model selection."
                },
                "authors": [
                    {
                        "name": "Jasper Dekoninck"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10140v1",
                "updated": "2025-02-14T13:10:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    10,
                    39,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:10:39Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    10,
                    39,
                    4,
                    45,
                    0
                ],
                "title": "Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of\n  Small Multilingual Language Models for Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of\n  Small Multilingual Language Models for Low-Resource Languages"
                },
                "summary": "Low-resource languages (LRLs) face significant challenges in natural language\nprocessing (NLP) due to limited data. While current state-of-the-art large\nlanguage models (LLMs) still struggle with LRLs, smaller multilingual models\n(mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of\ntheir capacity to low training data sizes. This study systematically\ninvestigates parameter-efficient adapter-based methods for adapting mLMs to\nLRLs, evaluating three architectures: Sequential Bottleneck, Invertible\nBottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and\nstructured knowledge from ConceptNet, we show that small adaptation datasets\n(e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains\nin intrinsic (masked language modeling) and extrinsic tasks (topic\nclassification, sentiment analysis, and named entity recognition). We find that\nSequential Bottleneck adapters excel in language modeling, while Invertible\nBottleneck adapters slightly outperform other methods on downstream tasks due\nto better embedding alignment and larger parameter counts. Adapter-based\nmethods match or outperform full fine-tuning while using far fewer parameters,\nand smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3,\nGPT-4, and DeepSeek-R1-based distilled models. While adaptation improves\nperformance, pre-training data size remains the dominant factor, especially for\nlanguages with extensive pre-training coverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-resource languages (LRLs) face significant challenges in natural language\nprocessing (NLP) due to limited data. While current state-of-the-art large\nlanguage models (LLMs) still struggle with LRLs, smaller multilingual models\n(mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of\ntheir capacity to low training data sizes. This study systematically\ninvestigates parameter-efficient adapter-based methods for adapting mLMs to\nLRLs, evaluating three architectures: Sequential Bottleneck, Invertible\nBottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and\nstructured knowledge from ConceptNet, we show that small adaptation datasets\n(e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains\nin intrinsic (masked language modeling) and extrinsic tasks (topic\nclassification, sentiment analysis, and named entity recognition). We find that\nSequential Bottleneck adapters excel in language modeling, while Invertible\nBottleneck adapters slightly outperform other methods on downstream tasks due\nto better embedding alignment and larger parameter counts. Adapter-based\nmethods match or outperform full fine-tuning while using far fewer parameters,\nand smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3,\nGPT-4, and DeepSeek-R1-based distilled models. While adaptation improves\nperformance, pre-training data size remains the dominant factor, especially for\nlanguages with extensive pre-training coverage."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Ivan Vykopal"
                    },
                    {
                        "name": "Josef van Genabith"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14497v2",
                "updated": "2025-02-14T12:38:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    12,
                    38,
                    15,
                    4,
                    45,
                    0
                ],
                "published": "2025-01-24T13:53:54Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    53,
                    54,
                    4,
                    24,
                    0
                ],
                "title": "Evaluating and Improving Graph to Text Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Improving Graph to Text Generation with Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated immense potential across\nvarious tasks. However, research for exploring and improving the capabilities\nof LLMs in interpreting graph structures remains limited. To address this gap,\nwe conduct a comprehensive evaluation of prompting current open-source LLMs on\ngraph-to-text generation tasks. Although we explored the optimal prompting\nstrategies and proposed a novel and effective diversity-difficulty-based\nfew-shot sample selection method, we found that the improvements from\ntuning-free approaches were incremental, as LLMs struggle with planning on\ncomplex graphs, particularly those with a larger number of triplets. To further\nimprove LLMs in planning with graph sequences and grounding in truth, we\nintroduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks:\nreordering and attribution. Through extensive automatic and human evaluations,\nwe demonstrate significant improvements in the quality of generated text from\nboth few-shot learning and fine-tuning perspectives using the PlanGTG dataset.\nOur study paves the way for new research directions in graph-to-text\ngeneration. PlanGTG datasets can be found in https://github.com/probe2/kg_text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated immense potential across\nvarious tasks. However, research for exploring and improving the capabilities\nof LLMs in interpreting graph structures remains limited. To address this gap,\nwe conduct a comprehensive evaluation of prompting current open-source LLMs on\ngraph-to-text generation tasks. Although we explored the optimal prompting\nstrategies and proposed a novel and effective diversity-difficulty-based\nfew-shot sample selection method, we found that the improvements from\ntuning-free approaches were incremental, as LLMs struggle with planning on\ncomplex graphs, particularly those with a larger number of triplets. To further\nimprove LLMs in planning with graph sequences and grounding in truth, we\nintroduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks:\nreordering and attribution. Through extensive automatic and human evaluations,\nwe demonstrate significant improvements in the quality of generated text from\nboth few-shot learning and fine-tuning perspectives using the PlanGTG dataset.\nOur study paves the way for new research directions in graph-to-text\ngeneration. PlanGTG datasets can be found in https://github.com/probe2/kg_text."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Wanqiu Long"
                    },
                    {
                        "name": "Deyi Xiong"
                    },
                    {
                        "name": "Victor Gutierrez-Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10110v1",
                "updated": "2025-02-14T12:16:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    12,
                    16,
                    38,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T12:16:38Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    12,
                    16,
                    38,
                    4,
                    45,
                    0
                ],
                "title": "ScamFerret: Detecting Scam Websites Autonomously with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScamFerret: Detecting Scam Websites Autonomously with Large Language\n  Models"
                },
                "summary": "With the rise of sophisticated scam websites that exploit human psychological\nvulnerabilities, distinguishing between legitimate and scam websites has become\nincreasingly challenging. This paper presents ScamFerret, an innovative agent\nsystem employing a large language model (LLM) to autonomously collect and\nanalyze data from a given URL to determine whether it is a scam. Unlike\ntraditional machine learning models that require large datasets and feature\nengineering, ScamFerret leverages LLMs' natural language understanding to\naccurately identify scam websites of various types and languages without\nrequiring additional training or fine-tuning. Our evaluation demonstrated that\nScamFerret achieves 0.972 accuracy in classifying four scam types in English\nand 0.993 accuracy in classifying online shopping websites across three\ndifferent languages, particularly when using GPT-4. Furthermore, we confirmed\nthat ScamFerret collects and analyzes external information such as web content,\nDNS records, and user reviews as necessary, providing a basis for identifying\nscam websites from multiple perspectives. These results suggest that LLMs have\nsignificant potential in enhancing cybersecurity measures against sophisticated\nscam websites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of sophisticated scam websites that exploit human psychological\nvulnerabilities, distinguishing between legitimate and scam websites has become\nincreasingly challenging. This paper presents ScamFerret, an innovative agent\nsystem employing a large language model (LLM) to autonomously collect and\nanalyze data from a given URL to determine whether it is a scam. Unlike\ntraditional machine learning models that require large datasets and feature\nengineering, ScamFerret leverages LLMs' natural language understanding to\naccurately identify scam websites of various types and languages without\nrequiring additional training or fine-tuning. Our evaluation demonstrated that\nScamFerret achieves 0.972 accuracy in classifying four scam types in English\nand 0.993 accuracy in classifying online shopping websites across three\ndifferent languages, particularly when using GPT-4. Furthermore, we confirmed\nthat ScamFerret collects and analyzes external information such as web content,\nDNS records, and user reviews as necessary, providing a basis for identifying\nscam websites from multiple perspectives. These results suggest that LLMs have\nsignificant potential in enhancing cybersecurity measures against sophisticated\nscam websites."
                },
                "authors": [
                    {
                        "name": "Hiroki Nakano"
                    },
                    {
                        "name": "Takashi Koide"
                    },
                    {
                        "name": "Daiki Chiba"
                    }
                ],
                "author_detail": {
                    "name": "Daiki Chiba"
                },
                "author": "Daiki Chiba",
                "arxiv_comment": "Accepted for publication at DIMVA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04348v2",
                "updated": "2025-02-14T11:46:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    46,
                    43,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-04T15:16:17Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    15,
                    16,
                    17,
                    1,
                    35,
                    0
                ],
                "title": "Prompt-based Depth Pruning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-based Depth Pruning of Large Language Models"
                },
                "summary": "Depth pruning aims to reduce the inference cost of a large language model\nwithout any hardware-specific complications, by simply removing several less\nimportant transformer blocks. However, our empirical findings suggest that the\nimportance of a transformer block may be highly task-dependent -- a block that\nis crucial for a task can be removed without degrading the accuracy on another\ntask. Based on this observation, we develop a dynamic depth pruning algorithm,\ncoined PuDDing (Prompt-routed Dynamic Depth Pruning), which determines which\nblocks to omit from the model based on the input prompt. PuDDing operates by\ntraining a lightweight router to predict the best omission set among a set of\noptions, where this option set has also been constructed in a data-driven\nmanner. Empirical results on commonsense reasoning benchmarks demonstrate that\nPuDDing effectively accelerates the inference language models, and achieves\nbetter on-task performance than static depth pruning baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth pruning aims to reduce the inference cost of a large language model\nwithout any hardware-specific complications, by simply removing several less\nimportant transformer blocks. However, our empirical findings suggest that the\nimportance of a transformer block may be highly task-dependent -- a block that\nis crucial for a task can be removed without degrading the accuracy on another\ntask. Based on this observation, we develop a dynamic depth pruning algorithm,\ncoined PuDDing (Prompt-routed Dynamic Depth Pruning), which determines which\nblocks to omit from the model based on the input prompt. PuDDing operates by\ntraining a lightweight router to predict the best omission set among a set of\noptions, where this option set has also been constructed in a data-driven\nmanner. Empirical results on commonsense reasoning benchmarks demonstrate that\nPuDDing effectively accelerates the inference language models, and achieves\nbetter on-task performance than static depth pruning baselines."
                },
                "authors": [
                    {
                        "name": "Juyun Wee"
                    },
                    {
                        "name": "Minjae Park"
                    },
                    {
                        "name": "Jaeho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaeho Lee"
                },
                "author": "Jaeho Lee",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10097v1",
                "updated": "2025-02-14T11:44:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    44,
                    17,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T11:44:17Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    44,
                    17,
                    4,
                    45,
                    0
                ],
                "title": "Causal Information Prioritization for Efficient Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Information Prioritization for Efficient Reinforcement Learning"
                },
                "summary": "Current Reinforcement Learning (RL) methods often suffer from\nsample-inefficiency, resulting from blind exploration strategies that neglect\ncausal relationships among states, actions, and rewards. Although recent causal\napproaches aim to address this problem, they lack grounded modeling of\nreward-guided causal understanding of states and actions for goal-orientation,\nthus impairing learning efficiency. To tackle this issue, we propose a novel\nmethod named Causal Information Prioritization (CIP) that improves sample\nefficiency by leveraging factored MDPs to infer causal relationships between\ndifferent dimensions of states and actions with respect to rewards, enabling\nthe prioritization of causal information. Specifically, CIP identifies and\nleverages causal relationships between states and rewards to execute\ncounterfactual data augmentation to prioritize high-impact state features under\nthe causal understanding of the environments. Moreover, CIP integrates a\ncausality-aware empowerment learning objective, which significantly enhances\nthe agent's execution of reward-guided actions for more efficient exploration\nin complex environments. To fully assess the effectiveness of CIP, we conduct\nextensive experiments across 39 tasks in 5 diverse continuous control\nenvironments, encompassing both locomotion and manipulation skills learning\nwith pixel-based and sparse reward settings. Experimental results demonstrate\nthat CIP consistently outperforms existing RL methods across a wide range of\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Reinforcement Learning (RL) methods often suffer from\nsample-inefficiency, resulting from blind exploration strategies that neglect\ncausal relationships among states, actions, and rewards. Although recent causal\napproaches aim to address this problem, they lack grounded modeling of\nreward-guided causal understanding of states and actions for goal-orientation,\nthus impairing learning efficiency. To tackle this issue, we propose a novel\nmethod named Causal Information Prioritization (CIP) that improves sample\nefficiency by leveraging factored MDPs to infer causal relationships between\ndifferent dimensions of states and actions with respect to rewards, enabling\nthe prioritization of causal information. Specifically, CIP identifies and\nleverages causal relationships between states and rewards to execute\ncounterfactual data augmentation to prioritize high-impact state features under\nthe causal understanding of the environments. Moreover, CIP integrates a\ncausality-aware empowerment learning objective, which significantly enhances\nthe agent's execution of reward-guided actions for more efficient exploration\nin complex environments. To fully assess the effectiveness of CIP, we conduct\nextensive experiments across 39 tasks in 5 diverse continuous control\nenvironments, encompassing both locomotion and manipulation skills learning\nwith pixel-based and sparse reward settings. Experimental results demonstrate\nthat CIP consistently outperforms existing RL methods across a wide range of\nscenarios."
                },
                "authors": [
                    {
                        "name": "Hongye Cao"
                    },
                    {
                        "name": "Fan Feng"
                    },
                    {
                        "name": "Tianpei Yang"
                    },
                    {
                        "name": "Jing Huo"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07191v3",
                "updated": "2025-02-14T11:37:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    37,
                    0,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-11T02:31:11Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    2,
                    31,
                    11,
                    1,
                    42,
                    0
                ],
                "title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bag of Tricks for Inference-time Computation of LLM Reasoning"
                },
                "summary": "With the advancement of large language models (LLMs), solving complex\nreasoning tasks has gained increasing attention. Inference-time computation\nmethods (e.g., Best-of-N, beam search, et al.) are particularly valuable as\nthey can enhance reasoning performance without modifying model parameters or\nrequiring additional training. However, these techniques come with\nimplementation challenges, and most existing methods remain at the\nproof-of-concept stage with limited practical adoption due to their\ncomputational complexity and varying effectiveness across different tasks. In\nthis paper, we investigate and benchmark diverse inference-time computation\nstrategies across reasoning tasks of varying complexity. Since most current\nmethods rely on a proposer-verifier pipeline that first generates candidate\nsolutions (e.g., reasoning solutions) and then selects the best one based on\nreward signals (e.g., RLHF rewards, process rewards), our research focuses on\noptimizing both candidate solution generation (e.g., instructing prompts,\nhyperparameters such as temperature and top-p) and reward mechanisms (e.g.,\nself-evaluation, reward types). Through extensive experiments (more than 20,000\nA100-80G GPU hours with over 1,000 experiments) across a variety of models\n(e.g., Llama, Qwen, and Mistral families) of various sizes, our ablation\nstudies reveal that previously overlooked strategies can significantly enhance\nperformance (e.g., tuning temperature can improve reasoning task performance by\nup to 5%). Furthermore, we establish a standardized benchmark for\ninference-time computation by systematically evaluating six representative\nmethods across eight reasoning tasks. These findings provide a stronger\nfoundation for future research. The code is available at\nhttps://github.com/usail-hkust/benchmark_inference_time_computation_LL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of large language models (LLMs), solving complex\nreasoning tasks has gained increasing attention. Inference-time computation\nmethods (e.g., Best-of-N, beam search, et al.) are particularly valuable as\nthey can enhance reasoning performance without modifying model parameters or\nrequiring additional training. However, these techniques come with\nimplementation challenges, and most existing methods remain at the\nproof-of-concept stage with limited practical adoption due to their\ncomputational complexity and varying effectiveness across different tasks. In\nthis paper, we investigate and benchmark diverse inference-time computation\nstrategies across reasoning tasks of varying complexity. Since most current\nmethods rely on a proposer-verifier pipeline that first generates candidate\nsolutions (e.g., reasoning solutions) and then selects the best one based on\nreward signals (e.g., RLHF rewards, process rewards), our research focuses on\noptimizing both candidate solution generation (e.g., instructing prompts,\nhyperparameters such as temperature and top-p) and reward mechanisms (e.g.,\nself-evaluation, reward types). Through extensive experiments (more than 20,000\nA100-80G GPU hours with over 1,000 experiments) across a variety of models\n(e.g., Llama, Qwen, and Mistral families) of various sizes, our ablation\nstudies reveal that previously overlooked strategies can significantly enhance\nperformance (e.g., tuning temperature can improve reasoning task performance by\nup to 5%). Furthermore, we establish a standardized benchmark for\ninference-time computation by systematically evaluating six representative\nmethods across eight reasoning tasks. These findings provide a stronger\nfoundation for future research. The code is available at\nhttps://github.com/usail-hkust/benchmark_inference_time_computation_LL"
                },
                "authors": [
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Wenshuo Chao"
                    },
                    {
                        "name": "Naiqiang Tan"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01576v2",
                "updated": "2025-02-14T11:29:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    29,
                    39,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-02T14:57:27Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    14,
                    57,
                    27,
                    0,
                    337,
                    0
                ],
                "title": "Topological Signal Processing and Learning: Recent Advances and Future\n  Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topological Signal Processing and Learning: Recent Advances and Future\n  Challenges"
                },
                "summary": "Developing methods to process irregularly structured data is crucial in\napplications like gene-regulatory, brain, power, and socioeconomic networks.\nGraphs have been the go-to algebraic tool for modeling the structure via nodes\nand edges capturing their interactions, leading to the establishment of the\nfields of graph signal processing (GSP) and graph machine learning (GML). Key\ngraph-aware methods include Fourier transform, filtering, sampling, as well as\ntopology identification and spatiotemporal processing. Although versatile,\ngraphs can model only pairwise dependencies in the data. To this end,\ntopological structures such as simplicial and cell complexes have emerged as\nalgebraic representations for more intricate structure modeling in data-driven\nsystems, fueling the rapid development of novel topological-based processing\nand learning methods. This paper first presents the core principles of\ntopological signal processing through the Hodge theory, a framework\ninstrumental in propelling the field forward thanks to principled connections\nwith GSP-GML. It then outlines advances in topological signal representation,\nfiltering, and sampling, as well as inferring topological structures from data,\nprocessing spatiotemporal topological signals, and connections with topological\nmachine learning. The impact of topological signal processing and learning is\nfinally highlighted in applications dealing with flow data over networks,\ngeometric processing, statistical ranking, biology, and semantic communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing methods to process irregularly structured data is crucial in\napplications like gene-regulatory, brain, power, and socioeconomic networks.\nGraphs have been the go-to algebraic tool for modeling the structure via nodes\nand edges capturing their interactions, leading to the establishment of the\nfields of graph signal processing (GSP) and graph machine learning (GML). Key\ngraph-aware methods include Fourier transform, filtering, sampling, as well as\ntopology identification and spatiotemporal processing. Although versatile,\ngraphs can model only pairwise dependencies in the data. To this end,\ntopological structures such as simplicial and cell complexes have emerged as\nalgebraic representations for more intricate structure modeling in data-driven\nsystems, fueling the rapid development of novel topological-based processing\nand learning methods. This paper first presents the core principles of\ntopological signal processing through the Hodge theory, a framework\ninstrumental in propelling the field forward thanks to principled connections\nwith GSP-GML. It then outlines advances in topological signal representation,\nfiltering, and sampling, as well as inferring topological structures from data,\nprocessing spatiotemporal topological signals, and connections with topological\nmachine learning. The impact of topological signal processing and learning is\nfinally highlighted in applications dealing with flow data over networks,\ngeometric processing, statistical ranking, biology, and semantic communication."
                },
                "authors": [
                    {
                        "name": "Elvin Isufi"
                    },
                    {
                        "name": "Geert Leus"
                    },
                    {
                        "name": "Baltasar Beferull-Lozano"
                    },
                    {
                        "name": "Sergio Barbarossa"
                    },
                    {
                        "name": "Paolo Di Lorenzo"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Di Lorenzo"
                },
                "author": "Paolo Di Lorenzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10091v1",
                "updated": "2025-02-14T11:27:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    27,
                    2,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T11:27:02Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    27,
                    2,
                    4,
                    45,
                    0
                ],
                "title": "ELAA-ISAC: Environmental Mapping Utilizing the LoS State of\n  Communication Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELAA-ISAC: Environmental Mapping Utilizing the LoS State of\n  Communication Channel"
                },
                "summary": "In this paper, a novel environmental mapping method is proposed to outline\nthe indoor layout utilizing the line-of-sight (LoS) state information of\nextremely large aperture array (ELAA) channels. It leverages the spatial\nresolution provided by ELAA and the mobile terminal (MT)'s mobility to infer\nthe presence and location of obstacles in the environment. The LoS state\nestimation is formulated as a binary hypothesis testing problem, and the\noptimal decision rule is derived based on the likelihood ratio test.\nSubsequently, the theoretical error probability of LoS estimation is derived,\nshowing close alignment with simulation results. Then, an environmental mapping\nmethod is proposed, which progressively outlines the layout by combining LoS\nstate information from multiple MT locations. It is demonstrated that the\nproposed method can accurately outline the environment layout, with the mapping\naccuracy improving as the number of service-antennas and MT locations\nincreases. This paper also investigates the impact of channel estimation error\nand non-LoS (NLoS) components on the quality of environmental mapping. The\nproposed method exhibits particularly promising performance in LoS dominated\nwireless environments characterized by high Rician K-factor. Specifically, it\nachieves an average intersection over union (IoU) exceeding 80% when utilizing\n256 service antennas and 18 MT locations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, a novel environmental mapping method is proposed to outline\nthe indoor layout utilizing the line-of-sight (LoS) state information of\nextremely large aperture array (ELAA) channels. It leverages the spatial\nresolution provided by ELAA and the mobile terminal (MT)'s mobility to infer\nthe presence and location of obstacles in the environment. The LoS state\nestimation is formulated as a binary hypothesis testing problem, and the\noptimal decision rule is derived based on the likelihood ratio test.\nSubsequently, the theoretical error probability of LoS estimation is derived,\nshowing close alignment with simulation results. Then, an environmental mapping\nmethod is proposed, which progressively outlines the layout by combining LoS\nstate information from multiple MT locations. It is demonstrated that the\nproposed method can accurately outline the environment layout, with the mapping\naccuracy improving as the number of service-antennas and MT locations\nincreases. This paper also investigates the impact of channel estimation error\nand non-LoS (NLoS) components on the quality of environmental mapping. The\nproposed method exhibits particularly promising performance in LoS dominated\nwireless environments characterized by high Rician K-factor. Specifically, it\nachieves an average intersection over union (IoU) exceeding 80% when utilizing\n256 service antennas and 18 MT locations."
                },
                "authors": [
                    {
                        "name": "Jiuyu Liu"
                    },
                    {
                        "name": "Chunmei Xu"
                    },
                    {
                        "name": "Yi Ma"
                    },
                    {
                        "name": "Rahim Tafazolli"
                    },
                    {
                        "name": "Ahmed Elzanaty"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Elzanaty"
                },
                "author": "Ahmed Elzanaty",
                "arxiv_comment": "Accepted by IEEE ICC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10092v1",
                "updated": "2025-02-14T11:27:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    27,
                    2,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T11:27:02Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    27,
                    2,
                    4,
                    45,
                    0
                ],
                "title": "A novel approach to data generation in generative model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A novel approach to data generation in generative model"
                },
                "summary": "Variational Autoencoders (VAEs) and other generative models are widely\nemployed in artificial intelligence to synthesize new data. However, current\napproaches rely on Euclidean geometric assumptions and statistical\napproximations that fail to capture the structured and emergent nature of data\ngeneration. This paper introduces the Convergent Fusion Paradigm (CFP) theory,\na novel geometric framework that redefines data generation by integrating\ndimensional expansion accompanied by qualitative transformation. By modifying\nthe latent space geometry to interact with emergent high-dimensional\nstructures, CFP theory addresses key challenges such as identifiability issues\nand unintended artifacts like hallucinations in Large Language Models (LLMs).\nCFP theory is based on two key conceptual hypotheses that redefine how\ngenerative models structure relationships between data and algorithms. Through\nthe lens of CFP theory, we critically examine existing metric-learning\napproaches. CFP theory advances this perspective by introducing time-reversed\nmetric embeddings and structural convergence mechanisms, leading to a novel\ngeometric approach that better accounts for data generation as a structured\nepistemic process. Beyond its computational implications, CFP theory provides\nphilosophical insights into the ontological underpinnings of data generation.\nBy offering a systematic framework for high-dimensional learning dynamics, CFP\ntheory contributes to establishing a theoretical foundation for understanding\nthe data-relationship structures in AI. Finally, future research in CFP theory\nwill be led to its implications for fully realizing qualitative\ntransformations, introducing the potential of Hilbert space in generative\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Autoencoders (VAEs) and other generative models are widely\nemployed in artificial intelligence to synthesize new data. However, current\napproaches rely on Euclidean geometric assumptions and statistical\napproximations that fail to capture the structured and emergent nature of data\ngeneration. This paper introduces the Convergent Fusion Paradigm (CFP) theory,\na novel geometric framework that redefines data generation by integrating\ndimensional expansion accompanied by qualitative transformation. By modifying\nthe latent space geometry to interact with emergent high-dimensional\nstructures, CFP theory addresses key challenges such as identifiability issues\nand unintended artifacts like hallucinations in Large Language Models (LLMs).\nCFP theory is based on two key conceptual hypotheses that redefine how\ngenerative models structure relationships between data and algorithms. Through\nthe lens of CFP theory, we critically examine existing metric-learning\napproaches. CFP theory advances this perspective by introducing time-reversed\nmetric embeddings and structural convergence mechanisms, leading to a novel\ngeometric approach that better accounts for data generation as a structured\nepistemic process. Beyond its computational implications, CFP theory provides\nphilosophical insights into the ontological underpinnings of data generation.\nBy offering a systematic framework for high-dimensional learning dynamics, CFP\ntheory contributes to establishing a theoretical foundation for understanding\nthe data-relationship structures in AI. Finally, future research in CFP theory\nwill be led to its implications for fully realizing qualitative\ntransformations, introducing the potential of Hilbert space in generative\nmodeling."
                },
                "authors": [
                    {
                        "name": "JaeHong Kim"
                    },
                    {
                        "name": "Jaewon Shim"
                    }
                ],
                "author_detail": {
                    "name": "Jaewon Shim"
                },
                "arxiv_affiliation": "Center for 0D Nanofluidics, Institute of Applied Physics, Department of Physics and Astronomy, Seoul National University, Seoul 08826, Korea",
                "author": "Jaewon Shim",
                "arxiv_comment": "47 pages, 2 tables, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00A30 (Primary), 68T99 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.3; F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10089v1",
                "updated": "2025-02-14T11:21:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    21,
                    36,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T11:21:36Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    21,
                    36,
                    4,
                    45,
                    0
                ],
                "title": "A Hybrid Edge Classifier: Combining TinyML-Optimised CNN with RRAM-CMOS\n  ACAM for Energy-Efficient Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid Edge Classifier: Combining TinyML-Optimised CNN with RRAM-CMOS\n  ACAM for Energy-Efficient Inference"
                },
                "summary": "In recent years, the development of smart edge computing systems to process\ninformation locally is on the rise. Many near-sensor machine learning (ML)\napproaches have been implemented to introduce accurate and energy efficient\ntemplate matching operations in resource-constrained edge sensing systems, such\nas wearables. To introduce novel solutions that can be viable for extreme edge\ncases, hybrid solutions combining conventional and emerging technologies have\nstarted to be proposed. Deep Neural Networks (DNN) optimised for edge\napplication alongside new approaches of computing (both device and architecture\n-wise) could be a strong candidate in implementing edge ML solutions that aim\nat competitive accuracy classification while using a fraction of the power of\nconventional ML solutions. In this work, we are proposing a hybrid\nsoftware-hardware edge classifier aimed at the extreme edge near-sensor\nsystems. The classifier consists of two parts: (i) an optimised digital tinyML\nnetwork, working as a front-end feature extractor, and (ii) a back-end\nRRAM-CMOS analogue content addressable memory (ACAM), working as a final stage\ntemplate matching system. The combined hybrid system exhibits a competitive\ntrade-off in accuracy versus energy metric with $E_{front-end}$ = $96.23 nJ$\nand $E_{back-end}$ = $1.45 nJ$ for each classification operation compared with\n78.06$\\mu$J for the original teacher model, representing a 792-fold reduction,\nmaking it a viable solution for extreme edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the development of smart edge computing systems to process\ninformation locally is on the rise. Many near-sensor machine learning (ML)\napproaches have been implemented to introduce accurate and energy efficient\ntemplate matching operations in resource-constrained edge sensing systems, such\nas wearables. To introduce novel solutions that can be viable for extreme edge\ncases, hybrid solutions combining conventional and emerging technologies have\nstarted to be proposed. Deep Neural Networks (DNN) optimised for edge\napplication alongside new approaches of computing (both device and architecture\n-wise) could be a strong candidate in implementing edge ML solutions that aim\nat competitive accuracy classification while using a fraction of the power of\nconventional ML solutions. In this work, we are proposing a hybrid\nsoftware-hardware edge classifier aimed at the extreme edge near-sensor\nsystems. The classifier consists of two parts: (i) an optimised digital tinyML\nnetwork, working as a front-end feature extractor, and (ii) a back-end\nRRAM-CMOS analogue content addressable memory (ACAM), working as a final stage\ntemplate matching system. The combined hybrid system exhibits a competitive\ntrade-off in accuracy versus energy metric with $E_{front-end}$ = $96.23 nJ$\nand $E_{back-end}$ = $1.45 nJ$ for each classification operation compared with\n78.06$\\mu$J for the original teacher model, representing a 792-fold reduction,\nmaking it a viable solution for extreme edge applications."
                },
                "authors": [
                    {
                        "name": "Kieran Woodward"
                    },
                    {
                        "name": "Eiman Kanjo"
                    },
                    {
                        "name": "Georgios Papandroulidakis"
                    },
                    {
                        "name": "Shady Agwa"
                    },
                    {
                        "name": "Themis Prodromakis"
                    }
                ],
                "author_detail": {
                    "name": "Themis Prodromakis"
                },
                "author": "Themis Prodromakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10087v1",
                "updated": "2025-02-14T11:19:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    19,
                    27,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T11:19:27Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    19,
                    27,
                    4,
                    45,
                    0
                ],
                "title": "The implications of stochastic gas torques for asymmetric binaries in\n  the LISA band",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implications of stochastic gas torques for asymmetric binaries in\n  the LISA band"
                },
                "summary": "Gravitational waves from asymmetric mass-ratio black-hole binaries carry\nunique information about their astrophysical environment. For instance, the\nLaser Interferometer Space Antenna (LISA) could potentially measure the\namplitude and slope of gas torques in binaries embedded in the accretion disks\nof Active Galactic Nuclei, helping differentiate competing accretion disk\nmodels. However, this relies on simplified analytic models, which do not\naccount for the stochastic variability of torques seen in hydrodynamic\nsimulations. In this work, we use hydrodynamic simulations to create\ngravitational waveforms for extreme and intermediate mass-ratio inspirals in\nthe LISA band. We then analyze these simulated waveforms using simpler\ntemplates that assume analytic torques, without stochastic time variability. By\nperforming realistic Bayesian parameter estimation, we find no bias at 90%\nconfidence in the binary parameters; however, estimates of accretion disk\nparameters, such as torque amplitude and slope, may be biased. Typically, the\nposterior distribution is centered around the average value of the torques, but\nwhen stochastic variability is large, the posterior can indicate no torques,\neven though they are present in the simulation. Our results suggest that while\nsimplified analytic torque models work well for estimating binary parameters,\ncaution is needed when using them to infer properties of the accretion disk.\nThis work moves towards a more realistic assessment of one of the LISA science\nobjectives, i.e., probing the properties of the astrophysical environments of\nblack holes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves from asymmetric mass-ratio black-hole binaries carry\nunique information about their astrophysical environment. For instance, the\nLaser Interferometer Space Antenna (LISA) could potentially measure the\namplitude and slope of gas torques in binaries embedded in the accretion disks\nof Active Galactic Nuclei, helping differentiate competing accretion disk\nmodels. However, this relies on simplified analytic models, which do not\naccount for the stochastic variability of torques seen in hydrodynamic\nsimulations. In this work, we use hydrodynamic simulations to create\ngravitational waveforms for extreme and intermediate mass-ratio inspirals in\nthe LISA band. We then analyze these simulated waveforms using simpler\ntemplates that assume analytic torques, without stochastic time variability. By\nperforming realistic Bayesian parameter estimation, we find no bias at 90%\nconfidence in the binary parameters; however, estimates of accretion disk\nparameters, such as torque amplitude and slope, may be biased. Typically, the\nposterior distribution is centered around the average value of the torques, but\nwhen stochastic variability is large, the posterior can indicate no torques,\neven though they are present in the simulation. Our results suggest that while\nsimplified analytic torque models work well for estimating binary parameters,\ncaution is needed when using them to infer properties of the accretion disk.\nThis work moves towards a more realistic assessment of one of the LISA science\nobjectives, i.e., probing the properties of the astrophysical environments of\nblack holes."
                },
                "authors": [
                    {
                        "name": "Lorenzo Copparoni"
                    },
                    {
                        "name": "Lorenzo Speri"
                    },
                    {
                        "name": "Laura Sberna"
                    },
                    {
                        "name": "Andrea Derdzinski"
                    },
                    {
                        "name": "Enrico Barausse"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Barausse"
                },
                "author": "Enrico Barausse",
                "arxiv_comment": "16 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07016v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07016v3",
                "updated": "2025-02-14T11:01:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    1,
                    27,
                    4,
                    45,
                    0
                ],
                "published": "2024-06-11T07:16:34Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    7,
                    16,
                    34,
                    1,
                    163,
                    0
                ],
                "title": "Delving into LLM-assisted writing in biomedical publications through\n  excess vocabulary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delving into LLM-assisted writing in biomedical publications through\n  excess vocabulary"
                },
                "summary": "Large language models (LLMs) like ChatGPT can generate and revise text with\nhuman-level performance. These models come with clear limitations: they can\nproduce inaccurate information, reinforce existing biases, and be easily\nmisused. Yet, many scientists use them for their scholarly writing. But how\nwide-spread is such LLM usage in the academic literature? To answer this\nquestion for the field of biomedical research, we present an unbiased,\nlarge-scale approach: we study vocabulary changes in over 15 million biomedical\nabstracts from 2010--2024 indexed by PubMed, and show how the appearance of\nLLMs led to an abrupt increase in the frequency of certain style words. This\nexcess word analysis suggests that at least 13.5% of 2024 abstracts were\nprocessed with LLMs. This lower bound differed across disciplines, countries,\nand journals, reaching 40% for some subcorpora. We show that LLMs have had an\nunprecedented impact on scientific writing in biomedical research, surpassing\nthe effect of major world events such as the Covid pandemic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like ChatGPT can generate and revise text with\nhuman-level performance. These models come with clear limitations: they can\nproduce inaccurate information, reinforce existing biases, and be easily\nmisused. Yet, many scientists use them for their scholarly writing. But how\nwide-spread is such LLM usage in the academic literature? To answer this\nquestion for the field of biomedical research, we present an unbiased,\nlarge-scale approach: we study vocabulary changes in over 15 million biomedical\nabstracts from 2010--2024 indexed by PubMed, and show how the appearance of\nLLMs led to an abrupt increase in the frequency of certain style words. This\nexcess word analysis suggests that at least 13.5% of 2024 abstracts were\nprocessed with LLMs. This lower bound differed across disciplines, countries,\nand journals, reaching 40% for some subcorpora. We show that LLMs have had an\nunprecedented impact on scientific writing in biomedical research, surpassing\nthe effect of major world events such as the Covid pandemic."
                },
                "authors": [
                    {
                        "name": "Dmitry Kobak"
                    },
                    {
                        "name": "Rita González-Márquez"
                    },
                    {
                        "name": "Emőke-Ágnes Horvát"
                    },
                    {
                        "name": "Jan Lause"
                    }
                ],
                "author_detail": {
                    "name": "Jan Lause"
                },
                "author": "Jan Lause",
                "arxiv_comment": "v3: Updating the manuscript to include all PubMed abstracts until the\n  end of 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07016v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07016v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01874v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01874v2",
                "updated": "2025-02-14T11:00:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    0,
                    37,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-03T13:16:32Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    13,
                    16,
                    32,
                    1,
                    247,
                    0
                ],
                "title": "Partial membership models for soft clustering of multivariate football\n  player performance data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partial membership models for soft clustering of multivariate football\n  player performance data"
                },
                "summary": "The standard mixture modeling framework has been widely used to study\nheterogeneous populations, by modeling them as being composed of a finite\nnumber of homogeneous sub-populations. However, the standard mixture model\nassumes that each data point belongs to one and only one mixture component, or\ncluster, but when data points have fractional membership in multiple clusters\nthis assumption is unrealistic. It is in fact conceptually very different to\nrepresent an observation as partly belonging to multiple groups instead of\nbelonging to one group with uncertainty. For this purpose, various soft\nclustering approaches, or individual-level mixture models, have been developed.\nIn this context, Heller et al (2008) formulated the Bayesian partial membership\nmodel (PM) as an alternative structure for individual-level mixtures, which\nalso captures partial membership in the form of attribute-specific mixtures.\nOur work proposes using the PM for soft clustering of count data arising in\nfootball performance analysis and compares the results with those achieved with\nthe mixed membership model and finite mixture model. Learning and inference are\ncarried out using Markov chain Monte Carlo methods. The method is applied on\nSerie A football player data from the 2022/2023 football season, to estimate\nthe positions on the field where the players tend to play, in addition to their\nprimary position, based on their playing style. The application of partial\nmembership model to football data could have practical implications for\ncoaches, talent scouts, team managers and analysts. These stakeholders can\nutilize the findings to make informed decisions related to team strategy,\ntalent acquisition, and statistical research, ultimately enhancing performance\nand understanding in the field of football.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The standard mixture modeling framework has been widely used to study\nheterogeneous populations, by modeling them as being composed of a finite\nnumber of homogeneous sub-populations. However, the standard mixture model\nassumes that each data point belongs to one and only one mixture component, or\ncluster, but when data points have fractional membership in multiple clusters\nthis assumption is unrealistic. It is in fact conceptually very different to\nrepresent an observation as partly belonging to multiple groups instead of\nbelonging to one group with uncertainty. For this purpose, various soft\nclustering approaches, or individual-level mixture models, have been developed.\nIn this context, Heller et al (2008) formulated the Bayesian partial membership\nmodel (PM) as an alternative structure for individual-level mixtures, which\nalso captures partial membership in the form of attribute-specific mixtures.\nOur work proposes using the PM for soft clustering of count data arising in\nfootball performance analysis and compares the results with those achieved with\nthe mixed membership model and finite mixture model. Learning and inference are\ncarried out using Markov chain Monte Carlo methods. The method is applied on\nSerie A football player data from the 2022/2023 football season, to estimate\nthe positions on the field where the players tend to play, in addition to their\nprimary position, based on their playing style. The application of partial\nmembership model to football data could have practical implications for\ncoaches, talent scouts, team managers and analysts. These stakeholders can\nutilize the findings to make informed decisions related to team strategy,\ntalent acquisition, and statistical research, ultimately enhancing performance\nand understanding in the field of football."
                },
                "authors": [
                    {
                        "name": "Emiliano Seri"
                    },
                    {
                        "name": "Roberto Rocci"
                    },
                    {
                        "name": "Thomas Brendan Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Brendan Murphy"
                },
                "author": "Thomas Brendan Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01874v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01874v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10070v1",
                "updated": "2025-02-14T10:45:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    45,
                    36,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T10:45:36Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    45,
                    36,
                    4,
                    45,
                    0
                ],
                "title": "Topological Neural Networks over the Air",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topological Neural Networks over the Air"
                },
                "summary": "Topological neural networks (TNNs) are information processing architectures\nthat model representations from data lying over topological spaces (e.g.,\nsimplicial or cell complexes) and allow for decentralized implementation\nthrough localized communications over different neighborhoods. Existing TNN\narchitectures have not yet been considered in realistic communication\nscenarios, where channel effects typically introduce disturbances such as\nfading and noise. This paper aims to propose a novel TNN design, operating on\nregular cell complexes, that performs over-the-air computation, incorporating\nthe wireless communication model into its architecture. Specifically, during\ntraining and inference, the proposed method considers channel impairments such\nas fading and noise in the topological convolutional filtering operation, which\ntakes place over different signal orders and neighborhoods. Numerical results\nillustrate the architecture's robustness to channel impairments during testing\nand the superior performance with respect to existing architectures, which are\neither communication-agnostic or graph-based.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topological neural networks (TNNs) are information processing architectures\nthat model representations from data lying over topological spaces (e.g.,\nsimplicial or cell complexes) and allow for decentralized implementation\nthrough localized communications over different neighborhoods. Existing TNN\narchitectures have not yet been considered in realistic communication\nscenarios, where channel effects typically introduce disturbances such as\nfading and noise. This paper aims to propose a novel TNN design, operating on\nregular cell complexes, that performs over-the-air computation, incorporating\nthe wireless communication model into its architecture. Specifically, during\ntraining and inference, the proposed method considers channel impairments such\nas fading and noise in the topological convolutional filtering operation, which\ntakes place over different signal orders and neighborhoods. Numerical results\nillustrate the architecture's robustness to channel impairments during testing\nand the superior performance with respect to existing architectures, which are\neither communication-agnostic or graph-based."
                },
                "authors": [
                    {
                        "name": "Simone Fiorellino"
                    },
                    {
                        "name": "Claudio Battiloro"
                    },
                    {
                        "name": "Paolo Di Lorenzo"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Di Lorenzo"
                },
                "author": "Paolo Di Lorenzo",
                "arxiv_doi": "10.1109/ICASSP48485.2024.10446693",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICASSP48485.2024.10446693",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.10070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10065v1",
                "updated": "2025-02-14T10:42:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    42,
                    16,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T10:42:16Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    42,
                    16,
                    4,
                    45,
                    0
                ],
                "title": "Self-Normalized Inference in (Quantile, Expected Shortfall) Regressions\n  for Time Series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Normalized Inference in (Quantile, Expected Shortfall) Regressions\n  for Time Series"
                },
                "summary": "This paper is the first to propose valid inference tools, based on\nself-normalization, in time series expected shortfall regressions. In doing so,\nwe propose a novel two-step estimator for expected shortfall regressions which\nis based on convex optimization in both steps (rendering computation easy) and\nit only requires minimization of quantile losses and squared error losses\n(methods for both of which are implemented in every standard statistical\ncomputing package). As a corollary, we also derive self-normalized inference\ntools in time series quantile regressions. Extant methods, based on a bootstrap\nor direct estimation of the long-run variance, are computationally more\ninvolved, require the choice of tuning parameters and have serious size\ndistortions when the regression errors are strongly serially dependent. In\ncontrast, our inference tools only require estimates of the quantile regression\nparameters that are computed on an expanding window and are correctly sized.\nSimulations show the advantageous finite-sample properties of our methods.\nFinally, two applications to stock return predictability and to Growth-at-Risk\ndemonstrate the practical usefulness of the developed inference tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper is the first to propose valid inference tools, based on\nself-normalization, in time series expected shortfall regressions. In doing so,\nwe propose a novel two-step estimator for expected shortfall regressions which\nis based on convex optimization in both steps (rendering computation easy) and\nit only requires minimization of quantile losses and squared error losses\n(methods for both of which are implemented in every standard statistical\ncomputing package). As a corollary, we also derive self-normalized inference\ntools in time series quantile regressions. Extant methods, based on a bootstrap\nor direct estimation of the long-run variance, are computationally more\ninvolved, require the choice of tuning parameters and have serious size\ndistortions when the regression errors are strongly serially dependent. In\ncontrast, our inference tools only require estimates of the quantile regression\nparameters that are computed on an expanding window and are correctly sized.\nSimulations show the advantageous finite-sample properties of our methods.\nFinally, two applications to stock return predictability and to Growth-at-Risk\ndemonstrate the practical usefulness of the developed inference tools."
                },
                "authors": [
                    {
                        "name": "Yannick Hoga"
                    },
                    {
                        "name": "Christian Schulz"
                    }
                ],
                "author_detail": {
                    "name": "Christian Schulz"
                },
                "author": "Christian Schulz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10060v1",
                "updated": "2025-02-14T10:26:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    26,
                    14,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T10:26:14Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    26,
                    14,
                    4,
                    45,
                    0
                ],
                "title": "DiSciPLE: Learning Interpretable Programs for Scientific Visual\n  Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiSciPLE: Learning Interpretable Programs for Scientific Visual\n  Discovery"
                },
                "summary": "Visual data is used in numerous different scientific workflows ranging from\nremote sensing to ecology. As the amount of observation data increases, the\nchallenge is not just to make accurate predictions but also to understand the\nunderlying mechanisms for those predictions. Good interpretation is important\nin scientific workflows, as it allows for better decision-making by providing\ninsights into the data. This paper introduces an automatic way of obtaining\nsuch interpretable-by-design models, by learning programs that interleave\nneural networks. We propose DiSciPLE (Discovering Scientific Programs using\nLLMs and Evolution) an evolutionary algorithm that leverages common sense and\nprior knowledge of large language models (LLMs) to create Python programs\nexplaining visual data. Additionally, we propose two improvements: a program\ncritic and a program simplifier to improve our method further to synthesize\ngood programs. On three different real-world problems, DiSciPLE learns\nstate-of-the-art programs on novel tasks with no prior literature. For example,\nwe can learn programs with 35% lower error than the closest non-interpretable\nbaseline for population density estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual data is used in numerous different scientific workflows ranging from\nremote sensing to ecology. As the amount of observation data increases, the\nchallenge is not just to make accurate predictions but also to understand the\nunderlying mechanisms for those predictions. Good interpretation is important\nin scientific workflows, as it allows for better decision-making by providing\ninsights into the data. This paper introduces an automatic way of obtaining\nsuch interpretable-by-design models, by learning programs that interleave\nneural networks. We propose DiSciPLE (Discovering Scientific Programs using\nLLMs and Evolution) an evolutionary algorithm that leverages common sense and\nprior knowledge of large language models (LLMs) to create Python programs\nexplaining visual data. Additionally, we propose two improvements: a program\ncritic and a program simplifier to improve our method further to synthesize\ngood programs. On three different real-world problems, DiSciPLE learns\nstate-of-the-art programs on novel tasks with no prior literature. For example,\nwe can learn programs with 35% lower error than the closest non-interpretable\nbaseline for population density estimation."
                },
                "authors": [
                    {
                        "name": "Utkarsh Mall"
                    },
                    {
                        "name": "Cheng Perng Phoo"
                    },
                    {
                        "name": "Mia Chiquier"
                    },
                    {
                        "name": "Bharath Hariharan"
                    },
                    {
                        "name": "Kavita Bala"
                    },
                    {
                        "name": "Carl Vondrick"
                    }
                ],
                "author_detail": {
                    "name": "Carl Vondrick"
                },
                "author": "Carl Vondrick",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10059v1",
                "updated": "2025-02-14T10:21:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    21,
                    49,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T10:21:49Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    21,
                    49,
                    4,
                    45,
                    0
                ],
                "title": "RealCam-I2V: Real-World Image-to-Video Generation with Interactive\n  Complex Camera Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RealCam-I2V: Real-World Image-to-Video Generation with Interactive\n  Complex Camera Control"
                },
                "summary": "Recent advancements in camera-trajectory-guided image-to-video generation\noffer higher precision and better support for complex camera control compared\nto text-based approaches. However, they also introduce significant usability\nchallenges, as users often struggle to provide precise camera parameters when\nworking with arbitrary real-world images without knowledge of their depth nor\nscene scale. To address these real-world application issues, we propose\nRealCam-I2V, a novel diffusion-based video generation framework that integrates\nmonocular metric depth estimation to establish 3D scene reconstruction in a\npreprocessing step. During training, the reconstructed 3D scene enables scaling\ncamera parameters from relative to absolute values, ensuring compatibility and\nscale consistency across diverse real-world images. In inference, RealCam-I2V\noffers an intuitive interface where users can precisely draw camera\ntrajectories by dragging within the 3D scene. To further enhance precise camera\ncontrol and scene consistency, we propose scene-constrained noise shaping,\nwhich shapes high-level noise and also allows the framework to maintain\ndynamic, coherent video generation in lower noise stages. RealCam-I2V achieves\nsignificant improvements in controllability and video quality on the\nRealEstate10K and out-of-domain images. We further enables applications like\ncamera-controlled looping video generation and generative frame interpolation.\nWe will release our absolute-scale annotation, codes, and all checkpoints.\nPlease see dynamic results in https://zgctroy.github.io/RealCam-I2V.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in camera-trajectory-guided image-to-video generation\noffer higher precision and better support for complex camera control compared\nto text-based approaches. However, they also introduce significant usability\nchallenges, as users often struggle to provide precise camera parameters when\nworking with arbitrary real-world images without knowledge of their depth nor\nscene scale. To address these real-world application issues, we propose\nRealCam-I2V, a novel diffusion-based video generation framework that integrates\nmonocular metric depth estimation to establish 3D scene reconstruction in a\npreprocessing step. During training, the reconstructed 3D scene enables scaling\ncamera parameters from relative to absolute values, ensuring compatibility and\nscale consistency across diverse real-world images. In inference, RealCam-I2V\noffers an intuitive interface where users can precisely draw camera\ntrajectories by dragging within the 3D scene. To further enhance precise camera\ncontrol and scene consistency, we propose scene-constrained noise shaping,\nwhich shapes high-level noise and also allows the framework to maintain\ndynamic, coherent video generation in lower noise stages. RealCam-I2V achieves\nsignificant improvements in controllability and video quality on the\nRealEstate10K and out-of-domain images. We further enables applications like\ncamera-controlled looping video generation and generative frame interpolation.\nWe will release our absolute-scale annotation, codes, and all checkpoints.\nPlease see dynamic results in https://zgctroy.github.io/RealCam-I2V."
                },
                "authors": [
                    {
                        "name": "Teng Li"
                    },
                    {
                        "name": "Guangcong Zheng"
                    },
                    {
                        "name": "Rui Jiang"
                    },
                    {
                        "name": "Shuigenzhan"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Yehao Lu"
                    },
                    {
                        "name": "Yining Lin"
                    },
                    {
                        "name": "Xi Li"
                    }
                ],
                "author_detail": {
                    "name": "Xi Li"
                },
                "author": "Xi Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02370v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02370v4",
                "updated": "2025-02-14T10:04:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    4,
                    55,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-04T01:40:20Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    1,
                    40,
                    20,
                    2,
                    248,
                    0
                ],
                "title": "Do Large Language Models Possess Sensitive to Sentiment?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Possess Sensitive to Sentiment?"
                },
                "summary": "Large Language Models (LLMs) have recently displayed their extraordinary\ncapabilities in language understanding. However, how to comprehensively assess\nthe sentiment capabilities of LLMs continues to be a challenge. This paper\ninvestigates the ability of LLMs to detect and react to sentiment in text\nmodal. As the integration of LLMs into diverse applications is on the rise, it\nbecomes highly critical to comprehend their sensitivity to emotional tone, as\nit can influence the user experience and the efficacy of sentiment-driven\ntasks. We conduct a series of experiments to evaluate the performance of\nseveral prominent LLMs in identifying and responding appropriately to\nsentiments like positive, negative, and neutral emotions. The models' outputs\nare analyzed across various sentiment benchmarks, and their responses are\ncompared with human evaluations. Our discoveries indicate that although LLMs\nshow a basic sensitivity to sentiment, there are substantial variations in\ntheir accuracy and consistency, emphasizing the requirement for further\nenhancements in their training processes to better capture subtle emotional\ncues. Take an example in our findings, in some cases, the models might wrongly\nclassify a strongly positive sentiment as neutral, or fail to recognize sarcasm\nor irony in the text. Such misclassifications highlight the complexity of\nsentiment analysis and the areas where the models need to be refined. Another\naspect is that different LLMs might perform differently on the same set of\ndata, depending on their architecture and training datasets. This variance\ncalls for a more in-depth study of the factors that contribute to the\nperformance differences and how they can be optimized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently displayed their extraordinary\ncapabilities in language understanding. However, how to comprehensively assess\nthe sentiment capabilities of LLMs continues to be a challenge. This paper\ninvestigates the ability of LLMs to detect and react to sentiment in text\nmodal. As the integration of LLMs into diverse applications is on the rise, it\nbecomes highly critical to comprehend their sensitivity to emotional tone, as\nit can influence the user experience and the efficacy of sentiment-driven\ntasks. We conduct a series of experiments to evaluate the performance of\nseveral prominent LLMs in identifying and responding appropriately to\nsentiments like positive, negative, and neutral emotions. The models' outputs\nare analyzed across various sentiment benchmarks, and their responses are\ncompared with human evaluations. Our discoveries indicate that although LLMs\nshow a basic sensitivity to sentiment, there are substantial variations in\ntheir accuracy and consistency, emphasizing the requirement for further\nenhancements in their training processes to better capture subtle emotional\ncues. Take an example in our findings, in some cases, the models might wrongly\nclassify a strongly positive sentiment as neutral, or fail to recognize sarcasm\nor irony in the text. Such misclassifications highlight the complexity of\nsentiment analysis and the areas where the models need to be refined. Another\naspect is that different LLMs might perform differently on the same set of\ndata, depending on their architecture and training datasets. This variance\ncalls for a more in-depth study of the factors that contribute to the\nperformance differences and how they can be optimized."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xichou Zhu"
                    },
                    {
                        "name": "Zhou Shen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Yujun Chen"
                    },
                    {
                        "name": "Benzi John"
                    },
                    {
                        "name": "Zhenzhen Ma"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Zhiyang Xu"
                    },
                    {
                        "name": "Wei Luo"
                    },
                    {
                        "name": "Junhui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Junhui Wang"
                },
                "author": "Junhui Wang",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02370v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02370v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02375v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02375v4",
                "updated": "2025-02-14T10:02:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    2,
                    14,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-04T01:51:37Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    1,
                    51,
                    37,
                    2,
                    248,
                    0
                ],
                "title": "How Privacy-Savvy Are Large Language Models? A Case Study on Compliance\n  and Privacy Technical Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Privacy-Savvy Are Large Language Models? A Case Study on Compliance\n  and Privacy Technical Review"
                },
                "summary": "The recent advances in large language models (LLMs) have significantly\nexpanded their applications across various fields such as language generation,\nsummarization, and complex question answering. However, their application to\nprivacy compliance and technical privacy reviews remains under-explored,\nraising critical concerns about their ability to adhere to global privacy\nstandards and protect sensitive user data. This paper seeks to address this gap\nby providing a comprehensive case study evaluating LLMs' performance in\nprivacy-related tasks such as privacy information extraction (PIE), legal and\nregulatory key point detection (KPD), and question answering (QA) with respect\nto privacy policies and data protection regulations. We introduce a Privacy\nTechnical Review (PTR) framework, highlighting its role in mitigating privacy\nrisks during the software development life-cycle. Through an empirical\nassessment, we investigate the capacity of several prominent LLMs, including\nBERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks\nand technical privacy reviews. Our experiments benchmark the models across\nmultiple dimensions, focusing on their precision, recall, and F1-scores in\nextracting privacy-sensitive information and detecting key regulatory\ncompliance points. While LLMs show promise in automating privacy reviews and\nidentifying regulatory discrepancies, significant gaps persist in their ability\nto fully comply with evolving legal standards. We provide actionable\nrecommendations for enhancing LLMs' capabilities in privacy compliance,\nemphasizing the need for robust model improvements and better integration with\nlegal and regulatory requirements. This study underscores the growing\nimportance of developing privacy-aware LLMs that can both support businesses in\ncompliance efforts and safeguard user privacy rights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in large language models (LLMs) have significantly\nexpanded their applications across various fields such as language generation,\nsummarization, and complex question answering. However, their application to\nprivacy compliance and technical privacy reviews remains under-explored,\nraising critical concerns about their ability to adhere to global privacy\nstandards and protect sensitive user data. This paper seeks to address this gap\nby providing a comprehensive case study evaluating LLMs' performance in\nprivacy-related tasks such as privacy information extraction (PIE), legal and\nregulatory key point detection (KPD), and question answering (QA) with respect\nto privacy policies and data protection regulations. We introduce a Privacy\nTechnical Review (PTR) framework, highlighting its role in mitigating privacy\nrisks during the software development life-cycle. Through an empirical\nassessment, we investigate the capacity of several prominent LLMs, including\nBERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks\nand technical privacy reviews. Our experiments benchmark the models across\nmultiple dimensions, focusing on their precision, recall, and F1-scores in\nextracting privacy-sensitive information and detecting key regulatory\ncompliance points. While LLMs show promise in automating privacy reviews and\nidentifying regulatory discrepancies, significant gaps persist in their ability\nto fully comply with evolving legal standards. We provide actionable\nrecommendations for enhancing LLMs' capabilities in privacy compliance,\nemphasizing the need for robust model improvements and better integration with\nlegal and regulatory requirements. This study underscores the growing\nimportance of developing privacy-aware LLMs that can both support businesses in\ncompliance efforts and safeguard user privacy rights."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xichou Zhu"
                    },
                    {
                        "name": "Zhou Shen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Yujun Chen"
                    },
                    {
                        "name": "Benzi John"
                    },
                    {
                        "name": "Zhenzhen Ma"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Bolong Yang"
                    },
                    {
                        "name": "Manman Wang"
                    },
                    {
                        "name": "Zongxing Xie"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Dan Cai"
                    },
                    {
                        "name": "Junhui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Junhui Wang"
                },
                "author": "Junhui Wang",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02375v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02375v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10051v1",
                "updated": "2025-02-14T10:00:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    0,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T10:00:20Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    0,
                    20,
                    4,
                    45,
                    0
                ],
                "title": "ORI: O Routing Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORI: O Routing Intelligence"
                },
                "summary": "Single large language models (LLMs) often fall short when faced with the\never-growing range of tasks, making a single-model approach insufficient. We\naddress this challenge by proposing ORI (O Routing Intelligence), a dynamic\nframework that leverages a set of LLMs. By intelligently routing incoming\nqueries to the most suitable model, ORI not only improves task-specific\naccuracy, but also maintains efficiency. Comprehensive evaluations across\ndiverse benchmarks demonstrate consistent accuracy gains while controlling\ncomputational overhead. By intelligently routing queries, ORI outperforms the\nstrongest individual models by up to 2.7 points on MMLU and 1.8 points on MuSR,\nties the top performance on ARC, and on BBH. These results underscore the\nbenefits of a multi-model strategy and demonstrate how ORI's adaptive\narchitecture can more effectively handle diverse tasks, offering a scalable,\nhigh-performance solution for a system of multiple large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single large language models (LLMs) often fall short when faced with the\never-growing range of tasks, making a single-model approach insufficient. We\naddress this challenge by proposing ORI (O Routing Intelligence), a dynamic\nframework that leverages a set of LLMs. By intelligently routing incoming\nqueries to the most suitable model, ORI not only improves task-specific\naccuracy, but also maintains efficiency. Comprehensive evaluations across\ndiverse benchmarks demonstrate consistent accuracy gains while controlling\ncomputational overhead. By intelligently routing queries, ORI outperforms the\nstrongest individual models by up to 2.7 points on MMLU and 1.8 points on MuSR,\nties the top performance on ARC, and on BBH. These results underscore the\nbenefits of a multi-model strategy and demonstrate how ORI's adaptive\narchitecture can more effectively handle diverse tasks, offering a scalable,\nhigh-performance solution for a system of multiple large language models."
                },
                "authors": [
                    {
                        "name": "Ahmad Shadid"
                    },
                    {
                        "name": "Rahul Kumar"
                    },
                    {
                        "name": "Mohit Mayank"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Mayank"
                },
                "author": "Mohit Mayank",
                "arxiv_comment": "13 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10050v1",
                "updated": "2025-02-14T09:57:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    57,
                    7,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T09:57:07Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    57,
                    7,
                    4,
                    45,
                    0
                ],
                "title": "A Survey on LLM-powered Agents for Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LLM-powered Agents for Recommender Systems"
                },
                "summary": "Recommender systems are essential components of many online platforms, yet\ntraditional approaches still struggle with understanding complex user\npreferences and providing explainable recommendations. The emergence of Large\nLanguage Model (LLM)-powered agents offers a promising approach by enabling\nnatural language interactions and interpretable reasoning, potentially\ntransforming research in recommender systems. This survey provides a systematic\nreview of the emerging applications of LLM-powered agents in recommender\nsystems. We identify and analyze three key paradigms in current research: (1)\nRecommender-oriented approaches, which leverage intelligent agents to enhance\nthe fundamental recommendation mechanisms; (2) Interaction-oriented approaches,\nwhich facilitate dynamic user engagement through natural dialogue and\ninterpretable suggestions; and (3) Simulation-oriented approaches, which employ\nmulti-agent frameworks to model complex user-item interactions and system\ndynamics. Beyond paradigm categorization, we analyze the architectural\nfoundations of LLM-powered recommendation agents, examining their essential\ncomponents: profile construction, memory management, strategic planning, and\naction execution. Our investigation extends to a comprehensive analysis of\nbenchmark datasets and evaluation frameworks in this domain. This systematic\nexamination not only illuminates the current state of LLM-powered agent\nrecommender systems but also charts critical challenges and promising research\ndirections in this transformative field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems are essential components of many online platforms, yet\ntraditional approaches still struggle with understanding complex user\npreferences and providing explainable recommendations. The emergence of Large\nLanguage Model (LLM)-powered agents offers a promising approach by enabling\nnatural language interactions and interpretable reasoning, potentially\ntransforming research in recommender systems. This survey provides a systematic\nreview of the emerging applications of LLM-powered agents in recommender\nsystems. We identify and analyze three key paradigms in current research: (1)\nRecommender-oriented approaches, which leverage intelligent agents to enhance\nthe fundamental recommendation mechanisms; (2) Interaction-oriented approaches,\nwhich facilitate dynamic user engagement through natural dialogue and\ninterpretable suggestions; and (3) Simulation-oriented approaches, which employ\nmulti-agent frameworks to model complex user-item interactions and system\ndynamics. Beyond paradigm categorization, we analyze the architectural\nfoundations of LLM-powered recommendation agents, examining their essential\ncomponents: profile construction, memory management, strategic planning, and\naction execution. Our investigation extends to a comprehensive analysis of\nbenchmark datasets and evaluation frameworks in this domain. This systematic\nexamination not only illuminates the current state of LLM-powered agent\nrecommender systems but also charts critical challenges and promising research\ndirections in this transformative field."
                },
                "authors": [
                    {
                        "name": "Qiyao Peng"
                    },
                    {
                        "name": "Hongtao Liu"
                    },
                    {
                        "name": "Hua Huang"
                    },
                    {
                        "name": "Qing Yang"
                    },
                    {
                        "name": "Minglai Shao"
                    }
                ],
                "author_detail": {
                    "name": "Minglai Shao"
                },
                "author": "Minglai Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10049v1",
                "updated": "2025-02-14T09:56:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    56,
                    40,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T09:56:40Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    56,
                    40,
                    4,
                    45,
                    0
                ],
                "title": "The Probability of Tiered Benefit: Partial Identification with Robust\n  and Stable Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Probability of Tiered Benefit: Partial Identification with Robust\n  and Stable Inference"
                },
                "summary": "We define the probability of tiered benefit in scenarios with a binary\nexposure and an outcome that is either ordered categorical with $K \\geq 2$\ntiers or continuous partitioned by $K-1$ fixed thresholds into disjoint\nintervals. Similar to other pure counterfactual queries, this parameter is not\n$g$-identifiable without additional assumptions. We demonstrate that strong\nmonotonicity does not suffice for point identification when $K \\geq 3$ and\nprovide sharp bounds both with and without this constraint. Inference and\nuncertainty quantification for these bounds are challenging due to potential\nnonregularity induced by ambiguities in the individualized optimization\nproblems underlying the bounds. Such ambiguities can arise from immunities or\nnull treatment effects in subpopulations with positive probability, affecting\nthe lower bound estimate and hindering conservative inference. To address these\nissues, we extend the available stabilized one-step correction (S1S) procedure\nby incorporating stratum-specific stabilizing matrices. Through simulations, we\nillustrate the benefits of this approach over existing alternatives. We apply\nour method to estimate bounds on the probability of tiered benefit and harm\nfrom ADHD pharmacological treatment upon academic achievement, employing\nobservational data from Norwegian schoolchildren with ADHD. Our findings\nsuggest that although girls and children with previously low numeracy scores\nexperience moderate probabilities of both treatment benefit and harm, in no\ngroup does the minimum benefit surpass the maximum harm, complicating a\nclear-cut treatment recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We define the probability of tiered benefit in scenarios with a binary\nexposure and an outcome that is either ordered categorical with $K \\geq 2$\ntiers or continuous partitioned by $K-1$ fixed thresholds into disjoint\nintervals. Similar to other pure counterfactual queries, this parameter is not\n$g$-identifiable without additional assumptions. We demonstrate that strong\nmonotonicity does not suffice for point identification when $K \\geq 3$ and\nprovide sharp bounds both with and without this constraint. Inference and\nuncertainty quantification for these bounds are challenging due to potential\nnonregularity induced by ambiguities in the individualized optimization\nproblems underlying the bounds. Such ambiguities can arise from immunities or\nnull treatment effects in subpopulations with positive probability, affecting\nthe lower bound estimate and hindering conservative inference. To address these\nissues, we extend the available stabilized one-step correction (S1S) procedure\nby incorporating stratum-specific stabilizing matrices. Through simulations, we\nillustrate the benefits of this approach over existing alternatives. We apply\nour method to estimate bounds on the probability of tiered benefit and harm\nfrom ADHD pharmacological treatment upon academic achievement, employing\nobservational data from Norwegian schoolchildren with ADHD. Our findings\nsuggest that although girls and children with previously low numeracy scores\nexperience moderate probabilities of both treatment benefit and harm, in no\ngroup does the minimum benefit surpass the maximum harm, complicating a\nclear-cut treatment recommendation."
                },
                "authors": [
                    {
                        "name": "Johan de Aguas"
                    },
                    {
                        "name": "Sebastian Krumscheid"
                    },
                    {
                        "name": "Johan Pensar"
                    },
                    {
                        "name": "Guido Biele"
                    }
                ],
                "author_detail": {
                    "name": "Guido Biele"
                },
                "author": "Guido Biele",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01584v2",
                "updated": "2025-02-14T09:56:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    56,
                    31,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-03T03:42:56Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    3,
                    42,
                    56,
                    1,
                    247,
                    0
                ],
                "title": "Towards Cross-Lingual Explanation of Artwork in Large-scale Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Cross-Lingual Explanation of Artwork in Large-scale Vision\n  Language Models"
                },
                "summary": "As the performance of Large-scale Vision Language Models (LVLMs) improves,\nthey are increasingly capable of responding in multiple languages, and there is\nan expectation that the demand for explanations generated by LVLMs will grow.\nHowever, pre-training of Vision Encoder and the integrated training of LLMs\nwith Vision Encoder are mainly conducted using English training data, leaving\nit uncertain whether LVLMs can completely handle their potential when\ngenerating explanations in languages other than English. In addition,\nmultilingual QA benchmarks that create datasets using machine translation have\ncultural differences and biases, remaining issues for use as evaluation tasks.\nTo address these challenges, this study created an extended dataset in multiple\nlanguages without relying on machine translation. This dataset that takes into\naccount nuances and country-specific phrases was then used to evaluate the\ngeneration explanation abilities of LVLMs. Furthermore, this study examined\nwhether Instruction-Tuning in resource-rich English improves performance in\nother languages. Our findings indicate that LVLMs perform worse in languages\nother than English compared to English. In addition, it was observed that LVLMs\nstruggle to effectively manage the knowledge learned from English data. Our\ndataset is available at https://huggingface.co/datasets/naist-nlp/MultiExpArt",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the performance of Large-scale Vision Language Models (LVLMs) improves,\nthey are increasingly capable of responding in multiple languages, and there is\nan expectation that the demand for explanations generated by LVLMs will grow.\nHowever, pre-training of Vision Encoder and the integrated training of LLMs\nwith Vision Encoder are mainly conducted using English training data, leaving\nit uncertain whether LVLMs can completely handle their potential when\ngenerating explanations in languages other than English. In addition,\nmultilingual QA benchmarks that create datasets using machine translation have\ncultural differences and biases, remaining issues for use as evaluation tasks.\nTo address these challenges, this study created an extended dataset in multiple\nlanguages without relying on machine translation. This dataset that takes into\naccount nuances and country-specific phrases was then used to evaluate the\ngeneration explanation abilities of LVLMs. Furthermore, this study examined\nwhether Instruction-Tuning in resource-rich English improves performance in\nother languages. Our findings indicate that LVLMs perform worse in languages\nother than English compared to English. In addition, it was observed that LVLMs\nstruggle to effectively manage the knowledge learned from English data. Our\ndataset is available at https://huggingface.co/datasets/naist-nlp/MultiExpArt"
                },
                "authors": [
                    {
                        "name": "Shintaro Ozaki"
                    },
                    {
                        "name": "Kazuki Hayashi"
                    },
                    {
                        "name": "Yusuke Sakai"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Katsuhiko Hayashi"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "NAACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04344v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04344v3",
                "updated": "2025-02-14T09:51:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    51,
                    46,
                    4,
                    45,
                    0
                ],
                "published": "2024-06-06T17:59:56Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    17,
                    59,
                    56,
                    3,
                    158,
                    0
                ],
                "title": "Verbalized Machine Learning: Revisiting Machine Learning with Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbalized Machine Learning: Revisiting Machine Learning with Language\n  Models"
                },
                "summary": "Motivated by the progress made by large language models (LLMs), we introduce\nthe framework of verbalized machine learning (VML). In contrast to conventional\nmachine learning (ML) models that are typically optimized over a continuous\nparameter space, VML constrains the parameter space to be human-interpretable\nnatural language. Such a constraint leads to a new perspective of function\napproximation, where an LLM with a text prompt can be viewed as a function\nparameterized by the text prompt. Guided by this perspective, we revisit\nclassical ML problems, such as regression and classification, and find that\nthese problems can be solved by an LLM-parameterized learner and optimizer. The\nmajor advantages of VML include (1) easy encoding of inductive bias: prior\nknowledge about the problem and hypothesis class can be encoded in natural\nlanguage and fed into the LLM-parameterized learner; (2) automatic model class\nselection: the optimizer can automatically select a model class based on data\nand verbalized prior knowledge, and it can update the model class during\ntraining; and (3) interpretable learner updates: the LLM-parameterized\noptimizer can provide explanations for why an update is performed. We\nempirically verify the effectiveness of VML, and hope that VML can serve as a\nstepping stone to stronger interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the progress made by large language models (LLMs), we introduce\nthe framework of verbalized machine learning (VML). In contrast to conventional\nmachine learning (ML) models that are typically optimized over a continuous\nparameter space, VML constrains the parameter space to be human-interpretable\nnatural language. Such a constraint leads to a new perspective of function\napproximation, where an LLM with a text prompt can be viewed as a function\nparameterized by the text prompt. Guided by this perspective, we revisit\nclassical ML problems, such as regression and classification, and find that\nthese problems can be solved by an LLM-parameterized learner and optimizer. The\nmajor advantages of VML include (1) easy encoding of inductive bias: prior\nknowledge about the problem and hypothesis class can be encoded in natural\nlanguage and fed into the LLM-parameterized learner; (2) automatic model class\nselection: the optimizer can automatically select a model class based on data\nand verbalized prior knowledge, and it can update the model class during\ntraining; and (3) interpretable learner updates: the LLM-parameterized\noptimizer can provide explanations for why an update is performed. We\nempirically verify the effectiveness of VML, and hope that VML can serve as a\nstepping stone to stronger interpretability."
                },
                "authors": [
                    {
                        "name": "Tim Z. Xiao"
                    },
                    {
                        "name": "Robert Bamler"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Weiyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weiyang Liu"
                },
                "author": "Weiyang Liu",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (116 pages, 32\n  figures, v3: refined the paper structure and added more empirical results)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04344v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04344v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03930v2",
                "updated": "2025-02-14T09:49:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    49,
                    57,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-06T10:09:49Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    10,
                    9,
                    49,
                    3,
                    37,
                    0
                ],
                "title": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTAR: Diffusion Transformer Autoregressive Modeling for Speech\n  Generation"
                },
                "summary": "Several recent studies have attempted to autoregressively generate continuous\nspeech representations without discrete speech tokens by combining diffusion\nand autoregressive models, yet they often face challenges with excessive\ncomputational loads or suboptimal outcomes. In this work, we propose Diffusion\nTransformer Autoregressive Modeling (DiTAR), a patch-based autoregressive\nframework combining a language model with a diffusion transformer. This\napproach significantly enhances the efficacy of autoregressive models for\ncontinuous tokens and reduces computational demands. DiTAR utilizes a\ndivide-and-conquer strategy for patch generation, where the language model\nprocesses aggregated patch embeddings and the diffusion transformer\nsubsequently generates the next patch based on the output of the language\nmodel. For inference, we propose defining temperature as the time point of\nintroducing noise during the reverse diffusion ODE to balance diversity and\ndeterminism. We also show in the extensive scaling analysis that DiTAR has\nsuperb scalability. In zero-shot speech generation, DiTAR achieves\nstate-of-the-art performance in robustness, speaker similarity, and\nnaturalness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several recent studies have attempted to autoregressively generate continuous\nspeech representations without discrete speech tokens by combining diffusion\nand autoregressive models, yet they often face challenges with excessive\ncomputational loads or suboptimal outcomes. In this work, we propose Diffusion\nTransformer Autoregressive Modeling (DiTAR), a patch-based autoregressive\nframework combining a language model with a diffusion transformer. This\napproach significantly enhances the efficacy of autoregressive models for\ncontinuous tokens and reduces computational demands. DiTAR utilizes a\ndivide-and-conquer strategy for patch generation, where the language model\nprocesses aggregated patch embeddings and the diffusion transformer\nsubsequently generates the next patch based on the output of the language\nmodel. For inference, we propose defining temperature as the time point of\nintroducing noise during the reverse diffusion ODE to balance diversity and\ndeterminism. We also show in the extensive scaling analysis that DiTAR has\nsuperb scalability. In zero-shot speech generation, DiTAR achieves\nstate-of-the-art performance in robustness, speaker similarity, and\nnaturalness."
                },
                "authors": [
                    {
                        "name": "Dongya Jia"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Chenpeng Du"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Jian Cong"
                    },
                    {
                        "name": "Xiaobin Zhuang"
                    },
                    {
                        "name": "Chumin Li"
                    },
                    {
                        "name": "Zhen Wei"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Wang"
                },
                "author": "Yuxuan Wang",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10047v1",
                "updated": "2025-02-14T09:49:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    49,
                    52,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T09:49:52Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    49,
                    52,
                    4,
                    45,
                    0
                ],
                "title": "Janus: Collaborative Vision Transformer Under Dynamic Network\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Janus: Collaborative Vision Transformer Under Dynamic Network\n  Environment"
                },
                "summary": "Vision Transformers (ViTs) have outperformed traditional Convolutional Neural\nNetwork architectures and achieved state-of-the-art results in various computer\nvision tasks. Since ViTs are computationally expensive, the models either have\nto be pruned to run on resource-limited edge devices only or have to be\nexecuted on remote cloud servers after receiving the raw data transmitted over\nfluctuating networks. The resulting degraded performance or high latency all\nhinder their widespread applications. In this paper, we present Janus, the\nfirst framework for low-latency cloud-device collaborative Vision Transformer\ninference over dynamic networks. Janus overcomes the intrinsic model\nlimitations of ViTs and realizes collaboratively executing ViT models on both\ncloud and edge devices, achieving low latency, high accuracy, and low\ncommunication overhead. Specifically, Janus judiciously combines token pruning\ntechniques with a carefully designed fine-to-coarse model splitting policy and\nnon-static mixed pruning policy. It attains a balance between accuracy and\nlatency by dynamically selecting the optimal pruning level and split point.\nExperimental results across various tasks demonstrate that Janus enhances\nthroughput by up to 5.15 times and reduces latency violation ratios by up to\n98.7% when compared with baseline approaches under various network\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have outperformed traditional Convolutional Neural\nNetwork architectures and achieved state-of-the-art results in various computer\nvision tasks. Since ViTs are computationally expensive, the models either have\nto be pruned to run on resource-limited edge devices only or have to be\nexecuted on remote cloud servers after receiving the raw data transmitted over\nfluctuating networks. The resulting degraded performance or high latency all\nhinder their widespread applications. In this paper, we present Janus, the\nfirst framework for low-latency cloud-device collaborative Vision Transformer\ninference over dynamic networks. Janus overcomes the intrinsic model\nlimitations of ViTs and realizes collaboratively executing ViT models on both\ncloud and edge devices, achieving low latency, high accuracy, and low\ncommunication overhead. Specifically, Janus judiciously combines token pruning\ntechniques with a carefully designed fine-to-coarse model splitting policy and\nnon-static mixed pruning policy. It attains a balance between accuracy and\nlatency by dynamically selecting the optimal pruning level and split point.\nExperimental results across various tasks demonstrate that Janus enhances\nthroughput by up to 5.15 times and reduces latency violation ratios by up to\n98.7% when compared with baseline approaches under various network\nenvironments."
                },
                "authors": [
                    {
                        "name": "Linyi Jiang"
                    },
                    {
                        "name": "Silvery D. Fu"
                    },
                    {
                        "name": "Yifei Zhu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "arxiv_comment": "Accepted for publication in IEEE INFOCOM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10481v2",
                "updated": "2025-02-14T09:47:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    47,
                    42,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-14T13:18:20Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    18,
                    20,
                    0,
                    288,
                    0
                ],
                "title": "Model-Based Privacy-Preserving Knowledge Transfer for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Based Privacy-Preserving Knowledge Transfer for Large Language\n  Models"
                },
                "summary": "As large language models (LLMs) become more prevalent, effectively utilizing\ndomain-specific knowledge while ensuring privacy has become critical. Existing\nmethods often struggle to balance utility and privacy. For instance,\nretrieval-augmented generation (RAG) enables LLMs to access domain-specific\nknowledge but compromises the privacy of sensitive data. On the other hand,\ndifferentially private data synthesis techniques offer strong privacy\nguarantees but often result in poor utility. To address this challenge, we\npropose Llamdex, a novel framework that enhances LLMs using only models trained\non domain-specific data, integrated into LLMs through carefully designed\nconnection modules. Our approach significantly enhances the accuracy of\ndomain-specific tasks, achieving up to a 26% accuracy improvement compared to\nstate-of-the-art data synthesis methods under the same differential privacy\nconstraints. Experimental results show that Llamdex not only improves the\naccuracy of LLM responses but also maintains comparable inference efficiency to\nthe original LLM, highlighting its potential for real applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become more prevalent, effectively utilizing\ndomain-specific knowledge while ensuring privacy has become critical. Existing\nmethods often struggle to balance utility and privacy. For instance,\nretrieval-augmented generation (RAG) enables LLMs to access domain-specific\nknowledge but compromises the privacy of sensitive data. On the other hand,\ndifferentially private data synthesis techniques offer strong privacy\nguarantees but often result in poor utility. To address this challenge, we\npropose Llamdex, a novel framework that enhances LLMs using only models trained\non domain-specific data, integrated into LLMs through carefully designed\nconnection modules. Our approach significantly enhances the accuracy of\ndomain-specific tasks, achieving up to a 26% accuracy improvement compared to\nstate-of-the-art data synthesis methods under the same differential privacy\nconstraints. Experimental results show that Llamdex not only improves the\naccuracy of LLM responses but also maintains comparable inference efficiency to\nthe original LLM, highlighting its potential for real applications."
                },
                "authors": [
                    {
                        "name": "Zhaomin Wu"
                    },
                    {
                        "name": "Jizhou Guo"
                    },
                    {
                        "name": "Junyi Hou"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Lixin Fan"
                    },
                    {
                        "name": "Qiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yang"
                },
                "author": "Qiang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10046v1",
                "updated": "2025-02-14T09:46:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    46,
                    43,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T09:46:43Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    46,
                    43,
                    4,
                    45,
                    0
                ],
                "title": "ViRAC: A Vision-Reasoning Agent Head Movement Control Framework in\n  Arbitrary Virtual Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViRAC: A Vision-Reasoning Agent Head Movement Control Framework in\n  Arbitrary Virtual Environments"
                },
                "summary": "Creating lifelike virtual agents capable of interacting with their\nenvironments is a longstanding goal in computer graphics. This paper addresses\nthe challenge of generating natural head rotations, a critical aspect of\nbelievable agent behavior for visual information gathering and dynamic\nresponses to environmental cues. Although earlier methods have made significant\nstrides, many rely on data-driven or saliency-based approaches, which often\nunderperform in diverse settings and fail to capture deeper cognitive factors\nsuch as risk assessment, information seeking, and contextual prioritization.\nConsequently, generated behaviors can appear rigid or overlook critical scene\nelements, thereby diminishing the sense of realism. In this paper, we propose\n\\textbf{ViRAC}, a \\textbf{Vi}sion-\\textbf{R}easoning \\textbf{A}gent Head\nMovement \\textbf{C}ontrol framework, which exploits the common-sense knowledge\nand reasoning capabilities of large-scale models, including Vision-Language\nModels (VLMs) and Large-Language Models (LLMs). Rather than explicitly modeling\nevery cognitive mechanism, ViRAC leverages the biases and patterns internalized\nby these models from extensive training, thus emulating human-like perceptual\nprocesses without hand-tuned heuristics. Experimental results in multiple\nscenarios reveal that ViRAC produces more natural and context-aware head\nrotations than recent state-of-the-art techniques. Quantitative evaluations\nshow a closer alignment with real human head-movement data, while user studies\nconfirm improved realism and cognitive plausibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating lifelike virtual agents capable of interacting with their\nenvironments is a longstanding goal in computer graphics. This paper addresses\nthe challenge of generating natural head rotations, a critical aspect of\nbelievable agent behavior for visual information gathering and dynamic\nresponses to environmental cues. Although earlier methods have made significant\nstrides, many rely on data-driven or saliency-based approaches, which often\nunderperform in diverse settings and fail to capture deeper cognitive factors\nsuch as risk assessment, information seeking, and contextual prioritization.\nConsequently, generated behaviors can appear rigid or overlook critical scene\nelements, thereby diminishing the sense of realism. In this paper, we propose\n\\textbf{ViRAC}, a \\textbf{Vi}sion-\\textbf{R}easoning \\textbf{A}gent Head\nMovement \\textbf{C}ontrol framework, which exploits the common-sense knowledge\nand reasoning capabilities of large-scale models, including Vision-Language\nModels (VLMs) and Large-Language Models (LLMs). Rather than explicitly modeling\nevery cognitive mechanism, ViRAC leverages the biases and patterns internalized\nby these models from extensive training, thus emulating human-like perceptual\nprocesses without hand-tuned heuristics. Experimental results in multiple\nscenarios reveal that ViRAC produces more natural and context-aware head\nrotations than recent state-of-the-art techniques. Quantitative evaluations\nshow a closer alignment with real human head-movement data, while user studies\nconfirm improved realism and cognitive plausibility."
                },
                "authors": [
                    {
                        "name": "Juyeong Hwang"
                    },
                    {
                        "name": "Seong-Eun Hong"
                    },
                    {
                        "name": "Hyeongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongyeop Kang"
                },
                "author": "Hyeongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10038v1",
                "updated": "2025-02-14T09:34:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    34,
                    24,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T09:34:24Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    34,
                    24,
                    4,
                    45,
                    0
                ],
                "title": "POI-Enhancer: An LLM-based Semantic Enhancement Framework for POI\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POI-Enhancer: An LLM-based Semantic Enhancement Framework for POI\n  Representation Learning"
                },
                "summary": "POI representation learning plays a crucial role in handling tasks related to\nuser mobility data. Recent studies have shown that enriching POI\nrepresentations with multimodal information can significantly enhance their\ntask performance. Previously, the textual information incorporated into POI\nrepresentations typically involved only POI categories or check-in content,\nleading to relatively weak textual features in existing methods. In contrast,\nlarge language models (LLMs) trained on extensive text data have been found to\npossess rich textual knowledge. However leveraging such knowledge to enhance\nPOI representation learning presents two key challenges: first, how to extract\nPOI-related knowledge from LLMs effectively, and second, how to integrate the\nextracted information to enhance POI representations. To address these\nchallenges, we propose POI-Enhancer, a portable framework that leverages LLMs\nto improve POI representations produced by classic POI learning models. We\nfirst design three specialized prompts to extract semantic information from\nLLMs efficiently. Then, the Dual Feature Alignment module enhances the quality\nof the extracted information, while the Semantic Feature Fusion module\npreserves its integrity. The Cross Attention Fusion module then fully\nadaptively integrates such high-quality information into POI representations\nand Multi-View Contrastive Learning further injects human-understandable\nsemantic information into these representations. Extensive experiments on three\nreal-world datasets demonstrate the effectiveness of our framework, showing\nsignificant improvements across all baseline representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POI representation learning plays a crucial role in handling tasks related to\nuser mobility data. Recent studies have shown that enriching POI\nrepresentations with multimodal information can significantly enhance their\ntask performance. Previously, the textual information incorporated into POI\nrepresentations typically involved only POI categories or check-in content,\nleading to relatively weak textual features in existing methods. In contrast,\nlarge language models (LLMs) trained on extensive text data have been found to\npossess rich textual knowledge. However leveraging such knowledge to enhance\nPOI representation learning presents two key challenges: first, how to extract\nPOI-related knowledge from LLMs effectively, and second, how to integrate the\nextracted information to enhance POI representations. To address these\nchallenges, we propose POI-Enhancer, a portable framework that leverages LLMs\nto improve POI representations produced by classic POI learning models. We\nfirst design three specialized prompts to extract semantic information from\nLLMs efficiently. Then, the Dual Feature Alignment module enhances the quality\nof the extracted information, while the Semantic Feature Fusion module\npreserves its integrity. The Cross Attention Fusion module then fully\nadaptively integrates such high-quality information into POI representations\nand Multi-View Contrastive Learning further injects human-understandable\nsemantic information into these representations. Extensive experiments on three\nreal-world datasets demonstrate the effectiveness of our framework, showing\nsignificant improvements across all baseline representations."
                },
                "authors": [
                    {
                        "name": "Jiawei Cheng"
                    },
                    {
                        "name": "Jingyuan Wang"
                    },
                    {
                        "name": "Yichuan Zhang"
                    },
                    {
                        "name": "Jiahao Ji"
                    },
                    {
                        "name": "Yuanshao Zhu"
                    },
                    {
                        "name": "Zhibo Zhang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16345v2",
                "updated": "2025-02-14T09:32:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    32,
                    11,
                    4,
                    45,
                    0
                ],
                "published": "2024-11-25T12:44:02Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    44,
                    2,
                    0,
                    330,
                    0
                ],
                "title": "Preference Optimization for Reasoning with Pseudo Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Optimization for Reasoning with Pseudo Feedback"
                },
                "summary": "Preference optimization techniques, such as Direct Preference Optimization\n(DPO), are frequently employed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in domains like mathematical reasoning and coding,\ntypically following supervised fine-tuning. These methods rely on high-quality\nlabels for reasoning tasks to generate preference pairs; however, the\navailability of reasoning datasets with human-verified labels is limited. In\nthis study, we introduce a novel approach to generate pseudo feedback for\nreasoning tasks by framing the labeling of solutions to reason problems as an\nevaluation against associated test cases. We explore two forms of pseudo\nfeedback based on test cases: one generated by frontier LLMs and the other by\nextending self-consistency to multi-test-case. We conduct experiments on both\nmathematical reasoning and coding tasks using pseudo feedback for preference\noptimization, and observe improvements across both tasks. Specifically, using\nMathstral-7B as our base model, we improve MATH results from 58.3 to 68.6,\nsurpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and\nCollege Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3,\nrespectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on\nLiveCodeBench (from 21.1), surpassing Claude-3-Haiku.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference optimization techniques, such as Direct Preference Optimization\n(DPO), are frequently employed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in domains like mathematical reasoning and coding,\ntypically following supervised fine-tuning. These methods rely on high-quality\nlabels for reasoning tasks to generate preference pairs; however, the\navailability of reasoning datasets with human-verified labels is limited. In\nthis study, we introduce a novel approach to generate pseudo feedback for\nreasoning tasks by framing the labeling of solutions to reason problems as an\nevaluation against associated test cases. We explore two forms of pseudo\nfeedback based on test cases: one generated by frontier LLMs and the other by\nextending self-consistency to multi-test-case. We conduct experiments on both\nmathematical reasoning and coding tasks using pseudo feedback for preference\noptimization, and observe improvements across both tasks. Specifically, using\nMathstral-7B as our base model, we improve MATH results from 58.3 to 68.6,\nsurpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and\nCollege Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3,\nrespectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on\nLiveCodeBench (from 21.1), surpassing Claude-3-Haiku."
                },
                "authors": [
                    {
                        "name": "Fangkai Jiao"
                    },
                    {
                        "name": "Geyang Guo"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "28 pages, 11 figures. ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19008v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19008v3",
                "updated": "2025-02-14T09:29:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    29,
                    16,
                    4,
                    45,
                    0
                ],
                "published": "2024-07-26T18:00:02Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    18,
                    0,
                    2,
                    4,
                    208,
                    0
                ],
                "title": "GA-NIFS: Multi-phase analysis of a star-forming galaxy at $z \\sim 5.5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GA-NIFS: Multi-phase analysis of a star-forming galaxy at $z \\sim 5.5$"
                },
                "summary": "In this study, we present a detailed multiphase analysis of HZ4, a\nmain-sequence star-forming galaxy at z ~ 5.5, known for being a turbulent\nrotating disk and having a detection of a [CII] outflow in the ALMA\nobservations. We exploit JWST/NIRSpec observations in the integral field\nspectroscopy mode with low- and high-spectral resolution that allow us for the\nfirst time to spatially resolve the rest-frame UV and optical emission of the\ngalaxy to investigate the galaxy properties. In particular, the high-resolution\ndataset allows us to study the kinematics of the ionized gas phase, and the\nconditions of the interstellar medium, such as the excitation mechanism, dust\nattenuation, and metallicity. The lower-spectral resolution observations allow\nus to study the continuum emission and infer the stellar populations' ages and\nproperties. Our findings suggest that HZ4 is a galaxy merger rather than a\nrotating disk as previously inferred from lower resolution [CII] data. The\nmerger is associated with an extended broad, blueshifted emission, potentially\nindicative of an outflow originating from a region of intense star formation\nand extending up to 4 kpc. In light of these new observations we reanalyzed the\nALMA data to compare the multiphase gas properties. If we interpret the broad\ncomponents seen in [CII] and [OIII]$\\lambda$5007\\.A as outflows, the neutral\nand ionized components are co-spatial, the mass loading factor of the ionized\nphase is significantly lower than that of the neutral phase, aligning with\ntrends observed in multi-phase systems at lower redshifts. Nonetheless,\nadditional observations and larger statistical samples are essential to\ndetermine the role of mergers and outflows in the early Universe and to clarify\nthe origin of the broad emission components observed in this system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we present a detailed multiphase analysis of HZ4, a\nmain-sequence star-forming galaxy at z ~ 5.5, known for being a turbulent\nrotating disk and having a detection of a [CII] outflow in the ALMA\nobservations. We exploit JWST/NIRSpec observations in the integral field\nspectroscopy mode with low- and high-spectral resolution that allow us for the\nfirst time to spatially resolve the rest-frame UV and optical emission of the\ngalaxy to investigate the galaxy properties. In particular, the high-resolution\ndataset allows us to study the kinematics of the ionized gas phase, and the\nconditions of the interstellar medium, such as the excitation mechanism, dust\nattenuation, and metallicity. The lower-spectral resolution observations allow\nus to study the continuum emission and infer the stellar populations' ages and\nproperties. Our findings suggest that HZ4 is a galaxy merger rather than a\nrotating disk as previously inferred from lower resolution [CII] data. The\nmerger is associated with an extended broad, blueshifted emission, potentially\nindicative of an outflow originating from a region of intense star formation\nand extending up to 4 kpc. In light of these new observations we reanalyzed the\nALMA data to compare the multiphase gas properties. If we interpret the broad\ncomponents seen in [CII] and [OIII]$\\lambda$5007\\.A as outflows, the neutral\nand ionized components are co-spatial, the mass loading factor of the ionized\nphase is significantly lower than that of the neutral phase, aligning with\ntrends observed in multi-phase systems at lower redshifts. Nonetheless,\nadditional observations and larger statistical samples are essential to\ndetermine the role of mergers and outflows in the early Universe and to clarify\nthe origin of the broad emission components observed in this system."
                },
                "authors": [
                    {
                        "name": "Eleonora Parlanti"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Giacomo Venturi"
                    },
                    {
                        "name": "Rodrigo Herrera-Camus"
                    },
                    {
                        "name": "Santiago Arribas"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Stephane Charlot"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Michele Perna"
                    },
                    {
                        "name": "Hannah Übler"
                    },
                    {
                        "name": "Torsten Böker"
                    },
                    {
                        "name": "Giovanni Cresci"
                    },
                    {
                        "name": "Mirko Curti"
                    },
                    {
                        "name": "Gareth C. Jones"
                    },
                    {
                        "name": "Isabella Lamperti"
                    },
                    {
                        "name": "Pablo G. Pérez-González"
                    },
                    {
                        "name": "Bruno Rodríguez Del Pino"
                    },
                    {
                        "name": "Sandra Zamora"
                    }
                ],
                "author_detail": {
                    "name": "Sandra Zamora"
                },
                "author": "Sandra Zamora",
                "arxiv_comment": "23 pages, 20 figures, accepted in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19008v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19008v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2002.09377v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2002.09377v5",
                "updated": "2025-02-14T09:25:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    25,
                    24,
                    4,
                    45,
                    0
                ],
                "published": "2020-02-21T16:06:11Z",
                "published_parsed": [
                    2020,
                    2,
                    21,
                    16,
                    6,
                    11,
                    4,
                    52,
                    0
                ],
                "title": "Misspecification-robust likelihood-free inference in high dimensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misspecification-robust likelihood-free inference in high dimensions"
                },
                "summary": "Likelihood-free inference for simulator-based statistical models has\ndeveloped rapidly from its infancy to a useful tool for practitioners. However,\nmodels with more than a handful of parameters still generally remain a\nchallenge for the Approximate Bayesian Computation (ABC) based inference. To\nadvance the possibilities for performing likelihood-free inference in higher\ndimensional parameter spaces, we introduce an extension of the popular Bayesian\noptimisation based approach to approximate discrepancy functions in a\nprobabilistic manner which lends itself to an efficient exploration of the\nparameter space. Our approach achieves computational scalability for higher\ndimensional parameter spaces by using separate acquisition functions and\ndiscrepancies for each parameter. The efficient additive acquisition structure\nis combined with exponentiated loss -likelihood to provide a\nmisspecification-robust characterisation of the marginal posterior distribution\nfor all model parameters. The method successfully performs computationally\nefficient inference in a 100-dimensional space on canonical examples and\ncompares favourably to existing modularised ABC methods. We further illustrate\nthe potential of this approach by fitting a bacterial transmission dynamics\nmodel to a real data set, which provides biologically coherent results on\nstrain competition in a 30-dimensional parameter space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Likelihood-free inference for simulator-based statistical models has\ndeveloped rapidly from its infancy to a useful tool for practitioners. However,\nmodels with more than a handful of parameters still generally remain a\nchallenge for the Approximate Bayesian Computation (ABC) based inference. To\nadvance the possibilities for performing likelihood-free inference in higher\ndimensional parameter spaces, we introduce an extension of the popular Bayesian\noptimisation based approach to approximate discrepancy functions in a\nprobabilistic manner which lends itself to an efficient exploration of the\nparameter space. Our approach achieves computational scalability for higher\ndimensional parameter spaces by using separate acquisition functions and\ndiscrepancies for each parameter. The efficient additive acquisition structure\nis combined with exponentiated loss -likelihood to provide a\nmisspecification-robust characterisation of the marginal posterior distribution\nfor all model parameters. The method successfully performs computationally\nefficient inference in a 100-dimensional space on canonical examples and\ncompares favourably to existing modularised ABC methods. We further illustrate\nthe potential of this approach by fitting a bacterial transmission dynamics\nmodel to a real data set, which provides biologically coherent results on\nstrain competition in a 30-dimensional parameter space."
                },
                "authors": [
                    {
                        "name": "Owen Thomas"
                    },
                    {
                        "name": "Raquel Sá-Leão"
                    },
                    {
                        "name": "Hermínia de Lencastre"
                    },
                    {
                        "name": "Samuel Kaski"
                    },
                    {
                        "name": "Jukka Corander"
                    },
                    {
                        "name": "Henri Pesonen"
                    }
                ],
                "author_detail": {
                    "name": "Henri Pesonen"
                },
                "author": "Henri Pesonen",
                "arxiv_comment": "Final published version in Computational Statistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2002.09377v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2002.09377v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.10391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10391v1",
                "updated": "2025-02-14T18:59:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    51,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:59:51Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    51,
                    4,
                    45,
                    0
                ],
                "title": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MM-RLHF: The Next Step Forward in Multimodal LLM Alignment"
                },
                "summary": "Despite notable advancements in Multimodal Large Language Models (MLLMs),\nmost state-of-the-art models have not undergone thorough alignment with human\npreferences. This gap exists because current alignment research has primarily\nachieved progress in specific areas (e.g., hallucination reduction), while the\nbroader question of whether aligning models with human preferences can\nsystematically enhance MLLM capability remains largely unexplored. To this end,\nwe introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained,\nhuman-annotated preference comparison pairs. This dataset represents a\nsubstantial advancement over existing resources, offering superior size,\ndiversity, annotation granularity, and quality. Leveraging this dataset, we\npropose several key innovations to improve both the quality of reward models\nand the efficiency of alignment algorithms. Notably, we introduce a\nCritique-Based Reward Model, which generates critiques of model outputs before\nassigning scores, offering enhanced interpretability and more informative\nfeedback compared to traditional scalar reward mechanisms. Additionally, we\npropose Dynamic Reward Scaling, a method that adjusts the loss weight of each\nsample according to the reward signal, thereby optimizing the use of\nhigh-quality comparison pairs. Our approach is rigorously evaluated across\n$\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results\ndemonstrating significant and consistent improvements in model performance.\nSpecifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm\nleads to a $\\mathbf{19.5}$% increase in conversational abilities and a\n$\\mathbf{60}$% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and\nevaluation code, as well as reward modeling and safety benchmarks. For more\ndetails, please visit our project page: https://mm-rlhf.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite notable advancements in Multimodal Large Language Models (MLLMs),\nmost state-of-the-art models have not undergone thorough alignment with human\npreferences. This gap exists because current alignment research has primarily\nachieved progress in specific areas (e.g., hallucination reduction), while the\nbroader question of whether aligning models with human preferences can\nsystematically enhance MLLM capability remains largely unexplored. To this end,\nwe introduce MM-RLHF, a dataset containing $\\mathbf{120k}$ fine-grained,\nhuman-annotated preference comparison pairs. This dataset represents a\nsubstantial advancement over existing resources, offering superior size,\ndiversity, annotation granularity, and quality. Leveraging this dataset, we\npropose several key innovations to improve both the quality of reward models\nand the efficiency of alignment algorithms. Notably, we introduce a\nCritique-Based Reward Model, which generates critiques of model outputs before\nassigning scores, offering enhanced interpretability and more informative\nfeedback compared to traditional scalar reward mechanisms. Additionally, we\npropose Dynamic Reward Scaling, a method that adjusts the loss weight of each\nsample according to the reward signal, thereby optimizing the use of\nhigh-quality comparison pairs. Our approach is rigorously evaluated across\n$\\mathbf{10}$ distinct dimensions and $\\mathbf{27}$ benchmarks, with results\ndemonstrating significant and consistent improvements in model performance.\nSpecifically, fine-tuning LLaVA-ov-7B with MM-RLHF and our alignment algorithm\nleads to a $\\mathbf{19.5}$% increase in conversational abilities and a\n$\\mathbf{60}$% improvement in safety.\n  We have open-sourced the preference dataset, reward model, training and\nevaluation code, as well as reward modeling and safety benchmarks. For more\ndetails, please visit our project page: https://mm-rlhf.github.io."
                },
                "authors": [
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Haochen Tian"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Peiyan Li"
                    },
                    {
                        "name": "Jianshu Zeng"
                    },
                    {
                        "name": "Wulin Xie"
                    },
                    {
                        "name": "Yang Shi"
                    },
                    {
                        "name": "Huanyu Zhang"
                    },
                    {
                        "name": "Junkang Wu"
                    },
                    {
                        "name": "Xue Wang"
                    },
                    {
                        "name": "Yibo Hu"
                    },
                    {
                        "name": "Bin Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Rong Jin"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "Project Page: https://mm-rlhf.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10388v1",
                "updated": "2025-02-14T18:59:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    28,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:59:28Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    28,
                    4,
                    45,
                    0
                ],
                "title": "Aspect-Oriented Summarization for Psychiatric Short-Term Readmission\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-Oriented Summarization for Psychiatric Short-Term Readmission\n  Prediction"
                },
                "summary": "Recent progress in large language models (LLMs) has enabled the automated\nprocessing of lengthy documents even without supervised training on a\ntask-specific dataset. Yet, their zero-shot performance in complex tasks as\nopposed to straightforward information extraction tasks remains suboptimal. One\nfeasible approach for tasks with lengthy, complex input is to first summarize\nthe document and then apply supervised fine-tuning to the summary. However, the\nsummarization process inevitably results in some loss of information. In this\nstudy we present a method for processing the summaries of long documents aimed\nto capture different important aspects of the original document. We hypothesize\nthat LLM summaries generated with different aspect-oriented prompts contain\ndifferent \\textit{information signals}, and we propose methods to measure these\ndifferences. We introduce approaches to effectively integrate signals from\nthese different summaries for supervised training of transformer models. We\nvalidate our hypotheses on a high-impact task -- 30-day readmission prediction\nfrom a psychiatric discharge -- using real-world data from four hospitals, and\nshow that our proposed method increases the prediction performance for the\ncomplex task of predicting patient outcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in large language models (LLMs) has enabled the automated\nprocessing of lengthy documents even without supervised training on a\ntask-specific dataset. Yet, their zero-shot performance in complex tasks as\nopposed to straightforward information extraction tasks remains suboptimal. One\nfeasible approach for tasks with lengthy, complex input is to first summarize\nthe document and then apply supervised fine-tuning to the summary. However, the\nsummarization process inevitably results in some loss of information. In this\nstudy we present a method for processing the summaries of long documents aimed\nto capture different important aspects of the original document. We hypothesize\nthat LLM summaries generated with different aspect-oriented prompts contain\ndifferent \\textit{information signals}, and we propose methods to measure these\ndifferences. We introduce approaches to effectively integrate signals from\nthese different summaries for supervised training of transformer models. We\nvalidate our hypotheses on a high-impact task -- 30-day readmission prediction\nfrom a psychiatric discharge -- using real-world data from four hospitals, and\nshow that our proposed method increases the prediction performance for the\ncomplex task of predicting patient outcome."
                },
                "authors": [
                    {
                        "name": "WonJin Yoon"
                    },
                    {
                        "name": "Boyu Ren"
                    },
                    {
                        "name": "Spencer Thomas"
                    },
                    {
                        "name": "Chanwhi Kim"
                    },
                    {
                        "name": "Guergana Savova"
                    },
                    {
                        "name": "Mei-Hua Hall"
                    },
                    {
                        "name": "Timothy Miller"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Miller"
                },
                "author": "Timothy Miller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08008v2",
                "updated": "2025-02-14T18:52:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    52,
                    34,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-11T23:07:14Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    23,
                    7,
                    14,
                    1,
                    42,
                    0
                ],
                "title": "An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Interactive Framework for Implementing Privacy-Preserving Federated\n  Learning: Experiments on Large Language Models"
                },
                "summary": "Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) enhances privacy by keeping user data on local\ndevices. However, emerging attacks have demonstrated that the updates shared by\nusers during training can reveal significant information about their data. This\nhas greatly thwart the adoption of FL methods for training robust AI models in\nsensitive applications. Differential Privacy (DP) is considered the gold\nstandard for safeguarding user data. However, DP guarantees are highly\nconservative, providing worst-case privacy guarantees. This can result in\noverestimating privacy needs, which may compromise the model's accuracy.\nAdditionally, interpretations of these privacy guarantees have proven to be\nchallenging in different contexts. This is further exacerbated when other\nfactors, such as the number of training iterations, data distribution, and\nspecific application requirements, can add further complexity to this problem.\nIn this work, we proposed a framework that integrates a human entity as a\nprivacy practitioner to determine an optimal trade-off between the model's\nprivacy and utility. Our framework is the first to address the variable memory\nrequirement of existing DP methods in FL settings, where resource-limited\ndevices (e.g., cell phones) can participate. To support such settings, we adopt\na recent DP method with fixed memory usage to ensure scalable private FL. We\nevaluated our proposed framework by fine-tuning a BERT-based LLM model using\nthe GLUE dataset (a common approach in literature), leveraging the new\naccountant, and employing diverse data partitioning strategies to mimic\nreal-world conditions. As a result, we achieved stable memory usage, with an\naverage accuracy reduction of 1.33% for $\\epsilon = 10$ and 1.9% for $\\epsilon\n= 6$, when compared to the state-of-the-art DP accountant which does not\nsupport fixed memory usage."
                },
                "authors": [
                    {
                        "name": "Kasra Ahmadi"
                    },
                    {
                        "name": "Rouzbeh Behnia"
                    },
                    {
                        "name": "Reza Ebrahimi"
                    },
                    {
                        "name": "Mehran Mozaffari Kermani"
                    },
                    {
                        "name": "Jeremiah Birrell"
                    },
                    {
                        "name": "Jason Pacheco"
                    },
                    {
                        "name": "Attila A Yavuz"
                    }
                ],
                "author_detail": {
                    "name": "Attila A Yavuz"
                },
                "author": "Attila A Yavuz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10363v1",
                "updated": "2025-02-14T18:42:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    42,
                    42,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:42:42Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    42,
                    42,
                    4,
                    45,
                    0
                ],
                "title": "BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds"
                },
                "summary": "Traversing risky terrains with sparse footholds poses a significant challenge\nfor humanoid robots, requiring precise foot placements and stable locomotion.\nExisting approaches designed for quadrupedal robots often fail to generalize to\nhumanoid robots due to differences in foot geometry and unstable morphology,\nwhile learning-based approaches for humanoid locomotion still face great\nchallenges on complex terrains due to sparse foothold reward signals and\ninefficient learning processes. To address these challenges, we introduce\nBeamDojo, a reinforcement learning (RL) framework designed for enabling agile\nhumanoid locomotion on sparse footholds. BeamDojo begins by introducing a\nsampling-based foothold reward tailored for polygonal feet, along with a double\ncritic to balancing the learning process between dense locomotion rewards and\nsparse foothold rewards. To encourage sufficient trail-and-error exploration,\nBeamDojo incorporates a two-stage RL approach: the first stage relaxes the\nterrain dynamics by training the humanoid on flat terrain while providing it\nwith task terrain perceptive observations, and the second stage fine-tunes the\npolicy on the actual task terrain. Moreover, we implement a onboard LiDAR-based\nelevation map to enable real-world deployment. Extensive simulation and\nreal-world experiments demonstrate that BeamDojo achieves efficient learning in\nsimulation and enables agile locomotion with precise foot placement on sparse\nfootholds in the real world, maintaining a high success rate even under\nsignificant external disturbances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traversing risky terrains with sparse footholds poses a significant challenge\nfor humanoid robots, requiring precise foot placements and stable locomotion.\nExisting approaches designed for quadrupedal robots often fail to generalize to\nhumanoid robots due to differences in foot geometry and unstable morphology,\nwhile learning-based approaches for humanoid locomotion still face great\nchallenges on complex terrains due to sparse foothold reward signals and\ninefficient learning processes. To address these challenges, we introduce\nBeamDojo, a reinforcement learning (RL) framework designed for enabling agile\nhumanoid locomotion on sparse footholds. BeamDojo begins by introducing a\nsampling-based foothold reward tailored for polygonal feet, along with a double\ncritic to balancing the learning process between dense locomotion rewards and\nsparse foothold rewards. To encourage sufficient trail-and-error exploration,\nBeamDojo incorporates a two-stage RL approach: the first stage relaxes the\nterrain dynamics by training the humanoid on flat terrain while providing it\nwith task terrain perceptive observations, and the second stage fine-tunes the\npolicy on the actual task terrain. Moreover, we implement a onboard LiDAR-based\nelevation map to enable real-world deployment. Extensive simulation and\nreal-world experiments demonstrate that BeamDojo achieves efficient learning in\nsimulation and enables agile locomotion with precise foot placement on sparse\nfootholds in the real world, maintaining a high success rate even under\nsignificant external disturbances."
                },
                "authors": [
                    {
                        "name": "Huayi Wang"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Junli Ren"
                    },
                    {
                        "name": "Qingwei Ben"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Project website: https://why618188.github.io/beamdojo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10361v1",
                "updated": "2025-02-14T18:42:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    42,
                    7,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:42:07Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    42,
                    7,
                    4,
                    45,
                    0
                ],
                "title": "Enhancing Multilingual LLM Pretraining with Model-Based Data Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multilingual LLM Pretraining with Model-Based Data Selection"
                },
                "summary": "Dataset curation has become a basis for strong large language model (LLM)\nperformance. While various rule-based filtering heuristics exist for English\nand multilingual datasets, model-based filtering techniques have primarily\nfocused on English. To address the disparity stemming from limited research on\nnon-English languages, we propose a model-based filtering framework for\nmultilingual datasets that aims to identify a diverse set of structured and\nknowledge-rich samples. Our approach emphasizes transparency, simplicity, and\nefficiency, leveraging Transformer- and FastText-based classifiers to ensure\nthe broad accessibility of our technique and data. We conduct comprehensive\nablation studies on the FineWeb-2 web crawl dataset across diverse language\nfamilies, scripts, and resource availability to demonstrate the effectiveness\nof our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our\napproach can match the baseline MMLU score with as little as 15% of the\ntraining tokens, while also improving across other benchmarks. These findings\nprovide strong evidence for the generalizability of our approach to other\nlanguages. As a result, we extend our framework to 20 languages for which we\nrelease the refined pretraining datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset curation has become a basis for strong large language model (LLM)\nperformance. While various rule-based filtering heuristics exist for English\nand multilingual datasets, model-based filtering techniques have primarily\nfocused on English. To address the disparity stemming from limited research on\nnon-English languages, we propose a model-based filtering framework for\nmultilingual datasets that aims to identify a diverse set of structured and\nknowledge-rich samples. Our approach emphasizes transparency, simplicity, and\nefficiency, leveraging Transformer- and FastText-based classifiers to ensure\nthe broad accessibility of our technique and data. We conduct comprehensive\nablation studies on the FineWeb-2 web crawl dataset across diverse language\nfamilies, scripts, and resource availability to demonstrate the effectiveness\nof our method. Training a 1B-parameter Llama model for 70B and 119B tokens, our\napproach can match the baseline MMLU score with as little as 15% of the\ntraining tokens, while also improving across other benchmarks. These findings\nprovide strong evidence for the generalizability of our approach to other\nlanguages. As a result, we extend our framework to 20 languages for which we\nrelease the refined pretraining datasets."
                },
                "authors": [
                    {
                        "name": "Bettina Messmer"
                    },
                    {
                        "name": "Vinko Sabolčec"
                    },
                    {
                        "name": "Martin Jaggi"
                    }
                ],
                "author_detail": {
                    "name": "Martin Jaggi"
                },
                "author": "Martin Jaggi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00843v2",
                "updated": "2025-02-14T18:35:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    35,
                    3,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-30T04:20:10Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    4,
                    20,
                    10,
                    2,
                    304,
                    0
                ],
                "title": "The Graph's Apprentice: Teaching an LLM Low Level Knowledge for Circuit\n  Quality Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Graph's Apprentice: Teaching an LLM Low Level Knowledge for Circuit\n  Quality Estimation"
                },
                "summary": "Logic synthesis is a crucial phase in the circuit design process, responsible\nfor transforming hardware description language (HDL) designs into optimized\nnetlists. However, traditional logic synthesis methods are computationally\nintensive, restricting their iterative use in refining chip designs. Recent\nadvancements in large language models (LLMs), particularly those fine-tuned on\nprogramming languages, present a promising alternative. This work proposes\naugmenting LLMs with predictor networks trained to estimate circuit quality\ndirectly from HDL code. To enhance performance, the model is regularized using\nembeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT)\ngraphs, thereby incorporating lower-level circuit insights. The proposed method\ndemonstrates superior performance compared to existing graph-based RTL-level\nestimation techniques on the established benchmark OpenABCD, while providing\ninstant feedback on HDL code quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic synthesis is a crucial phase in the circuit design process, responsible\nfor transforming hardware description language (HDL) designs into optimized\nnetlists. However, traditional logic synthesis methods are computationally\nintensive, restricting their iterative use in refining chip designs. Recent\nadvancements in large language models (LLMs), particularly those fine-tuned on\nprogramming languages, present a promising alternative. This work proposes\naugmenting LLMs with predictor networks trained to estimate circuit quality\ndirectly from HDL code. To enhance performance, the model is regularized using\nembeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT)\ngraphs, thereby incorporating lower-level circuit insights. The proposed method\ndemonstrates superior performance compared to existing graph-based RTL-level\nestimation techniques on the established benchmark OpenABCD, while providing\ninstant feedback on HDL code quality."
                },
                "authors": [
                    {
                        "name": "Reza Moravej"
                    },
                    {
                        "name": "Saurabh Bodhe"
                    },
                    {
                        "name": "Zhanguang Zhang"
                    },
                    {
                        "name": "Didier Chetelat"
                    },
                    {
                        "name": "Dimitrios Tsaras"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10352v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10352v1",
                "updated": "2025-02-14T18:31:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    31,
                    39,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:31:39Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    31,
                    39,
                    4,
                    45,
                    0
                ],
                "title": "Agentic Verification for Ambiguous Query Disambiguation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Verification for Ambiguous Query Disambiguation"
                },
                "summary": "In this work, we tackle the challenge of disambiguating queries in\nretrieval-augmented generation (RAG) to diverse yet answerable interpretations.\nState-of-the-arts follow a Diversify-then-Verify (DtV) pipeline, where diverse\ninterpretations are generated by an LLM, later used as search queries to\nretrieve supporting passages. Such a process may introduce noise in either\ninterpretations or retrieval, particularly in enterprise settings, where LLMs\n-- trained on static data -- may struggle with domain-specific disambiguations.\nThus, a post-hoc verification phase is introduced to prune noises. Our\ndistinction is to unify diversification with verification by incorporating\nfeedback from retriever and generator early on. This joint approach improves\nboth efficiency and robustness by reducing reliance on multiple retrieval and\ninference steps, which are susceptible to cascading errors. We validate the\nefficiency and effectiveness of our method, Verified-Diversification with\nConsolidation (VERDICT), on the widely adopted ASQA benchmark to achieve\ndiverse yet verifiable interpretations. Empirical results show that VERDICT\nimproves grounding-aware F1 score by an average of 23% over the strongest\nbaseline across different backbone LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we tackle the challenge of disambiguating queries in\nretrieval-augmented generation (RAG) to diverse yet answerable interpretations.\nState-of-the-arts follow a Diversify-then-Verify (DtV) pipeline, where diverse\ninterpretations are generated by an LLM, later used as search queries to\nretrieve supporting passages. Such a process may introduce noise in either\ninterpretations or retrieval, particularly in enterprise settings, where LLMs\n-- trained on static data -- may struggle with domain-specific disambiguations.\nThus, a post-hoc verification phase is introduced to prune noises. Our\ndistinction is to unify diversification with verification by incorporating\nfeedback from retriever and generator early on. This joint approach improves\nboth efficiency and robustness by reducing reliance on multiple retrieval and\ninference steps, which are susceptible to cascading errors. We validate the\nefficiency and effectiveness of our method, Verified-Diversification with\nConsolidation (VERDICT), on the widely adopted ASQA benchmark to achieve\ndiverse yet verifiable interpretations. Empirical results show that VERDICT\nimproves grounding-aware F1 score by an average of 23% over the strongest\nbaseline across different backbone LLMs."
                },
                "authors": [
                    {
                        "name": "Youngwon Lee"
                    },
                    {
                        "name": "Seung-won Hwang"
                    },
                    {
                        "name": "Ruofan Wu"
                    },
                    {
                        "name": "Feng Yan"
                    },
                    {
                        "name": "Danmei Xu"
                    },
                    {
                        "name": "Moutasem Akkad"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10352v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13502v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13502v3",
                "updated": "2025-02-14T18:15:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    15,
                    1,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-17T12:48:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    12,
                    48,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily\n  Complex Proofs"
                },
                "summary": "Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to more complex\nproblems. This is difficult to study, as (i) much of the available evaluation\ndata has already been seen by the most capable models during training, and (ii)\nexisting benchmarks do not capture how problem proofs may be arbitrarily\ncomplex in various ways. In this paper, we present a data-generation framework\nfor evaluating LLMs on problems with arbitrarily complex arithmetic proofs,\ncalled MathGAP. MathGAP generates problem statements and chain-of-thought\nreasoning traces according to specifications about their arithmetic proof\nstructure, enabling systematic studies on easy-to-hard generalization with\nrespect to complexity of proof trees. Using MathGAP, we find that LLMs show a\nsignificant decrease in performance as proofs get deeper and wider. This effect\nis more pronounced in complex, nonlinear proof structures, which are\nchallenging even for the most capable models. The models are also sensitive to\nsimple changes in sentence ordering. However, they remain capable of solving\nsome complex problems, suggesting that reasoning generalization is noisy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can solve arithmetic word problems with high\naccuracy, but little is known about how well they generalize to more complex\nproblems. This is difficult to study, as (i) much of the available evaluation\ndata has already been seen by the most capable models during training, and (ii)\nexisting benchmarks do not capture how problem proofs may be arbitrarily\ncomplex in various ways. In this paper, we present a data-generation framework\nfor evaluating LLMs on problems with arbitrarily complex arithmetic proofs,\ncalled MathGAP. MathGAP generates problem statements and chain-of-thought\nreasoning traces according to specifications about their arithmetic proof\nstructure, enabling systematic studies on easy-to-hard generalization with\nrespect to complexity of proof trees. Using MathGAP, we find that LLMs show a\nsignificant decrease in performance as proofs get deeper and wider. This effect\nis more pronounced in complex, nonlinear proof structures, which are\nchallenging even for the most capable models. The models are also sensitive to\nsimple changes in sentence ordering. However, they remain capable of solving\nsome complex problems, suggesting that reasoning generalization is noisy."
                },
                "authors": [
                    {
                        "name": "Andreas Opedal"
                    },
                    {
                        "name": "Haruki Shirakami"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Abulhair Saparov"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    }
                ],
                "author_detail": {
                    "name": "Mrinmaya Sachan"
                },
                "author": "Mrinmaya Sachan",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13502v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13502v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17331v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17331v4",
                "updated": "2025-02-14T18:09:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    9,
                    50,
                    4,
                    45,
                    0
                ],
                "published": "2023-11-29T03:10:42Z",
                "published_parsed": [
                    2023,
                    11,
                    29,
                    3,
                    10,
                    42,
                    2,
                    333,
                    0
                ],
                "title": "Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for\n  Visual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for\n  Visual Question Answering"
                },
                "summary": "Recently, to comprehensively improve Vision Language Models (VLMs) for Visual\nQuestion Answering (VQA), several methods have been proposed to further\nreinforce the inference capabilities of VLMs to independently tackle VQA tasks\nrather than some methods that only utilize VLMs as aids to Large Language\nModels (LLMs). However, these methods ignore the rich common-sense knowledge\ninside the given VQA image sampled from the real world. Thus, they cannot fully\nuse the powerful VLM for the given VQA question to achieve optimal performance.\nAttempt to overcome this limitation and inspired by the human top-down\nreasoning process, i.e., systematically exploring relevant issues to derive a\ncomprehensive answer, this work introduces a novel, explainable multi-agent\ncollaboration framework by leveraging the expansive knowledge of Large Language\nModels (LLMs) to enhance the capabilities of VLMs themselves. Specifically, our\nframework comprises three agents, i.e., Responder, Seeker, and Integrator, to\ncollaboratively answer the given VQA question by seeking its relevant issues\nand generating the final answer in such a top-down reasoning process. The\nVLM-based Responder agent generates the answer candidates for the question and\nresponds to other relevant issues. The Seeker agent, primarily based on LLM,\nidentifies relevant issues related to the question to inform the Responder\nagent and constructs a Multi-View Knowledge Base (MVKB) for the given visual\nscene by leveraging the build-in world knowledge of LLM. The Integrator agent\ncombines knowledge from the Seeker agent and the Responder agent to produce the\nfinal VQA answer. Extensive and comprehensive evaluations on diverse VQA\ndatasets with a variety of VLMs demonstrate the superior performance and\ninterpretability of our framework over the baseline method in the zero-shot\nsetting without extra training cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, to comprehensively improve Vision Language Models (VLMs) for Visual\nQuestion Answering (VQA), several methods have been proposed to further\nreinforce the inference capabilities of VLMs to independently tackle VQA tasks\nrather than some methods that only utilize VLMs as aids to Large Language\nModels (LLMs). However, these methods ignore the rich common-sense knowledge\ninside the given VQA image sampled from the real world. Thus, they cannot fully\nuse the powerful VLM for the given VQA question to achieve optimal performance.\nAttempt to overcome this limitation and inspired by the human top-down\nreasoning process, i.e., systematically exploring relevant issues to derive a\ncomprehensive answer, this work introduces a novel, explainable multi-agent\ncollaboration framework by leveraging the expansive knowledge of Large Language\nModels (LLMs) to enhance the capabilities of VLMs themselves. Specifically, our\nframework comprises three agents, i.e., Responder, Seeker, and Integrator, to\ncollaboratively answer the given VQA question by seeking its relevant issues\nand generating the final answer in such a top-down reasoning process. The\nVLM-based Responder agent generates the answer candidates for the question and\nresponds to other relevant issues. The Seeker agent, primarily based on LLM,\nidentifies relevant issues related to the question to inform the Responder\nagent and constructs a Multi-View Knowledge Base (MVKB) for the given visual\nscene by leveraging the build-in world knowledge of LLM. The Integrator agent\ncombines knowledge from the Seeker agent and the Responder agent to produce the\nfinal VQA answer. Extensive and comprehensive evaluations on diverse VQA\ndatasets with a variety of VLMs demonstrate the superior performance and\ninterpretability of our framework over the baseline method in the zero-shot\nsetting without extra training cost."
                },
                "authors": [
                    {
                        "name": "Zeqing Wang"
                    },
                    {
                        "name": "Wentao Wan"
                    },
                    {
                        "name": "Qiqing Lao"
                    },
                    {
                        "name": "Runmeng Chen"
                    },
                    {
                        "name": "Minjie Lang"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Keze Wang"
                    },
                    {
                        "name": "Liang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liang Lin"
                },
                "author": "Liang Lin",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17331v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17331v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10338v1",
                "updated": "2025-02-14T17:55:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    55,
                    43,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T17:55:43Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    55,
                    43,
                    4,
                    45,
                    0
                ],
                "title": "Evaluating the Meta- and Object-Level Reasoning of Large Language Models\n  for Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Meta- and Object-Level Reasoning of Large Language Models\n  for Question Answering"
                },
                "summary": "Large Language Models (LLMs) excel in natural language tasks but still face\nchallenges in Question Answering (QA) tasks requiring complex, multi-step\nreasoning. We outline the types of reasoning required in some of these tasks,\nand reframe them in terms of meta-level reasoning (akin to high-level strategic\nreasoning or planning) and object-level reasoning (embodied in lower-level\ntasks such as mathematical reasoning). Franklin, a novel dataset with\nrequirements of meta- and object-level reasoning, is introduced and used along\nwith three other datasets to evaluate four LLMs at question answering tasks\nrequiring multiple steps of reasoning. Results from human annotation studies\nsuggest LLMs demonstrate meta-level reasoning with high frequency, but struggle\nwith object-level reasoning tasks in some of the datasets used. Additionally,\nevidence suggests that LLMs find the object-level reasoning required for the\nquestions in the Franklin dataset challenging, yet they do exhibit strong\nperformance with respect to the meta-level reasoning requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language tasks but still face\nchallenges in Question Answering (QA) tasks requiring complex, multi-step\nreasoning. We outline the types of reasoning required in some of these tasks,\nand reframe them in terms of meta-level reasoning (akin to high-level strategic\nreasoning or planning) and object-level reasoning (embodied in lower-level\ntasks such as mathematical reasoning). Franklin, a novel dataset with\nrequirements of meta- and object-level reasoning, is introduced and used along\nwith three other datasets to evaluate four LLMs at question answering tasks\nrequiring multiple steps of reasoning. Results from human annotation studies\nsuggest LLMs demonstrate meta-level reasoning with high frequency, but struggle\nwith object-level reasoning tasks in some of the datasets used. Additionally,\nevidence suggests that LLMs find the object-level reasoning required for the\nquestions in the Franklin dataset challenging, yet they do exhibit strong\nperformance with respect to the meta-level reasoning requirements."
                },
                "authors": [
                    {
                        "name": "Nick Ferguson"
                    },
                    {
                        "name": "Liane Guillou"
                    },
                    {
                        "name": "Alan Bundy"
                    },
                    {
                        "name": "Kwabena Nuamah"
                    }
                ],
                "author_detail": {
                    "name": "Kwabena Nuamah"
                },
                "author": "Kwabena Nuamah",
                "arxiv_comment": "8 pages. Accepted to the Workshop on Planning in the Era of LLMs\n  (LM4Plan @ AAAI 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10291v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10291v2",
                "updated": "2025-02-14T17:37:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    37,
                    35,
                    4,
                    45,
                    0
                ],
                "published": "2024-06-13T03:26:30Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    3,
                    26,
                    30,
                    3,
                    165,
                    0
                ],
                "title": "ResearchArena: Benchmarking Large Language Models' Ability to Collect\n  and Organize Information as Research Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResearchArena: Benchmarking Large Language Models' Ability to Collect\n  and Organize Information as Research Agents"
                },
                "summary": "Large language models (LLMs) excel across many natural language processing\ntasks but face challenges in domain-specific, analytical tasks such as\nconducting research surveys. This study introduces ResearchArena, a benchmark\ndesigned to evaluate LLMs' capabilities in conducting academic\nsurveys$\\unicode{x2013}$a foundational step in academic research. ResearchArena\nmodels the process in three stages: (1) information discovery, identifying\nrelevant literature; (2) information selection, evaluating papers' relevance\nand impact; and (3) information organization, structuring knowledge into\nhierarchical frameworks such as mind-maps. Notably, mind-map construction is\ntreated as a bonus task, reflecting its supplementary role in survey-writing.\nTo support these evaluations, we construct an offline environment of 12M\nfull-text academic papers and 7.9K survey papers. To ensure ethical compliance,\nwe do not redistribute copyrighted materials; instead, we provide code to\nconstruct the environment from the Semantic Scholar Open Research Corpus\n(S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform\ncompared to simpler keyword-based retrieval methods, underscoring significant\nopportunities for advancing LLMs in autonomous research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel across many natural language processing\ntasks but face challenges in domain-specific, analytical tasks such as\nconducting research surveys. This study introduces ResearchArena, a benchmark\ndesigned to evaluate LLMs' capabilities in conducting academic\nsurveys$\\unicode{x2013}$a foundational step in academic research. ResearchArena\nmodels the process in three stages: (1) information discovery, identifying\nrelevant literature; (2) information selection, evaluating papers' relevance\nand impact; and (3) information organization, structuring knowledge into\nhierarchical frameworks such as mind-maps. Notably, mind-map construction is\ntreated as a bonus task, reflecting its supplementary role in survey-writing.\nTo support these evaluations, we construct an offline environment of 12M\nfull-text academic papers and 7.9K survey papers. To ensure ethical compliance,\nwe do not redistribute copyrighted materials; instead, we provide code to\nconstruct the environment from the Semantic Scholar Open Research Corpus\n(S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform\ncompared to simpler keyword-based retrieval methods, underscoring significant\nopportunities for advancing LLMs in autonomous research."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Chenyan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Chenyan Xiong"
                },
                "author": "Chenyan Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10291v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10291v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10325v1",
                "updated": "2025-02-14T17:34:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    34,
                    28,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T17:34:28Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    34,
                    28,
                    4,
                    45,
                    0
                ],
                "title": "Process Reward Models for LLM Agents: Practical Framework and Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models for LLM Agents: Practical Framework and Directions"
                },
                "summary": "We introduce Agent Process Reward Models (AgentPRM), a simple and scalable\nframework for training LLM agents to continually improve through interactions.\nAgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo\nrollouts to compute reward targets and optimize policies. It requires minimal\nmodifications to existing RLHF pipelines, making it easy to integrate at scale.\nBeyond AgentPRM, we propose InversePRM, which learns process rewards directly\nfrom demonstrations without explicit outcome supervision. We also explore key\nchallenges and opportunities, including exploration, process reward shaping,\nand model-predictive reasoning. We evaluate on ALFWorld benchmark, show that\nsmall 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o\nbaselines, and analyze test-time scaling, reward hacking, and more. Our code is\navailable at: https://github.com/sanjibanc/agent_prm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Agent Process Reward Models (AgentPRM), a simple and scalable\nframework for training LLM agents to continually improve through interactions.\nAgentPRM follows a lightweight actor-critic paradigm, using Monte Carlo\nrollouts to compute reward targets and optimize policies. It requires minimal\nmodifications to existing RLHF pipelines, making it easy to integrate at scale.\nBeyond AgentPRM, we propose InversePRM, which learns process rewards directly\nfrom demonstrations without explicit outcome supervision. We also explore key\nchallenges and opportunities, including exploration, process reward shaping,\nand model-predictive reasoning. We evaluate on ALFWorld benchmark, show that\nsmall 3B models trained with AgentPRM and InversePRM outperform strong GPT-4o\nbaselines, and analyze test-time scaling, reward hacking, and more. Our code is\navailable at: https://github.com/sanjibanc/agent_prm."
                },
                "authors": [
                    {
                        "name": "Sanjiban Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Sanjiban Choudhury"
                },
                "author": "Sanjiban Choudhury",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v2",
                "updated": "2025-02-14T17:17:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    17,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_doi": "10.1145/3701716.3715192",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715192",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.09057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by WWW 2025 (Demo Track)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10308v1",
                "updated": "2025-02-14T17:12:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    12,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T17:12:20Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    12,
                    20,
                    4,
                    45,
                    0
                ],
                "title": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Powered Preference Elicitation in Combinatorial Assignment"
                },
                "summary": "We study the potential of large language models (LLMs) as proxies for humans\nto simplify preference elicitation (PE) in combinatorial assignment. While\ntraditional PE methods rely on iterative queries to capture preferences, LLMs\noffer a one-shot alternative with reduced human effort. We propose a framework\nfor LLM proxies that can work in tandem with SOTA ML-powered preference\nelicitation schemes. Our framework handles the novel challenges introduced by\nLLMs, such as response variability and increased computational costs. We\nexperimentally evaluate the efficiency of LLM proxies against human queries in\nthe well-studied course allocation domain, and we investigate the model\ncapabilities required for success. We find that our approach improves\nallocative efficiency by up to 20%, and these results are robust across\ndifferent LLMs and to differences in quality and accuracy of reporting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the potential of large language models (LLMs) as proxies for humans\nto simplify preference elicitation (PE) in combinatorial assignment. While\ntraditional PE methods rely on iterative queries to capture preferences, LLMs\noffer a one-shot alternative with reduced human effort. We propose a framework\nfor LLM proxies that can work in tandem with SOTA ML-powered preference\nelicitation schemes. Our framework handles the novel challenges introduced by\nLLMs, such as response variability and increased computational costs. We\nexperimentally evaluate the efficiency of LLM proxies against human queries in\nthe well-studied course allocation domain, and we investigate the model\ncapabilities required for success. We find that our approach improves\nallocative efficiency by up to 20%, and these results are robust across\ndifferent LLMs and to differences in quality and accuracy of reporting."
                },
                "authors": [
                    {
                        "name": "Ermis Soumalias"
                    },
                    {
                        "name": "Yanchen Jiang"
                    },
                    {
                        "name": "Kehang Zhu"
                    },
                    {
                        "name": "Michael Curry"
                    },
                    {
                        "name": "Sven Seuken"
                    },
                    {
                        "name": "David C. Parkes"
                    }
                ],
                "author_detail": {
                    "name": "David C. Parkes"
                },
                "author": "David C. Parkes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10299v1",
                "updated": "2025-02-14T17:01:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    1,
                    6,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T17:01:06Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    1,
                    6,
                    4,
                    45,
                    0
                ],
                "title": "Open-Source AI-Powered Optimization in Scalene: Advancing Python\n  Performance Profiling with DeepSeek-R1 and LLaMA 3.2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Source AI-Powered Optimization in Scalene: Advancing Python\n  Performance Profiling with DeepSeek-R1 and LLaMA 3.2"
                },
                "summary": "Python's flexibility and ease of use come at the cost of performance\ninefficiencies, requiring developers to rely on profilers to optimize\nexecution. SCALENE, a high-performance CPU, GPU, and memory profiler, provides\nfine-grained insights into Python applications while running significantly\nfaster than traditional profilers. Originally, SCALENE integrated OpenAI's API\nto generate AI-powered optimization suggestions, but its reliance on a\nproprietary API limited accessibility. This study explores the feasibility of\nusing opensource large language models (LLMs), such as DeepSeek-R1 and Llama\n3.2, to generate optimization recommendations within SCALENE. Our evaluation\nreveals that DeepSeek-R1 provides effective code optimizations comparable to\nproprietary models. We integrate DeepSeek-R1 into SCALENE to automatically\nanalyze performance bottlenecks and suggest improvements, enhancing SCALENE's\nutility while maintaining its open-source nature. This study demonstrates that\nopen-source LLMs can be viable alternatives for AI-driven code optimization,\npaving the way for more accessible and cost-effective performance analysis\ntools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python's flexibility and ease of use come at the cost of performance\ninefficiencies, requiring developers to rely on profilers to optimize\nexecution. SCALENE, a high-performance CPU, GPU, and memory profiler, provides\nfine-grained insights into Python applications while running significantly\nfaster than traditional profilers. Originally, SCALENE integrated OpenAI's API\nto generate AI-powered optimization suggestions, but its reliance on a\nproprietary API limited accessibility. This study explores the feasibility of\nusing opensource large language models (LLMs), such as DeepSeek-R1 and Llama\n3.2, to generate optimization recommendations within SCALENE. Our evaluation\nreveals that DeepSeek-R1 provides effective code optimizations comparable to\nproprietary models. We integrate DeepSeek-R1 into SCALENE to automatically\nanalyze performance bottlenecks and suggest improvements, enhancing SCALENE's\nutility while maintaining its open-source nature. This study demonstrates that\nopen-source LLMs can be viable alternatives for AI-driven code optimization,\npaving the way for more accessible and cost-effective performance analysis\ntools."
                },
                "authors": [
                    {
                        "name": "Saem Hasan"
                    },
                    {
                        "name": "Sanju Basak"
                    }
                ],
                "author_detail": {
                    "name": "Sanju Basak"
                },
                "author": "Sanju Basak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10294v1",
                "updated": "2025-02-14T16:56:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    56,
                    24,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T16:56:24Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    56,
                    24,
                    4,
                    45,
                    0
                ],
                "title": "QMaxViT-Unet+: A Query-Based MaxViT-Unet with Edge Enhancement for\n  Scribble-Supervised Segmentation of Medical Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMaxViT-Unet+: A Query-Based MaxViT-Unet with Edge Enhancement for\n  Scribble-Supervised Segmentation of Medical Images"
                },
                "summary": "The deployment of advanced deep learning models for medical image\nsegmentation is often constrained by the requirement for extensively annotated\ndatasets. Weakly-supervised learning, which allows less precise labels, has\nbecome a promising solution to this challenge. Building on this approach, we\npropose QMaxViT-Unet+, a novel framework for scribble-supervised medical image\nsegmentation. This framework is built on the U-Net architecture, with the\nencoder and decoder replaced by Multi-Axis Vision Transformer (MaxViT) blocks.\nThese blocks enhance the model's ability to learn local and global features\nefficiently. Additionally, our approach integrates a query-based Transformer\ndecoder to refine features and an edge enhancement module to compensate for the\nlimited boundary information in the scribble label. We evaluate the proposed\nQMaxViT-Unet+ on four public datasets focused on cardiac structures, colorectal\npolyps, and breast cancer: ACDC, MS-CMRSeg, SUN-SEG, and BUSI. Evaluation\nmetrics include the Dice similarity coefficient (DSC) and the 95th percentile\nof Hausdorff distance (HD95). Experimental results show that QMaxViT-Unet+\nachieves 89.1\\% DSC and 1.316mm HD95 on ACDC, 88.4\\% DSC and 2.226mm HD95 on\nMS-CMRSeg, 71.4\\% DSC and 4.996mm HD95 on SUN-SEG, and 69.4\\% DSC and 50.122mm\nHD95 on BUSI. These results demonstrate that our method outperforms existing\napproaches in terms of accuracy, robustness, and efficiency while remaining\ncompetitive with fully-supervised learning approaches. This makes it ideal for\nmedical image analysis, where high-quality annotations are often scarce and\nrequire significant effort and expense. The code is available at:\nhttps://github.com/anpc849/QMaxViT-Unet",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of advanced deep learning models for medical image\nsegmentation is often constrained by the requirement for extensively annotated\ndatasets. Weakly-supervised learning, which allows less precise labels, has\nbecome a promising solution to this challenge. Building on this approach, we\npropose QMaxViT-Unet+, a novel framework for scribble-supervised medical image\nsegmentation. This framework is built on the U-Net architecture, with the\nencoder and decoder replaced by Multi-Axis Vision Transformer (MaxViT) blocks.\nThese blocks enhance the model's ability to learn local and global features\nefficiently. Additionally, our approach integrates a query-based Transformer\ndecoder to refine features and an edge enhancement module to compensate for the\nlimited boundary information in the scribble label. We evaluate the proposed\nQMaxViT-Unet+ on four public datasets focused on cardiac structures, colorectal\npolyps, and breast cancer: ACDC, MS-CMRSeg, SUN-SEG, and BUSI. Evaluation\nmetrics include the Dice similarity coefficient (DSC) and the 95th percentile\nof Hausdorff distance (HD95). Experimental results show that QMaxViT-Unet+\nachieves 89.1\\% DSC and 1.316mm HD95 on ACDC, 88.4\\% DSC and 2.226mm HD95 on\nMS-CMRSeg, 71.4\\% DSC and 4.996mm HD95 on SUN-SEG, and 69.4\\% DSC and 50.122mm\nHD95 on BUSI. These results demonstrate that our method outperforms existing\napproaches in terms of accuracy, robustness, and efficiency while remaining\ncompetitive with fully-supervised learning approaches. This makes it ideal for\nmedical image analysis, where high-quality annotations are often scarce and\nrequire significant effort and expense. The code is available at:\nhttps://github.com/anpc849/QMaxViT-Unet"
                },
                "authors": [
                    {
                        "name": "Thien B. Nguyen-Tat"
                    },
                    {
                        "name": "Hoang-An Vo"
                    },
                    {
                        "name": "Phuoc-Sang Dang"
                    }
                ],
                "author_detail": {
                    "name": "Phuoc-Sang Dang"
                },
                "author": "Phuoc-Sang Dang",
                "arxiv_doi": "10.1016/j.compbiomed.2025.109762",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.compbiomed.2025.109762",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.10294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17115v2",
                "updated": "2025-02-14T16:44:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    44,
                    8,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-25T17:28:13Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    28,
                    13,
                    2,
                    269,
                    0
                ],
                "title": "Programming Every Example: Lifting Pre-training Data Quality Like\n  Experts at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Programming Every Example: Lifting Pre-training Data Quality Like\n  Experts at Scale"
                },
                "summary": "Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, FineWeb,\nFineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in\ndomain-specific continual pre-training: without domain specific design, models\ntrained on OpenWebMath refined by ProX outperform human-crafted rule-based\nmethods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for\nLlama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable\nto models like Llemma-7B trained on 200B tokens. Further analysis highlights\nthat ProX significantly saves training FLOPs, offering a promising path for\nefficient LLM pre-training. We are open-sourcing ProX with >500B corpus,\nmodels, and sharing all training and implementation details for reproducible\nresearch and future innovation. Code: https://github.com/GAIR-NLP/ProX",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model pre-training has traditionally relied on human experts\nto craft heuristics for improving the corpora quality, resulting in numerous\nrules developed to date. However, these rules lack the flexibility to address\nthe unique characteristics of individual example effectively. Meanwhile,\napplying tailored rules to every example is impractical for human experts. In\nthis paper, we demonstrate that even small language models, with as few as 0.3B\nparameters, can exhibit substantial data refining capabilities comparable to\nthose of human experts. We introduce Programming Every Example (ProX), a novel\nframework that treats data refinement as a programming task, enabling models to\nrefine corpora by generating and executing fine-grained operations, such as\nstring normalization, for each individual example at scale. Experimental\nresults show that models pre-trained on ProX-curated data outperform either\noriginal data or data filtered by other selection methods by more than 2%\nacross various downstream benchmarks. Its effectiveness spans various model\nsizes and pre-training corpora, including C4, RedPajama-V2, FineWeb,\nFineWeb-Edu, and DCLM. Furthermore, ProX exhibits significant potential in\ndomain-specific continual pre-training: without domain specific design, models\ntrained on OpenWebMath refined by ProX outperform human-crafted rule-based\nmethods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for\nLlama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable\nto models like Llemma-7B trained on 200B tokens. Further analysis highlights\nthat ProX significantly saves training FLOPs, offering a promising path for\nefficient LLM pre-training. We are open-sourcing ProX with >500B corpus,\nmodels, and sharing all training and implementation details for reproducible\nresearch and future innovation. Code: https://github.com/GAIR-NLP/ProX"
                },
                "authors": [
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Zengzhi Wang"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Junlong Li"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "arxiv_comment": "47 pages, 13 figures, 34 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01980v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01980v3",
                "updated": "2025-02-14T16:35:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    35,
                    51,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-03T15:22:41Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    22,
                    41,
                    1,
                    247,
                    0
                ],
                "title": "Large Language Models for Anomaly and Out-of-Distribution Detection: A\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Anomaly and Out-of-Distribution Detection: A\n  Survey"
                },
                "summary": "Detecting anomalies or out-of-distribution (OOD) samples is critical for\nmaintaining the reliability and trustworthiness of machine learning systems.\nRecently, Large Language Models (LLMs) have demonstrated their effectiveness\nnot only in natural language processing but also in broader applications due to\ntheir advanced comprehension and generative capabilities. The integration of\nLLMs into anomaly and OOD detection marks a significant shift from the\ntraditional paradigm in the field. This survey focuses on the problem of\nanomaly and OOD detection under the context of LLMs. We propose a new taxonomy\nto categorize existing approaches into two classes based on the role played by\nLLMs. Following our proposed taxonomy, we further discuss the related work\nunder each of the categories and finally discuss potential challenges and\ndirections for future research in this field. We also provide an up-to-date\nreading list of relevant papers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting anomalies or out-of-distribution (OOD) samples is critical for\nmaintaining the reliability and trustworthiness of machine learning systems.\nRecently, Large Language Models (LLMs) have demonstrated their effectiveness\nnot only in natural language processing but also in broader applications due to\ntheir advanced comprehension and generative capabilities. The integration of\nLLMs into anomaly and OOD detection marks a significant shift from the\ntraditional paradigm in the field. This survey focuses on the problem of\nanomaly and OOD detection under the context of LLMs. We propose a new taxonomy\nto categorize existing approaches into two classes based on the role played by\nLLMs. Following our proposed taxonomy, we further discuss the related work\nunder each of the categories and finally discuss potential challenges and\ndirections for future research in this field. We also provide an up-to-date\nreading list of relevant papers."
                },
                "authors": [
                    {
                        "name": "Ruiyao Xu"
                    },
                    {
                        "name": "Kaize Ding"
                    }
                ],
                "author_detail": {
                    "name": "Kaize Ding"
                },
                "author": "Kaize Ding",
                "arxiv_comment": "Accepted to NAACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01980v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01980v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06426v2",
                "updated": "2025-02-14T16:32:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    32,
                    54,
                    4,
                    45,
                    0
                ],
                "published": "2024-11-10T11:08:28Z",
                "published_parsed": [
                    2024,
                    11,
                    10,
                    11,
                    8,
                    28,
                    6,
                    315,
                    0
                ],
                "title": "SequentialBreak: Large Language Models Can be Fooled by Embedding\n  Jailbreak Prompts into Sequential Prompt Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SequentialBreak: Large Language Models Can be Fooled by Embedding\n  Jailbreak Prompts into Sequential Prompt Chains"
                },
                "summary": "As the integration of the Large Language Models (LLMs) into various\napplications increases, so does their susceptibility to misuse, raising\nsignificant security concerns. Numerous jailbreak attacks have been proposed to\nassess the security defense of LLMs. Current jailbreak attacks mainly rely on\nscenario camouflage, prompt obfuscation, prompt optimization, and prompt\niterative optimization to conceal malicious prompts. In particular, sequential\nprompt chains in a single query can lead LLMs to focus on certain prompts while\nignoring others, facilitating context manipulation. This paper introduces\nSequentialBreak, a novel jailbreak attack that exploits this vulnerability. We\ndiscuss several scenarios, not limited to examples like Question Bank, Dialog\nCompletion, and Game Environment, where the harmful prompt is embedded within\nbenign ones that can fool LLMs into generating harmful responses. The distinct\nnarrative structures of these scenarios show that SequentialBreak is flexible\nenough to adapt to various prompt formats beyond those discussed. Extensive\nexperiments demonstrate that SequentialBreak uses only a single query to\nachieve a substantial gain of attack success rate over existing baselines\nagainst both open-source and closed-source models. Through our research, we\nhighlight the urgent need for more robust and resilient safeguards to enhance\nLLM security and prevent potential misuse. All the result files and website\nassociated with this research are available in this GitHub repository:\nhttps://anonymous.4open.science/r/JailBreakAttack-4F3B/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the integration of the Large Language Models (LLMs) into various\napplications increases, so does their susceptibility to misuse, raising\nsignificant security concerns. Numerous jailbreak attacks have been proposed to\nassess the security defense of LLMs. Current jailbreak attacks mainly rely on\nscenario camouflage, prompt obfuscation, prompt optimization, and prompt\niterative optimization to conceal malicious prompts. In particular, sequential\nprompt chains in a single query can lead LLMs to focus on certain prompts while\nignoring others, facilitating context manipulation. This paper introduces\nSequentialBreak, a novel jailbreak attack that exploits this vulnerability. We\ndiscuss several scenarios, not limited to examples like Question Bank, Dialog\nCompletion, and Game Environment, where the harmful prompt is embedded within\nbenign ones that can fool LLMs into generating harmful responses. The distinct\nnarrative structures of these scenarios show that SequentialBreak is flexible\nenough to adapt to various prompt formats beyond those discussed. Extensive\nexperiments demonstrate that SequentialBreak uses only a single query to\nachieve a substantial gain of attack success rate over existing baselines\nagainst both open-source and closed-source models. Through our research, we\nhighlight the urgent need for more robust and resilient safeguards to enhance\nLLM security and prevent potential misuse. All the result files and website\nassociated with this research are available in this GitHub repository:\nhttps://anonymous.4open.science/r/JailBreakAttack-4F3B/."
                },
                "authors": [
                    {
                        "name": "Bijoy Ahmed Saiem"
                    },
                    {
                        "name": "MD Sadik Hossain Shanto"
                    },
                    {
                        "name": "Rakib Ahsan"
                    },
                    {
                        "name": "Md Rafi ur Rashid"
                    }
                ],
                "author_detail": {
                    "name": "Md Rafi ur Rashid"
                },
                "author": "Md Rafi ur Rashid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13610v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13610v2",
                "updated": "2025-02-14T16:27:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    27,
                    25,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-17T14:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    14,
                    46,
                    22,
                    3,
                    291,
                    0
                ],
                "title": "MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool\n  Calling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeNTi: Bridging Medical Calculator and LLM Agent with Nested Tool\n  Calling"
                },
                "summary": "Integrating tools into Large Language Models (LLMs) has facilitated the\nwidespread application. Despite this, in specialized downstream task contexts,\nreliance solely on tools is insufficient to fully address the complexities of\nthe real world. This particularly restricts the effective deployment of LLMs in\nfields such as medicine. In this paper, we focus on the downstream tasks of\nmedical calculators, which use standardized tests to assess an individual's\nhealth status. We introduce MeNTi, a universal agent architecture for LLMs.\nMeNTi integrates a specialized medical toolkit and employs meta-tool and nested\ncalling mechanisms to enhance LLM tool utilization. Specifically, it achieves\nflexible tool selection and nested tool calling to address practical issues\nfaced in intricate medical scenarios, including calculator selection, slot\nfilling, and unit conversion. To assess the capabilities of LLMs for\nquantitative assessment throughout the clinical process of calculator\nscenarios, we introduce CalcQA. This benchmark requires LLMs to use medical\ncalculators to perform calculations and assess patient health status. CalcQA is\nconstructed by professional physicians and includes 100 case-calculator pairs,\ncomplemented by a toolkit of 281 medical tools. The experimental results\ndemonstrate significant performance improvements with our framework. This\nresearch paves new directions for applying LLMs in demanding scenarios of\nmedicine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating tools into Large Language Models (LLMs) has facilitated the\nwidespread application. Despite this, in specialized downstream task contexts,\nreliance solely on tools is insufficient to fully address the complexities of\nthe real world. This particularly restricts the effective deployment of LLMs in\nfields such as medicine. In this paper, we focus on the downstream tasks of\nmedical calculators, which use standardized tests to assess an individual's\nhealth status. We introduce MeNTi, a universal agent architecture for LLMs.\nMeNTi integrates a specialized medical toolkit and employs meta-tool and nested\ncalling mechanisms to enhance LLM tool utilization. Specifically, it achieves\nflexible tool selection and nested tool calling to address practical issues\nfaced in intricate medical scenarios, including calculator selection, slot\nfilling, and unit conversion. To assess the capabilities of LLMs for\nquantitative assessment throughout the clinical process of calculator\nscenarios, we introduce CalcQA. This benchmark requires LLMs to use medical\ncalculators to perform calculations and assess patient health status. CalcQA is\nconstructed by professional physicians and includes 100 case-calculator pairs,\ncomplemented by a toolkit of 281 medical tools. The experimental results\ndemonstrate significant performance improvements with our framework. This\nresearch paves new directions for applying LLMs in demanding scenarios of\nmedicine."
                },
                "authors": [
                    {
                        "name": "Yakun Zhu"
                    },
                    {
                        "name": "Shaohang Wei"
                    },
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Kui Xue"
                    },
                    {
                        "name": "Xiaofan Zhang"
                    },
                    {
                        "name": "Shaoting Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shaoting Zhang"
                },
                "author": "Shaoting Zhang",
                "arxiv_comment": "NAACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13610v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13610v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10266v1",
                "updated": "2025-02-14T16:23:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    23,
                    39,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T16:23:39Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    23,
                    39,
                    4,
                    45,
                    0
                ],
                "title": "Are Large Language Models the future crowd workers of Linguistics?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models the future crowd workers of Linguistics?"
                },
                "summary": "Data elicitation from human participants is one of the core data collection\nstrategies used in empirical linguistic research. The amount of participants in\nsuch studies may vary considerably, ranging from a handful to crowdsourcing\ndimensions. Even if they provide resourceful extensive data, both of these\nsettings come alongside many disadvantages, such as low control of\nparticipants' attention during task completion, precarious working conditions\nin crowdsourcing environments, and time-consuming experimental designs. For\nthese reasons, this research aims to answer the question of whether Large\nLanguage Models (LLMs) may overcome those obstacles if included in empirical\nlinguistic pipelines. Two reproduction case studies are conducted to gain\nclarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced\nelicitation tasks, originally designed for human participants, are reproduced\nin the proposed framework with the help of OpenAI's GPT-4o-mini model. Its\nperformance with our zero-shot prompting baseline shows the effectiveness and\nhigh versatility of LLMs, that tend to outperform human informants in\nlinguistic tasks. The findings of the second replication further highlight the\nneed to explore additional prompting techniques, such as Chain-of-Thought (CoT)\nprompting, which, in a second follow-up experiment, demonstrates higher\nalignment to human performance on both critical and filler items. Given the\nlimited scale of this study, it is worthwhile to further explore the\nperformance of LLMs in empirical Linguistics and in other future applications\nin the humanities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data elicitation from human participants is one of the core data collection\nstrategies used in empirical linguistic research. The amount of participants in\nsuch studies may vary considerably, ranging from a handful to crowdsourcing\ndimensions. Even if they provide resourceful extensive data, both of these\nsettings come alongside many disadvantages, such as low control of\nparticipants' attention during task completion, precarious working conditions\nin crowdsourcing environments, and time-consuming experimental designs. For\nthese reasons, this research aims to answer the question of whether Large\nLanguage Models (LLMs) may overcome those obstacles if included in empirical\nlinguistic pipelines. Two reproduction case studies are conducted to gain\nclarity into this matter: Cruz (2023) and Lombard et al. (2021). The two forced\nelicitation tasks, originally designed for human participants, are reproduced\nin the proposed framework with the help of OpenAI's GPT-4o-mini model. Its\nperformance with our zero-shot prompting baseline shows the effectiveness and\nhigh versatility of LLMs, that tend to outperform human informants in\nlinguistic tasks. The findings of the second replication further highlight the\nneed to explore additional prompting techniques, such as Chain-of-Thought (CoT)\nprompting, which, in a second follow-up experiment, demonstrates higher\nalignment to human performance on both critical and filler items. Given the\nlimited scale of this study, it is worthwhile to further explore the\nperformance of LLMs in empirical Linguistics and in other future applications\nin the humanities."
                },
                "authors": [
                    {
                        "name": "Iris Ferrazzo"
                    }
                ],
                "author_detail": {
                    "name": "Iris Ferrazzo"
                },
                "author": "Iris Ferrazzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20299v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20299v2",
                "updated": "2025-02-14T16:16:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    16,
                    15,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-27T00:42:21Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    0,
                    42,
                    21,
                    6,
                    301,
                    0
                ],
                "title": "EACO-RAG: Towards Distributed Tiered LLM Deployment using Edge-Assisted\n  and Collaborative RAG with Adaptive Knowledge Update",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EACO-RAG: Towards Distributed Tiered LLM Deployment using Edge-Assisted\n  and Collaborative RAG with Adaptive Knowledge Update"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nlanguage tasks, but they require high computing power and rely on static\nknowledge. To overcome these limitations, Retrieval-Augmented Generation (RAG)\nincorporates up-to-date external information into LLMs without extensive\nfine-tuning. Meanwhile, small language models (SLMs) deployed on edge devices\noffer efficiency and low latency but often struggle with complex reasoning\ntasks. Unfortunately, current RAG approaches are predominantly based on\ncentralized databases and have not been adapted to address the distinct\nconstraints associated with deploying SLMs in edge environments. To bridge this\ngap, we propose Edge-Assisted and Collaborative RAG (EACO-RAG), a lightweight\nframework that leverages distributed edge nodes for adaptive knowledge updates\nand retrieval. EACO-RAG also employs a hierarchical collaborative gating\nmechanism to dynamically select among local, edge-assisted, and cloud-based\nstrategies, with a carefully designed algorithm based on Safe Online Bayesian\nOptimization to maximize the potential performance enhancements. Experimental\nresults demonstrate that EACO-RAG matches the accuracy of cloud-based knowledge\ngraph RAG systems while reducing total costs by up to 84.6% under relaxed delay\nconstraints and by 65.3% under stricter delay requirements. This work\nrepresents our initial effort toward achieving a distributed and scalable\ntiered LLM deployments, with EACO-RAG serving as a promising first step in\nunlocking the full potential of hybrid edge-cloud intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities in\nlanguage tasks, but they require high computing power and rely on static\nknowledge. To overcome these limitations, Retrieval-Augmented Generation (RAG)\nincorporates up-to-date external information into LLMs without extensive\nfine-tuning. Meanwhile, small language models (SLMs) deployed on edge devices\noffer efficiency and low latency but often struggle with complex reasoning\ntasks. Unfortunately, current RAG approaches are predominantly based on\ncentralized databases and have not been adapted to address the distinct\nconstraints associated with deploying SLMs in edge environments. To bridge this\ngap, we propose Edge-Assisted and Collaborative RAG (EACO-RAG), a lightweight\nframework that leverages distributed edge nodes for adaptive knowledge updates\nand retrieval. EACO-RAG also employs a hierarchical collaborative gating\nmechanism to dynamically select among local, edge-assisted, and cloud-based\nstrategies, with a carefully designed algorithm based on Safe Online Bayesian\nOptimization to maximize the potential performance enhancements. Experimental\nresults demonstrate that EACO-RAG matches the accuracy of cloud-based knowledge\ngraph RAG systems while reducing total costs by up to 84.6% under relaxed delay\nconstraints and by 65.3% under stricter delay requirements. This work\nrepresents our initial effort toward achieving a distributed and scalable\ntiered LLM deployments, with EACO-RAG serving as a promising first step in\nunlocking the full potential of hybrid edge-cloud intelligence."
                },
                "authors": [
                    {
                        "name": "Jiaxing Li"
                    },
                    {
                        "name": "Chi Xu"
                    },
                    {
                        "name": "Lianchen Jia"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Cong Zhang"
                    },
                    {
                        "name": "Jiangchuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangchuan Liu"
                },
                "author": "Jiangchuan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20299v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20299v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10263v1",
                "updated": "2025-02-14T16:16:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    16,
                    2,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T16:16:02Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    16,
                    2,
                    4,
                    45,
                    0
                ],
                "title": "Large Language Models and Synthetic Data for Monitoring Dataset Mentions\n  in Research Papers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Synthetic Data for Monitoring Dataset Mentions\n  in Research Papers"
                },
                "summary": "Tracking how data is mentioned and used in research papers provides critical\ninsights for improving data discoverability, quality, and production. However,\nmanually identifying and classifying dataset mentions across vast academic\nliterature is resource-intensive and not scalable. This paper presents a\nmachine learning framework that automates dataset mention detection across\nresearch domains by leveraging large language models (LLMs), synthetic data,\nand a two-stage fine-tuning process. We employ zero-shot extraction from\nresearch papers, an LLM-as-a-Judge for quality assessment, and a reasoning\nagent for refinement to generate a weakly supervised synthetic dataset. The\nPhi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by\nfine-tuning on a manually annotated subset. At inference, a ModernBERT-based\nclassifier efficiently filters dataset mentions, reducing computational\noverhead while maintaining high recall. Evaluated on a held-out manually\nannotated sample, our fine-tuned model outperforms NuExtract-v1.5 and\nGLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how\nLLM-generated synthetic data can effectively address training data scarcity,\nimproving generalization in low-resource settings. This framework offers a\npathway toward scalable monitoring of dataset usage, enhancing transparency,\nand supporting researchers, funders, and policymakers in identifying data gaps\nand strengthening data accessibility for informed decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracking how data is mentioned and used in research papers provides critical\ninsights for improving data discoverability, quality, and production. However,\nmanually identifying and classifying dataset mentions across vast academic\nliterature is resource-intensive and not scalable. This paper presents a\nmachine learning framework that automates dataset mention detection across\nresearch domains by leveraging large language models (LLMs), synthetic data,\nand a two-stage fine-tuning process. We employ zero-shot extraction from\nresearch papers, an LLM-as-a-Judge for quality assessment, and a reasoning\nagent for refinement to generate a weakly supervised synthetic dataset. The\nPhi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by\nfine-tuning on a manually annotated subset. At inference, a ModernBERT-based\nclassifier efficiently filters dataset mentions, reducing computational\noverhead while maintaining high recall. Evaluated on a held-out manually\nannotated sample, our fine-tuned model outperforms NuExtract-v1.5 and\nGLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how\nLLM-generated synthetic data can effectively address training data scarcity,\nimproving generalization in low-resource settings. This framework offers a\npathway toward scalable monitoring of dataset usage, enhancing transparency,\nand supporting researchers, funders, and policymakers in identifying data gaps\nand strengthening data accessibility for informed decision-making."
                },
                "authors": [
                    {
                        "name": "Aivin V. Solatorio"
                    },
                    {
                        "name": "Rafael Macalaba"
                    },
                    {
                        "name": "James Liounis"
                    }
                ],
                "author_detail": {
                    "name": "James Liounis"
                },
                "author": "James Liounis",
                "arxiv_comment": "Project GitHub repository at https://github.com/worldbank/ai4data-use",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20856v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20856v2",
                "updated": "2025-02-14T16:09:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    16,
                    9,
                    49,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-28T09:19:29Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    9,
                    19,
                    29,
                    0,
                    302,
                    0
                ],
                "title": "Strada-LLM: Graph LLM for traffic prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strada-LLM: Graph LLM for traffic prediction"
                },
                "summary": "Traffic prediction is a vital component of intelligent transportation\nsystems. By reasoning about traffic patterns in both the spatial and temporal\ndimensions, accurate and interpretable predictions can be provided. A\nconsiderable challenge in traffic prediction lies in handling the diverse data\ndistributions caused by vastly different traffic conditions occurring at\ndifferent locations. LLMs have been a dominant solution due to their remarkable\ncapacity to adapt to new datasets with very few labeled data samples, i.e.,\nfew-shot adaptability. However, existing forecasting techniques mainly focus on\nextracting local graph information and forming a text-like prompt, leaving LLM-\nbased traffic prediction an open problem. This work presents a probabilistic\nLLM for traffic forecasting with three highlights. We propose a graph-aware LLM\nfor traffic prediction that considers proximal traffic information.\nSpecifically, by considering the traffic of neighboring nodes as covariates,\nour model outperforms the corresponding time-series LLM. Furthermore, we adopt\na lightweight approach for efficient domain adaptation when facing new data\ndistributions in few-shot fashion. The comparative experiment demonstrates the\nproposed method outperforms the state-of-the-art LLM-based methods and the\ntraditional GNN- based supervised approaches. Furthermore, Strada-LLM can be\neasily adapted to different LLM backbones without a noticeable performance\ndrop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic prediction is a vital component of intelligent transportation\nsystems. By reasoning about traffic patterns in both the spatial and temporal\ndimensions, accurate and interpretable predictions can be provided. A\nconsiderable challenge in traffic prediction lies in handling the diverse data\ndistributions caused by vastly different traffic conditions occurring at\ndifferent locations. LLMs have been a dominant solution due to their remarkable\ncapacity to adapt to new datasets with very few labeled data samples, i.e.,\nfew-shot adaptability. However, existing forecasting techniques mainly focus on\nextracting local graph information and forming a text-like prompt, leaving LLM-\nbased traffic prediction an open problem. This work presents a probabilistic\nLLM for traffic forecasting with three highlights. We propose a graph-aware LLM\nfor traffic prediction that considers proximal traffic information.\nSpecifically, by considering the traffic of neighboring nodes as covariates,\nour model outperforms the corresponding time-series LLM. Furthermore, we adopt\na lightweight approach for efficient domain adaptation when facing new data\ndistributions in few-shot fashion. The comparative experiment demonstrates the\nproposed method outperforms the state-of-the-art LLM-based methods and the\ntraditional GNN- based supervised approaches. Furthermore, Strada-LLM can be\neasily adapted to different LLM backbones without a noticeable performance\ndrop."
                },
                "authors": [
                    {
                        "name": "Seyed Mohamad Moghadas"
                    },
                    {
                        "name": "Yangxintong Lyu"
                    },
                    {
                        "name": "Bruno Cornelis"
                    },
                    {
                        "name": "Alexandre Alahi"
                    },
                    {
                        "name": "Adrian Munteanu"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Munteanu"
                },
                "author": "Adrian Munteanu",
                "arxiv_comment": "The reviewers decided to reject it. After getting the reviews, we\n  wanted to study more.",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20856v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20856v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10250v1",
                "updated": "2025-02-14T15:59:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    59,
                    33,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:59:33Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    59,
                    33,
                    4,
                    45,
                    0
                ],
                "title": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VisCon-100K: Leveraging Contextual Web Data for Fine-tuning Vision\n  Language Models"
                },
                "summary": "Vision-language models (VLMs) excel in various visual benchmarks but are\noften constrained by the lack of high-quality visual fine-tuning data. To\naddress this challenge, we introduce VisCon-100K, a novel dataset derived from\ninterleaved image-text web documents. Our approach transforms 45K web documents\nfrom the OBELICS dataset into 100K image conversation samples. We utilize\nGPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert\nthese captions into diverse free-form and multiple-choice question-answer\npairs. Integrating this dataset for fine-tuning considerably enhances VLM\nperformance across multiple benchmarks. Unlike methods that focus solely on\nfine-grained visual content, our approach leverages accompanying web context,\nyielding superior results. We also discover that a `leaky modality mix,' where\nconversation samples contain questions answerable from both the image and its\ncontextual caption, outperforms non-leaky combinations of captions and Q\\&A\npairs. VisCon-100k dataset shows strong performance with two popular VLM\napproaches: text-only large language model (LLM) aligned with a vision encoder\nusing image captions data (ShareGPT4V-7b) and multimodally pretrained LLM\n(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the\nVisCon-100K dataset, we provide a contextual captioner trained on this dataset,\nfacilitating scalable fine-tuning data generation for future research and\nopen-source applications. Using the same pipeline, but substituting our trained\ncontextual captioner for GPT-4V, we also release the larger VisCon-1M dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) excel in various visual benchmarks but are\noften constrained by the lack of high-quality visual fine-tuning data. To\naddress this challenge, we introduce VisCon-100K, a novel dataset derived from\ninterleaved image-text web documents. Our approach transforms 45K web documents\nfrom the OBELICS dataset into 100K image conversation samples. We utilize\nGPT-4V to generate image-contextual captions and OpenChat 3.5 model to convert\nthese captions into diverse free-form and multiple-choice question-answer\npairs. Integrating this dataset for fine-tuning considerably enhances VLM\nperformance across multiple benchmarks. Unlike methods that focus solely on\nfine-grained visual content, our approach leverages accompanying web context,\nyielding superior results. We also discover that a `leaky modality mix,' where\nconversation samples contain questions answerable from both the image and its\ncontextual caption, outperforms non-leaky combinations of captions and Q\\&A\npairs. VisCon-100k dataset shows strong performance with two popular VLM\napproaches: text-only large language model (LLM) aligned with a vision encoder\nusing image captions data (ShareGPT4V-7b) and multimodally pretrained LLM\n(IDEFICS2-8b) using interleaved image-text data. In addition to releasing the\nVisCon-100K dataset, we provide a contextual captioner trained on this dataset,\nfacilitating scalable fine-tuning data generation for future research and\nopen-source applications. Using the same pipeline, but substituting our trained\ncontextual captioner for GPT-4V, we also release the larger VisCon-1M dataset."
                },
                "authors": [
                    {
                        "name": "Gokul Karthik Kumar"
                    },
                    {
                        "name": "Iheb Chaabane"
                    },
                    {
                        "name": "Kebin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kebin Wu"
                },
                "author": "Kebin Wu",
                "arxiv_comment": "Accepted at PAKDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10239v1",
                "updated": "2025-02-14T15:49:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    49,
                    2,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:49:02Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    49,
                    2,
                    4,
                    45,
                    0
                ],
                "title": "Efficient Zero-Order Federated Finetuning of Language Models for\n  Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Zero-Order Federated Finetuning of Language Models for\n  Resource-Constrained Devices"
                },
                "summary": "Federated fine-tuning offers a promising approach for tuning Large Language\nModels (LLMs) on edge devices while preserving data privacy. However,\nfine-tuning these models on edge devices remains challenging due to high\nmemory, communication, and computational demands. Zero-order optimization with\ntask alignment provides a potential solution, enabling fine-tuning with\ninference-level memory requirements but requires a longer convergence time. In\nthis paper, we propose Federated Split-Perturbation Zero-order Optimization\n(FedSPZO) that divides the network into two blocks, applying a different number\nof perturbations per block in a computationally effective way, achieving faster\nconvergence. Our evaluation shows a $2.5 - 7\\times $ reduction in computation\noverhead compared to zero-order state of the art techniques in federated\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated fine-tuning offers a promising approach for tuning Large Language\nModels (LLMs) on edge devices while preserving data privacy. However,\nfine-tuning these models on edge devices remains challenging due to high\nmemory, communication, and computational demands. Zero-order optimization with\ntask alignment provides a potential solution, enabling fine-tuning with\ninference-level memory requirements but requires a longer convergence time. In\nthis paper, we propose Federated Split-Perturbation Zero-order Optimization\n(FedSPZO) that divides the network into two blocks, applying a different number\nof perturbations per block in a computationally effective way, achieving faster\nconvergence. Our evaluation shows a $2.5 - 7\\times $ reduction in computation\noverhead compared to zero-order state of the art techniques in federated\nlearning."
                },
                "authors": [
                    {
                        "name": "Mohamed Aboelenien Ahmed"
                    },
                    {
                        "name": "Kilian Pfeiffer"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Heba Khdr"
                    },
                    {
                        "name": "Jörg Henkel"
                    }
                ],
                "author_detail": {
                    "name": "Jörg Henkel"
                },
                "author": "Jörg Henkel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07780v2",
                "updated": "2025-02-14T15:46:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    46,
                    51,
                    4,
                    45,
                    0
                ],
                "published": "2024-06-12T00:19:40Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    0,
                    19,
                    40,
                    2,
                    164,
                    0
                ],
                "title": "A Critical Look At Tokenwise Reward-Guided Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Critical Look At Tokenwise Reward-Guided Text Generation"
                },
                "summary": "Large language models (LLMs) can be improved by aligning with human\npreferences through fine-tuning -- the so-called reinforcement learning from\nhuman feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive\nfor many users. Due to their ability to bypass LLM fine-tuning, prediction-time\ntokenwise reward-guided text generation (RGTG) methods have recently been\nproposed. They use a reward model trained on full sequences to score partial\nsequences during decoding in a bid to steer the generation towards sequences\nwith high rewards. However, these methods have so far been only heuristically\nmotivated and poorly analyzed. In this work, we show that reward models trained\non full sequences are not compatible with scoring partial sequences. To\nalleviate this issue, we propose to train a Bradley-Terry reward model on\npartial sequences explicitly, and autoregressively sample from the implied\ntokenwise policy during decoding time. We study the properties of this reward\nmodel and the resulting policy: we show that this policy is proportional to the\nratio of two distinct RLHF policies. Our simple approach outperforms previous\nRGTG methods and performs similarly to strong offline baselines without\nlarge-scale LLM finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can be improved by aligning with human\npreferences through fine-tuning -- the so-called reinforcement learning from\nhuman feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive\nfor many users. Due to their ability to bypass LLM fine-tuning, prediction-time\ntokenwise reward-guided text generation (RGTG) methods have recently been\nproposed. They use a reward model trained on full sequences to score partial\nsequences during decoding in a bid to steer the generation towards sequences\nwith high rewards. However, these methods have so far been only heuristically\nmotivated and poorly analyzed. In this work, we show that reward models trained\non full sequences are not compatible with scoring partial sequences. To\nalleviate this issue, we propose to train a Bradley-Terry reward model on\npartial sequences explicitly, and autoregressively sample from the implied\ntokenwise policy during decoding time. We study the properties of this reward\nmodel and the resulting policy: we show that this policy is proportional to the\nratio of two distinct RLHF policies. Our simple approach outperforms previous\nRGTG methods and performs similarly to strong offline baselines without\nlarge-scale LLM finetuning."
                },
                "authors": [
                    {
                        "name": "Ahmad Rashid"
                    },
                    {
                        "name": "Ruotian Wu"
                    },
                    {
                        "name": "Julia Grosse"
                    },
                    {
                        "name": "Agustinus Kristiadi"
                    },
                    {
                        "name": "Pascal Poupart"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Poupart"
                },
                "author": "Pascal Poupart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14050v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14050v3",
                "updated": "2025-02-14T15:39:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    39,
                    29,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-18T17:05:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    5,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual\n  LLMs: An Extensive Investigation"
                },
                "summary": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. We reduce biases by\nfinetuning on curated non-harmful text, but find only direct preference\noptimization to be effective for mitigating toxicity. The mitigation caused by\napplying these methods in English also transfers to non-English languages. We\nfind evidence that the extent to which transfer takes place can be predicted by\nthe amount of data in a given language present in the model's pretraining data.\nHowever, this transfer of bias and toxicity mitigation often comes at the\nexpense of decreased language generation ability in non-English languages,\nhighlighting the importance of developing language-specific bias and toxicity\nmitigation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. We reduce biases by\nfinetuning on curated non-harmful text, but find only direct preference\noptimization to be effective for mitigating toxicity. The mitigation caused by\napplying these methods in English also transfers to non-English languages. We\nfind evidence that the extent to which transfer takes place can be predicted by\nthe amount of data in a given language present in the model's pretraining data.\nHowever, this transfer of bias and toxicity mitigation often comes at the\nexpense of decreased language generation ability in non-English languages,\nhighlighting the importance of developing language-specific bias and toxicity\nmitigation methods."
                },
                "authors": [
                    {
                        "name": "Vera Neplenbroek"
                    },
                    {
                        "name": "Arianna Bisazza"
                    },
                    {
                        "name": "Raquel Fernández"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Fernández"
                },
                "author": "Raquel Fernández",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14050v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14050v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07787v2",
                "updated": "2025-02-14T15:34:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    34,
                    2,
                    4,
                    45,
                    0
                ],
                "published": "2025-01-20T00:12:31Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    0,
                    12,
                    31,
                    0,
                    20,
                    0
                ],
                "title": "A Simulation-Based Framework for Leveraging Shared Autonomous Vehicles\n  to Enhance Disaster Evacuations in Rural Regions with a Focus on Vulnerable\n  Populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simulation-Based Framework for Leveraging Shared Autonomous Vehicles\n  to Enhance Disaster Evacuations in Rural Regions with a Focus on Vulnerable\n  Populations"
                },
                "summary": "Rapid advancements in autonomous vehicles (AVs) are poised to revolutionize\ntransportation and communities, including disaster evacuations, particularly\nthrough the deployment of Shared Autonomous Vehicles (SAVs). Despite the\npotential, the use of SAVs in rural disaster evacuations remains an\nunderexplored area. To address this gap, this study proposes a simulation-based\nframework that integrates both mathematical programming and SUMO traffic\nsimulation to deploy SAVs in pre- and post-disaster evacuations in rural areas.\nThe framework prioritizes the needs of vulnerable groups, including individuals\nwith disabilities, limited English proficiency, and elderly residents. Sumter\nCounty, Florida, serves as the case study due to its unique characteristics: a\nhigh concentration of vulnerable individuals and limited access to public\ntransportation, making it one of the most transportation-insecure counties in\nthe state. These conditions present significant challenges for evacuation\nplanning in the region. To explore potential solutions, we conducted mass\nevacuation simulations by incorporating SAVs across seven scenarios. These\nscenarios represented varying SAV penetration levels, ranging from 20% to 100%\nof the vulnerable population, and were compared to a baseline scenario using\nonly passenger cars. Additionally, we examined both pre-disaster and\npost-disaster conditions, accounting for infrastructure failures and road\nclosures. According to the simulation results, higher SAV integration\nsignificantly improves traffic distribution and reduces congestion. Scenarios\nfeaturing more SAVs exhibited lower congestion peaks and more stable traffic\nflow. Conversely, mixed traffic environments demonstrate reduced average speeds\nattributable to interactions between SAVs and passenger cars, while exclusive\nuse of SAVs results in higher speeds and more stable travel patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advancements in autonomous vehicles (AVs) are poised to revolutionize\ntransportation and communities, including disaster evacuations, particularly\nthrough the deployment of Shared Autonomous Vehicles (SAVs). Despite the\npotential, the use of SAVs in rural disaster evacuations remains an\nunderexplored area. To address this gap, this study proposes a simulation-based\nframework that integrates both mathematical programming and SUMO traffic\nsimulation to deploy SAVs in pre- and post-disaster evacuations in rural areas.\nThe framework prioritizes the needs of vulnerable groups, including individuals\nwith disabilities, limited English proficiency, and elderly residents. Sumter\nCounty, Florida, serves as the case study due to its unique characteristics: a\nhigh concentration of vulnerable individuals and limited access to public\ntransportation, making it one of the most transportation-insecure counties in\nthe state. These conditions present significant challenges for evacuation\nplanning in the region. To explore potential solutions, we conducted mass\nevacuation simulations by incorporating SAVs across seven scenarios. These\nscenarios represented varying SAV penetration levels, ranging from 20% to 100%\nof the vulnerable population, and were compared to a baseline scenario using\nonly passenger cars. Additionally, we examined both pre-disaster and\npost-disaster conditions, accounting for infrastructure failures and road\nclosures. According to the simulation results, higher SAV integration\nsignificantly improves traffic distribution and reduces congestion. Scenarios\nfeaturing more SAVs exhibited lower congestion peaks and more stable traffic\nflow. Conversely, mixed traffic environments demonstrate reduced average speeds\nattributable to interactions between SAVs and passenger cars, while exclusive\nuse of SAVs results in higher speeds and more stable travel patterns."
                },
                "authors": [
                    {
                        "name": "Alican Sevim"
                    },
                    {
                        "name": "Qian-wen Guo"
                    },
                    {
                        "name": "Eren Erman Ozguven"
                    }
                ],
                "author_detail": {
                    "name": "Eren Erman Ozguven"
                },
                "author": "Eren Erman Ozguven",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02844v3",
                "updated": "2025-02-14T15:32:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    32,
                    0,
                    4,
                    45,
                    0
                ],
                "published": "2025-01-06T08:43:31Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    8,
                    43,
                    31,
                    0,
                    6,
                    0
                ],
                "title": "Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based Retrieval Augmented Generation for Dynamic Few-shot Text\n  Classification"
                },
                "summary": "Text classification is a fundamental task in data mining, pivotal to various\napplications such as tabular understanding and recommendation. Although neural\nnetwork-based models, such as CNN and BERT, have demonstrated remarkable\nperformance in text classification, their effectiveness heavily relies on\nabundant labeled training data. This dependency makes these models less\neffective in dynamic few-shot text classification, where labeled data is\nscarce, and new target labels frequently appear based on application needs.\nRecently, large language models (LLMs) have shown promise due to their\nextensive pretraining and contextual understanding ability. Current approaches\nprovide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to classify texts. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. Rather than treating each input\nindependently, GORAG constructs and maintains a weighted graph by extracting\nside information across all target texts. In this graph, text keywords and\nlabels are represented as nodes, with edges indicating the correlations between\nthem. To model these correlations, GORAG employs an edge weighting mechanism to\nprioritize the importance and reliability of extracted information and\ndynamically retrieves relevant context using a minimum-cost spanning tree\ntailored for each text input. Empirical evaluations demonstrate that GORAG\noutperforms existing approaches by providing more comprehensive and precise\ncontextual information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text classification is a fundamental task in data mining, pivotal to various\napplications such as tabular understanding and recommendation. Although neural\nnetwork-based models, such as CNN and BERT, have demonstrated remarkable\nperformance in text classification, their effectiveness heavily relies on\nabundant labeled training data. This dependency makes these models less\neffective in dynamic few-shot text classification, where labeled data is\nscarce, and new target labels frequently appear based on application needs.\nRecently, large language models (LLMs) have shown promise due to their\nextensive pretraining and contextual understanding ability. Current approaches\nprovide LLMs with text inputs, candidate labels, and additional side\ninformation (e.g., descriptions) to classify texts. However, their\neffectiveness is hindered by the increased input size and the noise introduced\nthrough side information processing. To address these limitations, we propose a\ngraph-based online retrieval-augmented generation framework, namely GORAG, for\ndynamic few-shot text classification. Rather than treating each input\nindependently, GORAG constructs and maintains a weighted graph by extracting\nside information across all target texts. In this graph, text keywords and\nlabels are represented as nodes, with edges indicating the correlations between\nthem. To model these correlations, GORAG employs an edge weighting mechanism to\nprioritize the importance and reliability of extracted information and\ndynamically retrieves relevant context using a minimum-cost spanning tree\ntailored for each text input. Empirical evaluations demonstrate that GORAG\noutperforms existing approaches by providing more comprehensive and precise\ncontextual information."
                },
                "authors": [
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Fei Teng"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08346v2",
                "updated": "2025-02-14T15:25:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    25,
                    51,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-12T12:13:51Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    13,
                    51,
                    2,
                    43,
                    0
                ],
                "title": "Graph Foundation Models for Recommendation: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Foundation Models for Recommendation: A Comprehensive Survey"
                },
                "summary": "Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RS) serve as a fundamental tool for navigating the vast\nexpanse of online information, with deep learning advancements playing an\nincreasingly important role in improving ranking accuracy. Among these, graph\nneural networks (GNNs) excel at extracting higher-order structural information,\nwhile large language models (LLMs) are designed to process and comprehend\nnatural language, making both approaches highly effective and widely adopted.\nRecent research has focused on graph foundation models (GFMs), which integrate\nthe strengths of GNNs and LLMs to model complex RS problems more efficiently by\nleveraging the graph-based structure of user-item relationships alongside\ntextual understanding. In this survey, we provide a comprehensive overview of\nGFM-based RS technologies by introducing a clear taxonomy of current\napproaches, diving into methodological details, and highlighting key challenges\nand future directions. By synthesizing recent advancements, we aim to offer\nvaluable insights into the evolving landscape of GFM-based recommender systems."
                },
                "authors": [
                    {
                        "name": "Bin Wu"
                    },
                    {
                        "name": "Yihang Wang"
                    },
                    {
                        "name": "Yuanhao Zeng"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jiashu Zhao"
                    },
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yawen Li"
                    },
                    {
                        "name": "Long Xia"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Chuan Shi"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Shi"
                },
                "author": "Chuan Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07902v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07902v2",
                "updated": "2025-02-14T15:23:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    23,
                    32,
                    4,
                    45,
                    0
                ],
                "published": "2024-11-12T16:21:22Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    16,
                    21,
                    22,
                    1,
                    317,
                    0
                ],
                "title": "Bayes2IMC: In-Memory Computing for Bayesian Binary Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayes2IMC: In-Memory Computing for Bayesian Binary Neural Networks"
                },
                "summary": "Bayesian Neural Networks (BNNs) provide superior estimates of uncertainty by\ngenerating an ensemble of predictive distributions. However, inference via\nensembling is resource-intensive, requiring additional entropy sources to\ngenerate stochasticity which increases resource consumption. We introduce\nBayes2IMC, an in-memory computing (IMC) architecture designed for binary\nBayesian neural networks that leverage nanoscale device stochasticity to\ngenerate desired distributions. Our novel approach utilizes Phase-Change Memory\n(PCM) to harness inherent noise characteristics, enabling the creation of a\nbinary neural network. This design eliminates the necessity for a pre-neuron\nAnalog-to-Digital Converter (ADC), significantly improving power and area\nefficiency. We also develop a hardware-software co-optimized correction method\napplied solely on the logits in the final layer to reduce device-induced\naccuracy variations across deployments on hardware. Additionally, we devise a\nsimple compensation technique that ensures no drop in classification accuracy\ndespite conductance drift of PCM. We validate the effectiveness of our approach\non the CIFAR-10 dataset with a VGGBinaryConnect model, achieving accuracy\nmetrics comparable to ideal software implementations as well as results\nreported in the literature using other technologies. Finally, we present a\ncomplete core architecture and compare its projected power, performance, and\narea efficiency against an equivalent SRAM baseline, showing a $3.8$ to $9.6\n\\times$ improvement in total efficiency (in GOPS/W/mm$^2$) and a $2.2 $ to $5.6\n\\times$ improvement in power efficiency (in GOPS/W). In addition, the projected\nhardware performance of Bayes2IMC surpasses that of most of the BNN\narchitectures based on memristive devices reported in the literature, and\nachieves up to $20\\%$ higher power efficiency compared to the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Neural Networks (BNNs) provide superior estimates of uncertainty by\ngenerating an ensemble of predictive distributions. However, inference via\nensembling is resource-intensive, requiring additional entropy sources to\ngenerate stochasticity which increases resource consumption. We introduce\nBayes2IMC, an in-memory computing (IMC) architecture designed for binary\nBayesian neural networks that leverage nanoscale device stochasticity to\ngenerate desired distributions. Our novel approach utilizes Phase-Change Memory\n(PCM) to harness inherent noise characteristics, enabling the creation of a\nbinary neural network. This design eliminates the necessity for a pre-neuron\nAnalog-to-Digital Converter (ADC), significantly improving power and area\nefficiency. We also develop a hardware-software co-optimized correction method\napplied solely on the logits in the final layer to reduce device-induced\naccuracy variations across deployments on hardware. Additionally, we devise a\nsimple compensation technique that ensures no drop in classification accuracy\ndespite conductance drift of PCM. We validate the effectiveness of our approach\non the CIFAR-10 dataset with a VGGBinaryConnect model, achieving accuracy\nmetrics comparable to ideal software implementations as well as results\nreported in the literature using other technologies. Finally, we present a\ncomplete core architecture and compare its projected power, performance, and\narea efficiency against an equivalent SRAM baseline, showing a $3.8$ to $9.6\n\\times$ improvement in total efficiency (in GOPS/W/mm$^2$) and a $2.2 $ to $5.6\n\\times$ improvement in power efficiency (in GOPS/W). In addition, the projected\nhardware performance of Bayes2IMC surpasses that of most of the BNN\narchitectures based on memristive devices reported in the literature, and\nachieves up to $20\\%$ higher power efficiency compared to the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Prabodh Katti"
                    },
                    {
                        "name": "Clement Ruah"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    },
                    {
                        "name": "Bashir M. Al-Hashimi"
                    },
                    {
                        "name": "Bipin Rajendran"
                    }
                ],
                "author_detail": {
                    "name": "Bipin Rajendran"
                },
                "author": "Bipin Rajendran",
                "arxiv_comment": "Accepted for publication in IEEE Transactions On Circuits and Systems\n  I: Regular Papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07902v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07902v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10053v2",
                "updated": "2025-02-14T15:20:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    20,
                    47,
                    4,
                    45,
                    0
                ],
                "published": "2025-01-17T09:16:13Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    16,
                    13,
                    4,
                    17,
                    0
                ],
                "title": "AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented\n  Generation using Tree-based Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirRAG: Activating Intrinsic Reasoning for Retrieval Augmented\n  Generation using Tree-based Search"
                },
                "summary": "Leveraging the autonomous decision-making capabilities of large language\nmodels (LLMs) has demonstrated superior performance in reasoning tasks.\nHowever, despite the success of iterative or recursive retrieval-augmented\ngeneration (RAG) techniques, these methods are often constrained to a single\nsolution space when confronted with complex problems. In this paper, we propose\na novel thinking pattern in RAG that integrates system analysis with efficient\nreasoning actions, significantly activating intrinsic reasoning capabilities\nand expanding the solution space of specific tasks via Monte Carlo Tree Search\n(MCTS), which we refer to as AirRAG. Specifically, our approach designs five\nfundamental reasoning actions, which are expanded to a broad tree-based\nreasoning space using MCTS. The approach also incorporates self-consistency\nverification to explore potential reasoning paths and inference scaling law.\nAdditionally, computationally optimal strategies are employed to allocate more\ninference resources to key actions, thereby enhancing overall performance.\nExperimental results demonstrate the effectiveness of AirRAG, showing\nsignificant performance gains on complex question-answering datasets.\nFurthermore, AirRAG is flexible and lightweight, making it easy to integrate\nwith other advanced technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the autonomous decision-making capabilities of large language\nmodels (LLMs) has demonstrated superior performance in reasoning tasks.\nHowever, despite the success of iterative or recursive retrieval-augmented\ngeneration (RAG) techniques, these methods are often constrained to a single\nsolution space when confronted with complex problems. In this paper, we propose\na novel thinking pattern in RAG that integrates system analysis with efficient\nreasoning actions, significantly activating intrinsic reasoning capabilities\nand expanding the solution space of specific tasks via Monte Carlo Tree Search\n(MCTS), which we refer to as AirRAG. Specifically, our approach designs five\nfundamental reasoning actions, which are expanded to a broad tree-based\nreasoning space using MCTS. The approach also incorporates self-consistency\nverification to explore potential reasoning paths and inference scaling law.\nAdditionally, computationally optimal strategies are employed to allocate more\ninference resources to key actions, thereby enhancing overall performance.\nExperimental results demonstrate the effectiveness of AirRAG, showing\nsignificant performance gains on complex question-answering datasets.\nFurthermore, AirRAG is flexible and lightweight, making it easy to integrate\nwith other advanced technologies."
                },
                "authors": [
                    {
                        "name": "Wenfeng Feng"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Jingyi Song"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "arxiv_comment": "17 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04184v2",
                "updated": "2025-02-14T15:19:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    19,
                    1,
                    4,
                    45,
                    0
                ],
                "published": "2024-06-06T15:40:29Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    15,
                    40,
                    29,
                    3,
                    158,
                    0
                ],
                "title": "Shield Synthesis for LTL Modulo Theories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shield Synthesis for LTL Modulo Theories"
                },
                "summary": "In recent years, Machine Learning (ML) models have achieved remarkable\nsuccess in various domains. However, these models also tend to demonstrate\nunsafe behaviors, precluding their deployment in safety-critical systems. To\ncope with this issue, ample research focuses on developing methods that\nguarantee the safe behaviour of a given ML model. A prominent example is\nshielding which incorporates an external component (a ``shield'') that blocks\nunwanted behavior. Despite significant progress, shielding suffers from a main\nsetback: it is currently geared towards properties encoded solely in\npropositional logics (e.g., LTL) and is unsuitable for richer logics. This, in\nturn, limits the widespread applicability of shielding in many real-world\nsystems. In this work, we address this gap, and extend shielding to LTL modulo\ntheories, by building upon recent advances in reactive synthesis modulo\ntheories. This allowed us to develop a novel approach for generating shields\nconforming to complex safety specifications in these more expressive, logics.\nWe evaluated our shields and demonstrate their ability to handle rich data with\ntemporal dynamics. To the best of our knowledge, this is the first approach for\nsynthesizing shields for such expressivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Machine Learning (ML) models have achieved remarkable\nsuccess in various domains. However, these models also tend to demonstrate\nunsafe behaviors, precluding their deployment in safety-critical systems. To\ncope with this issue, ample research focuses on developing methods that\nguarantee the safe behaviour of a given ML model. A prominent example is\nshielding which incorporates an external component (a ``shield'') that blocks\nunwanted behavior. Despite significant progress, shielding suffers from a main\nsetback: it is currently geared towards properties encoded solely in\npropositional logics (e.g., LTL) and is unsuitable for richer logics. This, in\nturn, limits the widespread applicability of shielding in many real-world\nsystems. In this work, we address this gap, and extend shielding to LTL modulo\ntheories, by building upon recent advances in reactive synthesis modulo\ntheories. This allowed us to develop a novel approach for generating shields\nconforming to complex safety specifications in these more expressive, logics.\nWe evaluated our shields and demonstrate their ability to handle rich data with\ntemporal dynamics. To the best of our knowledge, this is the first approach for\nsynthesizing shields for such expressivity."
                },
                "authors": [
                    {
                        "name": "Andoni Rodriguez"
                    },
                    {
                        "name": "Guy Amir"
                    },
                    {
                        "name": "Davide Corsi"
                    },
                    {
                        "name": "Cesar Sanchez"
                    },
                    {
                        "name": "Guy Katz"
                    }
                ],
                "author_detail": {
                    "name": "Guy Katz"
                },
                "author": "Guy Katz",
                "arxiv_comment": "To appear in AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10216v1",
                "updated": "2025-02-14T15:10:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    10,
                    43,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    10,
                    43,
                    4,
                    45,
                    0
                ],
                "title": "Forget the Data and Fine-Tuning! Just Fold the Network to Compress",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forget the Data and Fine-Tuning! Just Fold the Network to Compress"
                },
                "summary": "We introduce model folding, a novel data-free model compression technique\nthat merges structurally similar neurons across layers, significantly reducing\nthe model size without the need for fine-tuning or access to training data.\nUnlike existing methods, model folding preserves data statistics during\ncompression by leveraging k-means clustering, and using novel data-free\ntechniques to prevent variance collapse or explosion. Our theoretical framework\nand experiments across standard benchmarks, including ResNet18 and LLaMA-7B,\ndemonstrate that model folding achieves comparable performance to data-driven\ncompression techniques and outperforms recently proposed data-free methods,\nespecially at high sparsity levels. This approach is particularly effective for\ncompressing large-scale models, making it suitable for deployment in\nresource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce model folding, a novel data-free model compression technique\nthat merges structurally similar neurons across layers, significantly reducing\nthe model size without the need for fine-tuning or access to training data.\nUnlike existing methods, model folding preserves data statistics during\ncompression by leveraging k-means clustering, and using novel data-free\ntechniques to prevent variance collapse or explosion. Our theoretical framework\nand experiments across standard benchmarks, including ResNet18 and LLaMA-7B,\ndemonstrate that model folding achieves comparable performance to data-driven\ncompression techniques and outperforms recently proposed data-free methods,\nespecially at high sparsity levels. This approach is particularly effective for\ncompressing large-scale models, making it suitable for deployment in\nresource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Dong Wang"
                    },
                    {
                        "name": "Haris Šikić"
                    },
                    {
                        "name": "Lothar Thiele"
                    },
                    {
                        "name": "Olga Saukh"
                    }
                ],
                "author_detail": {
                    "name": "Olga Saukh"
                },
                "author": "Olga Saukh",
                "arxiv_comment": "This paper has been accepted by The Thirteenth International\n  Conference on Learning Representations(ICLR), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10215v1",
                "updated": "2025-02-14T15:09:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    9,
                    15,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:09:15Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    9,
                    15,
                    4,
                    45,
                    0
                ],
                "title": "Do Large Language Models Reason Causally Like Us? Even Better?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Reason Causally Like Us? Even Better?"
                },
                "summary": "Causal reasoning is a core component of intelligence. Large language models\n(LLMs) have shown impressive capabilities in generating human-like text,\nraising questions about whether their responses reflect true understanding or\nstatistical patterns. We compared causal reasoning in humans and four LLMs\nusing tasks based on collider graphs, rating the likelihood of a query variable\noccurring given evidence from other variables. We find that LLMs reason\ncausally along a spectrum from human-like to normative inference, with\nalignment shifting based on model, context, and task. Overall, GPT-4o and\nClaude showed the most normative behavior, including \"explaining away\", whereas\nGemini-Pro and GPT-3.5 did not. Although all agents deviated from the expected\nindependence of causes - Claude the least - they exhibited strong associative\nreasoning and predictive inference when assessing the likelihood of the effect\ngiven its causes. These findings underscore the need to assess AI biases as\nthey increasingly assist human decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal reasoning is a core component of intelligence. Large language models\n(LLMs) have shown impressive capabilities in generating human-like text,\nraising questions about whether their responses reflect true understanding or\nstatistical patterns. We compared causal reasoning in humans and four LLMs\nusing tasks based on collider graphs, rating the likelihood of a query variable\noccurring given evidence from other variables. We find that LLMs reason\ncausally along a spectrum from human-like to normative inference, with\nalignment shifting based on model, context, and task. Overall, GPT-4o and\nClaude showed the most normative behavior, including \"explaining away\", whereas\nGemini-Pro and GPT-3.5 did not. Although all agents deviated from the expected\nindependence of causes - Claude the least - they exhibited strong associative\nreasoning and predictive inference when assessing the likelihood of the effect\ngiven its causes. These findings underscore the need to assess AI biases as\nthey increasingly assist human decision-making."
                },
                "authors": [
                    {
                        "name": "Hanna M. Dettki"
                    },
                    {
                        "name": "Brenden M. Lake"
                    },
                    {
                        "name": "Charley M. Wu"
                    },
                    {
                        "name": "Bob Rehder"
                    }
                ],
                "author_detail": {
                    "name": "Bob Rehder"
                },
                "author": "Bob Rehder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10202v1",
                "updated": "2025-02-14T14:56:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    56,
                    19,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T14:56:19Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    56,
                    19,
                    4,
                    45,
                    0
                ],
                "title": "Can Post-Training Quantization Benefit from an Additional QLoRA\n  Integration?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Post-Training Quantization Benefit from an Additional QLoRA\n  Integration?"
                },
                "summary": "Large language models (LLMs) have transformed natural language processing but\npose significant challenges for real-world deployment. These models necessitate\nconsiderable computing resources, which can be costly and frequently\nunavailable. Model compression techniques such as quantization are often\nleveraged to alleviate resource demand, but they may have a negative impact on\nthe generation quality. In this study, we explore the integration of 4-bit\nPost-training Quantization (PTQ) with QLoRA to address these issues. We\ndemonstrate through extensive experiments that this integration outperforms\nstandard PTQ, and in some cases even 16-bit full-parameter fine-tuning on LLMs,\nvalidated across proprietary and public datasets with different quantization\nalgorithms. The results demonstrate the efficacy of PTQ-QLoRA integration,\noffering a viable solution for deploying powerful LLMs in resource-constrained\nenvironments without compromising on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed natural language processing but\npose significant challenges for real-world deployment. These models necessitate\nconsiderable computing resources, which can be costly and frequently\nunavailable. Model compression techniques such as quantization are often\nleveraged to alleviate resource demand, but they may have a negative impact on\nthe generation quality. In this study, we explore the integration of 4-bit\nPost-training Quantization (PTQ) with QLoRA to address these issues. We\ndemonstrate through extensive experiments that this integration outperforms\nstandard PTQ, and in some cases even 16-bit full-parameter fine-tuning on LLMs,\nvalidated across proprietary and public datasets with different quantization\nalgorithms. The results demonstrate the efficacy of PTQ-QLoRA integration,\noffering a viable solution for deploying powerful LLMs in resource-constrained\nenvironments without compromising on performance."
                },
                "authors": [
                    {
                        "name": "Xiliang Zhu"
                    },
                    {
                        "name": "Elena Khasanova"
                    },
                    {
                        "name": "Cheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Chen"
                },
                "author": "Cheng Chen",
                "arxiv_comment": "Accepted to NAACL 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10201v1",
                "updated": "2025-02-14T14:52:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    52,
                    41,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T14:52:41Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    52,
                    41,
                    4,
                    45,
                    0
                ],
                "title": "Prediction hubs are context-informed frequent tokens in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prediction hubs are context-informed frequent tokens in LLMs"
                },
                "summary": "Hubness, the tendency for few points to be among the nearest neighbours of a\ndisproportionate number of other points, commonly arises when applying standard\ndistance measures to high-dimensional data, often negatively impacting\ndistance-based analysis. As autoregressive large language models (LLMs) operate\non high-dimensional representations, we ask whether they are also affected by\nhubness. We first show, theoretically, that the only representation comparison\noperation performed by LLMs, namely that between context and unembedding\nvectors to determine continuation probabilities, is not characterized by the\nconcentration of distances phenomenon that typically causes the appeareance of\nnuisance hubness. We then empirically show that this comparison still leads to\na high degree of hubness, but the hubs in this case do not constitute a\ndisturbance. They are rather the result of context-modulated frequent tokens\noften appearing in the pool of likely candidates for next token prediction. On\nthe other hand, when other distance computations involving LLM representations\nare performed, we do not have the same theoretical guarantees, and, indeed, we\nsee nuisance hubs appear. In summary, our work highlights, on the one hand, how\nhubness, while omnipresent in high-dimensional spaces, is not always a negative\nproperty that needs to be mitigated, and, on the other hand, it shows that\nvarious widely-used LLMs have developed a guessing strategy that consists in\nconstantly assigning a high probability to frequent tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hubness, the tendency for few points to be among the nearest neighbours of a\ndisproportionate number of other points, commonly arises when applying standard\ndistance measures to high-dimensional data, often negatively impacting\ndistance-based analysis. As autoregressive large language models (LLMs) operate\non high-dimensional representations, we ask whether they are also affected by\nhubness. We first show, theoretically, that the only representation comparison\noperation performed by LLMs, namely that between context and unembedding\nvectors to determine continuation probabilities, is not characterized by the\nconcentration of distances phenomenon that typically causes the appeareance of\nnuisance hubness. We then empirically show that this comparison still leads to\na high degree of hubness, but the hubs in this case do not constitute a\ndisturbance. They are rather the result of context-modulated frequent tokens\noften appearing in the pool of likely candidates for next token prediction. On\nthe other hand, when other distance computations involving LLM representations\nare performed, we do not have the same theoretical guarantees, and, indeed, we\nsee nuisance hubs appear. In summary, our work highlights, on the one hand, how\nhubness, while omnipresent in high-dimensional spaces, is not always a negative\nproperty that needs to be mitigated, and, on the other hand, it shows that\nvarious widely-used LLMs have developed a guessing strategy that consists in\nconstantly assigning a high probability to frequent tokens."
                },
                "authors": [
                    {
                        "name": "Beatrix M. G. Nielsen"
                    },
                    {
                        "name": "Iuri Macocco"
                    },
                    {
                        "name": "Marco Baroni"
                    }
                ],
                "author_detail": {
                    "name": "Marco Baroni"
                },
                "author": "Marco Baroni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01237v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01237v2",
                "updated": "2025-02-14T14:47:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    47,
                    26,
                    4,
                    45,
                    0
                ],
                "published": "2025-01-02T12:55:27Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    12,
                    55,
                    27,
                    3,
                    2,
                    0
                ],
                "title": "Self-Refinement Strategies for LLM-based Product Attribute Value\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Refinement Strategies for LLM-based Product Attribute Value\n  Extraction"
                },
                "summary": "Structured product data, in the form of attribute-value pairs, is essential\nfor e-commerce platforms to support features such as faceted product search and\nattribute-based product comparison. However, vendors often provide unstructured\nproduct descriptions, making attribute value extraction necessary to ensure\ndata consistency and usability. Large language models (LLMs) have demonstrated\ntheir potential for product attribute value extraction in few-shot scenarios.\nRecent research has shown that self-refinement techniques can improve the\nperformance of LLMs on tasks such as code generation and text-to-SQL\ntranslation. For other tasks, the application of these techniques has resulted\nin increased costs due to processing additional tokens, without achieving any\nimprovement in performance. This paper investigates applying two\nself-refinement techniques (error-based prompt rewriting and self-correction)\nto the product attribute value extraction task. The self-refinement techniques\nare evaluated across zero-shot, few-shot in-context learning, and fine-tuning\nscenarios using GPT-4o. The experiments show that both self-refinement\ntechniques fail to significantly improve the extraction performance while\nsubstantially increasing processing costs. For scenarios with development data,\nfine-tuning yields the highest performance, while the ramp-up costs of\nfine-tuning are balanced out as the amount of product descriptions increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured product data, in the form of attribute-value pairs, is essential\nfor e-commerce platforms to support features such as faceted product search and\nattribute-based product comparison. However, vendors often provide unstructured\nproduct descriptions, making attribute value extraction necessary to ensure\ndata consistency and usability. Large language models (LLMs) have demonstrated\ntheir potential for product attribute value extraction in few-shot scenarios.\nRecent research has shown that self-refinement techniques can improve the\nperformance of LLMs on tasks such as code generation and text-to-SQL\ntranslation. For other tasks, the application of these techniques has resulted\nin increased costs due to processing additional tokens, without achieving any\nimprovement in performance. This paper investigates applying two\nself-refinement techniques (error-based prompt rewriting and self-correction)\nto the product attribute value extraction task. The self-refinement techniques\nare evaluated across zero-shot, few-shot in-context learning, and fine-tuning\nscenarios using GPT-4o. The experiments show that both self-refinement\ntechniques fail to significantly improve the extraction performance while\nsubstantially increasing processing costs. For scenarios with development data,\nfine-tuning yields the highest performance, while the ramp-up costs of\nfine-tuning are balanced out as the amount of product descriptions increases."
                },
                "authors": [
                    {
                        "name": "Alexander Brinkmann"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01237v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10197v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10197v1",
                "updated": "2025-02-14T14:44:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    44,
                    22,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T14:44:22Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    44,
                    22,
                    4,
                    45,
                    0
                ],
                "title": "MathConstruct: Challenging LLM Reasoning with Constructive Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathConstruct: Challenging LLM Reasoning with Constructive Proofs"
                },
                "summary": "While Large Language Models (LLMs) demonstrate impressive performance in\nmathematics, existing math benchmarks come with significant limitations. Many\nfocus on problems with fixed ground-truth answers, and are often saturated due\nto problem simplicity or the viability of guessing or memorization. Crucially,\nthey capture only a narrow subset of relevant math problems. To address this\nresearch gap, we introduce \\mc, a new benchmark of 126 challenging problems\nsourced from various math competitions, which targets constructive proofs, a\nwidely encountered problem type requiring the construction of mathematical\nobjects with specific properties. These proofs are particularly suitable for\nLLM evaluation, as solution correctness can be easily verified. Our automated\nverifiers also enable MathConstruct to generate problem variations, used to\nevaluate robustness. State-of-the-art LLMs solve only 54% of MathConstruct\nproblems, highlighting its complexity and importance for LLM evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) demonstrate impressive performance in\nmathematics, existing math benchmarks come with significant limitations. Many\nfocus on problems with fixed ground-truth answers, and are often saturated due\nto problem simplicity or the viability of guessing or memorization. Crucially,\nthey capture only a narrow subset of relevant math problems. To address this\nresearch gap, we introduce \\mc, a new benchmark of 126 challenging problems\nsourced from various math competitions, which targets constructive proofs, a\nwidely encountered problem type requiring the construction of mathematical\nobjects with specific properties. These proofs are particularly suitable for\nLLM evaluation, as solution correctness can be easily verified. Our automated\nverifiers also enable MathConstruct to generate problem variations, used to\nevaluate robustness. State-of-the-art LLMs solve only 54% of MathConstruct\nproblems, highlighting its complexity and importance for LLM evaluation."
                },
                "authors": [
                    {
                        "name": "Mislav Balunović"
                    },
                    {
                        "name": "Jasper Dekoninck"
                    },
                    {
                        "name": "Nikola Jovanović"
                    },
                    {
                        "name": "Ivo Petrov"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10197v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10197v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07640v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07640v2",
                "updated": "2025-02-14T14:40:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    40,
                    12,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-11T15:27:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    27,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem\n  Proving"
                },
                "summary": "We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. Despite using only supervised\nfine-tuning, our final prover significantly outperforms the previous best\nopen-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning.\nOn the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32),\nsurpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover\nsuccessfully solves 7 problems (Pass@512), ranking first on the leaderboard.\nFurthermore, it generates 29.7K formal proofs for Lean Workbook problems,\nnearly doubling the 15.7K produced by earlier works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Goedel-Prover, an open-source large language model (LLM) that\nachieves the state-of-the-art (SOTA) performance in automated formal proof\ngeneration for mathematical problems. The key challenge in this field is the\nscarcity of formalized math statements and proofs, which we tackle in the\nfollowing ways. We train statement formalizers to translate the natural\nlanguage math problems from Numina into formal language (Lean 4), creating a\ndataset of 1.64 million formal statements. LLMs are used to check that the\nformal statements accurately preserve the content of the original natural\nlanguage problems. We then iteratively build a large dataset of formal proofs\nby training a series of provers. Each prover succeeds in proving many\nstatements that the previous ones could not, and these new proofs are added to\nthe training set for the next prover. Despite using only supervised\nfine-tuning, our final prover significantly outperforms the previous best\nopen-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning.\nOn the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32),\nsurpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover\nsuccessfully solves 7 problems (Pass@512), ranking first on the leaderboard.\nFurthermore, it generates 29.7K formal proofs for Lean Workbook problems,\nnearly doubling the 15.7K produced by earlier works."
                },
                "authors": [
                    {
                        "name": "Yong Lin"
                    },
                    {
                        "name": "Shange Tang"
                    },
                    {
                        "name": "Bohan Lyu"
                    },
                    {
                        "name": "Jiayun Wu"
                    },
                    {
                        "name": "Hongzhou Lin"
                    },
                    {
                        "name": "Kaiyu Yang"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Mengzhou Xia"
                    },
                    {
                        "name": "Danqi Chen"
                    },
                    {
                        "name": "Sanjeev Arora"
                    },
                    {
                        "name": "Chi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chi Jin"
                },
                "author": "Chi Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07640v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07640v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03970v2",
                "updated": "2025-02-14T14:37:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    37,
                    7,
                    4,
                    45,
                    0
                ],
                "published": "2024-02-06T12:59:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    12,
                    59,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Is Deep Learning finally better than Decision Trees on Tabular Data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Deep Learning finally better than Decision Trees on Tabular Data?"
                },
                "summary": "Tabular data is a ubiquitous data modality due to its versatility and ease of\nuse in many real-world applications. The predominant heuristics for handling\nclassification tasks on tabular data rely on classical machine learning\ntechniques, as the superiority of deep learning models has not yet been\ndemonstrated. This raises the question of whether new deep learning paradigms\ncan surpass classical approaches. Recent studies on tabular data offer a unique\nperspective on the limitations of neural networks in this domain and highlight\nthe superiority of gradient boosted decision trees (GBDTs) in terms of\nscalability and robustness across various datasets. However, novel foundation\nmodels have not been thoroughly assessed regarding quality or fairly compared\nto existing methods for tabular classification. Our study categorizes ten\nstate-of-the-art neural models based on their underlying learning paradigm,\ndemonstrating specifically that meta-learned foundation models outperform GBDTs\nin small data regimes. Although dataset-specific neural networks generally\noutperform LLM-based tabular classifiers, they are surpassed by an AutoML\nlibrary which exhibits the best performance but at the cost of higher\ncomputational demands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data is a ubiquitous data modality due to its versatility and ease of\nuse in many real-world applications. The predominant heuristics for handling\nclassification tasks on tabular data rely on classical machine learning\ntechniques, as the superiority of deep learning models has not yet been\ndemonstrated. This raises the question of whether new deep learning paradigms\ncan surpass classical approaches. Recent studies on tabular data offer a unique\nperspective on the limitations of neural networks in this domain and highlight\nthe superiority of gradient boosted decision trees (GBDTs) in terms of\nscalability and robustness across various datasets. However, novel foundation\nmodels have not been thoroughly assessed regarding quality or fairly compared\nto existing methods for tabular classification. Our study categorizes ten\nstate-of-the-art neural models based on their underlying learning paradigm,\ndemonstrating specifically that meta-learned foundation models outperform GBDTs\nin small data regimes. Although dataset-specific neural networks generally\noutperform LLM-based tabular classifiers, they are surpassed by an AutoML\nlibrary which exhibits the best performance but at the cost of higher\ncomputational demands."
                },
                "authors": [
                    {
                        "name": "Guri Zabërgja"
                    },
                    {
                        "name": "Arlind Kadra"
                    },
                    {
                        "name": "Christian M. M. Frey"
                    },
                    {
                        "name": "Josif Grabocka"
                    }
                ],
                "author_detail": {
                    "name": "Josif Grabocka"
                },
                "author": "Josif Grabocka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10194v1",
                "updated": "2025-02-14T14:36:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    36,
                    47,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T14:36:47Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    36,
                    47,
                    4,
                    45,
                    0
                ],
                "title": "Translating Common Security Assertions Across Processor Designs: A\n  RISC-V Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating Common Security Assertions Across Processor Designs: A\n  RISC-V Case Study"
                },
                "summary": "RISC-V is gaining popularity for its adaptability and cost-effectiveness in\nprocessor design. With the increasing adoption of RISC-V, the importance of\nimplementing robust security verification has grown significantly. In the state\nof the art, various approaches have been developed to strengthen the security\nverification process. Among these methods, assertion-based security\nverification has proven to be a promising approach for ensuring that security\nfeatures are effectively met. To this end, some approaches manually define\nsecurity assertions for processor designs; however, these manual methods\nrequire significant time, cost, and human expertise. Consequently, recent\napproaches focus on translating pre-defined security assertions from one design\nto another. Nonetheless, these methods are not primarily centered on processor\nsecurity, particularly RISC-V. Furthermore, many of these approaches have not\nbeen validated against real-world attacks, such as hardware Trojans. In this\nwork, we introduce a methodology for translating security assertions across\nprocessors with different architectures, using RISC-V as a case study. Our\napproach reduces time and cost compared to developing security assertions\nmanually from the outset. Our methodology was applied to five critical security\nmodules with assertion translation achieving nearly 100% success across all\nmodules. These results validate the efficacy of our approach and highlight its\npotential for enhancing security verification in modern processor designs. The\neffectiveness of the translated assertions was rigorously tested against\nhardware Trojans defined by large language models (LLMs), demonstrating their\nreliability in detecting security breaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RISC-V is gaining popularity for its adaptability and cost-effectiveness in\nprocessor design. With the increasing adoption of RISC-V, the importance of\nimplementing robust security verification has grown significantly. In the state\nof the art, various approaches have been developed to strengthen the security\nverification process. Among these methods, assertion-based security\nverification has proven to be a promising approach for ensuring that security\nfeatures are effectively met. To this end, some approaches manually define\nsecurity assertions for processor designs; however, these manual methods\nrequire significant time, cost, and human expertise. Consequently, recent\napproaches focus on translating pre-defined security assertions from one design\nto another. Nonetheless, these methods are not primarily centered on processor\nsecurity, particularly RISC-V. Furthermore, many of these approaches have not\nbeen validated against real-world attacks, such as hardware Trojans. In this\nwork, we introduce a methodology for translating security assertions across\nprocessors with different architectures, using RISC-V as a case study. Our\napproach reduces time and cost compared to developing security assertions\nmanually from the outset. Our methodology was applied to five critical security\nmodules with assertion translation achieving nearly 100% success across all\nmodules. These results validate the efficacy of our approach and highlight its\npotential for enhancing security verification in modern processor designs. The\neffectiveness of the translated assertions was rigorously tested against\nhardware Trojans defined by large language models (LLMs), demonstrating their\nreliability in detecting security breaches."
                },
                "authors": [
                    {
                        "name": "Sharjeel Imtiaz"
                    },
                    {
                        "name": "Uljana Reinsalu"
                    },
                    {
                        "name": "Tara Ghasempouri"
                    }
                ],
                "author_detail": {
                    "name": "Tara Ghasempouri"
                },
                "author": "Tara Ghasempouri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18940v2",
                "updated": "2025-02-14T14:22:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    22,
                    55,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-25T16:23:32Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    16,
                    23,
                    32,
                    2,
                    360,
                    0
                ],
                "title": "Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations"
                },
                "summary": "Songwriting is often driven by multimodal inspirations, such as imagery,\nnarratives, or existing music, yet songwriters remain unsupported by current\nmusic AI systems in incorporating these multimodal inputs into their creative\nprocesses. We introduce Amuse, a songwriting assistant that transforms\nmultimodal (image, text, or audio) inputs into chord progressions that can be\nseamlessly incorporated into songwriters' creative processes. A key feature of\nAmuse is its novel method for generating coherent chords that are relevant to\nmusic keywords in the absence of datasets with paired examples of multimodal\ninputs and chords. Specifically, we propose a method that leverages multimodal\nlarge language models (LLMs) to convert multimodal inputs into noisy chord\nsuggestions and uses a unimodal chord model to filter the suggestions. A user\nstudy with songwriters shows that Amuse effectively supports transforming\nmultimodal ideas into coherent musical suggestions, enhancing users' agency and\ncreativity throughout the songwriting process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Songwriting is often driven by multimodal inspirations, such as imagery,\nnarratives, or existing music, yet songwriters remain unsupported by current\nmusic AI systems in incorporating these multimodal inputs into their creative\nprocesses. We introduce Amuse, a songwriting assistant that transforms\nmultimodal (image, text, or audio) inputs into chord progressions that can be\nseamlessly incorporated into songwriters' creative processes. A key feature of\nAmuse is its novel method for generating coherent chords that are relevant to\nmusic keywords in the absence of datasets with paired examples of multimodal\ninputs and chords. Specifically, we propose a method that leverages multimodal\nlarge language models (LLMs) to convert multimodal inputs into noisy chord\nsuggestions and uses a unimodal chord model to filter the suggestions. A user\nstudy with songwriters shows that Amuse effectively supports transforming\nmultimodal ideas into coherent musical suggestions, enhancing users' agency and\ncreativity throughout the songwriting process."
                },
                "authors": [
                    {
                        "name": "Yewon Kim"
                    },
                    {
                        "name": "Sung-Ju Lee"
                    },
                    {
                        "name": "Chris Donahue"
                    }
                ],
                "author_detail": {
                    "name": "Chris Donahue"
                },
                "author": "Chris Donahue",
                "arxiv_comment": "Published as a conference paper at CHI 2025. Project page:\n  https://yewon-kim.com/amuse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10174v1",
                "updated": "2025-02-14T14:09:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    9,
                    43,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T14:09:43Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    9,
                    43,
                    4,
                    45,
                    0
                ],
                "title": "Technical Risks of (Lethal) Autonomous Weapons Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technical Risks of (Lethal) Autonomous Weapons Systems"
                },
                "summary": "The autonomy and adaptability of (Lethal) Autonomous Weapons Systems, (L)AWS\nin short, promise unprecedented operational capabilities, but they also\nintroduce profound risks that challenge the principles of control,\naccountability, and stability in international security. This report outlines\nthe key technological risks associated with (L)AWS deployment, emphasizing\ntheir unpredictability, lack of transparency, and operational unreliability,\nwhich can lead to severe unintended consequences.\n  Key Takeaways:\n  1. Proposed advantages of (L)AWS can only be achieved through objectification\nand classification, but a range of systematic risks limit the reliability and\npredictability of classifying algorithms.\n  2. These systematic risks include the black-box nature of AI decision-making,\nsusceptibility to reward hacking, goal misgeneralization and potential for\nemergent behaviors that escape human control.\n  3. (L)AWS could act in ways that are not just unexpected but also\nuncontrollable, undermining mission objectives and potentially escalating\nconflicts.\n  4. Even rigorously tested systems may behave unpredictably and harmfully in\nreal-world conditions, jeopardizing both strategic stability and humanitarian\nprinciples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The autonomy and adaptability of (Lethal) Autonomous Weapons Systems, (L)AWS\nin short, promise unprecedented operational capabilities, but they also\nintroduce profound risks that challenge the principles of control,\naccountability, and stability in international security. This report outlines\nthe key technological risks associated with (L)AWS deployment, emphasizing\ntheir unpredictability, lack of transparency, and operational unreliability,\nwhich can lead to severe unintended consequences.\n  Key Takeaways:\n  1. Proposed advantages of (L)AWS can only be achieved through objectification\nand classification, but a range of systematic risks limit the reliability and\npredictability of classifying algorithms.\n  2. These systematic risks include the black-box nature of AI decision-making,\nsusceptibility to reward hacking, goal misgeneralization and potential for\nemergent behaviors that escape human control.\n  3. (L)AWS could act in ways that are not just unexpected but also\nuncontrollable, undermining mission objectives and potentially escalating\nconflicts.\n  4. Even rigorously tested systems may behave unpredictably and harmfully in\nreal-world conditions, jeopardizing both strategic stability and humanitarian\nprinciples."
                },
                "authors": [
                    {
                        "name": "Heramb Podar"
                    },
                    {
                        "name": "Alycia Colijn"
                    }
                ],
                "author_detail": {
                    "name": "Alycia Colijn"
                },
                "author": "Alycia Colijn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15451v2",
                "updated": "2025-02-14T14:03:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    14,
                    3,
                    43,
                    4,
                    45,
                    0
                ],
                "published": "2025-01-26T08:45:37Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    8,
                    45,
                    37,
                    6,
                    26,
                    0
                ],
                "title": "STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity\n  Extraction in Chinese Hate Speech Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity\n  Extraction in Chinese Hate Speech Detection"
                },
                "summary": "The proliferation of hate speech has caused significant harm to society. The\nintensity and directionality of hate are closely tied to the target and\nargument it is associated with. However, research on hate speech detection in\nChinese has lagged behind, and existing datasets lack span-level fine-grained\nannotations. Furthermore, the lack of research on Chinese hateful slang poses a\nsignificant challenge. In this paper, we provide a solution for fine-grained\ndetection of Chinese hate speech. First, we construct a dataset containing\nTarget-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first\nspan-level Chinese hate speech dataset. Secondly, we evaluate the span-level\nhate speech detection performance of existing models using STATE ToxiCN.\nFinally, we conduct the first study on Chinese hateful slang and evaluate the\nability of LLMs to detect such expressions. Our work contributes valuable\nresources and insights to advance span-level hate speech detection in Chinese.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of hate speech has caused significant harm to society. The\nintensity and directionality of hate are closely tied to the target and\nargument it is associated with. However, research on hate speech detection in\nChinese has lagged behind, and existing datasets lack span-level fine-grained\nannotations. Furthermore, the lack of research on Chinese hateful slang poses a\nsignificant challenge. In this paper, we provide a solution for fine-grained\ndetection of Chinese hate speech. First, we construct a dataset containing\nTarget-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first\nspan-level Chinese hate speech dataset. Secondly, we evaluate the span-level\nhate speech detection performance of existing models using STATE ToxiCN.\nFinally, we conduct the first study on Chinese hateful slang and evaluate the\nability of LLMs to detect such expressions. Our work contributes valuable\nresources and insights to advance span-level hate speech detection in Chinese."
                },
                "authors": [
                    {
                        "name": "Zewen Bai"
                    },
                    {
                        "name": "Yuanyuan Sun"
                    },
                    {
                        "name": "Shengdi Yin"
                    },
                    {
                        "name": "Junyu Lu"
                    },
                    {
                        "name": "Jingjie Zeng"
                    },
                    {
                        "name": "Haohao Zhu"
                    },
                    {
                        "name": "Liang Yang"
                    },
                    {
                        "name": "Hongfei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Hongfei Lin"
                },
                "author": "Hongfei Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09078v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09078v3",
                "updated": "2025-02-14T13:46:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    46,
                    53,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-12T09:01:18Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    9,
                    1,
                    18,
                    3,
                    347,
                    0
                ],
                "title": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forest-of-Thought: Scaling Test-Time Compute for Enhancing LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency.Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable abilities across\nvarious language tasks, but solving complex reasoning problems remains a\nsignificant challenge. While existing methods, such as Chain-of-Thought (CoT)\nand Tree-of-Thought (ToT), enhance reasoning by decomposing problems or\nstructuring prompts, they typically perform a single pass of reasoning and may\nfail to revisit flawed paths, compromising accuracy. To address this\nlimitation, we propose a novel reasoning framework called Forest-of-Thought\n(FoT), which integrates multiple reasoning trees to leverage collective\ndecision-making for solving complex logical problems. FoT employs sparse\nactivation strategies to select the most relevant reasoning paths, improving\nboth efficiency and accuracy. Additionally, we introduce a dynamic\nself-correction strategy that enables real-time error correction, along with\nconsensus-guided decision-making strategies to optimize both correctness and\ncomputational resources. Experimental results demonstrate that the FoT\nframework, combined with these strategies, significantly enhances the reasoning\ncapabilities of LLMs, enabling them to solve complex tasks with greater\nprecision and efficiency.Code will be available at\nhttps://github.com/iamhankai/Forest-of-Thought."
                },
                "authors": [
                    {
                        "name": "Zhenni Bi"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Chuanjian Liu"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "arxiv_comment": "Code will be available at\n  https://github.com/iamhankai/Forest-of-Thought",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09078v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09078v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10157v1",
                "updated": "2025-02-14T13:36:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    36,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:36:20Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    36,
                    20,
                    4,
                    45,
                    0
                ],
                "title": "SessionRec: Next Session Prediction Paradigm For Generative Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SessionRec: Next Session Prediction Paradigm For Generative Sequential\n  Recommendation"
                },
                "summary": "We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for\ngenerative sequential recommendation, addressing the fundamental misalignment\nbetween conventional next-item prediction paradigm (NIPP) and real-world\nrecommendation scenarios. Unlike NIPP's item-level autoregressive generation\nthat contradicts actual session-based user interactions, our framework\nintroduces a session-aware representation learning through hierarchical\nsequence aggregation (intra/inter-session), reducing attention computation\ncomplexity while enabling implicit modeling of massive negative interactions,\nand a session-based prediction objective that better captures users' diverse\ninterests through multi-item recommendation in next sessions. Moreover, we\nfound that incorporating a rank loss for items within the session under the\nnext session prediction paradigm can significantly improve the ranking\neffectiveness of generative sequence recommendation models. We also verified\nthat SessionRec exhibits clear power-law scaling laws similar to those observed\nin LLMs. Extensive experiments conducted on public datasets and online A/B test\nin Meituan App demonstrate the effectiveness of SessionRec. The proposed\nparadigm establishes new foundations for developing industrial-scale generative\nrecommendation systems through its model-agnostic architecture and\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SessionRec, a novel next-session prediction paradigm (NSPP) for\ngenerative sequential recommendation, addressing the fundamental misalignment\nbetween conventional next-item prediction paradigm (NIPP) and real-world\nrecommendation scenarios. Unlike NIPP's item-level autoregressive generation\nthat contradicts actual session-based user interactions, our framework\nintroduces a session-aware representation learning through hierarchical\nsequence aggregation (intra/inter-session), reducing attention computation\ncomplexity while enabling implicit modeling of massive negative interactions,\nand a session-based prediction objective that better captures users' diverse\ninterests through multi-item recommendation in next sessions. Moreover, we\nfound that incorporating a rank loss for items within the session under the\nnext session prediction paradigm can significantly improve the ranking\neffectiveness of generative sequence recommendation models. We also verified\nthat SessionRec exhibits clear power-law scaling laws similar to those observed\nin LLMs. Extensive experiments conducted on public datasets and online A/B test\nin Meituan App demonstrate the effectiveness of SessionRec. The proposed\nparadigm establishes new foundations for developing industrial-scale generative\nrecommendation systems through its model-agnostic architecture and\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Lei Huang"
                    },
                    {
                        "name": "Hao Guo"
                    },
                    {
                        "name": "Linzhi Peng"
                    },
                    {
                        "name": "Long Zhang"
                    },
                    {
                        "name": "Xiaoteng Wang"
                    },
                    {
                        "name": "Daoyuan Wang"
                    },
                    {
                        "name": "Shichao Wang"
                    },
                    {
                        "name": "Jinpeng Wang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Sheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Chen"
                },
                "author": "Sheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10151v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10151v1",
                "updated": "2025-02-14T13:25:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    25,
                    29,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:25:29Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    25,
                    29,
                    4,
                    45,
                    0
                ],
                "title": "Semantica: Decentralized Search using a LLM-Guided Semantic Tree Overlay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantica: Decentralized Search using a LLM-Guided Semantic Tree Overlay"
                },
                "summary": "Centralized search engines are key for the Internet, but lead to undesirable\nconcentration of power. Decentralized alternatives fail to offer equal document\nretrieval accuracy and speed. Nevertheless, Semantic Overlay Networks can come\nclose to the performance of centralized solutions when the semantics of\ndocuments are properly captured. This work uses embeddings from Large Language\nModels to capture semantics and fulfill the promise of Semantic Overlay\nNetworks. Our proposed algorithm, called Semantica, constructs a prefix tree\n(trie) utilizing document embeddings calculated by a language model. Users\nconnect to each other based on the embeddings of their documents, ensuring that\nsemantically similar users are directly linked. Thereby, this construction\nmakes it more likely for user searches to be answered by the users that they\nare directly connected to, or by the users they are close to in the network\nconnection graph. The implementation of our algorithm also accommodates the\nsemantic diversity of individual users by spawning \"clone\" user identifiers in\nthe tree. Our experiments use emulation with a real-world workload to show\nSemantica's ability to identify and connect to similar users quickly. Semantica\nfinds up to ten times more semantically similar users than current\nstate-of-the-art approaches. At the same time, Semantica can retrieve more than\ntwo times the number of relevant documents given the same network load. We also\nmake our code publicly available to facilitate further research in the area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Centralized search engines are key for the Internet, but lead to undesirable\nconcentration of power. Decentralized alternatives fail to offer equal document\nretrieval accuracy and speed. Nevertheless, Semantic Overlay Networks can come\nclose to the performance of centralized solutions when the semantics of\ndocuments are properly captured. This work uses embeddings from Large Language\nModels to capture semantics and fulfill the promise of Semantic Overlay\nNetworks. Our proposed algorithm, called Semantica, constructs a prefix tree\n(trie) utilizing document embeddings calculated by a language model. Users\nconnect to each other based on the embeddings of their documents, ensuring that\nsemantically similar users are directly linked. Thereby, this construction\nmakes it more likely for user searches to be answered by the users that they\nare directly connected to, or by the users they are close to in the network\nconnection graph. The implementation of our algorithm also accommodates the\nsemantic diversity of individual users by spawning \"clone\" user identifiers in\nthe tree. Our experiments use emulation with a real-world workload to show\nSemantica's ability to identify and connect to similar users quickly. Semantica\nfinds up to ten times more semantically similar users than current\nstate-of-the-art approaches. At the same time, Semantica can retrieve more than\ntwo times the number of relevant documents given the same network load. We also\nmake our code publicly available to facilitate further research in the area."
                },
                "authors": [
                    {
                        "name": "Petru Neague"
                    },
                    {
                        "name": "Quinten Stokkink"
                    },
                    {
                        "name": "Naman Goel"
                    },
                    {
                        "name": "Johan Pouwelse"
                    }
                ],
                "author_detail": {
                    "name": "Johan Pouwelse"
                },
                "author": "Johan Pouwelse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10151v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10148v1",
                "updated": "2025-02-14T13:23:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    23,
                    18,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:23:18Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    23,
                    18,
                    4,
                    45,
                    0
                ],
                "title": "Cooperative Multi-Agent Planning with Adaptive Skill Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Multi-Agent Planning with Adaptive Skill Synthesis"
                },
                "summary": "Despite much progress in training distributed artificial intelligence (AI),\nbuilding cooperative multi-agent systems with multi-agent reinforcement\nlearning (MARL) faces challenges in sample efficiency, interpretability, and\ntransferability. Unlike traditional learning-based methods that require\nextensive interaction with the environment, large language models (LLMs)\ndemonstrate remarkable capabilities in zero-shot planning and complex\nreasoning. However, existing LLM-based approaches heavily rely on text-based\nobservations and struggle with the non-Markovian nature of multi-agent\ninteractions under partial observability. We present COMPASS, a novel\nmulti-agent architecture that integrates vision-language models (VLMs) with a\ndynamic skill library and structured communication for decentralized\nclosed-loop decision-making. The skill library, bootstrapped from\ndemonstrations, evolves via planner-guided tasks to enable adaptive strategies.\nCOMPASS propagates entity information through multi-hop communication under\npartial observability. Evaluations on the improved StarCraft Multi-Agent\nChallenge (SMACv2) demonstrate COMPASS achieves up to 30\\% higher win rates\nthan state-of-the-art MARL algorithms in symmetric scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite much progress in training distributed artificial intelligence (AI),\nbuilding cooperative multi-agent systems with multi-agent reinforcement\nlearning (MARL) faces challenges in sample efficiency, interpretability, and\ntransferability. Unlike traditional learning-based methods that require\nextensive interaction with the environment, large language models (LLMs)\ndemonstrate remarkable capabilities in zero-shot planning and complex\nreasoning. However, existing LLM-based approaches heavily rely on text-based\nobservations and struggle with the non-Markovian nature of multi-agent\ninteractions under partial observability. We present COMPASS, a novel\nmulti-agent architecture that integrates vision-language models (VLMs) with a\ndynamic skill library and structured communication for decentralized\nclosed-loop decision-making. The skill library, bootstrapped from\ndemonstrations, evolves via planner-guided tasks to enable adaptive strategies.\nCOMPASS propagates entity information through multi-hop communication under\npartial observability. Evaluations on the improved StarCraft Multi-Agent\nChallenge (SMACv2) demonstrate COMPASS achieves up to 30\\% higher win rates\nthan state-of-the-art MARL algorithms in symmetric scenarios."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Wenshuai Zhao"
                    },
                    {
                        "name": "Joni Pajarinen"
                    }
                ],
                "author_detail": {
                    "name": "Joni Pajarinen"
                },
                "author": "Joni Pajarinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14391v2",
                "updated": "2025-02-14T13:15:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    15,
                    13,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-18T11:52:10Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    52,
                    10,
                    4,
                    292,
                    0
                ],
                "title": "Context-Aware or Context-Insensitive? Assessing LLMs' Performance in\n  Document-Level Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Aware or Context-Insensitive? Assessing LLMs' Performance in\n  Document-Level Translation"
                },
                "summary": "Large language models (LLMs) are increasingly strong contenders in machine\ntranslation. In this work, we focus on document-level translation, where some\nwords cannot be translated without context from outside the sentence.\nSpecifically, we investigate the ability of prominent LLMs to utilize the\ndocument context during translation through a perturbation analysis (analyzing\nmodels' robustness to perturbed and randomized document context) and an\nattribution analysis (examining the contribution of relevant context to the\ntranslation). We conduct an extensive evaluation across nine LLMs from diverse\nmodel families and training paradigms, including translation-specialized LLMs,\nalongside two encoder-decoder transformer baselines. We find that LLMs'\nimproved document-translation performance compared to encoder-decoder models is\nnot reflected in pronoun translation performance. Our analysis highlight the\nneed for context-aware finetuning of LLMs with a focus on relevant parts of the\ncontext to improve their reliability for document-level translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly strong contenders in machine\ntranslation. In this work, we focus on document-level translation, where some\nwords cannot be translated without context from outside the sentence.\nSpecifically, we investigate the ability of prominent LLMs to utilize the\ndocument context during translation through a perturbation analysis (analyzing\nmodels' robustness to perturbed and randomized document context) and an\nattribution analysis (examining the contribution of relevant context to the\ntranslation). We conduct an extensive evaluation across nine LLMs from diverse\nmodel families and training paradigms, including translation-specialized LLMs,\nalongside two encoder-decoder transformer baselines. We find that LLMs'\nimproved document-translation performance compared to encoder-decoder models is\nnot reflected in pronoun translation performance. Our analysis highlight the\nneed for context-aware finetuning of LLMs with a focus on relevant parts of the\ncontext to improve their reliability for document-level translation."
                },
                "authors": [
                    {
                        "name": "Wafaa Mohammed"
                    },
                    {
                        "name": "Vlad Niculae"
                    }
                ],
                "author_detail": {
                    "name": "Vlad Niculae"
                },
                "author": "Vlad Niculae",
                "arxiv_comment": "9 pages, 3 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10347v2",
                "updated": "2025-02-14T13:13:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    13,
                    33,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-14T10:00:49Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    0,
                    49,
                    0,
                    288,
                    0
                ],
                "title": "A Unified Approach to Routing and Cascading for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Approach to Routing and Cascading for LLMs"
                },
                "summary": "The availability of a wide range of large language models (LLMs) embedded in\nvarious agentic systems has significantly increased the potential of model\nselection strategies to improve the cost-performance tradeoff. Existing\nstrategies involve either routing, where a single model is chosen per query, or\ncascading, which sequentially runs increasingly larger models until a\nsatisfactory answer is found. However, current approaches face three key\nlimitations: they (1) lack formal proofs of optimality, (2) fail to identify\nthe conditions under which these strategies are most effective to improve the\ncost-performance tradeoff, and (3) are unable to combine both paradigms for\nfurther improvements. To address these issues, we first derive a novel optimal\nstrategy for cascading and prove the optimality of an existing routing\nstrategy. Further, we propose cascade routing, a unified framework that\nintegrates routing and cascading into a theoretically optimal strategy. Through\nour analysis, we identify good quality estimators as the critical factor for\nthe success of model selection paradigms. Finally, in our experiments, we show\nthat cascade routing consistently outperforms the individual approaches by a\nlarge margin and we analyze quality estimators to determine when routing and/or\ncascading are useful paradigms for model selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of a wide range of large language models (LLMs) embedded in\nvarious agentic systems has significantly increased the potential of model\nselection strategies to improve the cost-performance tradeoff. Existing\nstrategies involve either routing, where a single model is chosen per query, or\ncascading, which sequentially runs increasingly larger models until a\nsatisfactory answer is found. However, current approaches face three key\nlimitations: they (1) lack formal proofs of optimality, (2) fail to identify\nthe conditions under which these strategies are most effective to improve the\ncost-performance tradeoff, and (3) are unable to combine both paradigms for\nfurther improvements. To address these issues, we first derive a novel optimal\nstrategy for cascading and prove the optimality of an existing routing\nstrategy. Further, we propose cascade routing, a unified framework that\nintegrates routing and cascading into a theoretically optimal strategy. Through\nour analysis, we identify good quality estimators as the critical factor for\nthe success of model selection paradigms. Finally, in our experiments, we show\nthat cascade routing consistently outperforms the individual approaches by a\nlarge margin and we analyze quality estimators to determine when routing and/or\ncascading are useful paradigms for model selection."
                },
                "authors": [
                    {
                        "name": "Jasper Dekoninck"
                    },
                    {
                        "name": "Maximilian Baader"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10140v1",
                "updated": "2025-02-14T13:10:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    10,
                    39,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:10:39Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    10,
                    39,
                    4,
                    45,
                    0
                ],
                "title": "Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of\n  Small Multilingual Language Models for Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of\n  Small Multilingual Language Models for Low-Resource Languages"
                },
                "summary": "Low-resource languages (LRLs) face significant challenges in natural language\nprocessing (NLP) due to limited data. While current state-of-the-art large\nlanguage models (LLMs) still struggle with LRLs, smaller multilingual models\n(mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of\ntheir capacity to low training data sizes. This study systematically\ninvestigates parameter-efficient adapter-based methods for adapting mLMs to\nLRLs, evaluating three architectures: Sequential Bottleneck, Invertible\nBottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and\nstructured knowledge from ConceptNet, we show that small adaptation datasets\n(e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains\nin intrinsic (masked language modeling) and extrinsic tasks (topic\nclassification, sentiment analysis, and named entity recognition). We find that\nSequential Bottleneck adapters excel in language modeling, while Invertible\nBottleneck adapters slightly outperform other methods on downstream tasks due\nto better embedding alignment and larger parameter counts. Adapter-based\nmethods match or outperform full fine-tuning while using far fewer parameters,\nand smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3,\nGPT-4, and DeepSeek-R1-based distilled models. While adaptation improves\nperformance, pre-training data size remains the dominant factor, especially for\nlanguages with extensive pre-training coverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-resource languages (LRLs) face significant challenges in natural language\nprocessing (NLP) due to limited data. While current state-of-the-art large\nlanguage models (LLMs) still struggle with LRLs, smaller multilingual models\n(mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of\ntheir capacity to low training data sizes. This study systematically\ninvestigates parameter-efficient adapter-based methods for adapting mLMs to\nLRLs, evaluating three architectures: Sequential Bottleneck, Invertible\nBottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and\nstructured knowledge from ConceptNet, we show that small adaptation datasets\n(e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains\nin intrinsic (masked language modeling) and extrinsic tasks (topic\nclassification, sentiment analysis, and named entity recognition). We find that\nSequential Bottleneck adapters excel in language modeling, while Invertible\nBottleneck adapters slightly outperform other methods on downstream tasks due\nto better embedding alignment and larger parameter counts. Adapter-based\nmethods match or outperform full fine-tuning while using far fewer parameters,\nand smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3,\nGPT-4, and DeepSeek-R1-based distilled models. While adaptation improves\nperformance, pre-training data size remains the dominant factor, especially for\nlanguages with extensive pre-training coverage."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Ivan Vykopal"
                    },
                    {
                        "name": "Josef van Genabith"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14497v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14497v2",
                "updated": "2025-02-14T12:38:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    12,
                    38,
                    15,
                    4,
                    45,
                    0
                ],
                "published": "2025-01-24T13:53:54Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    53,
                    54,
                    4,
                    24,
                    0
                ],
                "title": "Evaluating and Improving Graph to Text Generation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating and Improving Graph to Text Generation with Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have demonstrated immense potential across\nvarious tasks. However, research for exploring and improving the capabilities\nof LLMs in interpreting graph structures remains limited. To address this gap,\nwe conduct a comprehensive evaluation of prompting current open-source LLMs on\ngraph-to-text generation tasks. Although we explored the optimal prompting\nstrategies and proposed a novel and effective diversity-difficulty-based\nfew-shot sample selection method, we found that the improvements from\ntuning-free approaches were incremental, as LLMs struggle with planning on\ncomplex graphs, particularly those with a larger number of triplets. To further\nimprove LLMs in planning with graph sequences and grounding in truth, we\nintroduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks:\nreordering and attribution. Through extensive automatic and human evaluations,\nwe demonstrate significant improvements in the quality of generated text from\nboth few-shot learning and fine-tuning perspectives using the PlanGTG dataset.\nOur study paves the way for new research directions in graph-to-text\ngeneration. PlanGTG datasets can be found in https://github.com/probe2/kg_text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated immense potential across\nvarious tasks. However, research for exploring and improving the capabilities\nof LLMs in interpreting graph structures remains limited. To address this gap,\nwe conduct a comprehensive evaluation of prompting current open-source LLMs on\ngraph-to-text generation tasks. Although we explored the optimal prompting\nstrategies and proposed a novel and effective diversity-difficulty-based\nfew-shot sample selection method, we found that the improvements from\ntuning-free approaches were incremental, as LLMs struggle with planning on\ncomplex graphs, particularly those with a larger number of triplets. To further\nimprove LLMs in planning with graph sequences and grounding in truth, we\nintroduce a new graph-to-text dataset, PlanGTG, annotated with two sub-tasks:\nreordering and attribution. Through extensive automatic and human evaluations,\nwe demonstrate significant improvements in the quality of generated text from\nboth few-shot learning and fine-tuning perspectives using the PlanGTG dataset.\nOur study paves the way for new research directions in graph-to-text\ngeneration. PlanGTG datasets can be found in https://github.com/probe2/kg_text."
                },
                "authors": [
                    {
                        "name": "Jie He"
                    },
                    {
                        "name": "Yijun Yang"
                    },
                    {
                        "name": "Wanqiu Long"
                    },
                    {
                        "name": "Deyi Xiong"
                    },
                    {
                        "name": "Victor Gutierrez-Basulto"
                    },
                    {
                        "name": "Jeff Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Z. Pan"
                },
                "author": "Jeff Z. Pan",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14497v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14497v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10110v1",
                "updated": "2025-02-14T12:16:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    12,
                    16,
                    38,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T12:16:38Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    12,
                    16,
                    38,
                    4,
                    45,
                    0
                ],
                "title": "ScamFerret: Detecting Scam Websites Autonomously with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScamFerret: Detecting Scam Websites Autonomously with Large Language\n  Models"
                },
                "summary": "With the rise of sophisticated scam websites that exploit human psychological\nvulnerabilities, distinguishing between legitimate and scam websites has become\nincreasingly challenging. This paper presents ScamFerret, an innovative agent\nsystem employing a large language model (LLM) to autonomously collect and\nanalyze data from a given URL to determine whether it is a scam. Unlike\ntraditional machine learning models that require large datasets and feature\nengineering, ScamFerret leverages LLMs' natural language understanding to\naccurately identify scam websites of various types and languages without\nrequiring additional training or fine-tuning. Our evaluation demonstrated that\nScamFerret achieves 0.972 accuracy in classifying four scam types in English\nand 0.993 accuracy in classifying online shopping websites across three\ndifferent languages, particularly when using GPT-4. Furthermore, we confirmed\nthat ScamFerret collects and analyzes external information such as web content,\nDNS records, and user reviews as necessary, providing a basis for identifying\nscam websites from multiple perspectives. These results suggest that LLMs have\nsignificant potential in enhancing cybersecurity measures against sophisticated\nscam websites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of sophisticated scam websites that exploit human psychological\nvulnerabilities, distinguishing between legitimate and scam websites has become\nincreasingly challenging. This paper presents ScamFerret, an innovative agent\nsystem employing a large language model (LLM) to autonomously collect and\nanalyze data from a given URL to determine whether it is a scam. Unlike\ntraditional machine learning models that require large datasets and feature\nengineering, ScamFerret leverages LLMs' natural language understanding to\naccurately identify scam websites of various types and languages without\nrequiring additional training or fine-tuning. Our evaluation demonstrated that\nScamFerret achieves 0.972 accuracy in classifying four scam types in English\nand 0.993 accuracy in classifying online shopping websites across three\ndifferent languages, particularly when using GPT-4. Furthermore, we confirmed\nthat ScamFerret collects and analyzes external information such as web content,\nDNS records, and user reviews as necessary, providing a basis for identifying\nscam websites from multiple perspectives. These results suggest that LLMs have\nsignificant potential in enhancing cybersecurity measures against sophisticated\nscam websites."
                },
                "authors": [
                    {
                        "name": "Hiroki Nakano"
                    },
                    {
                        "name": "Takashi Koide"
                    },
                    {
                        "name": "Daiki Chiba"
                    }
                ],
                "author_detail": {
                    "name": "Daiki Chiba"
                },
                "author": "Daiki Chiba",
                "arxiv_comment": "Accepted for publication at DIMVA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11345v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11345v3",
                "updated": "2025-02-14T11:55:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    55,
                    30,
                    4,
                    45,
                    0
                ],
                "published": "2024-05-18T17:28:35Z",
                "published_parsed": [
                    2024,
                    5,
                    18,
                    17,
                    28,
                    35,
                    5,
                    139,
                    0
                ],
                "title": "City-Scale Multi-Camera Vehicle Tracking System with Improved\n  Self-Supervised Camera Link Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "City-Scale Multi-Camera Vehicle Tracking System with Improved\n  Self-Supervised Camera Link Model"
                },
                "summary": "Multi-Target Multi-Camera Tracking (MTMCT) has broad applications and forms\nthe basis for numerous future city-wide systems (e.g. traffic management, crash\ndetection, etc.). However, the challenge of matching vehicle trajectories\nacross different cameras based solely on feature extraction poses significant\ndifficulties. This article introduces an innovative multi-camera vehicle\ntracking system that utilizes a self-supervised camera link model. In contrast\nto related works that rely on manual spatial-temporal annotations, our model\nautomatically extracts crucial multi-camera relationships for vehicle matching.\nThe camera link is established through a pre-matching process that evaluates\nfeature similarities, pair numbers, and time variance for high-quality tracks.\nThis process calculates the probability of spatial linkage for all camera\ncombinations, selecting the highest scoring pairs to create camera links. Our\napproach significantly improves deployment times by eliminating the need for\nhuman annotation, offering substantial improvements in efficiency and\ncost-effectiveness when it comes to real-world application. This pairing\nprocess supports cross camera matching by setting spatial-temporal constraints,\nreducing the searching space for potential vehicle matches. According to our\nexperimental results, the proposed method achieves a new state-of-the-art among\nautomatic camera-link based methods in CityFlow V2 benchmarks with 61.07% IDF1\nScore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Target Multi-Camera Tracking (MTMCT) has broad applications and forms\nthe basis for numerous future city-wide systems (e.g. traffic management, crash\ndetection, etc.). However, the challenge of matching vehicle trajectories\nacross different cameras based solely on feature extraction poses significant\ndifficulties. This article introduces an innovative multi-camera vehicle\ntracking system that utilizes a self-supervised camera link model. In contrast\nto related works that rely on manual spatial-temporal annotations, our model\nautomatically extracts crucial multi-camera relationships for vehicle matching.\nThe camera link is established through a pre-matching process that evaluates\nfeature similarities, pair numbers, and time variance for high-quality tracks.\nThis process calculates the probability of spatial linkage for all camera\ncombinations, selecting the highest scoring pairs to create camera links. Our\napproach significantly improves deployment times by eliminating the need for\nhuman annotation, offering substantial improvements in efficiency and\ncost-effectiveness when it comes to real-world application. This pairing\nprocess supports cross camera matching by setting spatial-temporal constraints,\nreducing the searching space for potential vehicle matches. According to our\nexperimental results, the proposed method achieves a new state-of-the-art among\nautomatic camera-link based methods in CityFlow V2 benchmarks with 61.07% IDF1\nScore."
                },
                "authors": [
                    {
                        "name": "Yuqiang Lin"
                    },
                    {
                        "name": "Sam Lockyer"
                    },
                    {
                        "name": "Nic Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Nic Zhang"
                },
                "author": "Nic Zhang",
                "arxiv_comment": "Upload the revised manuscript with the publisher's requirement",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11345v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11345v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07191v3",
                "updated": "2025-02-14T11:37:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    37,
                    0,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-11T02:31:11Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    2,
                    31,
                    11,
                    1,
                    42,
                    0
                ],
                "title": "Bag of Tricks for Inference-time Computation of LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bag of Tricks for Inference-time Computation of LLM Reasoning"
                },
                "summary": "With the advancement of large language models (LLMs), solving complex\nreasoning tasks has gained increasing attention. Inference-time computation\nmethods (e.g., Best-of-N, beam search, et al.) are particularly valuable as\nthey can enhance reasoning performance without modifying model parameters or\nrequiring additional training. However, these techniques come with\nimplementation challenges, and most existing methods remain at the\nproof-of-concept stage with limited practical adoption due to their\ncomputational complexity and varying effectiveness across different tasks. In\nthis paper, we investigate and benchmark diverse inference-time computation\nstrategies across reasoning tasks of varying complexity. Since most current\nmethods rely on a proposer-verifier pipeline that first generates candidate\nsolutions (e.g., reasoning solutions) and then selects the best one based on\nreward signals (e.g., RLHF rewards, process rewards), our research focuses on\noptimizing both candidate solution generation (e.g., instructing prompts,\nhyperparameters such as temperature and top-p) and reward mechanisms (e.g.,\nself-evaluation, reward types). Through extensive experiments (more than 20,000\nA100-80G GPU hours with over 1,000 experiments) across a variety of models\n(e.g., Llama, Qwen, and Mistral families) of various sizes, our ablation\nstudies reveal that previously overlooked strategies can significantly enhance\nperformance (e.g., tuning temperature can improve reasoning task performance by\nup to 5%). Furthermore, we establish a standardized benchmark for\ninference-time computation by systematically evaluating six representative\nmethods across eight reasoning tasks. These findings provide a stronger\nfoundation for future research. The code is available at\nhttps://github.com/usail-hkust/benchmark_inference_time_computation_LL",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of large language models (LLMs), solving complex\nreasoning tasks has gained increasing attention. Inference-time computation\nmethods (e.g., Best-of-N, beam search, et al.) are particularly valuable as\nthey can enhance reasoning performance without modifying model parameters or\nrequiring additional training. However, these techniques come with\nimplementation challenges, and most existing methods remain at the\nproof-of-concept stage with limited practical adoption due to their\ncomputational complexity and varying effectiveness across different tasks. In\nthis paper, we investigate and benchmark diverse inference-time computation\nstrategies across reasoning tasks of varying complexity. Since most current\nmethods rely on a proposer-verifier pipeline that first generates candidate\nsolutions (e.g., reasoning solutions) and then selects the best one based on\nreward signals (e.g., RLHF rewards, process rewards), our research focuses on\noptimizing both candidate solution generation (e.g., instructing prompts,\nhyperparameters such as temperature and top-p) and reward mechanisms (e.g.,\nself-evaluation, reward types). Through extensive experiments (more than 20,000\nA100-80G GPU hours with over 1,000 experiments) across a variety of models\n(e.g., Llama, Qwen, and Mistral families) of various sizes, our ablation\nstudies reveal that previously overlooked strategies can significantly enhance\nperformance (e.g., tuning temperature can improve reasoning task performance by\nup to 5%). Furthermore, we establish a standardized benchmark for\ninference-time computation by systematically evaluating six representative\nmethods across eight reasoning tasks. These findings provide a stronger\nfoundation for future research. The code is available at\nhttps://github.com/usail-hkust/benchmark_inference_time_computation_LL"
                },
                "authors": [
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Wenshuo Chao"
                    },
                    {
                        "name": "Naiqiang Tan"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10092v1",
                "updated": "2025-02-14T11:27:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    27,
                    2,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T11:27:02Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    27,
                    2,
                    4,
                    45,
                    0
                ],
                "title": "A novel approach to data generation in generative model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A novel approach to data generation in generative model"
                },
                "summary": "Variational Autoencoders (VAEs) and other generative models are widely\nemployed in artificial intelligence to synthesize new data. However, current\napproaches rely on Euclidean geometric assumptions and statistical\napproximations that fail to capture the structured and emergent nature of data\ngeneration. This paper introduces the Convergent Fusion Paradigm (CFP) theory,\na novel geometric framework that redefines data generation by integrating\ndimensional expansion accompanied by qualitative transformation. By modifying\nthe latent space geometry to interact with emergent high-dimensional\nstructures, CFP theory addresses key challenges such as identifiability issues\nand unintended artifacts like hallucinations in Large Language Models (LLMs).\nCFP theory is based on two key conceptual hypotheses that redefine how\ngenerative models structure relationships between data and algorithms. Through\nthe lens of CFP theory, we critically examine existing metric-learning\napproaches. CFP theory advances this perspective by introducing time-reversed\nmetric embeddings and structural convergence mechanisms, leading to a novel\ngeometric approach that better accounts for data generation as a structured\nepistemic process. Beyond its computational implications, CFP theory provides\nphilosophical insights into the ontological underpinnings of data generation.\nBy offering a systematic framework for high-dimensional learning dynamics, CFP\ntheory contributes to establishing a theoretical foundation for understanding\nthe data-relationship structures in AI. Finally, future research in CFP theory\nwill be led to its implications for fully realizing qualitative\ntransformations, introducing the potential of Hilbert space in generative\nmodeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Autoencoders (VAEs) and other generative models are widely\nemployed in artificial intelligence to synthesize new data. However, current\napproaches rely on Euclidean geometric assumptions and statistical\napproximations that fail to capture the structured and emergent nature of data\ngeneration. This paper introduces the Convergent Fusion Paradigm (CFP) theory,\na novel geometric framework that redefines data generation by integrating\ndimensional expansion accompanied by qualitative transformation. By modifying\nthe latent space geometry to interact with emergent high-dimensional\nstructures, CFP theory addresses key challenges such as identifiability issues\nand unintended artifacts like hallucinations in Large Language Models (LLMs).\nCFP theory is based on two key conceptual hypotheses that redefine how\ngenerative models structure relationships between data and algorithms. Through\nthe lens of CFP theory, we critically examine existing metric-learning\napproaches. CFP theory advances this perspective by introducing time-reversed\nmetric embeddings and structural convergence mechanisms, leading to a novel\ngeometric approach that better accounts for data generation as a structured\nepistemic process. Beyond its computational implications, CFP theory provides\nphilosophical insights into the ontological underpinnings of data generation.\nBy offering a systematic framework for high-dimensional learning dynamics, CFP\ntheory contributes to establishing a theoretical foundation for understanding\nthe data-relationship structures in AI. Finally, future research in CFP theory\nwill be led to its implications for fully realizing qualitative\ntransformations, introducing the potential of Hilbert space in generative\nmodeling."
                },
                "authors": [
                    {
                        "name": "JaeHong Kim"
                    },
                    {
                        "name": "Jaewon Shim"
                    }
                ],
                "author_detail": {
                    "name": "Jaewon Shim"
                },
                "arxiv_affiliation": "Center for 0D Nanofluidics, Institute of Applied Physics, Department of Physics and Astronomy, Seoul National University, Seoul 08826, Korea",
                "author": "Jaewon Shim",
                "arxiv_comment": "47 pages, 2 tables, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "00A30 (Primary), 68T99 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.3; F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07016v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07016v3",
                "updated": "2025-02-14T11:01:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    11,
                    1,
                    27,
                    4,
                    45,
                    0
                ],
                "published": "2024-06-11T07:16:34Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    7,
                    16,
                    34,
                    1,
                    163,
                    0
                ],
                "title": "Delving into LLM-assisted writing in biomedical publications through\n  excess vocabulary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delving into LLM-assisted writing in biomedical publications through\n  excess vocabulary"
                },
                "summary": "Large language models (LLMs) like ChatGPT can generate and revise text with\nhuman-level performance. These models come with clear limitations: they can\nproduce inaccurate information, reinforce existing biases, and be easily\nmisused. Yet, many scientists use them for their scholarly writing. But how\nwide-spread is such LLM usage in the academic literature? To answer this\nquestion for the field of biomedical research, we present an unbiased,\nlarge-scale approach: we study vocabulary changes in over 15 million biomedical\nabstracts from 2010--2024 indexed by PubMed, and show how the appearance of\nLLMs led to an abrupt increase in the frequency of certain style words. This\nexcess word analysis suggests that at least 13.5% of 2024 abstracts were\nprocessed with LLMs. This lower bound differed across disciplines, countries,\nand journals, reaching 40% for some subcorpora. We show that LLMs have had an\nunprecedented impact on scientific writing in biomedical research, surpassing\nthe effect of major world events such as the Covid pandemic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like ChatGPT can generate and revise text with\nhuman-level performance. These models come with clear limitations: they can\nproduce inaccurate information, reinforce existing biases, and be easily\nmisused. Yet, many scientists use them for their scholarly writing. But how\nwide-spread is such LLM usage in the academic literature? To answer this\nquestion for the field of biomedical research, we present an unbiased,\nlarge-scale approach: we study vocabulary changes in over 15 million biomedical\nabstracts from 2010--2024 indexed by PubMed, and show how the appearance of\nLLMs led to an abrupt increase in the frequency of certain style words. This\nexcess word analysis suggests that at least 13.5% of 2024 abstracts were\nprocessed with LLMs. This lower bound differed across disciplines, countries,\nand journals, reaching 40% for some subcorpora. We show that LLMs have had an\nunprecedented impact on scientific writing in biomedical research, surpassing\nthe effect of major world events such as the Covid pandemic."
                },
                "authors": [
                    {
                        "name": "Dmitry Kobak"
                    },
                    {
                        "name": "Rita González-Márquez"
                    },
                    {
                        "name": "Emőke-Ágnes Horvát"
                    },
                    {
                        "name": "Jan Lause"
                    }
                ],
                "author_detail": {
                    "name": "Jan Lause"
                },
                "author": "Jan Lause",
                "arxiv_comment": "v3: Updating the manuscript to include all PubMed abstracts until the\n  end of 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07016v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07016v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10060v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10060v1",
                "updated": "2025-02-14T10:26:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    26,
                    14,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T10:26:14Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    26,
                    14,
                    4,
                    45,
                    0
                ],
                "title": "DiSciPLE: Learning Interpretable Programs for Scientific Visual\n  Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiSciPLE: Learning Interpretable Programs for Scientific Visual\n  Discovery"
                },
                "summary": "Visual data is used in numerous different scientific workflows ranging from\nremote sensing to ecology. As the amount of observation data increases, the\nchallenge is not just to make accurate predictions but also to understand the\nunderlying mechanisms for those predictions. Good interpretation is important\nin scientific workflows, as it allows for better decision-making by providing\ninsights into the data. This paper introduces an automatic way of obtaining\nsuch interpretable-by-design models, by learning programs that interleave\nneural networks. We propose DiSciPLE (Discovering Scientific Programs using\nLLMs and Evolution) an evolutionary algorithm that leverages common sense and\nprior knowledge of large language models (LLMs) to create Python programs\nexplaining visual data. Additionally, we propose two improvements: a program\ncritic and a program simplifier to improve our method further to synthesize\ngood programs. On three different real-world problems, DiSciPLE learns\nstate-of-the-art programs on novel tasks with no prior literature. For example,\nwe can learn programs with 35% lower error than the closest non-interpretable\nbaseline for population density estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual data is used in numerous different scientific workflows ranging from\nremote sensing to ecology. As the amount of observation data increases, the\nchallenge is not just to make accurate predictions but also to understand the\nunderlying mechanisms for those predictions. Good interpretation is important\nin scientific workflows, as it allows for better decision-making by providing\ninsights into the data. This paper introduces an automatic way of obtaining\nsuch interpretable-by-design models, by learning programs that interleave\nneural networks. We propose DiSciPLE (Discovering Scientific Programs using\nLLMs and Evolution) an evolutionary algorithm that leverages common sense and\nprior knowledge of large language models (LLMs) to create Python programs\nexplaining visual data. Additionally, we propose two improvements: a program\ncritic and a program simplifier to improve our method further to synthesize\ngood programs. On three different real-world problems, DiSciPLE learns\nstate-of-the-art programs on novel tasks with no prior literature. For example,\nwe can learn programs with 35% lower error than the closest non-interpretable\nbaseline for population density estimation."
                },
                "authors": [
                    {
                        "name": "Utkarsh Mall"
                    },
                    {
                        "name": "Cheng Perng Phoo"
                    },
                    {
                        "name": "Mia Chiquier"
                    },
                    {
                        "name": "Bharath Hariharan"
                    },
                    {
                        "name": "Kavita Bala"
                    },
                    {
                        "name": "Carl Vondrick"
                    }
                ],
                "author_detail": {
                    "name": "Carl Vondrick"
                },
                "author": "Carl Vondrick",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10060v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10060v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02370v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02370v4",
                "updated": "2025-02-14T10:04:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    4,
                    55,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-04T01:40:20Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    1,
                    40,
                    20,
                    2,
                    248,
                    0
                ],
                "title": "Do Large Language Models Possess Sensitive to Sentiment?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Possess Sensitive to Sentiment?"
                },
                "summary": "Large Language Models (LLMs) have recently displayed their extraordinary\ncapabilities in language understanding. However, how to comprehensively assess\nthe sentiment capabilities of LLMs continues to be a challenge. This paper\ninvestigates the ability of LLMs to detect and react to sentiment in text\nmodal. As the integration of LLMs into diverse applications is on the rise, it\nbecomes highly critical to comprehend their sensitivity to emotional tone, as\nit can influence the user experience and the efficacy of sentiment-driven\ntasks. We conduct a series of experiments to evaluate the performance of\nseveral prominent LLMs in identifying and responding appropriately to\nsentiments like positive, negative, and neutral emotions. The models' outputs\nare analyzed across various sentiment benchmarks, and their responses are\ncompared with human evaluations. Our discoveries indicate that although LLMs\nshow a basic sensitivity to sentiment, there are substantial variations in\ntheir accuracy and consistency, emphasizing the requirement for further\nenhancements in their training processes to better capture subtle emotional\ncues. Take an example in our findings, in some cases, the models might wrongly\nclassify a strongly positive sentiment as neutral, or fail to recognize sarcasm\nor irony in the text. Such misclassifications highlight the complexity of\nsentiment analysis and the areas where the models need to be refined. Another\naspect is that different LLMs might perform differently on the same set of\ndata, depending on their architecture and training datasets. This variance\ncalls for a more in-depth study of the factors that contribute to the\nperformance differences and how they can be optimized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently displayed their extraordinary\ncapabilities in language understanding. However, how to comprehensively assess\nthe sentiment capabilities of LLMs continues to be a challenge. This paper\ninvestigates the ability of LLMs to detect and react to sentiment in text\nmodal. As the integration of LLMs into diverse applications is on the rise, it\nbecomes highly critical to comprehend their sensitivity to emotional tone, as\nit can influence the user experience and the efficacy of sentiment-driven\ntasks. We conduct a series of experiments to evaluate the performance of\nseveral prominent LLMs in identifying and responding appropriately to\nsentiments like positive, negative, and neutral emotions. The models' outputs\nare analyzed across various sentiment benchmarks, and their responses are\ncompared with human evaluations. Our discoveries indicate that although LLMs\nshow a basic sensitivity to sentiment, there are substantial variations in\ntheir accuracy and consistency, emphasizing the requirement for further\nenhancements in their training processes to better capture subtle emotional\ncues. Take an example in our findings, in some cases, the models might wrongly\nclassify a strongly positive sentiment as neutral, or fail to recognize sarcasm\nor irony in the text. Such misclassifications highlight the complexity of\nsentiment analysis and the areas where the models need to be refined. Another\naspect is that different LLMs might perform differently on the same set of\ndata, depending on their architecture and training datasets. This variance\ncalls for a more in-depth study of the factors that contribute to the\nperformance differences and how they can be optimized."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xichou Zhu"
                    },
                    {
                        "name": "Zhou Shen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Yujun Chen"
                    },
                    {
                        "name": "Benzi John"
                    },
                    {
                        "name": "Zhenzhen Ma"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Zhiyang Xu"
                    },
                    {
                        "name": "Wei Luo"
                    },
                    {
                        "name": "Junhui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Junhui Wang"
                },
                "author": "Junhui Wang",
                "arxiv_comment": "10 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02370v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02370v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02375v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02375v4",
                "updated": "2025-02-14T10:02:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    2,
                    14,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-04T01:51:37Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    1,
                    51,
                    37,
                    2,
                    248,
                    0
                ],
                "title": "How Privacy-Savvy Are Large Language Models? A Case Study on Compliance\n  and Privacy Technical Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Privacy-Savvy Are Large Language Models? A Case Study on Compliance\n  and Privacy Technical Review"
                },
                "summary": "The recent advances in large language models (LLMs) have significantly\nexpanded their applications across various fields such as language generation,\nsummarization, and complex question answering. However, their application to\nprivacy compliance and technical privacy reviews remains under-explored,\nraising critical concerns about their ability to adhere to global privacy\nstandards and protect sensitive user data. This paper seeks to address this gap\nby providing a comprehensive case study evaluating LLMs' performance in\nprivacy-related tasks such as privacy information extraction (PIE), legal and\nregulatory key point detection (KPD), and question answering (QA) with respect\nto privacy policies and data protection regulations. We introduce a Privacy\nTechnical Review (PTR) framework, highlighting its role in mitigating privacy\nrisks during the software development life-cycle. Through an empirical\nassessment, we investigate the capacity of several prominent LLMs, including\nBERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks\nand technical privacy reviews. Our experiments benchmark the models across\nmultiple dimensions, focusing on their precision, recall, and F1-scores in\nextracting privacy-sensitive information and detecting key regulatory\ncompliance points. While LLMs show promise in automating privacy reviews and\nidentifying regulatory discrepancies, significant gaps persist in their ability\nto fully comply with evolving legal standards. We provide actionable\nrecommendations for enhancing LLMs' capabilities in privacy compliance,\nemphasizing the need for robust model improvements and better integration with\nlegal and regulatory requirements. This study underscores the growing\nimportance of developing privacy-aware LLMs that can both support businesses in\ncompliance efforts and safeguard user privacy rights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in large language models (LLMs) have significantly\nexpanded their applications across various fields such as language generation,\nsummarization, and complex question answering. However, their application to\nprivacy compliance and technical privacy reviews remains under-explored,\nraising critical concerns about their ability to adhere to global privacy\nstandards and protect sensitive user data. This paper seeks to address this gap\nby providing a comprehensive case study evaluating LLMs' performance in\nprivacy-related tasks such as privacy information extraction (PIE), legal and\nregulatory key point detection (KPD), and question answering (QA) with respect\nto privacy policies and data protection regulations. We introduce a Privacy\nTechnical Review (PTR) framework, highlighting its role in mitigating privacy\nrisks during the software development life-cycle. Through an empirical\nassessment, we investigate the capacity of several prominent LLMs, including\nBERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks\nand technical privacy reviews. Our experiments benchmark the models across\nmultiple dimensions, focusing on their precision, recall, and F1-scores in\nextracting privacy-sensitive information and detecting key regulatory\ncompliance points. While LLMs show promise in automating privacy reviews and\nidentifying regulatory discrepancies, significant gaps persist in their ability\nto fully comply with evolving legal standards. We provide actionable\nrecommendations for enhancing LLMs' capabilities in privacy compliance,\nemphasizing the need for robust model improvements and better integration with\nlegal and regulatory requirements. This study underscores the growing\nimportance of developing privacy-aware LLMs that can both support businesses in\ncompliance efforts and safeguard user privacy rights."
                },
                "authors": [
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xichou Zhu"
                    },
                    {
                        "name": "Zhou Shen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Yujun Chen"
                    },
                    {
                        "name": "Benzi John"
                    },
                    {
                        "name": "Zhenzhen Ma"
                    },
                    {
                        "name": "Tao Hu"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Bolong Yang"
                    },
                    {
                        "name": "Manman Wang"
                    },
                    {
                        "name": "Zongxing Xie"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Dan Cai"
                    },
                    {
                        "name": "Junhui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Junhui Wang"
                },
                "author": "Junhui Wang",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02375v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02375v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10051v1",
                "updated": "2025-02-14T10:00:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    0,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T10:00:20Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    10,
                    0,
                    20,
                    4,
                    45,
                    0
                ],
                "title": "ORI: O Routing Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORI: O Routing Intelligence"
                },
                "summary": "Single large language models (LLMs) often fall short when faced with the\never-growing range of tasks, making a single-model approach insufficient. We\naddress this challenge by proposing ORI (O Routing Intelligence), a dynamic\nframework that leverages a set of LLMs. By intelligently routing incoming\nqueries to the most suitable model, ORI not only improves task-specific\naccuracy, but also maintains efficiency. Comprehensive evaluations across\ndiverse benchmarks demonstrate consistent accuracy gains while controlling\ncomputational overhead. By intelligently routing queries, ORI outperforms the\nstrongest individual models by up to 2.7 points on MMLU and 1.8 points on MuSR,\nties the top performance on ARC, and on BBH. These results underscore the\nbenefits of a multi-model strategy and demonstrate how ORI's adaptive\narchitecture can more effectively handle diverse tasks, offering a scalable,\nhigh-performance solution for a system of multiple large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single large language models (LLMs) often fall short when faced with the\never-growing range of tasks, making a single-model approach insufficient. We\naddress this challenge by proposing ORI (O Routing Intelligence), a dynamic\nframework that leverages a set of LLMs. By intelligently routing incoming\nqueries to the most suitable model, ORI not only improves task-specific\naccuracy, but also maintains efficiency. Comprehensive evaluations across\ndiverse benchmarks demonstrate consistent accuracy gains while controlling\ncomputational overhead. By intelligently routing queries, ORI outperforms the\nstrongest individual models by up to 2.7 points on MMLU and 1.8 points on MuSR,\nties the top performance on ARC, and on BBH. These results underscore the\nbenefits of a multi-model strategy and demonstrate how ORI's adaptive\narchitecture can more effectively handle diverse tasks, offering a scalable,\nhigh-performance solution for a system of multiple large language models."
                },
                "authors": [
                    {
                        "name": "Ahmad Shadid"
                    },
                    {
                        "name": "Rahul Kumar"
                    },
                    {
                        "name": "Mohit Mayank"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Mayank"
                },
                "author": "Mohit Mayank",
                "arxiv_comment": "13 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10050v1",
                "updated": "2025-02-14T09:57:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    57,
                    7,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T09:57:07Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    57,
                    7,
                    4,
                    45,
                    0
                ],
                "title": "A Survey on LLM-powered Agents for Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LLM-powered Agents for Recommender Systems"
                },
                "summary": "Recommender systems are essential components of many online platforms, yet\ntraditional approaches still struggle with understanding complex user\npreferences and providing explainable recommendations. The emergence of Large\nLanguage Model (LLM)-powered agents offers a promising approach by enabling\nnatural language interactions and interpretable reasoning, potentially\ntransforming research in recommender systems. This survey provides a systematic\nreview of the emerging applications of LLM-powered agents in recommender\nsystems. We identify and analyze three key paradigms in current research: (1)\nRecommender-oriented approaches, which leverage intelligent agents to enhance\nthe fundamental recommendation mechanisms; (2) Interaction-oriented approaches,\nwhich facilitate dynamic user engagement through natural dialogue and\ninterpretable suggestions; and (3) Simulation-oriented approaches, which employ\nmulti-agent frameworks to model complex user-item interactions and system\ndynamics. Beyond paradigm categorization, we analyze the architectural\nfoundations of LLM-powered recommendation agents, examining their essential\ncomponents: profile construction, memory management, strategic planning, and\naction execution. Our investigation extends to a comprehensive analysis of\nbenchmark datasets and evaluation frameworks in this domain. This systematic\nexamination not only illuminates the current state of LLM-powered agent\nrecommender systems but also charts critical challenges and promising research\ndirections in this transformative field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems are essential components of many online platforms, yet\ntraditional approaches still struggle with understanding complex user\npreferences and providing explainable recommendations. The emergence of Large\nLanguage Model (LLM)-powered agents offers a promising approach by enabling\nnatural language interactions and interpretable reasoning, potentially\ntransforming research in recommender systems. This survey provides a systematic\nreview of the emerging applications of LLM-powered agents in recommender\nsystems. We identify and analyze three key paradigms in current research: (1)\nRecommender-oriented approaches, which leverage intelligent agents to enhance\nthe fundamental recommendation mechanisms; (2) Interaction-oriented approaches,\nwhich facilitate dynamic user engagement through natural dialogue and\ninterpretable suggestions; and (3) Simulation-oriented approaches, which employ\nmulti-agent frameworks to model complex user-item interactions and system\ndynamics. Beyond paradigm categorization, we analyze the architectural\nfoundations of LLM-powered recommendation agents, examining their essential\ncomponents: profile construction, memory management, strategic planning, and\naction execution. Our investigation extends to a comprehensive analysis of\nbenchmark datasets and evaluation frameworks in this domain. This systematic\nexamination not only illuminates the current state of LLM-powered agent\nrecommender systems but also charts critical challenges and promising research\ndirections in this transformative field."
                },
                "authors": [
                    {
                        "name": "Qiyao Peng"
                    },
                    {
                        "name": "Hongtao Liu"
                    },
                    {
                        "name": "Hua Huang"
                    },
                    {
                        "name": "Qing Yang"
                    },
                    {
                        "name": "Minglai Shao"
                    }
                ],
                "author_detail": {
                    "name": "Minglai Shao"
                },
                "author": "Minglai Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01584v2",
                "updated": "2025-02-14T09:56:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    56,
                    31,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-03T03:42:56Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    3,
                    42,
                    56,
                    1,
                    247,
                    0
                ],
                "title": "Towards Cross-Lingual Explanation of Artwork in Large-scale Vision\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Cross-Lingual Explanation of Artwork in Large-scale Vision\n  Language Models"
                },
                "summary": "As the performance of Large-scale Vision Language Models (LVLMs) improves,\nthey are increasingly capable of responding in multiple languages, and there is\nan expectation that the demand for explanations generated by LVLMs will grow.\nHowever, pre-training of Vision Encoder and the integrated training of LLMs\nwith Vision Encoder are mainly conducted using English training data, leaving\nit uncertain whether LVLMs can completely handle their potential when\ngenerating explanations in languages other than English. In addition,\nmultilingual QA benchmarks that create datasets using machine translation have\ncultural differences and biases, remaining issues for use as evaluation tasks.\nTo address these challenges, this study created an extended dataset in multiple\nlanguages without relying on machine translation. This dataset that takes into\naccount nuances and country-specific phrases was then used to evaluate the\ngeneration explanation abilities of LVLMs. Furthermore, this study examined\nwhether Instruction-Tuning in resource-rich English improves performance in\nother languages. Our findings indicate that LVLMs perform worse in languages\nother than English compared to English. In addition, it was observed that LVLMs\nstruggle to effectively manage the knowledge learned from English data. Our\ndataset is available at https://huggingface.co/datasets/naist-nlp/MultiExpArt",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the performance of Large-scale Vision Language Models (LVLMs) improves,\nthey are increasingly capable of responding in multiple languages, and there is\nan expectation that the demand for explanations generated by LVLMs will grow.\nHowever, pre-training of Vision Encoder and the integrated training of LLMs\nwith Vision Encoder are mainly conducted using English training data, leaving\nit uncertain whether LVLMs can completely handle their potential when\ngenerating explanations in languages other than English. In addition,\nmultilingual QA benchmarks that create datasets using machine translation have\ncultural differences and biases, remaining issues for use as evaluation tasks.\nTo address these challenges, this study created an extended dataset in multiple\nlanguages without relying on machine translation. This dataset that takes into\naccount nuances and country-specific phrases was then used to evaluate the\ngeneration explanation abilities of LVLMs. Furthermore, this study examined\nwhether Instruction-Tuning in resource-rich English improves performance in\nother languages. Our findings indicate that LVLMs perform worse in languages\nother than English compared to English. In addition, it was observed that LVLMs\nstruggle to effectively manage the knowledge learned from English data. Our\ndataset is available at https://huggingface.co/datasets/naist-nlp/MultiExpArt"
                },
                "authors": [
                    {
                        "name": "Shintaro Ozaki"
                    },
                    {
                        "name": "Kazuki Hayashi"
                    },
                    {
                        "name": "Yusuke Sakai"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Katsuhiko Hayashi"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "NAACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04344v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04344v3",
                "updated": "2025-02-14T09:51:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    51,
                    46,
                    4,
                    45,
                    0
                ],
                "published": "2024-06-06T17:59:56Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    17,
                    59,
                    56,
                    3,
                    158,
                    0
                ],
                "title": "Verbalized Machine Learning: Revisiting Machine Learning with Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbalized Machine Learning: Revisiting Machine Learning with Language\n  Models"
                },
                "summary": "Motivated by the progress made by large language models (LLMs), we introduce\nthe framework of verbalized machine learning (VML). In contrast to conventional\nmachine learning (ML) models that are typically optimized over a continuous\nparameter space, VML constrains the parameter space to be human-interpretable\nnatural language. Such a constraint leads to a new perspective of function\napproximation, where an LLM with a text prompt can be viewed as a function\nparameterized by the text prompt. Guided by this perspective, we revisit\nclassical ML problems, such as regression and classification, and find that\nthese problems can be solved by an LLM-parameterized learner and optimizer. The\nmajor advantages of VML include (1) easy encoding of inductive bias: prior\nknowledge about the problem and hypothesis class can be encoded in natural\nlanguage and fed into the LLM-parameterized learner; (2) automatic model class\nselection: the optimizer can automatically select a model class based on data\nand verbalized prior knowledge, and it can update the model class during\ntraining; and (3) interpretable learner updates: the LLM-parameterized\noptimizer can provide explanations for why an update is performed. We\nempirically verify the effectiveness of VML, and hope that VML can serve as a\nstepping stone to stronger interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the progress made by large language models (LLMs), we introduce\nthe framework of verbalized machine learning (VML). In contrast to conventional\nmachine learning (ML) models that are typically optimized over a continuous\nparameter space, VML constrains the parameter space to be human-interpretable\nnatural language. Such a constraint leads to a new perspective of function\napproximation, where an LLM with a text prompt can be viewed as a function\nparameterized by the text prompt. Guided by this perspective, we revisit\nclassical ML problems, such as regression and classification, and find that\nthese problems can be solved by an LLM-parameterized learner and optimizer. The\nmajor advantages of VML include (1) easy encoding of inductive bias: prior\nknowledge about the problem and hypothesis class can be encoded in natural\nlanguage and fed into the LLM-parameterized learner; (2) automatic model class\nselection: the optimizer can automatically select a model class based on data\nand verbalized prior knowledge, and it can update the model class during\ntraining; and (3) interpretable learner updates: the LLM-parameterized\noptimizer can provide explanations for why an update is performed. We\nempirically verify the effectiveness of VML, and hope that VML can serve as a\nstepping stone to stronger interpretability."
                },
                "authors": [
                    {
                        "name": "Tim Z. Xiao"
                    },
                    {
                        "name": "Robert Bamler"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Weiyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weiyang Liu"
                },
                "author": "Weiyang Liu",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (116 pages, 32\n  figures, v3: refined the paper structure and added more empirical results)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04344v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04344v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10481v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10481v2",
                "updated": "2025-02-14T09:47:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    47,
                    42,
                    4,
                    45,
                    0
                ],
                "published": "2024-10-14T13:18:20Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    18,
                    20,
                    0,
                    288,
                    0
                ],
                "title": "Model-Based Privacy-Preserving Knowledge Transfer for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Based Privacy-Preserving Knowledge Transfer for Large Language\n  Models"
                },
                "summary": "As large language models (LLMs) become more prevalent, effectively utilizing\ndomain-specific knowledge while ensuring privacy has become critical. Existing\nmethods often struggle to balance utility and privacy. For instance,\nretrieval-augmented generation (RAG) enables LLMs to access domain-specific\nknowledge but compromises the privacy of sensitive data. On the other hand,\ndifferentially private data synthesis techniques offer strong privacy\nguarantees but often result in poor utility. To address this challenge, we\npropose Llamdex, a novel framework that enhances LLMs using only models trained\non domain-specific data, integrated into LLMs through carefully designed\nconnection modules. Our approach significantly enhances the accuracy of\ndomain-specific tasks, achieving up to a 26% accuracy improvement compared to\nstate-of-the-art data synthesis methods under the same differential privacy\nconstraints. Experimental results show that Llamdex not only improves the\naccuracy of LLM responses but also maintains comparable inference efficiency to\nthe original LLM, highlighting its potential for real applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become more prevalent, effectively utilizing\ndomain-specific knowledge while ensuring privacy has become critical. Existing\nmethods often struggle to balance utility and privacy. For instance,\nretrieval-augmented generation (RAG) enables LLMs to access domain-specific\nknowledge but compromises the privacy of sensitive data. On the other hand,\ndifferentially private data synthesis techniques offer strong privacy\nguarantees but often result in poor utility. To address this challenge, we\npropose Llamdex, a novel framework that enhances LLMs using only models trained\non domain-specific data, integrated into LLMs through carefully designed\nconnection modules. Our approach significantly enhances the accuracy of\ndomain-specific tasks, achieving up to a 26% accuracy improvement compared to\nstate-of-the-art data synthesis methods under the same differential privacy\nconstraints. Experimental results show that Llamdex not only improves the\naccuracy of LLM responses but also maintains comparable inference efficiency to\nthe original LLM, highlighting its potential for real applications."
                },
                "authors": [
                    {
                        "name": "Zhaomin Wu"
                    },
                    {
                        "name": "Jizhou Guo"
                    },
                    {
                        "name": "Junyi Hou"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Lixin Fan"
                    },
                    {
                        "name": "Qiang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yang"
                },
                "author": "Qiang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10481v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10481v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10046v1",
                "updated": "2025-02-14T09:46:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    46,
                    43,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T09:46:43Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    46,
                    43,
                    4,
                    45,
                    0
                ],
                "title": "ViRAC: A Vision-Reasoning Agent Head Movement Control Framework in\n  Arbitrary Virtual Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViRAC: A Vision-Reasoning Agent Head Movement Control Framework in\n  Arbitrary Virtual Environments"
                },
                "summary": "Creating lifelike virtual agents capable of interacting with their\nenvironments is a longstanding goal in computer graphics. This paper addresses\nthe challenge of generating natural head rotations, a critical aspect of\nbelievable agent behavior for visual information gathering and dynamic\nresponses to environmental cues. Although earlier methods have made significant\nstrides, many rely on data-driven or saliency-based approaches, which often\nunderperform in diverse settings and fail to capture deeper cognitive factors\nsuch as risk assessment, information seeking, and contextual prioritization.\nConsequently, generated behaviors can appear rigid or overlook critical scene\nelements, thereby diminishing the sense of realism. In this paper, we propose\n\\textbf{ViRAC}, a \\textbf{Vi}sion-\\textbf{R}easoning \\textbf{A}gent Head\nMovement \\textbf{C}ontrol framework, which exploits the common-sense knowledge\nand reasoning capabilities of large-scale models, including Vision-Language\nModels (VLMs) and Large-Language Models (LLMs). Rather than explicitly modeling\nevery cognitive mechanism, ViRAC leverages the biases and patterns internalized\nby these models from extensive training, thus emulating human-like perceptual\nprocesses without hand-tuned heuristics. Experimental results in multiple\nscenarios reveal that ViRAC produces more natural and context-aware head\nrotations than recent state-of-the-art techniques. Quantitative evaluations\nshow a closer alignment with real human head-movement data, while user studies\nconfirm improved realism and cognitive plausibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating lifelike virtual agents capable of interacting with their\nenvironments is a longstanding goal in computer graphics. This paper addresses\nthe challenge of generating natural head rotations, a critical aspect of\nbelievable agent behavior for visual information gathering and dynamic\nresponses to environmental cues. Although earlier methods have made significant\nstrides, many rely on data-driven or saliency-based approaches, which often\nunderperform in diverse settings and fail to capture deeper cognitive factors\nsuch as risk assessment, information seeking, and contextual prioritization.\nConsequently, generated behaviors can appear rigid or overlook critical scene\nelements, thereby diminishing the sense of realism. In this paper, we propose\n\\textbf{ViRAC}, a \\textbf{Vi}sion-\\textbf{R}easoning \\textbf{A}gent Head\nMovement \\textbf{C}ontrol framework, which exploits the common-sense knowledge\nand reasoning capabilities of large-scale models, including Vision-Language\nModels (VLMs) and Large-Language Models (LLMs). Rather than explicitly modeling\nevery cognitive mechanism, ViRAC leverages the biases and patterns internalized\nby these models from extensive training, thus emulating human-like perceptual\nprocesses without hand-tuned heuristics. Experimental results in multiple\nscenarios reveal that ViRAC produces more natural and context-aware head\nrotations than recent state-of-the-art techniques. Quantitative evaluations\nshow a closer alignment with real human head-movement data, while user studies\nconfirm improved realism and cognitive plausibility."
                },
                "authors": [
                    {
                        "name": "Juyeong Hwang"
                    },
                    {
                        "name": "Seong-Eun Hong"
                    },
                    {
                        "name": "Hyeongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongyeop Kang"
                },
                "author": "Hyeongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10038v1",
                "updated": "2025-02-14T09:34:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    34,
                    24,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T09:34:24Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    34,
                    24,
                    4,
                    45,
                    0
                ],
                "title": "POI-Enhancer: An LLM-based Semantic Enhancement Framework for POI\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POI-Enhancer: An LLM-based Semantic Enhancement Framework for POI\n  Representation Learning"
                },
                "summary": "POI representation learning plays a crucial role in handling tasks related to\nuser mobility data. Recent studies have shown that enriching POI\nrepresentations with multimodal information can significantly enhance their\ntask performance. Previously, the textual information incorporated into POI\nrepresentations typically involved only POI categories or check-in content,\nleading to relatively weak textual features in existing methods. In contrast,\nlarge language models (LLMs) trained on extensive text data have been found to\npossess rich textual knowledge. However leveraging such knowledge to enhance\nPOI representation learning presents two key challenges: first, how to extract\nPOI-related knowledge from LLMs effectively, and second, how to integrate the\nextracted information to enhance POI representations. To address these\nchallenges, we propose POI-Enhancer, a portable framework that leverages LLMs\nto improve POI representations produced by classic POI learning models. We\nfirst design three specialized prompts to extract semantic information from\nLLMs efficiently. Then, the Dual Feature Alignment module enhances the quality\nof the extracted information, while the Semantic Feature Fusion module\npreserves its integrity. The Cross Attention Fusion module then fully\nadaptively integrates such high-quality information into POI representations\nand Multi-View Contrastive Learning further injects human-understandable\nsemantic information into these representations. Extensive experiments on three\nreal-world datasets demonstrate the effectiveness of our framework, showing\nsignificant improvements across all baseline representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POI representation learning plays a crucial role in handling tasks related to\nuser mobility data. Recent studies have shown that enriching POI\nrepresentations with multimodal information can significantly enhance their\ntask performance. Previously, the textual information incorporated into POI\nrepresentations typically involved only POI categories or check-in content,\nleading to relatively weak textual features in existing methods. In contrast,\nlarge language models (LLMs) trained on extensive text data have been found to\npossess rich textual knowledge. However leveraging such knowledge to enhance\nPOI representation learning presents two key challenges: first, how to extract\nPOI-related knowledge from LLMs effectively, and second, how to integrate the\nextracted information to enhance POI representations. To address these\nchallenges, we propose POI-Enhancer, a portable framework that leverages LLMs\nto improve POI representations produced by classic POI learning models. We\nfirst design three specialized prompts to extract semantic information from\nLLMs efficiently. Then, the Dual Feature Alignment module enhances the quality\nof the extracted information, while the Semantic Feature Fusion module\npreserves its integrity. The Cross Attention Fusion module then fully\nadaptively integrates such high-quality information into POI representations\nand Multi-View Contrastive Learning further injects human-understandable\nsemantic information into these representations. Extensive experiments on three\nreal-world datasets demonstrate the effectiveness of our framework, showing\nsignificant improvements across all baseline representations."
                },
                "authors": [
                    {
                        "name": "Jiawei Cheng"
                    },
                    {
                        "name": "Jingyuan Wang"
                    },
                    {
                        "name": "Yichuan Zhang"
                    },
                    {
                        "name": "Jiahao Ji"
                    },
                    {
                        "name": "Yuanshao Zhu"
                    },
                    {
                        "name": "Zhibo Zhang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16345v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16345v2",
                "updated": "2025-02-14T09:32:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    32,
                    11,
                    4,
                    45,
                    0
                ],
                "published": "2024-11-25T12:44:02Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    44,
                    2,
                    0,
                    330,
                    0
                ],
                "title": "Preference Optimization for Reasoning with Pseudo Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Optimization for Reasoning with Pseudo Feedback"
                },
                "summary": "Preference optimization techniques, such as Direct Preference Optimization\n(DPO), are frequently employed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in domains like mathematical reasoning and coding,\ntypically following supervised fine-tuning. These methods rely on high-quality\nlabels for reasoning tasks to generate preference pairs; however, the\navailability of reasoning datasets with human-verified labels is limited. In\nthis study, we introduce a novel approach to generate pseudo feedback for\nreasoning tasks by framing the labeling of solutions to reason problems as an\nevaluation against associated test cases. We explore two forms of pseudo\nfeedback based on test cases: one generated by frontier LLMs and the other by\nextending self-consistency to multi-test-case. We conduct experiments on both\nmathematical reasoning and coding tasks using pseudo feedback for preference\noptimization, and observe improvements across both tasks. Specifically, using\nMathstral-7B as our base model, we improve MATH results from 58.3 to 68.6,\nsurpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and\nCollege Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3,\nrespectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on\nLiveCodeBench (from 21.1), surpassing Claude-3-Haiku.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference optimization techniques, such as Direct Preference Optimization\n(DPO), are frequently employed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in domains like mathematical reasoning and coding,\ntypically following supervised fine-tuning. These methods rely on high-quality\nlabels for reasoning tasks to generate preference pairs; however, the\navailability of reasoning datasets with human-verified labels is limited. In\nthis study, we introduce a novel approach to generate pseudo feedback for\nreasoning tasks by framing the labeling of solutions to reason problems as an\nevaluation against associated test cases. We explore two forms of pseudo\nfeedback based on test cases: one generated by frontier LLMs and the other by\nextending self-consistency to multi-test-case. We conduct experiments on both\nmathematical reasoning and coding tasks using pseudo feedback for preference\noptimization, and observe improvements across both tasks. Specifically, using\nMathstral-7B as our base model, we improve MATH results from 58.3 to 68.6,\nsurpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and\nCollege Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3,\nrespectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on\nLiveCodeBench (from 21.1), surpassing Claude-3-Haiku."
                },
                "authors": [
                    {
                        "name": "Fangkai Jiao"
                    },
                    {
                        "name": "Geyang Guo"
                    },
                    {
                        "name": "Xingxing Zhang"
                    },
                    {
                        "name": "Nancy F. Chen"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "28 pages, 11 figures. ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16345v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16345v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10036v1",
                "updated": "2025-02-14T09:26:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    26,
                    59,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T09:26:59Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    26,
                    59,
                    4,
                    45,
                    0
                ],
                "title": "Automation Bias in the AI Act: On the Legal Implications of Attempting\n  to De-Bias Human Oversight of AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automation Bias in the AI Act: On the Legal Implications of Attempting\n  to De-Bias Human Oversight of AI"
                },
                "summary": "This paper examines the legal implications of the explicit mentioning of\nautomation bias (AB) in the Artificial Intelligence Act (AIA). The AIA mandates\nhuman oversight for high-risk AI systems and requires providers to enable\nawareness of AB, i.e., the tendency to over-rely on AI outputs. The paper\nanalyses how this extra-juridical concept is embedded in the AIA, the division\nof responsibility between AI providers and deployers, and the challenges of\nlegally enforcing this novel awareness requirement. The analysis shows that the\nAIA's focus on providers does not adequately address design and context as\ncauses of AB, and questions whether the AIA should directly regulate the risk\nof AB rather than just mandating awareness. As the AIA's approach requires a\nbalance between legal mandates and behavioural science, the paper proposes that\nharmonised standards should reference the state of research on AB and human-AI\ninteraction. Ultimately, further empirical research will be essential for\neffective safeguards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines the legal implications of the explicit mentioning of\nautomation bias (AB) in the Artificial Intelligence Act (AIA). The AIA mandates\nhuman oversight for high-risk AI systems and requires providers to enable\nawareness of AB, i.e., the tendency to over-rely on AI outputs. The paper\nanalyses how this extra-juridical concept is embedded in the AIA, the division\nof responsibility between AI providers and deployers, and the challenges of\nlegally enforcing this novel awareness requirement. The analysis shows that the\nAIA's focus on providers does not adequately address design and context as\ncauses of AB, and questions whether the AIA should directly regulate the risk\nof AB rather than just mandating awareness. As the AIA's approach requires a\nbalance between legal mandates and behavioural science, the paper proposes that\nharmonised standards should reference the state of research on AB and human-AI\ninteraction. Ultimately, further empirical research will be essential for\neffective safeguards."
                },
                "authors": [
                    {
                        "name": "Johann Laux"
                    },
                    {
                        "name": "Hannah Ruschemeier"
                    }
                ],
                "author_detail": {
                    "name": "Hannah Ruschemeier"
                },
                "author": "Hannah Ruschemeier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16265v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16265v3",
                "updated": "2025-02-14T09:12:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    9,
                    12,
                    12,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-20T10:06:11Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    10,
                    6,
                    11,
                    4,
                    355,
                    0
                ],
                "title": "Autoware.Flex: Human-Instructed Dynamically Reconfigurable Autonomous\n  Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoware.Flex: Human-Instructed Dynamically Reconfigurable Autonomous\n  Driving Systems"
                },
                "summary": "Existing Autonomous Driving Systems (ADS) independently make driving\ndecisions, but they face two significant limitations. First, in complex\nscenarios, ADS may misinterpret the environment and make inappropriate driving\ndecisions. Second, these systems are unable to incorporate human driving\npreferences in their decision-making processes. This paper proposes\nAutoware$.$Flex, a novel ADS system that incorporates human input into the\ndriving process, allowing users to guide the ADS in making more appropriate\ndecisions and ensuring their preferences are satisfied. Achieving this needs to\naddress two key challenges: (1) translating human instructions, expressed in\nnatural language, into a format the ADS can understand, and (2) ensuring these\ninstructions are executed safely and consistently within the ADS' s\ndecision-making framework. For the first challenge, we employ a Large Language\nModel (LLM) assisted by an ADS-specialized knowledge base to enhance\ndomain-specific translation. For the second challenge, we design a validation\nmechanism to ensure that human instructions result in safe and consistent\ndriving behavior. Experiments conducted on both simulators and a real-world\nautonomous vehicle demonstrate that Autoware$.$Flex effectively interprets\nhuman instructions and executes them safely.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing Autonomous Driving Systems (ADS) independently make driving\ndecisions, but they face two significant limitations. First, in complex\nscenarios, ADS may misinterpret the environment and make inappropriate driving\ndecisions. Second, these systems are unable to incorporate human driving\npreferences in their decision-making processes. This paper proposes\nAutoware$.$Flex, a novel ADS system that incorporates human input into the\ndriving process, allowing users to guide the ADS in making more appropriate\ndecisions and ensuring their preferences are satisfied. Achieving this needs to\naddress two key challenges: (1) translating human instructions, expressed in\nnatural language, into a format the ADS can understand, and (2) ensuring these\ninstructions are executed safely and consistently within the ADS' s\ndecision-making framework. For the first challenge, we employ a Large Language\nModel (LLM) assisted by an ADS-specialized knowledge base to enhance\ndomain-specific translation. For the second challenge, we design a validation\nmechanism to ensure that human instructions result in safe and consistent\ndriving behavior. Experiments conducted on both simulators and a real-world\nautonomous vehicle demonstrate that Autoware$.$Flex effectively interprets\nhuman instructions and executes them safely."
                },
                "authors": [
                    {
                        "name": "Ziwei Song"
                    },
                    {
                        "name": "Mingsong Lv"
                    },
                    {
                        "name": "Tianchi Ren"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Jen-Ming Wu"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_comment": "14 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16265v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16265v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.00958v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.00958v3",
                "updated": "2025-02-14T08:51:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    51,
                    47,
                    4,
                    45,
                    0
                ],
                "published": "2023-02-02T08:53:32Z",
                "published_parsed": [
                    2023,
                    2,
                    2,
                    8,
                    53,
                    32,
                    3,
                    33,
                    0
                ],
                "title": "A Typed Lambda-Calculus for Establishing Trust in Probabilistic Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Typed Lambda-Calculus for Establishing Trust in Probabilistic Programs"
                },
                "summary": "The extensive deployment of probabilistic algorithms has radically changed\nour perspective on several well-established computational notions. Correctness\nis probably the most basic one. While a typical probabilistic program cannot be\nsaid to compute the correct result, we often have quite strong expectations\nabout the frequency with which it should return certain outputs. In these\ncases, trust as a generalisation of correctness fares better. One way to\nunderstand it is to say that a probabilistic computational process is\ntrustworthy if the frequency of its outputs is compliant with a probability\ndistribution which models its expected behaviour. We present a formal\ncomputational framework that formalises this idea. In order to do so, we define\na typed lambda-calculus that features operators for conducting experiments at\nruntime on probabilistic programs and for evaluating whether they compute\noutputs as determined by a target probability distribution. After proving some\nfundamental computational properties of the calculus, such as progress and\ntermination, we define a static notion of confidence that allows to prove that\nour notion of trust behaves correctly with respect to the basic tenets of\nprobability theory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extensive deployment of probabilistic algorithms has radically changed\nour perspective on several well-established computational notions. Correctness\nis probably the most basic one. While a typical probabilistic program cannot be\nsaid to compute the correct result, we often have quite strong expectations\nabout the frequency with which it should return certain outputs. In these\ncases, trust as a generalisation of correctness fares better. One way to\nunderstand it is to say that a probabilistic computational process is\ntrustworthy if the frequency of its outputs is compliant with a probability\ndistribution which models its expected behaviour. We present a formal\ncomputational framework that formalises this idea. In order to do so, we define\na typed lambda-calculus that features operators for conducting experiments at\nruntime on probabilistic programs and for evaluating whether they compute\noutputs as determined by a target probability distribution. After proving some\nfundamental computational properties of the calculus, such as progress and\ntermination, we define a static notion of confidence that allows to prove that\nour notion of trust behaves correctly with respect to the basic tenets of\nprobability theory."
                },
                "authors": [
                    {
                        "name": "Francesco A. Genco"
                    },
                    {
                        "name": "Giuseppe Primiero"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Primiero"
                },
                "author": "Giuseppe Primiero",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.00958v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.00958v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10013v1",
                "updated": "2025-02-14T08:47:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    47,
                    10,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T08:47:10Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    47,
                    10,
                    4,
                    45,
                    0
                ],
                "title": "Probabilistic Lexical Manifold Construction in Large Language Models via\n  Hierarchical Vector Field Interpolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Lexical Manifold Construction in Large Language Models via\n  Hierarchical Vector Field Interpolation"
                },
                "summary": "Hierarchical vector field interpolation introduces a structured probabilistic\nframework for lexical representation, ensuring that word embeddings transition\nsmoothly across a continuous manifold rather than being constrained to discrete\ntoken mappings. The proposed methodology constructs a probabilistic function\nspace where word representations adhere to topological consistency, mitigating\nrepresentational discontinuities commonly observed in transformer-based\nembeddings. Empirical evaluations reveal that probabilistic constraints enhance\nlexical coherence by refining contextual relationships, leading to improvements\nin semantic stability across multiple linguistic distributions. The application\nof divergence minimization techniques ensures that interpolated embeddings\nmaintain probabilistic consistency while preserving computational feasibility\nfor large-scale implementations. Experimental findings demonstrate that\ninterpolated lexical manifolds improve representation density alignment,\nreducing anisotropic distortions in contextual embedding distributions.\nComparative analyses with standard transformer-based models highlight that\nstructured interpolation yields more stable representations, particularly in\ntasks requiring fine-grained semantic differentiation. The statistical\nevaluation of embedding divergence confirms that probabilistic lexical\nmanifolds reduce representational inconsistencies while maintaining coherence\nacross varying scales of contextual abstraction. An assessment of computational\nefficiency reveals that while interpolation introduces minor processing\noverhead, the structured representation learning approach remains scalable for\npractical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical vector field interpolation introduces a structured probabilistic\nframework for lexical representation, ensuring that word embeddings transition\nsmoothly across a continuous manifold rather than being constrained to discrete\ntoken mappings. The proposed methodology constructs a probabilistic function\nspace where word representations adhere to topological consistency, mitigating\nrepresentational discontinuities commonly observed in transformer-based\nembeddings. Empirical evaluations reveal that probabilistic constraints enhance\nlexical coherence by refining contextual relationships, leading to improvements\nin semantic stability across multiple linguistic distributions. The application\nof divergence minimization techniques ensures that interpolated embeddings\nmaintain probabilistic consistency while preserving computational feasibility\nfor large-scale implementations. Experimental findings demonstrate that\ninterpolated lexical manifolds improve representation density alignment,\nreducing anisotropic distortions in contextual embedding distributions.\nComparative analyses with standard transformer-based models highlight that\nstructured interpolation yields more stable representations, particularly in\ntasks requiring fine-grained semantic differentiation. The statistical\nevaluation of embedding divergence confirms that probabilistic lexical\nmanifolds reduce representational inconsistencies while maintaining coherence\nacross varying scales of contextual abstraction. An assessment of computational\nefficiency reveals that while interpolation introduces minor processing\noverhead, the structured representation learning approach remains scalable for\npractical deployment."
                },
                "authors": [
                    {
                        "name": "Clive Pendleton"
                    },
                    {
                        "name": "Ewan Harrington"
                    },
                    {
                        "name": "Giles Fairbrother"
                    },
                    {
                        "name": "Jasper Arkwright"
                    },
                    {
                        "name": "Nigel Fenwick"
                    },
                    {
                        "name": "Richard Katrix"
                    }
                ],
                "author_detail": {
                    "name": "Richard Katrix"
                },
                "author": "Richard Katrix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.16466v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.16466v4",
                "updated": "2025-02-14T08:44:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    44,
                    16,
                    4,
                    45,
                    0
                ],
                "published": "2023-11-28T04:07:34Z",
                "published_parsed": [
                    2023,
                    11,
                    28,
                    4,
                    7,
                    34,
                    1,
                    332,
                    0
                ],
                "title": "The Adoption and Efficacy of Large Language Models: Evidence From\n  Consumer Complaints in the Financial Industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Adoption and Efficacy of Large Language Models: Evidence From\n  Consumer Complaints in the Financial Industry"
                },
                "summary": "Large Language Models (LLMs) are reshaping consumer decision-making,\nparticularly in communication with firms, yet our understanding of their impact\nremains limited. This research explores the effect of LLMs on consumer\ncomplaints submitted to the Consumer Financial Protection Bureau from 2015 to\n2024, documenting the adoption of LLMs for drafting complaints and evaluating\nthe likelihood of obtaining relief from financial firms. We analyzed over 1\nmillion complaints and identified a significant increase in LLM usage following\nthe release of ChatGPT. We find that LLM usage is associated with an increased\nlikelihood of obtaining relief from financial firms. To investigate this\nrelationship, we employ an instrumental variable approach to mitigate\nendogeneity concerns around LLM adoption. Although instrumental variables\nsuggest a potential causal link, they cannot fully capture all unobserved\nheterogeneity. To further establish this causal relationship, we conducted\ncontrolled experiments, which support that LLMs can enhance the clarity and\npersuasiveness of consumer narratives, thereby increasing the likelihood of\nobtaining relief. Our findings suggest that facilitating access to LLMs can\nhelp firms better understand consumer concerns and level the playing field\namong consumers. This underscores the importance of policies promoting\ntechnological accessibility, enabling all consumers to effectively voice their\nconcerns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are reshaping consumer decision-making,\nparticularly in communication with firms, yet our understanding of their impact\nremains limited. This research explores the effect of LLMs on consumer\ncomplaints submitted to the Consumer Financial Protection Bureau from 2015 to\n2024, documenting the adoption of LLMs for drafting complaints and evaluating\nthe likelihood of obtaining relief from financial firms. We analyzed over 1\nmillion complaints and identified a significant increase in LLM usage following\nthe release of ChatGPT. We find that LLM usage is associated with an increased\nlikelihood of obtaining relief from financial firms. To investigate this\nrelationship, we employ an instrumental variable approach to mitigate\nendogeneity concerns around LLM adoption. Although instrumental variables\nsuggest a potential causal link, they cannot fully capture all unobserved\nheterogeneity. To further establish this causal relationship, we conducted\ncontrolled experiments, which support that LLMs can enhance the clarity and\npersuasiveness of consumer narratives, thereby increasing the likelihood of\nobtaining relief. Our findings suggest that facilitating access to LLMs can\nhelp firms better understand consumer concerns and level the playing field\namong consumers. This underscores the importance of policies promoting\ntechnological accessibility, enabling all consumers to effectively voice their\nconcerns."
                },
                "authors": [
                    {
                        "name": "Minkyu Shin"
                    },
                    {
                        "name": "Jin Kim"
                    },
                    {
                        "name": "Jiwoong Shin"
                    }
                ],
                "author_detail": {
                    "name": "Jiwoong Shin"
                },
                "author": "Jiwoong Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.16466v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.16466v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12475v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12475v2",
                "updated": "2025-02-14T08:40:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    40,
                    39,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-17T02:22:24Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    22,
                    24,
                    1,
                    352,
                    0
                ],
                "title": "RareAgents: Advancing Rare Disease Care through LLM-Empowered\n  Multi-disciplinary Team",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RareAgents: Advancing Rare Disease Care through LLM-Empowered\n  Multi-disciplinary Team"
                },
                "summary": "Rare diseases, despite their low individual incidence, collectively impact\naround 300 million people worldwide due to the vast number of diseases. The\ninvolvement of multiple organs and systems, and the shortage of specialized\ndoctors with relevant experience make diagnosing and treating rare diseases\nmore challenging than common diseases. Recently, agents powered by large\nlanguage models (LLMs) have demonstrated notable applications across various\ndomains. In the medical field, some agent methods have outperformed direct\nprompts in question-answering tasks from medical examinations. However, current\nagent frameworks are not well-adapted to real-world clinical scenarios,\nespecially those involving the complex demands of rare diseases. To bridge this\ngap, we introduce RareAgents, the first LLM-driven multi-disciplinary team\nframework designed specifically for the complex clinical context of rare\ndiseases. RareAgents integrates advanced Multidisciplinary Team (MDT)\ncoordination, memory mechanisms, and medical tools utilization, leveraging\nLlama-3.1-8B/70B as the base model. Experimental results show that RareAgents\noutperforms state-of-the-art domain-specific models, GPT-4o, and current agent\nframeworks in differential diagnosis and medication recommendation for rare\ndiseases. Furthermore, we contribute a novel rare disease dataset,\nMIMIC-IV-Ext-Rare, to support further advancements in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rare diseases, despite their low individual incidence, collectively impact\naround 300 million people worldwide due to the vast number of diseases. The\ninvolvement of multiple organs and systems, and the shortage of specialized\ndoctors with relevant experience make diagnosing and treating rare diseases\nmore challenging than common diseases. Recently, agents powered by large\nlanguage models (LLMs) have demonstrated notable applications across various\ndomains. In the medical field, some agent methods have outperformed direct\nprompts in question-answering tasks from medical examinations. However, current\nagent frameworks are not well-adapted to real-world clinical scenarios,\nespecially those involving the complex demands of rare diseases. To bridge this\ngap, we introduce RareAgents, the first LLM-driven multi-disciplinary team\nframework designed specifically for the complex clinical context of rare\ndiseases. RareAgents integrates advanced Multidisciplinary Team (MDT)\ncoordination, memory mechanisms, and medical tools utilization, leveraging\nLlama-3.1-8B/70B as the base model. Experimental results show that RareAgents\noutperforms state-of-the-art domain-specific models, GPT-4o, and current agent\nframeworks in differential diagnosis and medication recommendation for rare\ndiseases. Furthermore, we contribute a novel rare disease dataset,\nMIMIC-IV-Ext-Rare, to support further advancements in this field."
                },
                "authors": [
                    {
                        "name": "Xuanzhong Chen"
                    },
                    {
                        "name": "Ye Jin"
                    },
                    {
                        "name": "Xiaohao Mao"
                    },
                    {
                        "name": "Lun Wang"
                    },
                    {
                        "name": "Shuyang Zhang"
                    },
                    {
                        "name": "Ting Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ting Chen"
                },
                "author": "Ting Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12475v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12475v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10001v1",
                "updated": "2025-02-14T08:33:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    33,
                    31,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T08:33:31Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    33,
                    31,
                    4,
                    45,
                    0
                ],
                "title": "EmbBERT-Q: Breaking Memory Barriers in Embedded NLP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbBERT-Q: Breaking Memory Barriers in Embedded NLP"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing,\nsetting new standards across a wide range of applications. However, their\nrelevant memory and computational demands make them impractical for deployment\non technologically-constrained tiny devices such as wearable devices and\nInternet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a\nnovel tiny language model specifically designed for tiny devices with stringent\nmemory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in\nNatural Language Processing tasks in this scenario, with a total memory\nfootprint (weights and activations) of just 781 kB, representing a 25x\nreduction in size with respect to SotA models. By combining architectural\ninnovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently\noutperforms several baseline models scaled down to a 2 MB memory budget (i.e.,\nthe maximum memory typically available in tiny devices), including heavily\ncompressed versions of BERT and MAMBA. Extensive experimental evaluations on\nboth a selected benchmark dataset, TinyNLP, specifically curated to evaluate\nTiny Language Models in NLP tasks and real-world scenarios, and the GLUE\nbenchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with\nrespect to existing approaches, achieving an unmatched balance between memory\nand performance. To ensure the complete and immediate reproducibility of all\nour results, we release all code, scripts, and model checkpoints at\nhttps://github.com/RiccardoBravin/tiny-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing,\nsetting new standards across a wide range of applications. However, their\nrelevant memory and computational demands make them impractical for deployment\non technologically-constrained tiny devices such as wearable devices and\nInternet-of-Things units. To address this limitation, we introduce EmbBERT-Q, a\nnovel tiny language model specifically designed for tiny devices with stringent\nmemory constraints. EmbBERT-Q achieves state-of-the-art (SotA) accuracy in\nNatural Language Processing tasks in this scenario, with a total memory\nfootprint (weights and activations) of just 781 kB, representing a 25x\nreduction in size with respect to SotA models. By combining architectural\ninnovations with hardware-compatible 8-bit quantization, EmbBERT-Q consistently\noutperforms several baseline models scaled down to a 2 MB memory budget (i.e.,\nthe maximum memory typically available in tiny devices), including heavily\ncompressed versions of BERT and MAMBA. Extensive experimental evaluations on\nboth a selected benchmark dataset, TinyNLP, specifically curated to evaluate\nTiny Language Models in NLP tasks and real-world scenarios, and the GLUE\nbenchmark, demonstrate EmbBERT-Q ability to deliver competitive accuracy with\nrespect to existing approaches, achieving an unmatched balance between memory\nand performance. To ensure the complete and immediate reproducibility of all\nour results, we release all code, scripts, and model checkpoints at\nhttps://github.com/RiccardoBravin/tiny-LLM."
                },
                "authors": [
                    {
                        "name": "Riccardo Bravin"
                    },
                    {
                        "name": "Massimo Pavan"
                    },
                    {
                        "name": "Hazem Hesham Yousef Shalby"
                    },
                    {
                        "name": "Fabrizio Pittorino"
                    },
                    {
                        "name": "Manuel Roveri"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Roveri"
                },
                "author": "Manuel Roveri",
                "arxiv_comment": "24 pages, 4 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09994v1",
                "updated": "2025-02-14T08:25:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    25,
                    6,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T08:25:06Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    25,
                    6,
                    4,
                    45,
                    0
                ],
                "title": "Decision Information Meets Large Language Models: The Future of\n  Explainable Operations Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision Information Meets Large Language Models: The Future of\n  Explainable Operations Research"
                },
                "summary": "Operations Research (OR) is vital for decision-making in many industries.\nWhile recent OR methods have seen significant improvements in automation and\nefficiency through integrating Large Language Models (LLMs), they still\nstruggle to produce meaningful explanations. This lack of clarity raises\nconcerns about transparency and trustworthiness in OR applications. To address\nthese challenges, we propose a comprehensive framework, Explainable Operations\nResearch (EOR), emphasizing actionable and understandable explanations\naccompanying optimization. The core of EOR is the concept of Decision\nInformation, which emerges from what-if analysis and focuses on evaluating the\nimpact of complex constraints (or parameters) changes on decision-making.\nSpecifically, we utilize bipartite graphs to quantify the changes in the OR\nmodel and adopt LLMs to improve the explanation capabilities. Additionally, we\nintroduce the first industrial benchmark to rigorously evaluate the\neffectiveness of explanations and analyses in OR, establishing a new standard\nfor transparency and clarity in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operations Research (OR) is vital for decision-making in many industries.\nWhile recent OR methods have seen significant improvements in automation and\nefficiency through integrating Large Language Models (LLMs), they still\nstruggle to produce meaningful explanations. This lack of clarity raises\nconcerns about transparency and trustworthiness in OR applications. To address\nthese challenges, we propose a comprehensive framework, Explainable Operations\nResearch (EOR), emphasizing actionable and understandable explanations\naccompanying optimization. The core of EOR is the concept of Decision\nInformation, which emerges from what-if analysis and focuses on evaluating the\nimpact of complex constraints (or parameters) changes on decision-making.\nSpecifically, we utilize bipartite graphs to quantify the changes in the OR\nmodel and adopt LLMs to improve the explanation capabilities. Additionally, we\nintroduce the first industrial benchmark to rigorously evaluate the\neffectiveness of explanations and analyses in OR, establishing a new standard\nfor transparency and clarity in the field."
                },
                "authors": [
                    {
                        "name": "Yansen Zhang"
                    },
                    {
                        "name": "Qingcan Kang"
                    },
                    {
                        "name": "Wing Yin Yu"
                    },
                    {
                        "name": "Hailei Gong"
                    },
                    {
                        "name": "Xiaojin Fu"
                    },
                    {
                        "name": "Xiongwei Han"
                    },
                    {
                        "name": "Tao Zhong"
                    },
                    {
                        "name": "Chen Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ma"
                },
                "author": "Chen Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09992v1",
                "updated": "2025-02-14T08:23:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    23,
                    51,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T08:23:51Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    23,
                    51,
                    4,
                    45,
                    0
                ],
                "title": "Large Language Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Diffusion Models"
                },
                "summary": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large\nlanguage models (LLMs). We challenge this notion by introducing LLaDA, a\ndiffusion model trained from scratch under the pre-training and supervised\nfine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\nmasking process and a reverse process, parameterized by a vanilla Transformer\nto predict masked tokens. By optimizing a likelihood bound, it provides a\nprincipled generative approach for probabilistic inference. Across extensive\nbenchmarks, LLaDA demonstrates strong scalability, outperforming our\nself-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\nLLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\ninstruction-following abilities in case studies such as multi-turn dialogue.\nMoreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\npoem completion task. Our findings establish diffusion models as a viable and\npromising alternative to ARMs, challenging the assumption that key LLM\ncapabilities discussed above are inherently tied to ARMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive models (ARMs) are widely regarded as the cornerstone of large\nlanguage models (LLMs). We challenge this notion by introducing LLaDA, a\ndiffusion model trained from scratch under the pre-training and supervised\nfine-tuning (SFT) paradigm. LLaDA models distributions through a forward data\nmasking process and a reverse process, parameterized by a vanilla Transformer\nto predict masked tokens. By optimizing a likelihood bound, it provides a\nprincipled generative approach for probabilistic inference. Across extensive\nbenchmarks, LLaDA demonstrates strong scalability, outperforming our\nself-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong\nLLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive\ninstruction-following abilities in case studies such as multi-turn dialogue.\nMoreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal\npoem completion task. Our findings establish diffusion models as a viable and\npromising alternative to ARMs, challenging the assumption that key LLM\ncapabilities discussed above are inherently tied to ARMs."
                },
                "authors": [
                    {
                        "name": "Shen Nie"
                    },
                    {
                        "name": "Fengqi Zhu"
                    },
                    {
                        "name": "Zebin You"
                    },
                    {
                        "name": "Xiaolu Zhang"
                    },
                    {
                        "name": "Jingyang Ou"
                    },
                    {
                        "name": "Jun Hu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09990v1",
                "updated": "2025-02-14T08:22:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    22,
                    51,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T08:22:51Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    22,
                    51,
                    4,
                    45,
                    0
                ],
                "title": "X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from\n  Multi-Turn Jailbreaks without Compromising Usability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from\n  Multi-Turn Jailbreaks without Compromising Usability"
                },
                "summary": "Despite the rapid development of safety alignment techniques for LLMs,\ndefending against multi-turn jailbreaks is still a challenging task. In this\npaper, we conduct a comprehensive comparison, revealing that some existing\ndefense methods can improve the robustness of LLMs against multi-turn\njailbreaks but compromise usability, i.e., reducing general capabilities or\ncausing the over-refusal problem. From the perspective of mechanism\ninterpretability of LLMs, we discover that these methods fail to establish a\nboundary that exactly distinguishes safe and harmful feature representations.\nTherefore, boundary-safe representations close to harmful representations are\ninevitably disrupted, leading to a decline in usability. To address this issue,\nwe propose X-Boundary to push harmful representations away from boundary-safe\nrepresentations and obtain an exact distinction boundary. In this way, harmful\nrepresentations can be precisely erased without disrupting safe ones.\nExperimental results show that X-Boundary achieves state-of-the-art defense\nperformance against multi-turn jailbreaks, while reducing the over-refusal rate\nby about 20% and maintaining nearly complete general capability. Furthermore,\nwe theoretically prove and empirically verify that X-Boundary can accelerate\nthe convergence process during training. Please see our code at:\nhttps://github.com/AI45Lab/X-Boundary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rapid development of safety alignment techniques for LLMs,\ndefending against multi-turn jailbreaks is still a challenging task. In this\npaper, we conduct a comprehensive comparison, revealing that some existing\ndefense methods can improve the robustness of LLMs against multi-turn\njailbreaks but compromise usability, i.e., reducing general capabilities or\ncausing the over-refusal problem. From the perspective of mechanism\ninterpretability of LLMs, we discover that these methods fail to establish a\nboundary that exactly distinguishes safe and harmful feature representations.\nTherefore, boundary-safe representations close to harmful representations are\ninevitably disrupted, leading to a decline in usability. To address this issue,\nwe propose X-Boundary to push harmful representations away from boundary-safe\nrepresentations and obtain an exact distinction boundary. In this way, harmful\nrepresentations can be precisely erased without disrupting safe ones.\nExperimental results show that X-Boundary achieves state-of-the-art defense\nperformance against multi-turn jailbreaks, while reducing the over-refusal rate\nby about 20% and maintaining nearly complete general capability. Furthermore,\nwe theoretically prove and empirically verify that X-Boundary can accelerate\nthe convergence process during training. Please see our code at:\nhttps://github.com/AI45Lab/X-Boundary."
                },
                "authors": [
                    {
                        "name": "Xiaoya Lu"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Yi Yu"
                    },
                    {
                        "name": "Luxin Xu"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09980v1",
                "updated": "2025-02-14T08:05:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    5,
                    41,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T08:05:41Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    5,
                    41,
                    4,
                    45,
                    0
                ],
                "title": "V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with\n  Multi-Modal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with\n  Multi-Modal Large Language Models"
                },
                "summary": "Current autonomous driving vehicles rely mainly on their individual sensors\nto understand surrounding scenes and plan for future trajectories, which can be\nunreliable when the sensors are malfunctioning or occluded. To address this\nproblem, cooperative perception methods via vehicle-to-vehicle (V2V)\ncommunication have been proposed, but they have tended to focus on detection\nand tracking. How those approaches contribute to overall cooperative planning\nperformance is still under-explored. Inspired by recent progress using Large\nLanguage Models (LLMs) to build autonomous driving systems, we propose a novel\nproblem setting that integrates an LLM into cooperative autonomous driving,\nwith the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and\nbenchmark. We also propose our baseline method Vehicle-to-Vehicle Large\nLanguage Model (V2V-LLM), which uses an LLM to fuse perception information from\nmultiple connected autonomous vehicles (CAVs) and answer driving-related\nquestions: grounding, notable object identification, and planning. Experimental\nresults show that our proposed V2V-LLM can be a promising unified model\narchitecture for performing various tasks in cooperative autonomous driving,\nand outperforms other baseline methods that use different fusion approaches.\nOur work also creates a new research direction that can improve the safety of\nfuture autonomous driving systems. Our project website:\nhttps://eddyhkchiu.github.io/v2vllm.github.io/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current autonomous driving vehicles rely mainly on their individual sensors\nto understand surrounding scenes and plan for future trajectories, which can be\nunreliable when the sensors are malfunctioning or occluded. To address this\nproblem, cooperative perception methods via vehicle-to-vehicle (V2V)\ncommunication have been proposed, but they have tended to focus on detection\nand tracking. How those approaches contribute to overall cooperative planning\nperformance is still under-explored. Inspired by recent progress using Large\nLanguage Models (LLMs) to build autonomous driving systems, we propose a novel\nproblem setting that integrates an LLM into cooperative autonomous driving,\nwith the proposed Vehicle-to-Vehicle Question-Answering (V2V-QA) dataset and\nbenchmark. We also propose our baseline method Vehicle-to-Vehicle Large\nLanguage Model (V2V-LLM), which uses an LLM to fuse perception information from\nmultiple connected autonomous vehicles (CAVs) and answer driving-related\nquestions: grounding, notable object identification, and planning. Experimental\nresults show that our proposed V2V-LLM can be a promising unified model\narchitecture for performing various tasks in cooperative autonomous driving,\nand outperforms other baseline methods that use different fusion approaches.\nOur work also creates a new research direction that can improve the safety of\nfuture autonomous driving systems. Our project website:\nhttps://eddyhkchiu.github.io/v2vllm.github.io/ ."
                },
                "authors": [
                    {
                        "name": "Hsu-kuang Chiu"
                    },
                    {
                        "name": "Ryo Hachiuma"
                    },
                    {
                        "name": "Chien-Yi Wang"
                    },
                    {
                        "name": "Stephen F. Smith"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    },
                    {
                        "name": "Min-Hung Chen"
                    }
                ],
                "author_detail": {
                    "name": "Min-Hung Chen"
                },
                "author": "Min-Hung Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09977v1",
                "updated": "2025-02-14T08:04:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    4,
                    22,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T08:04:22Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    4,
                    22,
                    4,
                    45,
                    0
                ],
                "title": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs\n  - No Silver Bullet for LC or RAG Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs\n  - No Silver Bullet for LC or RAG Routing"
                },
                "summary": "Effectively incorporating external knowledge into Large Language Models\n(LLMs) is crucial for enhancing their capabilities and addressing real-world\nneeds. Retrieval-Augmented Generation (RAG) offers an effective method for\nachieving this by retrieving the most relevant fragments into LLMs. However,\nthe advancements in context window size for LLMs offer an alternative approach,\nraising the question of whether RAG remains necessary for effectively handling\nexternal knowledge. Several existing studies provide inconclusive comparisons\nbetween RAG and long-context (LC) LLMs, largely due to limitations in the\nbenchmark designs. In this paper, we present LaRA, a novel benchmark\nspecifically designed to rigorously compare RAG and LC LLMs. LaRA encompasses\n2,326 test cases across four practical QA task categories and three types of\nnaturally occurring long texts. Through systematic evaluation of seven\nopen-source and four proprietary LLMs, we find that the optimal choice between\nRAG and LC depends on a complex interplay of factors, including the model's\nparameter size, long-text capabilities, context length, task type, and the\ncharacteristics of the retrieved chunks. Our findings provide actionable\nguidelines for practitioners to effectively leverage both RAG and LC approaches\nin developing and deploying LLM applications. Our code and dataset is provided\nat:\n\\href{https://github.com/likuanppd/LaRA}{\\textbf{https://github.com/likuanppd/LaRA}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively incorporating external knowledge into Large Language Models\n(LLMs) is crucial for enhancing their capabilities and addressing real-world\nneeds. Retrieval-Augmented Generation (RAG) offers an effective method for\nachieving this by retrieving the most relevant fragments into LLMs. However,\nthe advancements in context window size for LLMs offer an alternative approach,\nraising the question of whether RAG remains necessary for effectively handling\nexternal knowledge. Several existing studies provide inconclusive comparisons\nbetween RAG and long-context (LC) LLMs, largely due to limitations in the\nbenchmark designs. In this paper, we present LaRA, a novel benchmark\nspecifically designed to rigorously compare RAG and LC LLMs. LaRA encompasses\n2,326 test cases across four practical QA task categories and three types of\nnaturally occurring long texts. Through systematic evaluation of seven\nopen-source and four proprietary LLMs, we find that the optimal choice between\nRAG and LC depends on a complex interplay of factors, including the model's\nparameter size, long-text capabilities, context length, task type, and the\ncharacteristics of the retrieved chunks. Our findings provide actionable\nguidelines for practitioners to effectively leverage both RAG and LC approaches\nin developing and deploying LLM applications. Our code and dataset is provided\nat:\n\\href{https://github.com/likuanppd/LaRA}{\\textbf{https://github.com/likuanppd/LaRA}}."
                },
                "authors": [
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Minhao Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Minhao Cheng"
                },
                "author": "Minhao Cheng",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09974v1",
                "updated": "2025-02-14T08:00:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    0,
                    42,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T08:00:42Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    8,
                    0,
                    42,
                    4,
                    45,
                    0
                ],
                "title": "Has My System Prompt Been Used? Large Language Model Prompt Membership\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Has My System Prompt Been Used? Large Language Model Prompt Membership\n  Inference"
                },
                "summary": "Prompt engineering has emerged as a powerful technique for optimizing large\nlanguage models (LLMs) for specific applications, enabling faster prototyping\nand improved performance, and giving rise to the interest of the community in\nprotecting proprietary system prompts. In this work, we explore a novel\nperspective on prompt privacy through the lens of membership inference. We\ndevelop Prompt Detective, a statistical method to reliably determine whether a\ngiven system prompt was used by a third-party language model. Our approach\nrelies on a statistical test comparing the distributions of two groups of model\noutputs corresponding to different system prompts. Through extensive\nexperiments with a variety of language models, we demonstrate the effectiveness\nof Prompt Detective for prompt membership inference. Our work reveals that even\nminor changes in system prompts manifest in distinct response distributions,\nenabling us to verify prompt usage with statistical significance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt engineering has emerged as a powerful technique for optimizing large\nlanguage models (LLMs) for specific applications, enabling faster prototyping\nand improved performance, and giving rise to the interest of the community in\nprotecting proprietary system prompts. In this work, we explore a novel\nperspective on prompt privacy through the lens of membership inference. We\ndevelop Prompt Detective, a statistical method to reliably determine whether a\ngiven system prompt was used by a third-party language model. Our approach\nrelies on a statistical test comparing the distributions of two groups of model\noutputs corresponding to different system prompts. Through extensive\nexperiments with a variety of language models, we demonstrate the effectiveness\nof Prompt Detective for prompt membership inference. Our work reveals that even\nminor changes in system prompts manifest in distinct response distributions,\nenabling us to verify prompt usage with statistical significance."
                },
                "authors": [
                    {
                        "name": "Roman Levin"
                    },
                    {
                        "name": "Valeriia Cherepanova"
                    },
                    {
                        "name": "Abhimanyu Hans"
                    },
                    {
                        "name": "Avi Schwarzschild"
                    },
                    {
                        "name": "Tom Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Tom Goldstein"
                },
                "author": "Tom Goldstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19324v2",
                "updated": "2025-02-14T07:30:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    7,
                    30,
                    0,
                    4,
                    45,
                    0
                ],
                "published": "2025-01-31T17:19:57Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    19,
                    57,
                    4,
                    31,
                    0
                ],
                "title": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward-Guided Speculative Decoding for Efficient LLM Reasoning"
                },
                "summary": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios. The code is available at\nhttps://github.com/BaohaoLiao/RSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Reward-Guided Speculative Decoding (RSD), a novel framework\naimed at improving the efficiency of inference in large language models (LLMs).\nRSD synergistically combines a lightweight draft model with a more powerful\ntarget model, incorporating a controlled bias to prioritize high-reward\noutputs, in contrast to existing speculative decoding methods that enforce\nstrict unbiasedness. RSD employs a process reward model to evaluate\nintermediate decoding steps and dynamically decide whether to invoke the target\nmodel, optimizing the trade-off between computational cost and output quality.\nWe theoretically demonstrate that a threshold-based mixture strategy achieves\nan optimal balance between resource utilization and performance. Extensive\nevaluations on challenging reasoning benchmarks, including Olympiad-level\ntasks, show that RSD delivers significant efficiency gains against decoding\nwith the target model only (up to 4.4x fewer FLOPs), while achieving\nsignificant better accuracy than parallel decoding method on average (up to\n+3.5). These results highlight RSD as a robust and cost-effective approach for\ndeploying LLMs in resource-intensive scenarios. The code is available at\nhttps://github.com/BaohaoLiao/RSD."
                },
                "authors": [
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Junnan Li"
                    },
                    {
                        "name": "Christof Monz"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Doyen Sahoo"
                    },
                    {
                        "name": "Caiming Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Caiming Xiong"
                },
                "author": "Caiming Xiong",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09955v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09955v1",
                "updated": "2025-02-14T07:22:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    7,
                    22,
                    25,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T07:22:25Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    7,
                    22,
                    25,
                    4,
                    45,
                    0
                ],
                "title": "Diverse Inference and Verification for Advanced Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverse Inference and Verification for Advanced Reasoning"
                },
                "summary": "Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant\nprogress in mathematics and coding, yet find challenging advanced tasks such as\nInternational Mathematical Olympiad (IMO) combinatorics problems, Abstraction\nand Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions.\nWe use a diverse inference approach that combines multiple models and methods\nat test time. We find that verifying mathematics and code problems, and\nrejection sampling on other problems is simple and effective. We automatically\nverify correctness of solutions to IMO problems by Lean, and ARC puzzles by\ncode, and find that best-of-N effectively answers HLE questions. Our approach\nincreases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%,\naccuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that\n948 humans could not and 26.5% of ARC puzzles that o3 high compute does not.\nTest-time simulations, reinforcement learning, and meta-learning with inference\nfeedback improve generalization by adapting agent graph representations and\nvarying prompts, code, and datasets. Our approach is reliable, robust, and\nscalable, and in the spirit of reproducible research, we will make it publicly\navailable upon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning LLMs such as OpenAI o1, o3 and DeepSeek R1 have made significant\nprogress in mathematics and coding, yet find challenging advanced tasks such as\nInternational Mathematical Olympiad (IMO) combinatorics problems, Abstraction\nand Reasoning Corpus (ARC) puzzles, and Humanity's Last Exam (HLE) questions.\nWe use a diverse inference approach that combines multiple models and methods\nat test time. We find that verifying mathematics and code problems, and\nrejection sampling on other problems is simple and effective. We automatically\nverify correctness of solutions to IMO problems by Lean, and ARC puzzles by\ncode, and find that best-of-N effectively answers HLE questions. Our approach\nincreases answer accuracy on IMO combinatorics problems from 33.3% to 77.8%,\naccuracy on HLE questions from 8% to 37%, and solves 80% of ARC puzzles that\n948 humans could not and 26.5% of ARC puzzles that o3 high compute does not.\nTest-time simulations, reinforcement learning, and meta-learning with inference\nfeedback improve generalization by adapting agent graph representations and\nvarying prompts, code, and datasets. Our approach is reliable, robust, and\nscalable, and in the spirit of reproducible research, we will make it publicly\navailable upon publication."
                },
                "authors": [
                    {
                        "name": "Iddo Drori"
                    },
                    {
                        "name": "Gaston Longhitano"
                    },
                    {
                        "name": "Mao Mao"
                    },
                    {
                        "name": "Seunghwan Hyun"
                    },
                    {
                        "name": "Yuke Zhang"
                    },
                    {
                        "name": "Sungjun Park"
                    },
                    {
                        "name": "Zachary Meeks"
                    },
                    {
                        "name": "Xin-Yu Zhang"
                    },
                    {
                        "name": "Ben Segev"
                    },
                    {
                        "name": "Howard Yong"
                    },
                    {
                        "name": "Nakul Verma"
                    },
                    {
                        "name": "Avi Shporer"
                    },
                    {
                        "name": "Alon Amit"
                    },
                    {
                        "name": "Madeleine Udell"
                    }
                ],
                "author_detail": {
                    "name": "Madeleine Udell"
                },
                "author": "Madeleine Udell",
                "arxiv_comment": "165 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09955v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09955v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09641v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09641v4",
                "updated": "2025-02-14T07:14:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    7,
                    14,
                    14,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-15T07:23:07Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    7,
                    23,
                    7,
                    6,
                    259,
                    0
                ],
                "title": "AACessTalk: Fostering Communication between Minimally Verbal Autistic\n  Children and Parents with Contextual Guidance and Card Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AACessTalk: Fostering Communication between Minimally Verbal Autistic\n  Children and Parents with Contextual Guidance and Card Recommendation"
                },
                "summary": "As minimally verbal autistic (MVA) children communicate with parents through\nfew words and nonverbal cues, parents often struggle to encourage their\nchildren to express subtle emotions and needs and to grasp their nuanced\nsignals. We present AACessTalk, a tablet-based, AI-mediated communication\nsystem that facilitates meaningful exchanges between an MVA child and a parent.\nAACessTalk provides real-time guides to the parent to engage the child in\nconversation and, in turn, recommends contextual vocabulary cards to the child.\nThrough a two-week deployment study with 11 MVA child-parent dyads, we examine\nhow AACessTalk fosters everyday conversation practice and mutual engagement.\nOur findings show high engagement from all dyads, leading to increased\nfrequency of conversation and turn-taking. AACessTalk also encouraged parents\nto explore their own interaction strategies and empowered the children to have\nmore agency in communication. We discuss the implications of designing\ntechnologies for balanced communication dynamics in parent-MVA child\ninteraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As minimally verbal autistic (MVA) children communicate with parents through\nfew words and nonverbal cues, parents often struggle to encourage their\nchildren to express subtle emotions and needs and to grasp their nuanced\nsignals. We present AACessTalk, a tablet-based, AI-mediated communication\nsystem that facilitates meaningful exchanges between an MVA child and a parent.\nAACessTalk provides real-time guides to the parent to engage the child in\nconversation and, in turn, recommends contextual vocabulary cards to the child.\nThrough a two-week deployment study with 11 MVA child-parent dyads, we examine\nhow AACessTalk fosters everyday conversation practice and mutual engagement.\nOur findings show high engagement from all dyads, leading to increased\nfrequency of conversation and turn-taking. AACessTalk also encouraged parents\nto explore their own interaction strategies and empowered the children to have\nmore agency in communication. We discuss the implications of designing\ntechnologies for balanced communication dynamics in parent-MVA child\ninteraction."
                },
                "authors": [
                    {
                        "name": "Dasom Choi"
                    },
                    {
                        "name": "SoHyun Park"
                    },
                    {
                        "name": "Kyungah Lee"
                    },
                    {
                        "name": "Hwajung Hong"
                    },
                    {
                        "name": "Young-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young-Ho Kim"
                },
                "author": "Young-Ho Kim",
                "arxiv_comment": "21 pages excluding reference. Accepted at ACM CHI 2025.\n  https://naver-ai.github.io/aacesstalk/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09641v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09641v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08943v2",
                "updated": "2025-02-14T06:10:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    6,
                    10,
                    0,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-13T03:43:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    3,
                    43,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "Beyond the Singular: The Essential Role of Multiple Generations in\n  Effective Benchmark Evaluation and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Singular: The Essential Role of Multiple Generations in\n  Effective Benchmark Evaluation and Analysis"
                },
                "summary": "Large language models (LLMs) have demonstrated significant utilities in\nreal-world applications, exhibiting impressive capabilities in natural language\nprocessing and understanding. Benchmark evaluations are crucial for assessing\nthe capabilities of LLMs as they can provide a comprehensive assessment of\ntheir strengths and weaknesses. However, current evaluation methods often\noverlook the inherent randomness of LLMs by employing deterministic generation\nstrategies or relying on a single random sample, resulting in unaccounted\nsampling variance and unreliable benchmark score estimates. In this paper, we\npropose a hierarchical statistical model that provides a more comprehensive\nrepresentation of the benchmarking process by incorporating both benchmark\ncharacteristics and LLM randomness. We show that leveraging multiple\ngenerations improves the accuracy of estimating the benchmark score and reduces\nvariance. We also introduce $\\mathbb P\\left(\\text{correct}\\right)$, a\nprompt-level difficulty score based on correct ratios, providing fine-grained\ninsights into individual prompts. Additionally, we create a data map that\nvisualizes difficulty and semantic prompts, enabling error detection and\nquality control in benchmark construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant utilities in\nreal-world applications, exhibiting impressive capabilities in natural language\nprocessing and understanding. Benchmark evaluations are crucial for assessing\nthe capabilities of LLMs as they can provide a comprehensive assessment of\ntheir strengths and weaknesses. However, current evaluation methods often\noverlook the inherent randomness of LLMs by employing deterministic generation\nstrategies or relying on a single random sample, resulting in unaccounted\nsampling variance and unreliable benchmark score estimates. In this paper, we\npropose a hierarchical statistical model that provides a more comprehensive\nrepresentation of the benchmarking process by incorporating both benchmark\ncharacteristics and LLM randomness. We show that leveraging multiple\ngenerations improves the accuracy of estimating the benchmark score and reduces\nvariance. We also introduce $\\mathbb P\\left(\\text{correct}\\right)$, a\nprompt-level difficulty score based on correct ratios, providing fine-grained\ninsights into individual prompts. Additionally, we create a data map that\nvisualizes difficulty and semantic prompts, enabling error detection and\nquality control in benchmark construction."
                },
                "authors": [
                    {
                        "name": "Wenbo Zhang"
                    },
                    {
                        "name": "Hengrui Cai"
                    },
                    {
                        "name": "Wenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Chen"
                },
                "author": "Wenyu Chen",
                "arxiv_comment": "10 pages, 1 table, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09933v1",
                "updated": "2025-02-14T06:05:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    6,
                    5,
                    12,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T06:05:12Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    6,
                    5,
                    12,
                    4,
                    45,
                    0
                ],
                "title": "MIR-Bench: Benchmarking LLM's Long-Context Intelligence via Many-Shot\n  In-Context Inductive Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIR-Bench: Benchmarking LLM's Long-Context Intelligence via Many-Shot\n  In-Context Inductive Reasoning"
                },
                "summary": "Inductive Reasoning (IR), the ability to summarize rules from examples and\napply on new ones, has long been viewed as a primal ability for general\nintelligence and widely studied by cognitive science and AI researchers. Many\nbenchmarks have been proposed to measure such ability for Large Language Models\n(LLMs); however, they focus on few-shot (usually $<$10) setting and lack\nevaluation for aggregating many pieces of information from long contexts. On\nthe other hand, the ever-growing context length of LLMs have brought forth the\nnovel paradigm of many-shot In-Context Learning (ICL), which addresses new\ntasks with hundreds to thousands of examples without expensive and inefficient\nfine-tuning. However, many-shot evaluations are mostly focused on\nclassification (a very limited aspect of IR), and popular long-context LLM\ntasks such as Needle-In-A-Haystack (NIAH) seldom require complicated\nintelligence for integrating many pieces of information. To fix the issues from\nboth worlds, we propose MIR-Bench, the first many-shot in-context inductive\nreasoning benchmark that asks LLM to induce output via input-output examples\nfrom underlying functions with diverse data format. Based on MIR-Bench, we\nstudy many novel problems for inductive reasoning and many-shot ICL, including\nrobustness against erroneous shots and the effect of Chain-of-Thought (CoT),\nand acquired insightful findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inductive Reasoning (IR), the ability to summarize rules from examples and\napply on new ones, has long been viewed as a primal ability for general\nintelligence and widely studied by cognitive science and AI researchers. Many\nbenchmarks have been proposed to measure such ability for Large Language Models\n(LLMs); however, they focus on few-shot (usually $<$10) setting and lack\nevaluation for aggregating many pieces of information from long contexts. On\nthe other hand, the ever-growing context length of LLMs have brought forth the\nnovel paradigm of many-shot In-Context Learning (ICL), which addresses new\ntasks with hundreds to thousands of examples without expensive and inefficient\nfine-tuning. However, many-shot evaluations are mostly focused on\nclassification (a very limited aspect of IR), and popular long-context LLM\ntasks such as Needle-In-A-Haystack (NIAH) seldom require complicated\nintelligence for integrating many pieces of information. To fix the issues from\nboth worlds, we propose MIR-Bench, the first many-shot in-context inductive\nreasoning benchmark that asks LLM to induce output via input-output examples\nfrom underlying functions with diverse data format. Based on MIR-Bench, we\nstudy many novel problems for inductive reasoning and many-shot ICL, including\nrobustness against erroneous shots and the effect of Chain-of-Thought (CoT),\nand acquired insightful findings."
                },
                "authors": [
                    {
                        "name": "Kai Yan"
                    },
                    {
                        "name": "Zhan Ling"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Ting-Han Fan"
                    },
                    {
                        "name": "Lingfeng Shen"
                    },
                    {
                        "name": "Zhengyin Du"
                    },
                    {
                        "name": "Jiecao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jiecao Chen"
                },
                "author": "Jiecao Chen",
                "arxiv_comment": "32 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18279v9",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18279v9",
                "updated": "2025-02-14T05:42:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    42,
                    16,
                    4,
                    45,
                    0
                ],
                "published": "2024-11-27T12:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "Large Language Model-Brained GUI Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Brained GUI Agents: A Survey"
                },
                "summary": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18279v9",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18279v9",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09922v1",
                "updated": "2025-02-14T05:21:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    21,
                    48,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T05:21:48Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    21,
                    48,
                    4,
                    45,
                    0
                ],
                "title": "λScale: Enabling Fast Scaling for Serverless Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "λScale: Enabling Fast Scaling for Serverless Large Language\n  Model Inference"
                },
                "summary": "Serverless computing has emerged as a compelling solution for cloud-based\nmodel inference. However, as modern large language models (LLMs) continue to\ngrow in size, existing serverless platforms often face substantial model\nstartup overhead. This poses a significant challenge in efficiently scaling\nmodel instances to accommodate dynamic, bursty workloads commonly observed in\nreal-world inference services. In this paper, we introduce {\\lambda}Scale, an\nefficient serverless inference system to achieve fast model scaling. The key\nidea behind {\\lambda}Scale is to leverage high-speed RDMA networks between GPU\nnodes for fast model multicast, while enabling distributed inference execution\nduring model transmission -- referred to as \"execute-while-load\".\n{\\lambda}Scale proposes an efficient model scaling scheme, {\\lambda}Pipe, which\nsupports adaptive model multicast and dynamically constructs execution\npipelines across receiving nodes for collaborative, distributed inference.\nAdditionally, {\\lambda}Scale supports efficient model management across GPU and\nhost memory, allowing fast scaling for models across different storage tiers.\nEvaluation results show that {\\lambda}Scale enables fast model scaling and\neffectively handles load spikes, achieving up to 5x tail-latency improvement\nand 31.3% cost reduction compared to state-of-the-art solutions on real-world\nLLM inference traces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing has emerged as a compelling solution for cloud-based\nmodel inference. However, as modern large language models (LLMs) continue to\ngrow in size, existing serverless platforms often face substantial model\nstartup overhead. This poses a significant challenge in efficiently scaling\nmodel instances to accommodate dynamic, bursty workloads commonly observed in\nreal-world inference services. In this paper, we introduce {\\lambda}Scale, an\nefficient serverless inference system to achieve fast model scaling. The key\nidea behind {\\lambda}Scale is to leverage high-speed RDMA networks between GPU\nnodes for fast model multicast, while enabling distributed inference execution\nduring model transmission -- referred to as \"execute-while-load\".\n{\\lambda}Scale proposes an efficient model scaling scheme, {\\lambda}Pipe, which\nsupports adaptive model multicast and dynamically constructs execution\npipelines across receiving nodes for collaborative, distributed inference.\nAdditionally, {\\lambda}Scale supports efficient model management across GPU and\nhost memory, allowing fast scaling for models across different storage tiers.\nEvaluation results show that {\\lambda}Scale enables fast model scaling and\neffectively handles load spikes, achieving up to 5x tail-latency improvement\nand 31.3% cost reduction compared to state-of-the-art solutions on real-world\nLLM inference traces."
                },
                "authors": [
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Chaobo Jia"
                    },
                    {
                        "name": "Zhaoyuan Su"
                    },
                    {
                        "name": "Sheng Yao"
                    },
                    {
                        "name": "Tingfeng Lan"
                    },
                    {
                        "name": "Yuchen Yang"
                    },
                    {
                        "name": "Yue Cheng"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Ruichuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ruichuan Chen"
                },
                "author": "Ruichuan Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08098v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08098v2",
                "updated": "2025-02-14T05:21:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    21,
                    37,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-11T04:52:41Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    52,
                    41,
                    2,
                    346,
                    0
                ],
                "title": "What You See Is Not Always What You Get: An Empirical Study of Code\n  Comprehension by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What You See Is Not Always What You Get: An Empirical Study of Code\n  Comprehension by Large Language Models"
                },
                "summary": "Recent studies have demonstrated outstanding capabilities of large language\nmodels (LLMs) in software engineering tasks, including code generation and\ncomprehension. While LLMs have shown significant potential in assisting with\ncoding, it is perceived that LLMs are vulnerable to adversarial attacks. In\nthis paper, we investigate the vulnerability of LLMs to imperceptible attacks,\nwhere hidden character manipulation in source code misleads LLMs' behaviour\nwhile remaining undetectable to human reviewers. We devise these attacks into\nfour distinct categories and analyse their impacts on code analysis and\ncomprehension tasks. These four types of imperceptible coding character attacks\ninclude coding reordering, invisible coding characters, code deletions, and\ncode homoglyphs. To comprehensively benchmark the robustness of current LLMs\nsolutions against the attacks, we present a systematic experimental evaluation\non multiple state-of-the-art LLMs. Our experimental design introduces two key\nperformance metrics, namely model confidence using log probabilities of\nresponse, and the response correctness. A set of controlled experiments are\nconducted using a large-scale perturbed and unperturbed code snippets as the\nprimary prompt input. Our findings confirm the susceptibility of LLMs to\nimperceptible coding character attacks, while different LLMs present different\nnegative correlations between perturbation magnitude and performance. These\nresults highlight the urgent need for robust LLMs capable of manoeuvring\nbehaviours under imperceptible adversarial conditions. We anticipate this work\nprovides valuable insights for enhancing the security and trustworthiness of\nLLMs in software engineering applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have demonstrated outstanding capabilities of large language\nmodels (LLMs) in software engineering tasks, including code generation and\ncomprehension. While LLMs have shown significant potential in assisting with\ncoding, it is perceived that LLMs are vulnerable to adversarial attacks. In\nthis paper, we investigate the vulnerability of LLMs to imperceptible attacks,\nwhere hidden character manipulation in source code misleads LLMs' behaviour\nwhile remaining undetectable to human reviewers. We devise these attacks into\nfour distinct categories and analyse their impacts on code analysis and\ncomprehension tasks. These four types of imperceptible coding character attacks\ninclude coding reordering, invisible coding characters, code deletions, and\ncode homoglyphs. To comprehensively benchmark the robustness of current LLMs\nsolutions against the attacks, we present a systematic experimental evaluation\non multiple state-of-the-art LLMs. Our experimental design introduces two key\nperformance metrics, namely model confidence using log probabilities of\nresponse, and the response correctness. A set of controlled experiments are\nconducted using a large-scale perturbed and unperturbed code snippets as the\nprimary prompt input. Our findings confirm the susceptibility of LLMs to\nimperceptible coding character attacks, while different LLMs present different\nnegative correlations between perturbation magnitude and performance. These\nresults highlight the urgent need for robust LLMs capable of manoeuvring\nbehaviours under imperceptible adversarial conditions. We anticipate this work\nprovides valuable insights for enhancing the security and trustworthiness of\nLLMs in software engineering applications."
                },
                "authors": [
                    {
                        "name": "Bangshuo Zhu"
                    },
                    {
                        "name": "Jiawen Wen"
                    },
                    {
                        "name": "Huaming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huaming Chen"
                },
                "author": "Huaming Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08098v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08098v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09921v1",
                "updated": "2025-02-14T05:19:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T05:19:46Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "title": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing"
                },
                "summary": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption."
                },
                "authors": [
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Siung Noh"
                    },
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaewon Jung"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09913v1",
                "updated": "2025-02-14T04:58:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    4,
                    58,
                    28,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T04:58:28Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    4,
                    58,
                    28,
                    4,
                    45,
                    0
                ],
                "title": "AutoS$^2$earch: Unlocking the Reasoning Potential of Large Models for\n  Web-based Source Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoS$^2$earch: Unlocking the Reasoning Potential of Large Models for\n  Web-based Source Search"
                },
                "summary": "Web-based management systems have been widely used in risk control and\nindustrial safety. However, effectively integrating source search capabilities\ninto these systems, to enable decision-makers to locate and address the hazard\n(e.g., gas leak detection) remains a challenge. While prior efforts have\nexplored using web crowdsourcing and AI algorithms for source search decision\nsupport, these approaches suffer from overheads in recruiting human\nparticipants and slow response times in time-sensitive situations. To address\nthis, we introduce AutoS$^2$earch, a novel framework leveraging large models\nfor zero-shot source search in web applications. AutoS$^2$earch operates on a\nsimplified visual environment projected through a web-based display, utilizing\na chain-of-thought prompt designed to emulate human reasoning. The multi-modal\nlarge language model (MLLMs) dynamically converts visual observations into\nlanguage descriptions, enabling the LLM to perform linguistic reasoning on four\ndirectional choices. Extensive experiments demonstrate that AutoS$^2$earch\nachieves performance nearly equivalent to human-AI collaborative source search\nwhile eliminating dependency on crowdsourced labor. Our work offers valuable\ninsights in using web engineering to design such autonomous systems in other\nindustrial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web-based management systems have been widely used in risk control and\nindustrial safety. However, effectively integrating source search capabilities\ninto these systems, to enable decision-makers to locate and address the hazard\n(e.g., gas leak detection) remains a challenge. While prior efforts have\nexplored using web crowdsourcing and AI algorithms for source search decision\nsupport, these approaches suffer from overheads in recruiting human\nparticipants and slow response times in time-sensitive situations. To address\nthis, we introduce AutoS$^2$earch, a novel framework leveraging large models\nfor zero-shot source search in web applications. AutoS$^2$earch operates on a\nsimplified visual environment projected through a web-based display, utilizing\na chain-of-thought prompt designed to emulate human reasoning. The multi-modal\nlarge language model (MLLMs) dynamically converts visual observations into\nlanguage descriptions, enabling the LLM to perform linguistic reasoning on four\ndirectional choices. Extensive experiments demonstrate that AutoS$^2$earch\nachieves performance nearly equivalent to human-AI collaborative source search\nwhile eliminating dependency on crowdsourced labor. Our work offers valuable\ninsights in using web engineering to design such autonomous systems in other\nindustrial applications."
                },
                "authors": [
                    {
                        "name": "Zhengqiu Zhu"
                    },
                    {
                        "name": "Yatai Ji"
                    },
                    {
                        "name": "Jiaheng Huang"
                    },
                    {
                        "name": "Yong Zhao"
                    },
                    {
                        "name": "Sihang Qiu"
                    },
                    {
                        "name": "Rusheng Ju"
                    }
                ],
                "author_detail": {
                    "name": "Rusheng Ju"
                },
                "author": "Rusheng Ju",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.05816v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.05816v3",
                "updated": "2025-02-14T04:57:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    4,
                    57,
                    15,
                    4,
                    45,
                    0
                ],
                "published": "2023-06-09T11:30:08Z",
                "published_parsed": [
                    2023,
                    6,
                    9,
                    11,
                    30,
                    8,
                    4,
                    160,
                    0
                ],
                "title": "Detecting Phishing Sites Using ChatGPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Phishing Sites Using ChatGPT"
                },
                "summary": "The emergence of Large Language Models (LLMs), including ChatGPT, is having a\nsignificant impact on a wide range of fields. While LLMs have been extensively\nresearched for tasks such as code generation and text synthesis, their\napplication in detecting malicious web content, particularly phishing sites,\nhas been largely unexplored. To combat the rising tide of cyber attacks due to\nthe misuse of LLMs, it is important to automate detection by leveraging the\nadvanced capabilities of LLMs.\n  In this paper, we propose a novel system called ChatPhishDetector that\nutilizes LLMs to detect phishing sites. Our system involves leveraging a web\ncrawler to gather information from websites, generating prompts for LLMs based\non the crawled data, and then retrieving the detection results from the\nresponses generated by the LLMs. The system enables us to detect multilingual\nphishing sites with high accuracy by identifying impersonated brands and social\nengineering techniques in the context of the entire website, without the need\nto train machine learning models. To evaluate the performance of our system, we\nconducted experiments on our own dataset and compared it with baseline systems\nand several LLMs. The experimental results using GPT-4V demonstrated\noutstanding performance, with a precision of 98.7% and a recall of 99.6%,\noutperforming the detection results of other LLMs and existing systems. These\nfindings highlight the potential of LLMs for protecting users from online\nfraudulent activities and have important implications for enhancing\ncybersecurity measures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Large Language Models (LLMs), including ChatGPT, is having a\nsignificant impact on a wide range of fields. While LLMs have been extensively\nresearched for tasks such as code generation and text synthesis, their\napplication in detecting malicious web content, particularly phishing sites,\nhas been largely unexplored. To combat the rising tide of cyber attacks due to\nthe misuse of LLMs, it is important to automate detection by leveraging the\nadvanced capabilities of LLMs.\n  In this paper, we propose a novel system called ChatPhishDetector that\nutilizes LLMs to detect phishing sites. Our system involves leveraging a web\ncrawler to gather information from websites, generating prompts for LLMs based\non the crawled data, and then retrieving the detection results from the\nresponses generated by the LLMs. The system enables us to detect multilingual\nphishing sites with high accuracy by identifying impersonated brands and social\nengineering techniques in the context of the entire website, without the need\nto train machine learning models. To evaluate the performance of our system, we\nconducted experiments on our own dataset and compared it with baseline systems\nand several LLMs. The experimental results using GPT-4V demonstrated\noutstanding performance, with a precision of 98.7% and a recall of 99.6%,\noutperforming the detection results of other LLMs and existing systems. These\nfindings highlight the potential of LLMs for protecting users from online\nfraudulent activities and have important implications for enhancing\ncybersecurity measures."
                },
                "authors": [
                    {
                        "name": "Takashi Koide"
                    },
                    {
                        "name": "Naoki Fukushi"
                    },
                    {
                        "name": "Hiroki Nakano"
                    },
                    {
                        "name": "Daiki Chiba"
                    }
                ],
                "author_detail": {
                    "name": "Daiki Chiba"
                },
                "author": "Daiki Chiba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.05816v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.05816v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08952v2",
                "updated": "2025-02-14T04:56:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    4,
                    56,
                    16,
                    4,
                    45,
                    0
                ],
                "published": "2024-07-12T03:15:01Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    3,
                    15,
                    1,
                    4,
                    194,
                    0
                ],
                "title": "Detect, Investigate, Judge and Determine: A Knowledge-guided Framework\n  for Few-shot Fake News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detect, Investigate, Judge and Determine: A Knowledge-guided Framework\n  for Few-shot Fake News Detection"
                },
                "summary": "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-Shot Fake News Detection (FS-FND) aims to distinguish inaccurate news\nfrom real ones in extremely low-resource scenarios. This task has garnered\nincreased attention due to the widespread dissemination and harmful impact of\nfake news on social media. Large Language Models (LLMs) have demonstrated\ncompetitive performance with the help of their rich prior knowledge and\nexcellent in-context learning abilities. However, existing methods face\nsignificant limitations, such as the Understanding Ambiguity and Information\nScarcity, which significantly undermine the potential of LLMs. To address these\nshortcomings, we propose a Dual-perspective Knowledge-guided Fake News\nDetection (DKFND) model, designed to enhance LLMs from both inside and outside\nperspectives. Specifically, DKFND first identifies the knowledge concepts of\neach news article through a Detection Module. Subsequently, DKFND creatively\ndesigns an Investigation Module to retrieve inside and outside valuable\ninformation concerning to the current news, followed by another Judge Module to\nevaluate the relevance and confidence of them. Finally, a Determination Module\nfurther derives two respective predictions and obtain the final result.\nExtensive experiments on two public datasets show the efficacy of our proposed\nmethod, particularly in low-resource settings."
                },
                "authors": [
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Jiajun Zhu"
                    },
                    {
                        "name": "Xukai Liu"
                    },
                    {
                        "name": "Haoyu Tang"
                    },
                    {
                        "name": "Yanghai Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Xiaofang Zhou"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15156v2",
                "updated": "2025-02-14T04:54:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    4,
                    54,
                    23,
                    4,
                    45,
                    0
                ],
                "published": "2024-09-23T16:04:03Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    16,
                    4,
                    3,
                    0,
                    267,
                    0
                ],
                "title": "Rethinking Conventional Wisdom in Machine Learning: From Generalization\n  to Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Conventional Wisdom in Machine Learning: From Generalization\n  to Scaling"
                },
                "summary": "The remarkable success of large language pretraining and the discovery of\nscaling laws signify a paradigm shift in machine learning. Notably, the primary\nobjective has evolved from minimizing generalization error to reducing\napproximation error, and the most effective strategy has transitioned from\nregularization (in a broad sense) to scaling up models. This raises a critical\nquestion:\n  Do the established principles that proved successful in the\ngeneralization-centric era remain valid in this new era of scaling?\n  This paper examines several influential regularization-based principles that\nmay no longer hold true in the scaling-centric, large language model (LLM) era.\nThese principles include explicit L2 regularization and implicit regularization\nthrough small batch sizes and large learning rates. Additionally, we identify a\nnew phenomenon termed ``scaling law crossover,'' where two scaling curves\nintersect at a certain scale, implying that methods effective at smaller scales\nmay not generalize to larger ones. Together, these observations highlight two\nfundamental questions within this new paradigm:\n  $\\bullet$ Guiding Principles for Scaling: If regularization is no longer the\nprimary guiding principle for model design, what new principles are emerging to\nguide scaling?\n  $\\bullet$ Model Comparison at Scale: How to reliably and effectively compare\nmodels at the scale where only a single experiment is feasible?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable success of large language pretraining and the discovery of\nscaling laws signify a paradigm shift in machine learning. Notably, the primary\nobjective has evolved from minimizing generalization error to reducing\napproximation error, and the most effective strategy has transitioned from\nregularization (in a broad sense) to scaling up models. This raises a critical\nquestion:\n  Do the established principles that proved successful in the\ngeneralization-centric era remain valid in this new era of scaling?\n  This paper examines several influential regularization-based principles that\nmay no longer hold true in the scaling-centric, large language model (LLM) era.\nThese principles include explicit L2 regularization and implicit regularization\nthrough small batch sizes and large learning rates. Additionally, we identify a\nnew phenomenon termed ``scaling law crossover,'' where two scaling curves\nintersect at a certain scale, implying that methods effective at smaller scales\nmay not generalize to larger ones. Together, these observations highlight two\nfundamental questions within this new paradigm:\n  $\\bullet$ Guiding Principles for Scaling: If regularization is no longer the\nprimary guiding principle for model design, what new principles are emerging to\nguide scaling?\n  $\\bullet$ Model Comparison at Scale: How to reliably and effectively compare\nmodels at the scale where only a single experiment is feasible?"
                },
                "authors": [
                    {
                        "name": "Lechao Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Lechao Xiao"
                },
                "author": "Lechao Xiao",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08659v2",
                "updated": "2025-02-14T04:36:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    4,
                    36,
                    12,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-09T06:33:47Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    6,
                    33,
                    47,
                    6,
                    40,
                    0
                ],
                "title": "Deployment-friendly Lane-changing Intention Prediction Powered by\n  Brain-inspired Spiking Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment-friendly Lane-changing Intention Prediction Powered by\n  Brain-inspired Spiking Neural Networks"
                },
                "summary": "Accurate and real-time prediction of surrounding vehicles' lane-changing\nintentions is a critical challenge in deploying safe and efficient autonomous\ndriving systems in open-world scenarios. Existing high-performing methods\nremain hard to deploy due to their high computational cost, long training\ntimes, and excessive memory requirements. Here, we propose an efficient\nlane-changing intention prediction approach based on brain-inspired Spiking\nNeural Networks (SNN). By leveraging the event-driven nature of SNN, the\nproposed approach enables us to encode the vehicle's states in a more efficient\nmanner. Comparison experiments conducted on HighD and NGSIM datasets\ndemonstrate that our method significantly improves training efficiency and\nreduces deployment costs while maintaining comparable prediction accuracy.\nParticularly, compared to the baseline, our approach reduces training time by\n75% and memory usage by 99.9%. These results validate the efficiency and\nreliability of our method in lane-changing predictions, highlighting its\npotential for safe and efficient autonomous driving systems while offering\nsignificant advantages in deployment, including reduced training time, lower\nmemory usage, and faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and real-time prediction of surrounding vehicles' lane-changing\nintentions is a critical challenge in deploying safe and efficient autonomous\ndriving systems in open-world scenarios. Existing high-performing methods\nremain hard to deploy due to their high computational cost, long training\ntimes, and excessive memory requirements. Here, we propose an efficient\nlane-changing intention prediction approach based on brain-inspired Spiking\nNeural Networks (SNN). By leveraging the event-driven nature of SNN, the\nproposed approach enables us to encode the vehicle's states in a more efficient\nmanner. Comparison experiments conducted on HighD and NGSIM datasets\ndemonstrate that our method significantly improves training efficiency and\nreduces deployment costs while maintaining comparable prediction accuracy.\nParticularly, compared to the baseline, our approach reduces training time by\n75% and memory usage by 99.9%. These results validate the efficiency and\nreliability of our method in lane-changing predictions, highlighting its\npotential for safe and efficient autonomous driving systems while offering\nsignificant advantages in deployment, including reduced training time, lower\nmemory usage, and faster inference."
                },
                "authors": [
                    {
                        "name": "Shuqi Shen"
                    },
                    {
                        "name": "Junjie Yang"
                    },
                    {
                        "name": "Hui Zhong"
                    },
                    {
                        "name": "Qiming Zhang"
                    },
                    {
                        "name": "Hongliang Lu"
                    },
                    {
                        "name": "Hai Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hai Yang"
                },
                "author": "Hai Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08966v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08966v2",
                "updated": "2025-02-14T04:16:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    4,
                    16,
                    40,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-13T05:06:22Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    6,
                    22,
                    3,
                    44,
                    0
                ],
                "title": "RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage"
                },
                "summary": "Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external\ntools for tasks beyond their standalone capabilities, such as searching\nwebsites, booking flights, or making financial transactions. However, these\ntools greatly increase the risks of prompt injection attacks, where malicious\ncontent hijacks the LM agent to leak confidential data or trigger harmful\nactions. Existing defenses (OpenAI GPTs) require user confirmation before every\ntool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS),\nwhich automatically detects and executes tool calls that preserve integrity and\nconfidentiality, requiring user confirmation only when these safeguards cannot\nbe ensured. RTBAS adapts Information Flow Control to the unique challenges\npresented by TBAS. We present two novel dependency screeners, using\nLM-as-a-judge and attention-based saliency, to overcome these challenges.\nExperimental results on the AgentDojo Prompt Injection benchmark show RTBAS\nprevents all targeted attacks with only a 2% loss of task utility when under\nattack, and further tests confirm its ability to obtain near-oracle performance\non detecting both subtle and direct privacy leaks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external\ntools for tasks beyond their standalone capabilities, such as searching\nwebsites, booking flights, or making financial transactions. However, these\ntools greatly increase the risks of prompt injection attacks, where malicious\ncontent hijacks the LM agent to leak confidential data or trigger harmful\nactions. Existing defenses (OpenAI GPTs) require user confirmation before every\ntool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS),\nwhich automatically detects and executes tool calls that preserve integrity and\nconfidentiality, requiring user confirmation only when these safeguards cannot\nbe ensured. RTBAS adapts Information Flow Control to the unique challenges\npresented by TBAS. We present two novel dependency screeners, using\nLM-as-a-judge and attention-based saliency, to overcome these challenges.\nExperimental results on the AgentDojo Prompt Injection benchmark show RTBAS\nprevents all targeted attacks with only a 2% loss of task utility when under\nattack, and further tests confirm its ability to obtain near-oracle performance\non detecting both subtle and direct privacy leaks."
                },
                "authors": [
                    {
                        "name": "Peter Yong Zhong"
                    },
                    {
                        "name": "Siyuan Chen"
                    },
                    {
                        "name": "Ruiqi Wang"
                    },
                    {
                        "name": "McKenna McCall"
                    },
                    {
                        "name": "Ben L. Titzer"
                    },
                    {
                        "name": "Heather Miller"
                    },
                    {
                        "name": "Phillip B. Gibbons"
                    }
                ],
                "author_detail": {
                    "name": "Phillip B. Gibbons"
                },
                "author": "Phillip B. Gibbons",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08966v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08966v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09896v1",
                "updated": "2025-02-14T04:00:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    4,
                    0,
                    18,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T04:00:18Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    4,
                    0,
                    18,
                    4,
                    45,
                    0
                ],
                "title": "ChatIoT: Large Language Model-based Security Assistant for Internet of\n  Things with Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatIoT: Large Language Model-based Security Assistant for Internet of\n  Things with Retrieval-Augmented Generation"
                },
                "summary": "Internet of Things (IoT) has gained widespread popularity, revolutionizing\nindustries and daily life. However, it has also emerged as a prime target for\nattacks. Numerous efforts have been made to improve IoT security, and\nsubstantial IoT security and threat information, such as datasets and reports,\nhave been developed. However, existing research often falls short in leveraging\nthese insights to assist or guide users in harnessing IoT security practices in\na clear and actionable way. In this paper, we propose ChatIoT, a large language\nmodel (LLM)-based IoT security assistant designed to disseminate IoT security\nand threat intelligence. By leveraging the versatile property of\nretrieval-augmented generation (RAG), ChatIoT successfully integrates the\nadvanced language understanding and reasoning capabilities of LLM with\nfast-evolving IoT security information. Moreover, we develop an end-to-end data\nprocessing toolkit to handle heterogeneous datasets. This toolkit converts\ndatasets of various formats into retrievable documents and optimizes chunking\nstrategies for efficient retrieval. Additionally, we define a set of common use\ncase specifications to guide the LLM in generating answers aligned with users'\nspecific needs and expertise levels. Finally, we implement a prototype of\nChatIoT and conduct extensive experiments with different LLMs, such as LLaMA3,\nLLaMA3.1, and GPT-4o. Experimental evaluations demonstrate that ChatIoT can\ngenerate more reliable, relevant, and technical in-depth answers for most use\ncases. When evaluating the answers with LLaMA3:70B, ChatIoT improves the above\nmetrics by over 10% on average, particularly in relevance and technicality,\ncompared to using LLMs alone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internet of Things (IoT) has gained widespread popularity, revolutionizing\nindustries and daily life. However, it has also emerged as a prime target for\nattacks. Numerous efforts have been made to improve IoT security, and\nsubstantial IoT security and threat information, such as datasets and reports,\nhave been developed. However, existing research often falls short in leveraging\nthese insights to assist or guide users in harnessing IoT security practices in\na clear and actionable way. In this paper, we propose ChatIoT, a large language\nmodel (LLM)-based IoT security assistant designed to disseminate IoT security\nand threat intelligence. By leveraging the versatile property of\nretrieval-augmented generation (RAG), ChatIoT successfully integrates the\nadvanced language understanding and reasoning capabilities of LLM with\nfast-evolving IoT security information. Moreover, we develop an end-to-end data\nprocessing toolkit to handle heterogeneous datasets. This toolkit converts\ndatasets of various formats into retrievable documents and optimizes chunking\nstrategies for efficient retrieval. Additionally, we define a set of common use\ncase specifications to guide the LLM in generating answers aligned with users'\nspecific needs and expertise levels. Finally, we implement a prototype of\nChatIoT and conduct extensive experiments with different LLMs, such as LLaMA3,\nLLaMA3.1, and GPT-4o. Experimental evaluations demonstrate that ChatIoT can\ngenerate more reliable, relevant, and technical in-depth answers for most use\ncases. When evaluating the answers with LLaMA3:70B, ChatIoT improves the above\nmetrics by over 10% on average, particularly in relevance and technicality,\ncompared to using LLMs alone."
                },
                "authors": [
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Yan Lin Aung"
                    },
                    {
                        "name": "Sudipta Chattopadhyay"
                    },
                    {
                        "name": "Jianying Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jianying Zhou"
                },
                "author": "Jianying Zhou",
                "arxiv_comment": "preprint, under revision, 19 pages, 13 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.00228v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.00228v3",
                "updated": "2025-02-14T04:00:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    4,
                    0,
                    8,
                    4,
                    45,
                    0
                ],
                "published": "2023-04-01T05:04:06Z",
                "published_parsed": [
                    2023,
                    4,
                    1,
                    5,
                    4,
                    6,
                    5,
                    91,
                    0
                ],
                "title": "Accuracy and Political Bias of News Source Credibility Ratings by Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accuracy and Political Bias of News Source Credibility Ratings by Large\n  Language Models"
                },
                "summary": "Search engines increasingly leverage large language models (LLMs) to generate\ndirect answers, and AI chatbots now access the Internet for fresh data. As\ninformation curators for billions of users, LLMs must assess the accuracy and\nreliability of different sources. This paper audits nine widely used LLMs from\nthree leading providers -- OpenAI, Google, and Meta -- to evaluate their\nability to discern credible and high-quality information sources from\nlow-credibility ones. We find that while LLMs can rate most tested news\noutlets, larger models more frequently refuse to provide ratings due to\ninsufficient information, whereas smaller models are more prone to making\nerrors in their ratings. For sources where ratings are provided, LLMs exhibit a\nhigh level of agreement among themselves (average Spearman's $\\rho = 0.79$),\nbut their ratings align only moderately with human expert evaluations (average\n$\\rho = 0.50$). Analyzing news sources with different political leanings in the\nUS, we observe a liberal bias in credibility ratings yielded by all LLMs in\ndefault configurations. Additionally, assigning partisan roles to LLMs\nconsistently induces strong politically congruent bias in their ratings. These\nfindings have important implications for the use of LLMs in curating news and\npolitical information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search engines increasingly leverage large language models (LLMs) to generate\ndirect answers, and AI chatbots now access the Internet for fresh data. As\ninformation curators for billions of users, LLMs must assess the accuracy and\nreliability of different sources. This paper audits nine widely used LLMs from\nthree leading providers -- OpenAI, Google, and Meta -- to evaluate their\nability to discern credible and high-quality information sources from\nlow-credibility ones. We find that while LLMs can rate most tested news\noutlets, larger models more frequently refuse to provide ratings due to\ninsufficient information, whereas smaller models are more prone to making\nerrors in their ratings. For sources where ratings are provided, LLMs exhibit a\nhigh level of agreement among themselves (average Spearman's $\\rho = 0.79$),\nbut their ratings align only moderately with human expert evaluations (average\n$\\rho = 0.50$). Analyzing news sources with different political leanings in the\nUS, we observe a liberal bias in credibility ratings yielded by all LLMs in\ndefault configurations. Additionally, assigning partisan roles to LLMs\nconsistently induces strong politically congruent bias in their ratings. These\nfindings have important implications for the use of LLMs in curating news and\npolitical information."
                },
                "authors": [
                    {
                        "name": "Kai-Cheng Yang"
                    },
                    {
                        "name": "Filippo Menczer"
                    }
                ],
                "author_detail": {
                    "name": "Filippo Menczer"
                },
                "author": "Filippo Menczer",
                "arxiv_doi": "10.1145/3717867.3717903",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3717867.3717903",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2304.00228v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.00228v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]