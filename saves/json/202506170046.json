[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.06266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06266v3",
                "updated": "2025-06-13T17:58:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    58,
                    55,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-06T17:48:23Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    17,
                    48,
                    23,
                    4,
                    157,
                    0
                ],
                "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study"
                },
                "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining."
                },
                "authors": [
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "Emily Liu"
                    },
                    {
                        "name": "Will Tennien"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    },
                    {
                        "name": "Christopher Re"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Re"
                },
                "author": "Christopher Re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11970v1",
                "updated": "2025-06-13T17:28:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:28:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    28,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an\n  Efficient in-DRAM Rowhammer Mitigation"
                },
                "summary": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JEDEC has introduced the Per Row Activation Counting (PRAC) framework for\nDDR5 and future DRAMs to enable precise counting of DRAM row activations using\nper-row activation counts. While recent PRAC implementations enable holistic\nmitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the\nincreased DRAM timings for performing a read-modify-write of the counter.\nAlternatively, recent work, Chronus, addresses these slowdowns, but incurs\nenergy overheads due to the additional DRAM activations for counters. In this\npaper, we propose CnC-PRAC, a PRAC implementation that addresses both\nperformance and energy overheads. Unlike prior works focusing on caching\nactivation counts to reduce their overheads, our key idea is to reorder and\ncoalesce accesses to activation counts located in the same physical row. Our\ndesign achieves this by decoupling counter access from the critical path of\ndata accesses. This enables optimizations such as buffering counter\nread-modify-write requests and coalescing requests to the same row. Together,\nthese enable a reduction in row activations for counter accesses by almost\n75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC\nimplementation with negligible slowdown and a minimal dynamic energy overhead\nof 0.84%-1% compared to insecure DDR5 DRAM."
                },
                "authors": [
                    {
                        "name": "Chris S. Lin"
                    },
                    {
                        "name": "Jeonghyun Woo"
                    },
                    {
                        "name": "Prashant J. Nair"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "8 pages, including appendices. The paper is presented at DRAMSec\n  2025. (see https://dramsec.ethz.ch/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11886v1",
                "updated": "2025-06-13T15:35:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:35:54Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache"
                },
                "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Siyang He"
                    },
                    {
                        "name": "Qiqi Wang"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "10 pages, 7 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04065v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04065v4",
                "updated": "2025-06-13T08:32:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    32,
                    26,
                    4,
                    164,
                    0
                ],
                "published": "2024-05-07T07:14:38Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    7,
                    14,
                    38,
                    1,
                    128,
                    0
                ],
                "title": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference"
                },
                "summary": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost."
                },
                "authors": [
                    {
                        "name": "Runheng Liu"
                    },
                    {
                        "name": "Xingchen Xiao"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Zhijing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Wu"
                },
                "author": "Zhijing Wu",
                "arxiv_comment": "ACL 2025 Findings, 14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04065v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04065v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v2",
                "updated": "2025-06-13T07:04:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    7,
                    4,
                    46,
                    4,
                    164,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based\n  QA Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap,\nare essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, an extensible open-source framework that streamlines the\ncreation of reproducible, traceable map-based QA datasets. MapQaTor enables\nseamless integration with any maps API, allowing users to gather and visualize\ndata from diverse sources with minimal setup. By caching API responses, the\nplatform ensures consistent ground truth, enhancing the reliability of the data\neven as real-world information evolves. MapQaTor centralizes data retrieval,\nannotation, and visualization within a single platform, offering a unique\nopportunity to evaluate the current state of LLM-based geospatial reasoning\nwhile advancing their capabilities for improved geospatial understanding.\nEvaluation metrics show that, MapQaTor speeds up the annotation process by at\nleast 30 times compared to manual methods, underscoring its potential for\ndeveloping geospatial resources, such as complex map reasoning datasets. The\nwebsite is live at: https://mapqator.github.io/ and a demo video is available\nat: https://youtu.be/bVv7-NYRsTw."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "ACL 2025 (Demo)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11498v1",
                "updated": "2025-06-13T06:49:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T06:49:53Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    6,
                    49,
                    53,
                    4,
                    164,
                    0
                ],
                "title": "Lag-Relative Sparse Attention In Long Context Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lag-Relative Sparse Attention In Long Context Training"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Wanyi Huang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10657v2",
                "updated": "2025-06-13T02:54:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    54,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T12:46:49Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    12,
                    46,
                    49,
                    3,
                    163,
                    0
                ],
                "title": "Electric field control of third-order nonlinear Hall effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of third-order nonlinear Hall effect"
                },
                "summary": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of\nenergy band geometric property, providing a new paradigm for revealing the\nBerry curvature distribution and topological response of quantum materials. In\nthe Weyl semimetal TaIrTe4, we report for the first time that the sign of the\nthird-order NLHE reverses with decreasing temperature. Through scaling law\nanalysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23\nK) temperatures is dominated by Berry-connection polarizability (BCP) and\nimpurity scattering, respectively. The third-order NLHE response strength can\nbe effectively modulated by an additional applied in-plane constant electric\nfield. At the high temperature region, the BCP reduction induced by the\nelectric field leads to a decrease in the third-order NLHE response strength,\nwhile at the low temperature region, the electric field cause both BCP and\nimpurity scattering effects to weaken, resulting in a more significant\nmodulation of the third-order NLHE response strength. At 4 K and an electric\nfield strength of 0.3 kV/cm, the modulated relative response strength could\nreach up to 65.3%. This work provides a new means to explore the third-order\nNLHE and a valuable reference for the development of novel electronic devices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Jiarui Chen"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "20 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11418v1",
                "updated": "2025-06-13T02:36:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T02:36:15Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    36,
                    15,
                    4,
                    164,
                    0
                ],
                "title": "Efficient Long-Context LLM Inference via KV Cache Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Context LLM Inference via KV Cache Clustering"
                },
                "summary": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Shengnan Wang"
                    },
                    {
                        "name": "Yutong He"
                    },
                    {
                        "name": "Ping Gong"
                    },
                    {
                        "name": "Jiawei Yi"
                    },
                    {
                        "name": "Juncheng Zhang"
                    },
                    {
                        "name": "Youhui Bai"
                    },
                    {
                        "name": "Renhai Chen"
                    },
                    {
                        "name": "Gong Zhang"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Kun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yuan"
                },
                "author": "Kun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10848v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10848v2",
                "updated": "2025-06-13T02:28:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    2,
                    28,
                    47,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T16:08:28Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    16,
                    8,
                    28,
                    3,
                    163,
                    0
                ],
                "title": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Large Language Models with SlowFast Sampling: The\n  Three Golden Principles"
                },
                "summary": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models (dLLMs) have emerged as a promising\nalternative to traditional autoregressive LLMs by enabling parallel token\ngeneration and significantly reducing inference latency. However, existing\nsampling strategies for dLLMs, such as confidence-based or semi-autoregressive\ndecoding, often suffer from static behavior, leading to suboptimal efficiency\nand limited flexibility. In this paper, we propose SlowFast Sampling, a novel\ndynamic sampling strategy that adaptively alternates between exploratory and\naccelerated decoding stages. Our method is guided by three golden principles:\ncertainty principle, convergence principle, and positional principle, which\ngovern when and where tokens can be confidently and efficiently decoded. We\nfurther integrate our strategy with dLLM-Cache to reduce redundant computation.\nExtensive experiments across benchmarks and models show that SlowFast Sampling\nachieves up to 15.63$\\times$ speedup on LLaDA with minimal accuracy drop, and\nup to 34.22$\\times$ when combined with caching. Notably, our approach\noutperforms strong autoregressive baselines like LLaMA3 8B in throughput,\ndemonstrating that well-designed sampling can unlock the full potential of\ndLLMs for fast and high-quality generation."
                },
                "authors": [
                    {
                        "name": "Qingyan Wei"
                    },
                    {
                        "name": "Yaojie Zhang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages; 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10848v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10848v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11329v1",
                "updated": "2025-06-12T21:57:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:57:27Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    57,
                    27,
                    3,
                    163,
                    0
                ],
                "title": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A4: Microarchitecture-Aware LLC Management for Datacenter Servers with\n  Emerging I/O Devices"
                },
                "summary": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim\ncache for higher-level private caches but also as a buffer for low-latency DMA\ntransfers between CPU cores and I/O devices through Direct Cache Access (DCA).\nHowever, prior work has shown that high-bandwidth network-I/O devices can\nrapidly flood the LLC with packets, often causing significant contention with\nco-running workloads. One step further, this work explores hidden\nmicroarchitectural properties of the Intel Xeon CPUs, uncovering two previously\nunrecognized LLC contentions triggered by emerging high-bandwidth I/O devices.\nSpecifically, (C1) DMA-written cache lines in LLC ways designated for DCA\n(referred to as DCA ways) are migrated to certain LLC ways (denoted as\ninclusive ways) when accessed by CPU cores, unexpectedly contending with\nnon-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth\nstorage-I/O devices, which are increasingly common in datacenter servers,\nbenefit little from DCA while contending with (latency-sensitive) network-I/O\ndevices within DCA ways. To this end, we present \\design, a runtime LLC\nmanagement framework designed to alleviate both (C1) and (C2) among diverse\nco-running workloads, using a hidden knob and other hardware features\nimplemented in those CPUs. Additionally, we demonstrate that \\design can also\nalleviate other previously known network-I/O-driven LLC contentions. Overall,\nit improves the performance of latency-sensitive, high-priority workloads by\n51\\% without notably compromising that of low-priority workloads."
                },
                "authors": [
                    {
                        "name": "Haneul Park"
                    },
                    {
                        "name": "Jiaqi Lou"
                    },
                    {
                        "name": "Sangjin Lee"
                    },
                    {
                        "name": "Yifan Yuan"
                    },
                    {
                        "name": "Kyoung Soo Park"
                    },
                    {
                        "name": "Yongseok Son"
                    },
                    {
                        "name": "Ipoom Jeong"
                    },
                    {
                        "name": "Nam Sung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Nam Sung Kim"
                },
                "author": "Nam Sung Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11309v1",
                "updated": "2025-06-12T21:15:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "published": "2025-06-12T21:15:58Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    21,
                    15,
                    58,
                    3,
                    163,
                    0
                ],
                "title": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous\n  Speculative Decoding"
                },
                "summary": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency decoding for large language models (LLMs) is crucial for\napplications like chatbots and code assistants, yet generating long outputs\nremains slow in single-query settings. Prior work on speculative decoding\n(which combines a small draft model with a larger target model) and tensor\nparallelism has each accelerated decoding. However, conventional approaches\nfail to apply both simultaneously due to imbalanced compute requirements\n(between draft and target models), KV-cache inconsistencies, and communication\noverheads under small-batch tensor-parallelism. This paper introduces\nSwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec\nredesigns the speculative decoding pipeline in an asynchronous and\ndisaggregated manner, so that each component can be scaled flexibly and remove\ndraft overhead from the critical path. To realize this design, SwiftSpec\nproposes parallel tree generation, tree-aware KV cache management, and fused,\nlatency-optimized kernels to overcome the challenges listed above. Across 5\nmodel families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup\nover state-of-the-art speculative decoding systems and, as a highlight, serves\nLlama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known\nsystem for low-latency LLM serving at this scale."
                },
                "authors": [
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Ziheng Jiang"
                    },
                    {
                        "name": "Chengquan Jiang"
                    },
                    {
                        "name": "Menghan Yu"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Haibin Lin"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v4",
                "updated": "2025-06-12T20:38:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    20,
                    38,
                    42,
                    3,
                    163,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Main Memory-Based Covert and Side Channel Attacks in the\n  Context of Processing-in-Memory"
                },
                "summary": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of processing-in-memory (PiM)\narchitectures to establish covert and side channels. IMPACT enables\nhigh-throughput communication and private information leakage by exploiting the\nshared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates\nexpensive cache bypassing steps required by processor-centric memory-based\ntiming attacks and (ii) leverages the intrinsic parallelism of PiM operations.\nWe showcase two applications of IMPACT. First, we build two covert channels\nthat leverage different PiM approaches (i.e., processing-near-memory and\nprocessing-using-memory) to establish high-throughput covert communication\nchannels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication\nthroughput, respectively, which is 3.6x and 6.5x higher than the\nstate-of-the-art main memory-based covert channel. Second, we showcase a\nside-channel attack that leaks private information of concurrently-running\nvictim applications with a low error rate. Our source-code is openly and freely\navailable at https://github.com/CMU-SAFARI/IMPACT."
                },
                "authors": [
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "arxiv_comment": "DSN 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v4",
                "updated": "2025-06-12T13:33:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    13,
                    33,
                    52,
                    3,
                    163,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely adopted due to their remarkable\nperformance across various applications, driving the accelerated development of\na large number of diverse models. However, these individual LLMs show\nlimitations in generalization and performance on complex tasks due to inherent\ntraining biases, model size constraints, and the quality or diversity of\npre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13\\% on GSM8K and 70\\% on\nMMLU, compared to the top-performing baseline. Also, we establish a theoretical\nupper bound by an Oracle with LLMs and perform an in-depth linguistic analysis\nto understand the performance gap between the Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "ACL 2025 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v5",
                "updated": "2025-06-12T11:45:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    45,
                    57,
                    3,
                    163,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present TransMLA, a framework that seamlessly converts any\nGQA-based pre-trained model into an MLA-based model. Our approach enables\ndirect compatibility with DeepSeek's codebase, allowing these models to fully\nleverage DeepSeek-specific optimizations such as vLLM and SGlang. By\ncompressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x\ninference speedup at an 8K context length while preserving meaningful output\nquality. Additionally, the model requires only 6 billion tokens for fine-tuning\nto regain performance on par with the original across multiple benchmarks.\nTransMLA offers a practical solution for migrating GQA-based models to the MLA\nstructure. When combined with DeepSeek's advanced features, such as FP8\nquantization and Multi-Token Prediction, even greater inference acceleration\ncan be realized."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17911v2",
                "updated": "2025-06-12T11:26:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    11,
                    26,
                    10,
                    3,
                    163,
                    0
                ],
                "published": "2025-03-23T03:16:50Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    3,
                    16,
                    50,
                    6,
                    82,
                    0
                ],
                "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest\n  Neighbor Search"
                },
                "summary": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector\ndatabases and AI infrastructures. Recent graph-based ANNS algorithms have\nachieved high search accuracy with practical efficiency. Despite the\nadvancements, these algorithms still face performance bottlenecks in\nproduction, due to the random memory access patterns of graph-based search and\nthe high computational overheads of vector distance. In addition, the\nperformance of a graph-based ANNS algorithm is highly sensitive to parameters,\nwhile selecting the optimal parameters is cost-prohibitive, e.g., manual tuning\nrequires repeatedly re-building the index.\n  This paper introduces VSAG, an open-source framework that aims to enhance the\nin production performance of graph-based ANNS algorithms. VSAG has been\ndeployed at scale in the services of Ant Group, and it incorporates three key\noptimizations: (i) efficient memory access: it reduces L3 cache misses with\npre-fetching and cache-friendly vector organization; (ii) automated parameter\ntuning: it automatically selects performance-optimal parameters without\nrequiring index rebuilding; (iii) efficient distance computation: it leverages\nmodern hardware, scalar quantization, and smartly switches to low-precision\nrepresentation to dramatically reduce the distance computation costs. We\nevaluate VSAG on real-world datasets. The experimental results show that VSAG\nachieves the state-of-the-art performance and provides up to 4x speedup over\nHNSWlib (an industry-standard library) while ensuring the same accuracy."
                },
                "authors": [
                    {
                        "name": "Xiaoyao Zhong"
                    },
                    {
                        "name": "Haotian Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Deming Chu"
                    },
                    {
                        "name": "Xiangyu Wang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "George Gu"
                    },
                    {
                        "name": "Yi Xie"
                    },
                    {
                        "name": "Xuemin Lin"
                    },
                    {
                        "name": "Heng Tao Shen"
                    },
                    {
                        "name": "Jingkuan Song"
                    },
                    {
                        "name": "Peng Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Peng Cheng"
                },
                "author": "Peng Cheng",
                "arxiv_comment": "the report of open-source library VSAG\n  (https://github.com/antgroup/vsag)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11695v2",
                "updated": "2025-06-12T00:25:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    12,
                    0,
                    25,
                    14,
                    3,
                    163,
                    0
                ],
                "published": "2025-05-16T21:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    21,
                    4,
                    25,
                    4,
                    136,
                    0
                ],
                "title": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qronos: Correcting the Past by Shaping the Future... in Post-Training\n  Quantization"
                },
                "summary": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qronos -- a new state-of-the-art post-training quantization\nalgorithm that sequentially rounds and updates neural network weights. Qronos\nnot only explicitly corrects errors due to both weight and activation\nquantization, but also errors resulting from quantizing previous layers. Our\niterative algorithm is based on an interpretable and disciplined optimization\nframework that subsumes and surpasses existing data-driven approaches. At each\nstep, Qronos alternates between error correction and diffusion via optimal\nupdate rules. Importantly, we prove that Qronos admits an efficient\nimplementation that uses the Cholesky decomposition for solving least-squares\nproblems. We also demonstrate that Qronos is compatible with existing\ntransformation techniques such as Hadamard-based incoherence processing and\nweight-activation scaling equalization, among others. We evaluate Qronos using\nrecent autoregressive language generation models in the Llama3 family; Qronos\nconsistently outperforms previous state-of-the-art adaptive rounding methods\nwhen quantizing the weights, activations, and/or KV caches."
                },
                "authors": [
                    {
                        "name": "Shihao Zhang"
                    },
                    {
                        "name": "Haoyu Zhang"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Rayan Saab"
                    }
                ],
                "author_detail": {
                    "name": "Rayan Saab"
                },
                "author": "Rayan Saab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v3",
                "updated": "2025-06-11T22:50:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    22,
                    50,
                    44,
                    2,
                    162,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input context\nin order to perform complex tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nin order to process user inputs quickly, as they are received. We propose\nSqueezed Attention to accelerate LLM applications where a large portion of the\ninput context is fixed. We first leverage K-means clustering offline to group\nthe keys for the fixed context based on semantic similarity and represent each\ncluster with a single centroid value. During inference, we compare query tokens\nfrom the user input with the centroids to predict which keys from the fixed\ncontext are semantically relevant, and then compute exact attention using only\nthe important keys, thereby reducing bandwidth and computational costs. We also\npresent a hierarchical version of our algorithm which can reduce the complexity\nof attention from linear to logarithmic with respect to the fixed context\nlength. We evaluate our method on long-context benchmarks including LongBench,\nwhere it achieves a 3.1$\\times$ reduction in KV budget with no noticeable\naccuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy\ngap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.\nFuthermore, we implement kernels for centroid comparison and sparse\nFlashAttention with important keys, achieving more than 4$\\times$ speedups\nduring both the prefill and generation phases for long-context inference. Our\ncode is available at https://github.com/SqueezeAILab/SqueezedAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input context\nin order to perform complex tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nin order to process user inputs quickly, as they are received. We propose\nSqueezed Attention to accelerate LLM applications where a large portion of the\ninput context is fixed. We first leverage K-means clustering offline to group\nthe keys for the fixed context based on semantic similarity and represent each\ncluster with a single centroid value. During inference, we compare query tokens\nfrom the user input with the centroids to predict which keys from the fixed\ncontext are semantically relevant, and then compute exact attention using only\nthe important keys, thereby reducing bandwidth and computational costs. We also\npresent a hierarchical version of our algorithm which can reduce the complexity\nof attention from linear to logarithmic with respect to the fixed context\nlength. We evaluate our method on long-context benchmarks including LongBench,\nwhere it achieves a 3.1$\\times$ reduction in KV budget with no noticeable\naccuracy loss and up to an 8$\\times$ reduction with only a 0.5 point accuracy\ngap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models.\nFuthermore, we implement kernels for centroid comparison and sparse\nFlashAttention with important keys, achieving more than 4$\\times$ speedups\nduring both the prefill and generation phases for long-context inference. Our\ncode is available at https://github.com/SqueezeAILab/SqueezedAttention."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "22 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v2",
                "updated": "2025-06-11T21:59:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    21,
                    59,
                    20,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10100v1",
                "updated": "2025-06-11T18:34:57Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    18,
                    34,
                    57,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T18:34:57Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    18,
                    34,
                    57,
                    2,
                    162,
                    0
                ],
                "title": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark."
                },
                "authors": [
                    {
                        "name": "Yantai Yang"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Luo Zhongwei"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Zhipeng Zhang"
                    },
                    {
                        "name": "Chuan Wen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09758v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09758v1",
                "updated": "2025-06-11T14:03:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T14:03:13Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    14,
                    3,
                    13,
                    2,
                    162,
                    0
                ],
                "title": "Mainframe-style channel controllers for modern disaggregated memory\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe-style channel controllers for modern disaggregated memory\n  systems"
                },
                "summary": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promise of alleviating the main memory bottleneck, and the\nexistence of commercial hardware implementations, techniques for Near-Data\nProcessing have seen relatively little real-world deployment. The idea has\nreceived renewed interest with the appearance of disaggregated or \"far\" memory,\nfor example in the use of CXL memory pools.\n  However, we argue that the lack of a clear OS-centric abstraction of\nNear-Data Processing is a major barrier to adoption of the technology. Inspired\nby the channel controllers which interface the CPU to disk drives in mainframe\nsystems, we propose memory channel controllers as a convenient, portable, and\nvirtualizable abstraction of Near-Data Processing for modern disaggregated\nmemory systems.\n  In addition to providing a clean abstraction that enables OS integration\nwhile requiring no changes to CPU architecture, memory channel controllers\nincorporate another key innovation: they exploit the cache coherence provided\nby emerging interconnects to provide a much richer programming model, with more\nfine-grained interaction, than has been possible with existing designs."
                },
                "authors": [
                    {
                        "name": "Zikai Liu"
                    },
                    {
                        "name": "Jasmin Schult"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09758v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09758v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09536v1",
                "updated": "2025-06-11T09:08:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T09:08:59Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "title": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy"
                },
                "summary": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped and constructed the first prototype of a compact line-focus X-ray\ntube (LFXT) with technology potentially suitable for clinical translation of\nminibeams and microbeams. We give an overview of the commissioning process\npreceding the first operation, present optical and radiological focal spot\ncharacterization methods, and dosimetric measurements. Additionally, we report\non first preclinical in vitro cell and in vivo mouse brain irradiations\nconducted with the LFXT prototype. The focal spot characterization resulted in\na strongly eccentric electron distribution with a width of 72.3 $\\mu$m.\nDosimetry showed sharp microbeam dose profiles with steep lateral penumbras and\na peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An\nopen-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150\nkV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In\nvitro and in vivo experiments demonstrated the feasibility of the LFXT for\nminibeam and microbeam applications with field sizes of 1.5-2 cm. The mice\ndisplayed no observable side effects throughout the follow-up period after\nwhole-brain 260 $\\mu$m-minibeam irradiation. We successfully constructed and\ncommissioned the first proof-of-concept LFXT prototype. Dosimetric\ncharacterizations of the achieved microbeam field showed the superiority of the\nLFXT compared to conventional X-ray tubes in terms of beam quality. In future\ndevelopments, the remaining limitations of the prototype will be addressed for\nimproved minibeam and first ever microbeam radiation therapy in a clinical\nsetting."
                },
                "authors": [
                    {
                        "name": "Christian Petrich"
                    },
                    {
                        "name": "Johanna Winter"
                    },
                    {
                        "name": "Anton Dimroth"
                    },
                    {
                        "name": "Thomas Beiser"
                    },
                    {
                        "name": "Monika Dehn"
                    },
                    {
                        "name": "Jessica Stolz"
                    },
                    {
                        "name": "Jacopo Frignani"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Franz Schilling"
                    },
                    {
                        "name": "Ghaleb Natour"
                    },
                    {
                        "name": "Kurt Aulenbacher"
                    },
                    {
                        "name": "Thomas E. Schmid"
                    },
                    {
                        "name": "Jan J. Wilkens"
                    },
                    {
                        "name": "Stefan Bartzsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bartzsch"
                },
                "author": "Stefan Bartzsch",
                "arxiv_comment": "CP, JW, and AD share first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v2",
                "updated": "2025-06-11T06:01:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    6,
                    1,
                    15,
                    2,
                    162,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07564v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07564v3",
                "updated": "2025-06-11T03:14:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    14,
                    10,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-09T09:04:37Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    9,
                    4,
                    37,
                    0,
                    160,
                    0
                ],
                "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional\n  Autonomous Agent Systems"
                },
                "summary": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy."
                },
                "authors": [
                    {
                        "name": "Peiran Li"
                    },
                    {
                        "name": "Xinkai Zou"
                    },
                    {
                        "name": "Zhuohang Wu"
                    },
                    {
                        "name": "Ruifeng Li"
                    },
                    {
                        "name": "Shuo Xing"
                    },
                    {
                        "name": "Hanwen Zheng"
                    },
                    {
                        "name": "Zhikai Hu"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Haoxi Li"
                    },
                    {
                        "name": "Qin Yuan"
                    },
                    {
                        "name": "Yingmo Zhang"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhengzhong Tu"
                },
                "author": "Zhengzhong Tu",
                "arxiv_comment": "Former versions either contain unrelated content or cannot be\n  properly converted to PDF",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07564v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07564v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09350v1",
                "updated": "2025-06-11T03:04:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T03:04:23Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    3,
                    4,
                    23,
                    2,
                    162,
                    0
                ],
                "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation"
                },
                "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2"
                },
                "authors": [
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Jianwen Jiang"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Xuefeng Xiao"
                    },
                    {
                        "name": "Lu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lu Jiang"
                },
                "author": "Lu Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09342v1",
                "updated": "2025-06-11T02:48:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    48,
                    16,
                    2,
                    162,
                    0
                ],
                "published": "2025-06-11T02:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    48,
                    16,
                    2,
                    162,
                    0
                ],
                "title": "Latent Multi-Head Attention for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Multi-Head Attention for Small Language Models"
                },
                "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "arxiv_comment": "6 pages, 1 figure. 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04593v3",
                "updated": "2025-06-14T00:52:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    0,
                    52,
                    10,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-05T03:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    16,
                    51,
                    3,
                    156,
                    0
                ],
                "title": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning Assisted Edge Caching Scheme Based on Lightweight\n  Architecture DDPM"
                },
                "summary": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge caching is an emerging technology that empowers caching units at edge\nnodes, allowing users to fetch contents of interest that have been pre-cached\nat the edge nodes. The key to pre-caching is to maximize the cache hit\npercentage for cached content without compromising users' privacy. In this\nletter, we propose a federated learning (FL) assisted edge caching scheme based\non lightweight architecture denoising diffusion probabilistic model (LDPM). Our\nsimulation results verify that our proposed scheme achieves a higher cache hit\npercentage compared to existing FL-based methods and baseline methods."
                },
                "authors": [
                    {
                        "name": "Xun Li"
                    },
                    {
                        "name": "Qiong Wu"
                    },
                    {
                        "name": "Pingyi Fan"
                    },
                    {
                        "name": "Kezhi Wang"
                    },
                    {
                        "name": "Nan Cheng"
                    },
                    {
                        "name": "Khaled B. Letaief"
                    }
                ],
                "author_detail": {
                    "name": "Khaled B. Letaief"
                },
                "author": "Khaled B. Letaief",
                "arxiv_comment": "This paper has been submitted to IEEE letters. The source code has\n  been released at:\n  https://github.com/qiongwu86/Federated-Learning-Assisted-Edge-Caching-Scheme-Based-on-Lightweight-Architecture-DDPM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09282v1",
                "updated": "2025-06-10T22:46:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    46,
                    12,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T22:46:12Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    46,
                    12,
                    1,
                    161,
                    0
                ],
                "title": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing\n  Inference on Multi-Core CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScalableHD: Scalable and High-Throughput Hyperdimensional Computing\n  Inference on Multi-Core CPUs"
                },
                "summary": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that\nrepresents and manipulates information using high-dimensional vectors, called\nhypervectors (HV). Traditional HDC methods, while robust to noise and\ninherently parallel, rely on single-pass, non-parametric training and often\nsuffer from low accuracy. To address this, recent approaches adopt iterative\ntraining of base and class HVs, typically accelerated on GPUs. Inference,\nhowever, remains lightweight and well-suited for real-time execution. Yet,\nefficient HDC inference has been studied almost exclusively on specialized\nhardware such as FPGAs and GPUs, with limited attention to general-purpose\nmulti-core CPUs. To address this gap, we propose ScalableHD for scalable and\nhigh-throughput HDC inference on multi-core CPUs. ScalableHD employs a\ntwo-stage pipelined execution model, where each stage is parallelized across\ncores and processes chunks of base and class HVs. Intermediate results are\nstreamed between stages using a producer-consumer mechanism, enabling\non-the-fly consumption and improving cache locality. To maximize performance,\nScalableHD integrates memory tiling and NUMA-aware worker-to-core binding.\nFurther, it features two execution variants tailored for small and large batch\nsizes, each designed to exploit compute parallelism based on workload\ncharacteristics while mitigating the memory-bound compute pattern that limits\nHDC inference performance on modern multi-core CPUs. ScalableHD achieves up to\n10x speedup in throughput (samples per second) over state-of-the-art baselines\nsuch as TorchHD, across a diverse set of tasks ranging from human activity\nrecognition to image classification, while preserving task accuracy.\nFurthermore, ScalableHD exhibits robust scalability: increasing the number of\ncores yields near-proportional throughput improvements."
                },
                "authors": [
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IC3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07254v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07254v2",
                "updated": "2025-06-10T22:01:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    22,
                    1,
                    14,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-08T18:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    18,
                    43,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stable Whitening Optimizer for Efficient Neural Network Training"
                },
                "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07254v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07254v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09045v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09045v1",
                "updated": "2025-06-10T17:59:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T17:59:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    59,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagCache: Fast Video Generation with Magnitude-Aware Cache"
                },
                "summary": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing acceleration techniques for video diffusion models often rely on\nuniform heuristics or time-embedding variants to skip timesteps and reuse\ncached features. These approaches typically require extensive calibration with\ncurated prompts and risk inconsistent outputs due to prompt-specific\noverfitting. In this paper, we introduce a novel and robust discovery: a\nunified magnitude law observed across different models and prompts.\nSpecifically, the magnitude ratio of successive residual outputs decreases\nmonotonically and steadily in most timesteps while rapidly in the last several\nsteps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)\nthat adaptively skips unimportant timesteps using an error modeling mechanism\nand adaptive caching strategy. Unlike existing methods requiring dozens of\ncurated samples for calibration, MagCache only requires a single sample for\ncalibration. Experimental results show that MagCache achieves 2.1x and 2.68x\nspeedups on Open-Sora and Wan 2.1, respectively, while preserving superior\nvisual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,\nand PSNR, under comparable computational budgets."
                },
                "authors": [
                    {
                        "name": "Zehong Ma"
                    },
                    {
                        "name": "Longhui Wei"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Qi Tian"
                    }
                ],
                "author_detail": {
                    "name": "Qi Tian"
                },
                "author": "Qi Tian",
                "arxiv_comment": "Project Page: https://zehong-ma.github.io/MagCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09045v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09045v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08842v1",
                "updated": "2025-06-10T14:29:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    29,
                    2,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T14:29:02Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    14,
                    29,
                    2,
                    1,
                    161,
                    0
                ],
                "title": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN\n  Accelerator with Algorithm and Hardware Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN\n  Accelerator with Algorithm and Hardware Co-Design"
                },
                "summary": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for\ntheir event-driven characteristics and high energy efficiency. However, the\ntemporal dependency and irregularity of spikes present significant challenges\nfor hardware parallel processing and data reuse, leading to some existing\naccelerators falling short in processing latency and energy efficiency. To\novercome these challenges, we introduce the STI-SNN accelerator, designed for\nresource-constrained applications with high energy efficiency, flexibility, and\nlow latency. The accelerator is designed through algorithm and hardware\nco-design. Firstly, STI-SNN can perform inference in a single timestep. At the\nalgorithm level, we introduce a temporal pruning approach based on the temporal\nefficient training (TET) loss function. This approach alleviates spike\ndisappearance during timestep reduction, maintains inference accuracy, and\nexpands TET's application. In hardware design, we analyze data access patterns\nand adopt the output stationary (OS) dataflow, eliminating the need to store\nmembrane potentials and access memory operations. Furthermore, based on the OS\ndataflow, we propose a compressed and sorted representation of spikes, then\ncached in the line buffer to reduce the memory access cost and improve reuse\nefficiency. Secondly, STI-SNN supports different convolution methods. By\nadjusting the computation mode of processing elements (PEs) and parameterizing\nthe computation array, STI-SNN can accommodate lightweight models based on\ndepthwise separable convolutions (DSCs), further enhancing hardware\nflexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer\nparallel processing. For inter-layer parallelism, we ..."
                },
                "authors": [
                    {
                        "name": "Kainan Wang"
                    },
                    {
                        "name": "Chengyi Yang"
                    },
                    {
                        "name": "Chengting Yu"
                    },
                    {
                        "name": "Yee Sin Ang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Aili Wang"
                    }
                ],
                "author_detail": {
                    "name": "Aili Wang"
                },
                "author": "Aili Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v4",
                "updated": "2025-06-10T13:50:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    13,
                    50,
                    34,
                    1,
                    161,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. The codebase is at https://github.com/IBM/activated-lora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is inefficient, as the key-value (KV) cache of the entire\nturn history must be recomputed with the LoRA weights before generation can\nbegin. To address this problem, we propose Activated LoRA (aLoRA), an adapter\narchitecture which modifies the LoRA framework to only adapt weights for the\ntokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially\nallows aLoRA to accept the base model's KV cache of the input string, meaning\nthat aLoRA can be instantly activated whenever needed in a chain without\nrecomputing the cache. This enables building what we call \\emph{intrinsics},\ni.e. specialized models invoked to perform well-defined operations on portions\nof an input chain or conversation that otherwise uses the base model by\ndefault. We train a set of aLoRA-based intrinsics models, demonstrating\ncompetitive accuracy with standard LoRA while achieving significant inference\nbenefits. The codebase is at https://github.com/IBM/activated-lora."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08529v1",
                "updated": "2025-06-10T07:49:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    7,
                    49,
                    33,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T07:49:33Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    7,
                    49,
                    33,
                    1,
                    161,
                    0
                ],
                "title": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid\n  Temporal Modeling with Only 4$\\times$RTX 4090s",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid\n  Temporal Modeling with Only 4$\\times$RTX 4090s"
                },
                "summary": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have significantly advanced video super-resolution (VSR) by\nenhancing perceptual quality, largely through elaborately designed temporal\nmodeling to ensure inter-frame consistency. However, existing methods usually\nsuffer from limited temporal coherence and prohibitively high computational\ncosts (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for\nlong videos. In this work, we propose LiftVSR, an efficient VSR framework that\nleverages and elevates the image-wise diffusion prior from PixArt-$\\alpha$,\nachieving state-of-the-art results using only 4$\\times$RTX 4090 GPUs. To\nbalance long-term consistency and efficiency, we introduce a hybrid temporal\nmodeling mechanism that decomposes temporal learning into two complementary\ncomponents: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal\nmodeling within short frame segment ($\\textit{i.e.}$, low complexity), and (ii)\nAttention Memory Cache (AMC) for long-term temporal modeling across segments\n($\\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token\nflows across frames within multi-head query and key tokens to warp inter-frame\ncontexts in the value tokens. AMC adaptively aggregates historical segment\ninformation via a cache unit, ensuring long-term coherence with minimal\noverhead. To further stabilize the cache interaction during inference, we\nintroduce an asymmetric sampling strategy that mitigates feature mismatches\narising from different diffusion sampling steps. Extensive experiments on\nseveral typical VSR benchmarks have demonstrated that LiftVSR achieves\nimpressive performance with significantly lower computational costs."
                },
                "authors": [
                    {
                        "name": "Xijun Wang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Bingchen Li"
                    },
                    {
                        "name": "Zhibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhibo Chen"
                },
                "author": "Zhibo Chen",
                "arxiv_comment": "Project page: https://kopperx.github.io/projects/liftvsr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08373v1",
                "updated": "2025-06-10T02:37:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "published": "2025-06-10T02:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "Draft-based Approximate Inference for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft-based Approximate Inference for LLMs"
                },
                "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v1",
                "updated": "2025-06-09T19:13:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08009v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08009v1",
                "updated": "2025-06-09T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    17,
                    59,
                    55,
                    0,
                    160,
                    0
                ],
                "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion"
                },
                "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/"
                },
                "authors": [
                    {
                        "name": "Xun Huang"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Guande He"
                    },
                    {
                        "name": "Mingyuan Zhou"
                    },
                    {
                        "name": "Eli Shechtman"
                    }
                ],
                "author_detail": {
                    "name": "Eli Shechtman"
                },
                "author": "Eli Shechtman",
                "arxiv_comment": "Project website: http://self-forcing.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08009v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08009v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16886v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16886v2",
                "updated": "2025-06-09T15:31:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    15,
                    31,
                    53,
                    0,
                    160,
                    0
                ],
                "published": "2025-02-24T06:33:39Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    6,
                    33,
                    39,
                    0,
                    55,
                    0
                ],
                "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal\n  Performance"
                },
                "summary": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To alleviate memory burden during inference of large language models (LLMs),\nnumerous studies have focused on compressing the KV cache by exploring aspects\nsuch as attention sparsity. These techniques are often designed with a\npre-defined KV budget; however, as the optimal budget varies by different input\nlengths and task types, the existence of a fixed budget could result in\ninconsistent performance accepting inputs of diverse domains. To address this\nlimitation, we propose a new KV cache compression objective: to always ensure\nthe full-cache performance regardless of specific inputs, while maximizing KV\ncache pruning as much as possible. To achieve this goal, we introduce a novel\nKV cache compression method dubbed DBudgetKV, which features an attention-based\nmetric to signal when the remaining KV cache is unlikely to match the\nfull-cache performance, then halting the pruning process. Empirical evaluation\nspanning diverse context lengths, task types, and model sizes suggests that our\nmethod achieves lossless KV pruning effectively and robustly, exceeding 25%\ncompression ratio on average. Furthermore, our method is easy to integrate\nwithin LLM inference, not only optimizing memory space, but also showing\nreduced inference time compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Xuanfan Ni"
                    },
                    {
                        "name": "Liyan Xu"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Lemao Liu"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Piji Li"
                    }
                ],
                "author_detail": {
                    "name": "Piji Li"
                },
                "author": "Piji Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16886v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16886v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07703v1",
                "updated": "2025-06-09T12:41:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    12,
                    41,
                    31,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T12:41:31Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    12,
                    41,
                    31,
                    0,
                    160,
                    0
                ],
                "title": "$d$-Wave Flat Fermi Surface in Altermagnets Enables Maximum\n  Charge-to-Spin Conversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$d$-Wave Flat Fermi Surface in Altermagnets Enables Maximum\n  Charge-to-Spin Conversion"
                },
                "summary": "Altermagnets combine antiferromagnetic order with ferromagnet-like spin\nsplitting, a duality that unlocks ultrafast spin-dependent responses. This\nunique property creates unprecedented opportunities for spin-current\ngeneration, overcoming the intrinsic limitations of conventional spin-transfer\nand spin-orbit torque approaches in magnetic memory technologies. Here, we\nestablish a fundamental relationship between Fermi surface geometry and\ntime-reversal-odd ($\\mathcal{T}$-odd) spin currents in altermagnets through\ncombined model analysis and first-principles calculations. We demonstrate that\na $d$-wave altermagnet with a flat Fermi surface can achieve a theoretical\nupper limit of charge-to-spin conversion efficiency (CSE) of 100%. This\nmechanism is realized in the newly discovered room-temperature altermagnetic\nmetal KV$_2$O$_2$Se, which exhibits a CSE of $\\sim$78% at the charge neutrality\npoint, nearly double that of RuO$_2$, setting a new record for\n$\\mathcal{T}$-odd CSE. Under electron doping, this efficiency further increases\nto $\\sim$98%, approaching the theoretical limit. Our work advances the\nfundamental understanding of $\\mathcal{T}$-odd spin currents via Fermi surface\ngeometry engineering and provides key insights for developing next-generation\naltermagnet-based memory devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets combine antiferromagnetic order with ferromagnet-like spin\nsplitting, a duality that unlocks ultrafast spin-dependent responses. This\nunique property creates unprecedented opportunities for spin-current\ngeneration, overcoming the intrinsic limitations of conventional spin-transfer\nand spin-orbit torque approaches in magnetic memory technologies. Here, we\nestablish a fundamental relationship between Fermi surface geometry and\ntime-reversal-odd ($\\mathcal{T}$-odd) spin currents in altermagnets through\ncombined model analysis and first-principles calculations. We demonstrate that\na $d$-wave altermagnet with a flat Fermi surface can achieve a theoretical\nupper limit of charge-to-spin conversion efficiency (CSE) of 100%. This\nmechanism is realized in the newly discovered room-temperature altermagnetic\nmetal KV$_2$O$_2$Se, which exhibits a CSE of $\\sim$78% at the charge neutrality\npoint, nearly double that of RuO$_2$, setting a new record for\n$\\mathcal{T}$-odd CSE. Under electron doping, this efficiency further increases\nto $\\sim$98%, approaching the theoretical limit. Our work advances the\nfundamental understanding of $\\mathcal{T}$-odd spin currents via Fermi surface\ngeometry engineering and provides key insights for developing next-generation\naltermagnet-based memory devices."
                },
                "authors": [
                    {
                        "name": "Junwen Lai"
                    },
                    {
                        "name": "Tianye Yu"
                    },
                    {
                        "name": "Peitao Liu"
                    },
                    {
                        "name": "Long Liu"
                    },
                    {
                        "name": "Guozhong Xing"
                    },
                    {
                        "name": "Xing-Qiu Chen"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07639v1",
                "updated": "2025-06-09T11:04:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T11:04:13Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse"
                },
                "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment."
                },
                "authors": [
                    {
                        "name": "Zhekai Duan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Shikai Geng"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Chris Xiaoxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chris Xiaoxuan Lu"
                },
                "author": "Chris Xiaoxuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.14317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.14317v2",
                "updated": "2025-06-09T09:48:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    9,
                    48,
                    43,
                    0,
                    160,
                    0
                ],
                "published": "2025-02-20T07:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    7,
                    10,
                    43,
                    3,
                    51,
                    0
                ],
                "title": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParallelComp: Parallel Long-Context Compressor for Length Extrapolation"
                },
                "summary": "Extrapolating ultra-long contexts (text length >128K) remains a major\nchallenge for large language models (LLMs), as most training-free extrapolation\nmethods are not only severely limited by memory bottlenecks, but also suffer\nfrom the attention sink, which restricts their scalability and effectiveness in\npractice. In this work, we propose ParallelComp, a parallel long-context\ncompression method that effectively overcomes the memory bottleneck, enabling\n8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB\nGPU in a training-free setting. ParallelComp splits the input into chunks,\ndynamically evicting redundant chunks and irrelevant tokens, supported by a\nparallel KV cache eviction mechanism. Importantly, we present a systematic\ntheoretical and empirical analysis of attention biases in parallel\nattention-including the attention sink, recency bias, and middle bias-and\nreveal that these biases exhibit distinctive patterns under ultra-long context\nsettings. We further design a KV cache eviction technique to mitigate this\nphenomenon. Experimental results show that ParallelComp enables an 8B model\n(trained on 8K context) to achieve 91.17% of GPT-4's performance under\nultra-long contexts, outperforming closed-source models such as Claude-2 and\nKimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby\nachieving a 23.50x acceleration in the prefill stage with negligible\nperformance loss and pave the way for scalable and robust ultra-long contexts\nextrapolation in LLMs. We release the code at\nhttps://github.com/menik1126/ParallelComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extrapolating ultra-long contexts (text length >128K) remains a major\nchallenge for large language models (LLMs), as most training-free extrapolation\nmethods are not only severely limited by memory bottlenecks, but also suffer\nfrom the attention sink, which restricts their scalability and effectiveness in\npractice. In this work, we propose ParallelComp, a parallel long-context\ncompression method that effectively overcomes the memory bottleneck, enabling\n8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB\nGPU in a training-free setting. ParallelComp splits the input into chunks,\ndynamically evicting redundant chunks and irrelevant tokens, supported by a\nparallel KV cache eviction mechanism. Importantly, we present a systematic\ntheoretical and empirical analysis of attention biases in parallel\nattention-including the attention sink, recency bias, and middle bias-and\nreveal that these biases exhibit distinctive patterns under ultra-long context\nsettings. We further design a KV cache eviction technique to mitigate this\nphenomenon. Experimental results show that ParallelComp enables an 8B model\n(trained on 8K context) to achieve 91.17% of GPT-4's performance under\nultra-long contexts, outperforming closed-source models such as Claude-2 and\nKimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby\nachieving a 23.50x acceleration in the prefill stage with negligible\nperformance loss and pave the way for scalable and robust ultra-long contexts\nextrapolation in LLMs. We release the code at\nhttps://github.com/menik1126/ParallelComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Chiwun Yang"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Hongxia Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "This paper has been accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.14317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.14317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07533v1",
                "updated": "2025-06-09T08:16:24Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    8,
                    16,
                    24,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T08:16:24Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    8,
                    16,
                    24,
                    0,
                    160,
                    0
                ],
                "title": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via\n  Mixture of Quantization-Aware Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via\n  Mixture of Quantization-Aware Experts"
                },
                "summary": "One of the primary challenges in optimizing large language models (LLMs) for\nlong-context inference lies in the high memory consumption of the Key-Value\n(KV) cache. Existing approaches, such as quantization, have demonstrated\npromising results in reducing memory usage. However, current quantization\nmethods cannot take both effectiveness and efficiency into account. In this\npaper, we propose MoQAE, a novel mixed-precision quantization method via\nmixture of quantization-aware experts. First, we view different quantization\nbit-width configurations as experts and use the traditional mixture of experts\n(MoE) method to select the optimal configuration. To avoid the inefficiency\ncaused by inputting tokens one by one into the router in the traditional MoE\nmethod, we input the tokens into the router chunk by chunk. Second, we design a\nlightweight router-only fine-tuning process to train MoQAE with a comprehensive\nloss to learn the trade-off between model accuracy and memory usage. Finally,\nwe introduce a routing freezing (RF) and a routing sharing (RS) mechanism to\nfurther reduce the inference overhead. Extensive experiments on multiple\nbenchmark datasets demonstrate that our method outperforms state-of-the-art KV\ncache quantization approaches in both efficiency and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the primary challenges in optimizing large language models (LLMs) for\nlong-context inference lies in the high memory consumption of the Key-Value\n(KV) cache. Existing approaches, such as quantization, have demonstrated\npromising results in reducing memory usage. However, current quantization\nmethods cannot take both effectiveness and efficiency into account. In this\npaper, we propose MoQAE, a novel mixed-precision quantization method via\nmixture of quantization-aware experts. First, we view different quantization\nbit-width configurations as experts and use the traditional mixture of experts\n(MoE) method to select the optimal configuration. To avoid the inefficiency\ncaused by inputting tokens one by one into the router in the traditional MoE\nmethod, we input the tokens into the router chunk by chunk. Second, we design a\nlightweight router-only fine-tuning process to train MoQAE with a comprehensive\nloss to learn the trade-off between model accuracy and memory usage. Finally,\nwe introduce a routing freezing (RF) and a routing sharing (RS) mechanism to\nfurther reduce the inference overhead. Extensive experiments on multiple\nbenchmark datasets demonstrate that our method outperforms state-of-the-art KV\ncache quantization approaches in both efficiency and effectiveness."
                },
                "authors": [
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Haocheng Lu"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Kai Lu"
                    },
                    {
                        "name": "Jiguang Wan"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v2",
                "updated": "2025-06-09T07:58:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    7,
                    58,
                    19,
                    0,
                    160,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "The paper needs major modifications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07334v1",
                "updated": "2025-06-09T00:30:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    9,
                    0,
                    30,
                    8,
                    0,
                    160,
                    0
                ],
                "published": "2025-06-09T00:30:08Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    0,
                    30,
                    8,
                    0,
                    160,
                    0
                ],
                "title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large\n  Language Models"
                },
                "summary": "Modern large language models (LLMs) are inherently auto-regressive, requiring\ninput to be serialized into flat sequences regardless of their structural\ndependencies. This serialization hinders the model's ability to leverage\nstructural inductive biases, especially in tasks such as retrieval-augmented\ngeneration (RAG) and reasoning on data with native graph structures, where\ninter-segment dependencies are crucial. We introduce Graph-KV with the\npotential to overcome this limitation. Graph-KV leverages the KV-cache of text\nsegments as condensed representations and governs their interaction through\nstructural inductive biases. In this framework, 'target' segments selectively\nattend only to the KV-caches of their designated 'source' segments, rather than\nall preceding segments in a serialized sequence. This approach induces a\ngraph-structured block mask, sparsifying attention and enabling a\nmessage-passing-like step within the LLM. Furthermore, strategically allocated\npositional encodings for source and target segments reduce positional bias and\ncontext window consumption. We evaluate Graph-KV across three scenarios: (1)\nseven RAG benchmarks spanning direct inference, multi-hop reasoning, and\nlong-document understanding; (2) Arxiv-QA, a novel academic paper QA task with\nfull-text scientific papers structured as citation ego-graphs; and (3) paper\ntopic classification within a citation network. By effectively reducing\npositional bias and harnessing structural inductive biases, Graph-KV\nsubstantially outperforms baselines, including standard costly sequential\nencoding, across various settings. Code and the Graph-KV data are publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) are inherently auto-regressive, requiring\ninput to be serialized into flat sequences regardless of their structural\ndependencies. This serialization hinders the model's ability to leverage\nstructural inductive biases, especially in tasks such as retrieval-augmented\ngeneration (RAG) and reasoning on data with native graph structures, where\ninter-segment dependencies are crucial. We introduce Graph-KV with the\npotential to overcome this limitation. Graph-KV leverages the KV-cache of text\nsegments as condensed representations and governs their interaction through\nstructural inductive biases. In this framework, 'target' segments selectively\nattend only to the KV-caches of their designated 'source' segments, rather than\nall preceding segments in a serialized sequence. This approach induces a\ngraph-structured block mask, sparsifying attention and enabling a\nmessage-passing-like step within the LLM. Furthermore, strategically allocated\npositional encodings for source and target segments reduce positional bias and\ncontext window consumption. We evaluate Graph-KV across three scenarios: (1)\nseven RAG benchmarks spanning direct inference, multi-hop reasoning, and\nlong-document understanding; (2) Arxiv-QA, a novel academic paper QA task with\nfull-text scientific papers structured as citation ego-graphs; and (3) paper\ntopic classification within a citation network. By effectively reducing\npositional bias and harnessing structural inductive biases, Graph-KV\nsubstantially outperforms baselines, including standard costly sequential\nencoding, across various settings. Code and the Graph-KV data are publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Shikun Liu"
                    },
                    {
                        "name": "Siqi Miao"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07311v1",
                "updated": "2025-06-08T22:59:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    22,
                    59,
                    20,
                    6,
                    159,
                    0
                ],
                "published": "2025-06-08T22:59:20Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    22,
                    59,
                    20,
                    6,
                    159,
                    0
                ],
                "title": "Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency\n  in Deployed Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency\n  in Deployed Inference"
                },
                "summary": "Large Language Models (LLMs) encounter severe memory inefficiencies during\nlong-context inference due to conventional handling of key-value (KV) caches.\nIn this work, we introduce a novel integration of PagedAttention with PyTorch's\nFlexAttention, addressing internal fragmentation and inefficiencies associated\nwith monolithic KV cache allocations. Implemented within IBM's Foundation Model\nStack (FMS), our fused attention kernel efficiently gathers scattered KV data.\nOur benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced\ninference latency, growing only linearly (~2x) with sequence length from 128 to\n2048 tokens when utilizing a global KV cache, compared to exponential latency\nincreases without caching. While peak memory usage remains largely unchanged\nfor single-step evaluations (dominated by model weights and activations), paged\nattention causes minimal incremental memory usage, observable only at sequence\nlengths exceeding 2048 tokens due to its power-of-two cache allocations. We\nopen-source the full implementation and discuss its implications for future\nlong-context model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) encounter severe memory inefficiencies during\nlong-context inference due to conventional handling of key-value (KV) caches.\nIn this work, we introduce a novel integration of PagedAttention with PyTorch's\nFlexAttention, addressing internal fragmentation and inefficiencies associated\nwith monolithic KV cache allocations. Implemented within IBM's Foundation Model\nStack (FMS), our fused attention kernel efficiently gathers scattered KV data.\nOur benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced\ninference latency, growing only linearly (~2x) with sequence length from 128 to\n2048 tokens when utilizing a global KV cache, compared to exponential latency\nincreases without caching. While peak memory usage remains largely unchanged\nfor single-step evaluations (dominated by model weights and activations), paged\nattention causes minimal incremental memory usage, observable only at sequence\nlengths exceeding 2048 tokens due to its power-of-two cache allocations. We\nopen-source the full implementation and discuss its implications for future\nlong-context model deployment."
                },
                "authors": [
                    {
                        "name": "Thomas Joshi"
                    },
                    {
                        "name": "Herman Saini"
                    },
                    {
                        "name": "Neil Dhillon"
                    },
                    {
                        "name": "Antoni Viros i Martin"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    }
                ],
                "author_detail": {
                    "name": "Kaoutar El Maghraoui"
                },
                "author": "Kaoutar El Maghraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v3",
                "updated": "2025-06-08T21:23:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    21,
                    23,
                    22,
                    6,
                    159,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v3",
                "updated": "2025-06-08T20:04:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    20,
                    4,
                    17,
                    6,
                    159,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "FDC: Fast KV Dimensionality Compression for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FDC: Fast KV Dimensionality Compression for Efficient LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose FDC, a fast KV\ndimensionality compression system that eliminates the decompression overhead\nincurred in the existing KV dimensionality compression system, Palu, and\nreduces attention time. Moreover, FDC employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, FDC enhances the attention kernel to balance the\nuneven workloads caused by the adaptive compression approach to further reduce\nattention computation latency. Comprehensive experiments demonstrate that\ncompared to Palu, FDC can reduce Job Completion Time (JCT) by up to 64%, and\ndelivers up to 1.97X throughput under the same latency, while maintaining 99%\nof the accuracy without compression. When state-of-the-art eviction and\nquantization methods are combined with FDC, they exhibit similar improvements\ncompared to those combined with Palu. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose FDC, a fast KV\ndimensionality compression system that eliminates the decompression overhead\nincurred in the existing KV dimensionality compression system, Palu, and\nreduces attention time. Moreover, FDC employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, FDC enhances the attention kernel to balance the\nuneven workloads caused by the adaptive compression approach to further reduce\nattention computation latency. Comprehensive experiments demonstrate that\ncompared to Palu, FDC can reduce Job Completion Time (JCT) by up to 64%, and\ndelivers up to 1.97X throughput under the same latency, while maintaining 99%\nof the accuracy without compression. When state-of-the-art eviction and\nquantization methods are combined with FDC, they exhibit similar improvements\ncompared to those combined with Palu. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.08508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.08508v2",
                "updated": "2025-06-08T16:07:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    16,
                    7,
                    44,
                    6,
                    159,
                    0
                ],
                "published": "2022-10-16T11:21:26Z",
                "published_parsed": [
                    2022,
                    10,
                    16,
                    11,
                    21,
                    26,
                    6,
                    289,
                    0
                ],
                "title": "RevaMp3D: Architecting the Processor Core and Cache Hierarchy for\n  Systems with Monolithically-Integrated Logic and Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevaMp3D: Architecting the Processor Core and Cache Hierarchy for\n  Systems with Monolithically-Integrated Logic and Memory"
                },
                "summary": "Recent nano-technological advances enable the Monolithic 3D (M3D) integration\nof multiple memory and logic layers in a single chip, allowing for fine-grained\nconnections between layers and significantly alleviating main memory\nbottlenecks. We show for a variety of workloads, on a state-of-the-art\nM3D-based system, that the performance and energy bottlenecks shift from main\nmemory to the processor core and cache hierarchy. Therefore, there is a need to\nrevisit current designs that have been conventionally tailored to tackle the\nmemory bottleneck. Based on the insights from our design space exploration, we\npropose RevaMp3D, introducing five key changes. First, we propose removing the\nshared last-level cache, as this delivers speedups comparable to or exceeding\nthose from increasing its size or reducing its latency across all workloads.\nSecond, since improving L1 cache latency has a large impact on performance, we\nreduce L1 latency by leveraging an M3D layout to shorten its wires. Third, we\nrepurpose the area from the removed cache to widen and scale up pipeline\nstructures, accommodating more in-flight requests that are efficiently served\nby M3D memory. To avoid latency penalties from these larger structures, we\nleverage M3D layouts. Fourth, to facilitate high thread-level parallelism, we\npropose a new fine-grained synchronization technique, using M3D's dense\ninter-layer connectivity. Fifth, we leverage the M3D main memory to mitigate\nthe core bottlenecks. We propose a processor frontend design that memoizes the\nrepetitive fetched, decoded, and reordered instructions, stores them in main\nmemory, and turns off the relevant parts of the core when possible. RevaMp3D\nprovides 1.2x-2.9x speedup and 1.2x-1.4x energy reduction compared to a\nstate-of-the-art M3D system. We also analyze RevaMp3D's design decisions across\nvarious memory latencies to facilitate latency-aware design decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent nano-technological advances enable the Monolithic 3D (M3D) integration\nof multiple memory and logic layers in a single chip, allowing for fine-grained\nconnections between layers and significantly alleviating main memory\nbottlenecks. We show for a variety of workloads, on a state-of-the-art\nM3D-based system, that the performance and energy bottlenecks shift from main\nmemory to the processor core and cache hierarchy. Therefore, there is a need to\nrevisit current designs that have been conventionally tailored to tackle the\nmemory bottleneck. Based on the insights from our design space exploration, we\npropose RevaMp3D, introducing five key changes. First, we propose removing the\nshared last-level cache, as this delivers speedups comparable to or exceeding\nthose from increasing its size or reducing its latency across all workloads.\nSecond, since improving L1 cache latency has a large impact on performance, we\nreduce L1 latency by leveraging an M3D layout to shorten its wires. Third, we\nrepurpose the area from the removed cache to widen and scale up pipeline\nstructures, accommodating more in-flight requests that are efficiently served\nby M3D memory. To avoid latency penalties from these larger structures, we\nleverage M3D layouts. Fourth, to facilitate high thread-level parallelism, we\npropose a new fine-grained synchronization technique, using M3D's dense\ninter-layer connectivity. Fifth, we leverage the M3D main memory to mitigate\nthe core bottlenecks. We propose a processor frontend design that memoizes the\nrepetitive fetched, decoded, and reordered instructions, stores them in main\nmemory, and turns off the relevant parts of the core when possible. RevaMp3D\nprovides 1.2x-2.9x speedup and 1.2x-1.4x energy reduction compared to a\nstate-of-the-art M3D system. We also analyze RevaMp3D's design decisions across\nvarious memory latencies to facilitate latency-aware design decisions."
                },
                "authors": [
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Geraldo F. Oliveira"
                    },
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "Rachata Ausavarungnirun"
                    },
                    {
                        "name": "Juan Gómez Luna"
                    },
                    {
                        "name": "João Ferreira"
                    },
                    {
                        "name": "Jeremie S. Kim"
                    },
                    {
                        "name": "Christina Giannoula"
                    },
                    {
                        "name": "Nandita Vijaykumar"
                    },
                    {
                        "name": "Jisung Park"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.08508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.08508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v5",
                "updated": "2025-06-08T16:04:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    16,
                    4,
                    59,
                    6,
                    159,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 16.11\\% fewer model parameters and 20.3\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 16.11\\% fewer model parameters and 20.3\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07200v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07200v1",
                "updated": "2025-06-08T15:48:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    15,
                    48,
                    16,
                    6,
                    159,
                    0
                ],
                "published": "2025-06-08T15:48:16Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    15,
                    48,
                    16,
                    6,
                    159,
                    0
                ],
                "title": "Efficient RL-based Cache Vulnerability Exploration by Penalizing Useless\n  Agent Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient RL-based Cache Vulnerability Exploration by Penalizing Useless\n  Agent Actions"
                },
                "summary": "Cache-timing attacks exploit microarchitectural characteristics to leak\nsensitive data, posing a severe threat to modern systems. Despite its severity,\nanalyzing the vulnerability of a given cache structure against cache-timing\nattacks is challenging. To this end, a method based on Reinforcement Learning\n(RL) has been proposed to automatically explore vulnerabilities for a given\ncache structure. However, a naive RL-based approach suffers from inefficiencies\ndue to the agent performing actions that do not contribute to the exploration.\nIn this paper, we propose a method to identify these useless actions during\ntraining and penalize them so that the agent avoids them and the exploration\nefficiency is improved. Experiments on 17 cache structures show that our\ntraining mechanism reduces the number of useless actions by up to 43.08%. This\nresulted in the reduction of training time by 28\\% in the base case and 4.84\\%\nin the geomean compared to a naive RL-based approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-timing attacks exploit microarchitectural characteristics to leak\nsensitive data, posing a severe threat to modern systems. Despite its severity,\nanalyzing the vulnerability of a given cache structure against cache-timing\nattacks is challenging. To this end, a method based on Reinforcement Learning\n(RL) has been proposed to automatically explore vulnerabilities for a given\ncache structure. However, a naive RL-based approach suffers from inefficiencies\ndue to the agent performing actions that do not contribute to the exploration.\nIn this paper, we propose a method to identify these useless actions during\ntraining and penalize them so that the agent avoids them and the exploration\nefficiency is improved. Experiments on 17 cache structures show that our\ntraining mechanism reduces the number of useless actions by up to 43.08%. This\nresulted in the reduction of training time by 28\\% in the base case and 4.84\\%\nin the geomean compared to a naive RL-based approach."
                },
                "authors": [
                    {
                        "name": "Kanato Nakanishi"
                    },
                    {
                        "name": "Soramichi Akiyama"
                    }
                ],
                "author_detail": {
                    "name": "Soramichi Akiyama"
                },
                "author": "Soramichi Akiyama",
                "arxiv_comment": "Presented in Machine Learning for Computer Architecture and Systems\n  (MLArchSys), June 21, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07200v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07200v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16220v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16220v2",
                "updated": "2025-06-08T09:30:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    9,
                    30,
                    12,
                    6,
                    159,
                    0
                ],
                "published": "2024-08-29T02:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    2,
                    31,
                    28,
                    3,
                    242,
                    0
                ],
                "title": "Place Protections at the Right Place: Targeted Hardening for\n  Cryptographic Code against Spectre v1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Place Protections at the Right Place: Targeted Hardening for\n  Cryptographic Code against Spectre v1"
                },
                "summary": "Spectre v1 attacks pose a substantial threat to security-critical software,\nparticularly cryptographic implementations. Existing software mitigations,\nhowever, often introduce excessive overhead by indiscriminately hardening\ninstructions without assessing their vulnerability. We propose an analysis\nframework that employs a novel fixpoint algorithm to detect Spectre\nvulnerabilities and apply targeted hardening. The fixpoint algorithm accounts\nfor program behavior changes induced by stepwise hardening, enabling precise,\nsound and efficient vulnerability detection. This framework also provides\nflexibility for diverse hardening strategies and attacker models, enabling\ncustomized targeted hardening. We instantiate the framework as LightSLH, which\nhardens program with provable security.\n  We evaluate LightSLH on cryptographic algorithms from OpenSSL, Libsodium,\nNaCL and PQClean. Across all experimental cases, LightSLH provides the lowest\noverhead among current provable protection strategies, including 0\\% overhead\nin 50\\% cases. Notably, the analysis of LightSLH reveals two previously unknown\nsecurity issues: (1) The compiler can introduce risks overlooked by LLSCT, a\nhardening method proven secure at the LLVM IR level. We successfully construct\na side channel by exploiting compiler-inserted stack loads, confirming this\nrisk. (2) Memory access patterns generated by the scatter-gather algorithm\nstill depend on secrets, even for observers with cache line granularity. These\nfindings and results highlight the importance of applying accurate protections\nto specific instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectre v1 attacks pose a substantial threat to security-critical software,\nparticularly cryptographic implementations. Existing software mitigations,\nhowever, often introduce excessive overhead by indiscriminately hardening\ninstructions without assessing their vulnerability. We propose an analysis\nframework that employs a novel fixpoint algorithm to detect Spectre\nvulnerabilities and apply targeted hardening. The fixpoint algorithm accounts\nfor program behavior changes induced by stepwise hardening, enabling precise,\nsound and efficient vulnerability detection. This framework also provides\nflexibility for diverse hardening strategies and attacker models, enabling\ncustomized targeted hardening. We instantiate the framework as LightSLH, which\nhardens program with provable security.\n  We evaluate LightSLH on cryptographic algorithms from OpenSSL, Libsodium,\nNaCL and PQClean. Across all experimental cases, LightSLH provides the lowest\noverhead among current provable protection strategies, including 0\\% overhead\nin 50\\% cases. Notably, the analysis of LightSLH reveals two previously unknown\nsecurity issues: (1) The compiler can introduce risks overlooked by LLSCT, a\nhardening method proven secure at the LLVM IR level. We successfully construct\na side channel by exploiting compiler-inserted stack loads, confirming this\nrisk. (2) Memory access patterns generated by the scatter-gather algorithm\nstill depend on secrets, even for observers with cache line granularity. These\nfindings and results highlight the importance of applying accurate protections\nto specific instructions."
                },
                "authors": [
                    {
                        "name": "Yiming Zhu"
                    },
                    {
                        "name": "Wenchao Huang"
                    },
                    {
                        "name": "Yan Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yan Xiong"
                },
                "author": "Yan Xiong",
                "arxiv_comment": "Accepted to appear at USENIX Security 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16220v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16220v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v2",
                "updated": "2025-06-08T00:52:33Z",
                "updated_parsed": [
                    2025,
                    6,
                    8,
                    0,
                    52,
                    33,
                    6,
                    159,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17132v2",
                "updated": "2025-06-07T19:22:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    19,
                    22,
                    5,
                    5,
                    158,
                    0
                ],
                "published": "2025-05-22T03:00:39Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    3,
                    0,
                    39,
                    3,
                    142,
                    0
                ],
                "title": "Robustifying Vision-Language Models via Dynamic Token Reweighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Vision-Language Models via Dynamic Token Reweighting"
                },
                "summary": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. (warning: this paper contains potentially harmful content\ngenerated by VLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (VLMs) are highly vulnerable to jailbreak\nattacks that exploit visual-textual interactions to bypass safety guardrails.\nIn this paper, we present DTR, a novel inference-time defense that mitigates\nmultimodal jailbreak attacks through optimizing the model's key-value (KV)\ncaches. Rather than relying on curated safety-specific data or costly\nimage-to-text conversion, we introduce a new formulation of the safety-relevant\ndistributional shift induced by the visual modality. This formulation enables\nDTR to dynamically adjust visual token weights, minimizing the impact of\nadversarial visual inputs while preserving the model's general capabilities and\ninference efficiency. Extensive evaluation across diverse VLMs and attack\nbenchmarks demonstrates that \\sys outperforms existing defenses in both attack\nrobustness and benign task performance, marking the first successful\napplication of KV cache optimization for safety enhancement in multimodal\nfoundation models. (warning: this paper contains potentially harmful content\ngenerated by VLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Rongyi Zhu"
                    },
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Fenglong Ma"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.00979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.00979v2",
                "updated": "2025-06-07T14:03:06Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    14,
                    3,
                    6,
                    5,
                    158,
                    0
                ],
                "published": "2025-03-02T18:12:50Z",
                "published_parsed": [
                    2025,
                    3,
                    2,
                    18,
                    12,
                    50,
                    6,
                    61,
                    0
                ],
                "title": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dialogue Without Limits: Constant-Sized KV Caches for Extended Responses\n  in LLMs"
                },
                "summary": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Transformers rely on Key-Value (KV) caching to accelerate\ninference. However, the linear growth of the KV cache with context length leads\nto excessive memory consumption and bandwidth constraints. This bottleneck is\nparticularly problematic in real-time applications -- such as chatbots and\ninteractive assistants -- where low latency and high memory efficiency are\ncritical. Existing methods drop distant tokens or compress states in a lossy\nmanner, sacrificing accuracy by discarding vital context or introducing bias.\n  We propose MorphKV, an inference-time technique that maintains a\nconstant-sized KV cache while preserving accuracy. MorphKV balances long-range\ndependencies and local coherence during text generation. It eliminates\nearly-token bias while retaining high-fidelity context by adaptively ranking\ntokens through correlation-aware selection. Unlike heuristic retention or lossy\ncompression, MorphKV iteratively refines the KV cache via lightweight updates\nguided by attention patterns of recent tokens. This approach captures\ninter-token correlation with greater accuracy, crucial for tasks like content\ncreation and code generation. Our studies on long-response tasks show 52.9$\\%$\nmemory savings and 18.2$\\%$ higher accuracy on average compared to\nstate-of-the-art prior works, enabling efficient real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ravi Ghadia"
                    },
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Gaurav Jain"
                    },
                    {
                        "name": "Prashant Nair"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "arxiv_comment": "Published in the Proceedings of the 42nd International Conference on\n  Machine Learning (ICML), Vancouver, Canada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.00979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.00979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06773v1",
                "updated": "2025-06-07T11:50:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    11,
                    50,
                    11,
                    5,
                    158,
                    0
                ],
                "published": "2025-06-07T11:50:11Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    11,
                    50,
                    11,
                    5,
                    158,
                    0
                ],
                "title": "Taming Wild Branches: Overcoming Hard-to-Predict Branches using the\n  Bullseye Predictor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming Wild Branches: Overcoming Hard-to-Predict Branches using the\n  Bullseye Predictor"
                },
                "summary": "Branch prediction is key to the performance of out-of-order processors. While\nthe CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical\ncorrector, and a loop predictor, over half of its remaining mispredictions stem\nfrom a small set of hard-to-predict (H2P) branches. These branches occur under\ndiverse global histories, causing repeated thrashing in TAGE and eviction\nbefore usefulness counters can mature. Prior work shows that simply enlarging\nthe tables offers only marginal improvement.\n  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem\ncalled the Bullseye predictor. It identifies problematic PCs using a\nset-associative H2P Identification Table (HIT) and steers them to one of two\nbranch-specific perceptrons, one indexed by hashed local history and the other\nby folded global history. A short trial phase tracks head-to-head accuracy in\nan H2P cache. A branch becomes perceptron-resident only if the perceptron's\nsustained accuracy and output magnitude exceed dynamic thresholds, after which\nTAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,\nand perceptron operate fully in parallel with TAGE-SC-L, providing higher\nfidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI\nof 145.09.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Branch prediction is key to the performance of out-of-order processors. While\nthe CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical\ncorrector, and a loop predictor, over half of its remaining mispredictions stem\nfrom a small set of hard-to-predict (H2P) branches. These branches occur under\ndiverse global histories, causing repeated thrashing in TAGE and eviction\nbefore usefulness counters can mature. Prior work shows that simply enlarging\nthe tables offers only marginal improvement.\n  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem\ncalled the Bullseye predictor. It identifies problematic PCs using a\nset-associative H2P Identification Table (HIT) and steers them to one of two\nbranch-specific perceptrons, one indexed by hashed local history and the other\nby folded global history. A short trial phase tracks head-to-head accuracy in\nan H2P cache. A branch becomes perceptron-resident only if the perceptron's\nsustained accuracy and output magnitude exceed dynamic thresholds, after which\nTAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,\nand perceptron operate fully in parallel with TAGE-SC-L, providing higher\nfidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI\nof 145.09."
                },
                "authors": [
                    {
                        "name": "Emet Behrendt"
                    },
                    {
                        "name": "Shing Wai Pun"
                    },
                    {
                        "name": "Prashant J. Nair"
                    }
                ],
                "author_detail": {
                    "name": "Prashant J. Nair"
                },
                "author": "Prashant J. Nair",
                "arxiv_comment": "Paper accepted and presented at the 6th Championship Branch\n  Prediction (CBP) workshop, co-held with ISCA 2025, on June 21, 2025, Tokyo,\n  Japan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.2; B.2.1; C.4; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03296v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03296v2",
                "updated": "2025-06-07T01:36:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    7,
                    1,
                    36,
                    34,
                    5,
                    158,
                    0
                ],
                "published": "2025-06-03T18:35:56Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    35,
                    56,
                    1,
                    154,
                    0
                ],
                "title": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel CPU-GPU Execution for LLM Inference on Constrained GPUs"
                },
                "summary": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for online inference is often\nconstrained by limited GPU memory, particularly due to the growing KV cache\nduring auto-regressive decoding. Hybrid GPU-CPU execution has emerged as a\npromising solution by offloading KV cache management and parts of attention\ncomputation to the CPU. However, a key bottleneck remains: existing schedulers\nfail to effectively overlap CPU-offloaded tasks with GPU execution during the\nlatency-critical, bandwidth-bound decode phase. This particularly penalizes\nreal-time, decode-heavy applications (e.g., chat, Chain-of-Thought reasoning)\nwhich are currently underserved by existing systems, especially under memory\npressure typical of edge or low-cost deployments.\n  We present APEX, a novel, profiling-informed scheduling strategy that\nmaximizes CPU-GPU parallelism during hybrid LLM inference. Unlike systems\nrelying on static rules or purely heuristic approaches, APEX dynamically\ndispatches compute across heterogeneous resources by predicting execution times\nof CPU and GPU subtasks to maximize overlap while avoiding scheduling\noverheads. We evaluate APEX on diverse workloads and GPU architectures (NVIDIA\nT4, A10), using LLaMa-2-7B and LLaMa-3.1-8B models. Compared to GPU-only\nschedulers like VLLM, APEX improves throughput by 84% - 96% on T4 and 11% - 89%\non A10 GPUs, while preserving latency. Against the best existing hybrid\nschedulers, it delivers up to 49% (T4) and 37% (A10) higher throughput in\nlong-output settings. APEX significantly advances hybrid LLM inference\nefficiency on such memory-constrained hardware and provides a blueprint for\nscheduling in heterogeneous AI systems, filling a critical gap for efficient\nreal-time LLM applications."
                },
                "authors": [
                    {
                        "name": "Jiakun Fan"
                    },
                    {
                        "name": "Yanglin Zhang"
                    },
                    {
                        "name": "Xiangchen Li"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios S. Nikolopoulos"
                },
                "author": "Dimitrios S. Nikolopoulos",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03296v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03296v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06444v1",
                "updated": "2025-06-06T18:05:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-06T18:05:45Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    18,
                    5,
                    45,
                    4,
                    157,
                    0
                ],
                "title": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety\n  Assurance"
                },
                "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron ."
                },
                "authors": [
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Gaotang Li"
                    },
                    {
                        "name": "Tianxin Wei"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v1",
                "updated": "2025-06-06T09:55:59Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05811v1",
                "updated": "2025-06-06T07:20:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    7,
                    20,
                    25,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-06T07:20:25Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    7,
                    20,
                    25,
                    4,
                    157,
                    0
                ],
                "title": "Synchronous Clock and RF Carrier Transmission for Radio Access Network\n  Fronthaul",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synchronous Clock and RF Carrier Transmission for Radio Access Network\n  Fronthaul"
                },
                "summary": "We simultaneously achieve clock synchronisation, clock-synchronised data\ntransmission and ultra-low noise RF carrier generation by combining clock phase\ncaching and frequency comb transmission in radio access networks (RAN). We\ndemonstrate <100fs jitter for 25GHz RF carrier and 2.5GHz clock, and 16-hour\n6.6ps RMS wander.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We simultaneously achieve clock synchronisation, clock-synchronised data\ntransmission and ultra-low noise RF carrier generation by combining clock phase\ncaching and frequency comb transmission in radio access networks (RAN). We\ndemonstrate <100fs jitter for 25GHz RF carrier and 2.5GHz clock, and 16-hour\n6.6ps RMS wander."
                },
                "authors": [
                    {
                        "name": "Kari Aaron Clark"
                    },
                    {
                        "name": "Zun Htay"
                    },
                    {
                        "name": "Zichuan Zhou"
                    },
                    {
                        "name": "Amany Kassem"
                    },
                    {
                        "name": "Andrea Pertoldi"
                    },
                    {
                        "name": "Benjamin Rudin"
                    },
                    {
                        "name": "Florian Emaury"
                    },
                    {
                        "name": "Izzat Darwazeh"
                    },
                    {
                        "name": "Zhixin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhixin Liu"
                },
                "author": "Zhixin Liu",
                "arxiv_comment": "Conference manuscript submitted to the European Conference on Optical\n  Communication 2025 (ECOC 2025) on 2nd May 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.16800v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.16800v2",
                "updated": "2025-06-06T06:35:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    6,
                    35,
                    52,
                    4,
                    157,
                    0
                ],
                "published": "2023-05-26T10:29:25Z",
                "published_parsed": [
                    2023,
                    5,
                    26,
                    10,
                    29,
                    25,
                    4,
                    146,
                    0
                ],
                "title": "Joint Optimization of Triangle Mesh, Material, and Light from Neural\n  Fields with Neural Radiance Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Optimization of Triangle Mesh, Material, and Light from Neural\n  Fields with Neural Radiance Cache"
                },
                "summary": "Traditional inverse rendering techniques are based on textured meshes, which\nnaturally adapts to modern graphics pipelines, but costly differentiable\nmulti-bounce Monte Carlo (MC) ray tracing poses challenges for modeling global\nillumination. Recently, neural fields has demonstrated impressive\nreconstruction quality but falls short in modeling indirect illumination. In\nthis paper, we introduce a simple yet efficient inverse rendering framework\nthat combines the strengths of both methods. Specifically, given pre-trained\nneural field representing the scene, we can obtain an initial estimate of the\nsigned distance field (SDF) and create a Neural Radiance Cache (NRC), an\nenhancement over the traditional radiance cache used in real-time rendering. By\nusing the former to initialize differentiable marching tetrahedrons (DMTet) and\nthe latter to model indirect illumination, we can compute the global\nillumination via single-bounce differentiable MC ray tracing and jointly\noptimize the geometry, material, and light through back propagation.\nExperiments demonstrate that, compared to previous methods, our approach\neffectively prevents indirect illumination effects from being baked into\nmaterials, thus obtaining the high-quality reconstruction of triangle mesh,\nPhysically-Based (PBR) materials, and High Dynamic Range (HDR) light probe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional inverse rendering techniques are based on textured meshes, which\nnaturally adapts to modern graphics pipelines, but costly differentiable\nmulti-bounce Monte Carlo (MC) ray tracing poses challenges for modeling global\nillumination. Recently, neural fields has demonstrated impressive\nreconstruction quality but falls short in modeling indirect illumination. In\nthis paper, we introduce a simple yet efficient inverse rendering framework\nthat combines the strengths of both methods. Specifically, given pre-trained\nneural field representing the scene, we can obtain an initial estimate of the\nsigned distance field (SDF) and create a Neural Radiance Cache (NRC), an\nenhancement over the traditional radiance cache used in real-time rendering. By\nusing the former to initialize differentiable marching tetrahedrons (DMTet) and\nthe latter to model indirect illumination, we can compute the global\nillumination via single-bounce differentiable MC ray tracing and jointly\noptimize the geometry, material, and light through back propagation.\nExperiments demonstrate that, compared to previous methods, our approach\neffectively prevents indirect illumination effects from being baked into\nmaterials, thus obtaining the high-quality reconstruction of triangle mesh,\nPhysically-Based (PBR) materials, and High Dynamic Range (HDR) light probe."
                },
                "authors": [
                    {
                        "name": "Jiakai Sun"
                    },
                    {
                        "name": "Weijing Zhang"
                    },
                    {
                        "name": "Zhanjie Zhang"
                    },
                    {
                        "name": "Tianyi Chu"
                    },
                    {
                        "name": "Guangyuan Li"
                    },
                    {
                        "name": "Lei Zhao"
                    },
                    {
                        "name": "Wei Xing"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xing"
                },
                "author": "Wei Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.16800v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.16800v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v3",
                "updated": "2025-06-06T02:29:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    2,
                    29,
                    18,
                    4,
                    157,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures. Our code is available at https://github.com/OptimAI-Lab/RoSTE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia, Qwen and\nLlama models of different sizes demonstrate the effectiveness of RoSTE.\nCompared to existing post-SFT quantization baselines, our method consistently\nachieves superior performances across various tasks and different LLM\narchitectures. Our code is available at https://github.com/OptimAI-Lab/RoSTE."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang Katie Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "arxiv_comment": "accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05682v1",
                "updated": "2025-06-06T02:20:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    6,
                    2,
                    20,
                    49,
                    4,
                    157,
                    0
                ],
                "published": "2025-06-06T02:20:49Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    2,
                    20,
                    49,
                    4,
                    157,
                    0
                ],
                "title": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational\n  Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lumina: Real-Time Mobile Neural Rendering by Exploiting Computational\n  Redundancy"
                },
                "summary": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has vastly advanced the pace of neural\nrendering, but it remains computationally demanding on today's mobile SoCs. To\naddress this challenge, we propose Lumina, a hardware-algorithm co-designed\nsystem, which integrates two principal optimizations: a novel algorithm, S^2,\nand a radiance caching mechanism, RC, to improve the efficiency of neural\nrendering. S2 algorithm exploits temporal coherence in rendering to reduce the\ncomputational overhead, while RC leverages the color integration process of\n3DGS to decrease the frequency of intensive rasterization computations. Coupled\nwith these techniques, we propose an accelerator architecture, LuminCore, to\nfurther accelerate cache lookup and address the fundamental inefficiencies in\nRasterization. We show that Lumina achieves 4.5x speedup and 5.3x energy\nreduction against a mobile Volta GPU, with a marginal quality loss (< 0.2 dB\npeak signal-to-noise ratio reduction) across synthetic and real-world datasets."
                },
                "authors": [
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Weikai Lin"
                    },
                    {
                        "name": "Yuge Cheng"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Yuhao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yuhao Zhu"
                },
                "author": "Yuhao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v3",
                "updated": "2025-06-05T20:50:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    20,
                    50,
                    51,
                    3,
                    156,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11092v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11092v1",
                "updated": "2025-06-05T19:47:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    19,
                    47,
                    22,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T19:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    19,
                    47,
                    22,
                    3,
                    156,
                    0
                ],
                "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments."
                },
                "authors": [
                    {
                        "name": "Jubin Abhishek Soni"
                    },
                    {
                        "name": "Amit Anand"
                    },
                    {
                        "name": "Rajesh Kumar Pandey"
                    },
                    {
                        "name": "Aniket Abhishek Soni"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Abhishek Soni"
                },
                "author": "Aniket Abhishek Soni",
                "arxiv_comment": "6 pages, 5 figures, 3 tables. This manuscript has been submitted to\n  IEEE conference. Researchers are welcome to read and build upon this work;\n  please cite it appropriately. For questions or clarifications, feel free to\n  contact me",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11092v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11092v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05344v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are commonly derived by extending\npre-trained Large Language Models (LLMs) with visual capabilities. In this\nwork, we investigate how MLLMs process visual inputs by analyzing their\nattention mechanisms. We reveal a surprising sparsity phenomenon: only a small\nsubset (approximately less than 5%) of attention heads in LLMs actively\ncontribute to visual understanding, termed visual heads. To identify these\nheads efficiently, we design a training-free framework that quantifies\nhead-level visual relevance through targeted response analysis. Building on\nthis discovery, we introduce SparseMM, a KV-Cache optimization strategy that\nallocates asymmetric computation budgets to heads in LLMs based on their visual\nscores, leveraging the sparity of visual heads for accelerating the inference\nof MLLMs. Compared with prior KV-Cache acceleration methods that ignore the\nparticularity of visual, SparseMM prioritizes stress and retaining visual\nsemantics during decoding. Extensive evaluations across mainstream multimodal\nbenchmarks demonstrate that SparseMM achieves superior accuracy-efficiency\ntrade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52%\nmemory reduction during generation while maintaining performance parity on\nefficiency test. Our project is open sourced at\nhttps://github.com/CR400AF-A/SparseMM."
                },
                "authors": [
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05345v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Time Hyper-Scaling with KV Cache Compression"
                },
                "summary": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time scaling trades efficiency for increased reasoning accuracy by\ngenerating longer or more parallel sequences. However, in Transformer LLMs,\ngeneration cost is bottlenecked by the size of the key-value (KV) cache, rather\nthan the number of generated tokens. Hence, we explore inference-time\nhyper-scaling: by compressing the KV cache, we can generate more tokens within\nthe same compute budget and further improve the accuracy of scaled inference.\nThe success of this approach, however, hinges on the ability of compression\nmethods to preserve accuracy even at high compression ratios. To make\nhyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a\nnovel method for sparsifying KV caches that only requires 1K training steps to\nachieve 8$\\times$ compression, while maintaining better accuracy than\ntraining-free sparse attention. Instead of prematurely discarding cached\ntokens, DMS delays token eviction, implicitly merging representations and\npreserving critical information. We demonstrate the effectiveness of\ninference-time hyper-scaling with DMS on multiple families of LLMs, showing\nthat it boosts accuracy for comparable inference runtime and memory load. For\ninstance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on\nGPQA, and 9.6 on LiveCodeBench across compute budgets."
                },
                "authors": [
                    {
                        "name": "Adrian Łańcucki"
                    },
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05347v1",
                "updated": "2025-06-05T17:59:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:55Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    55,
                    3,
                    156,
                    0
                ],
                "title": "Neural Inverse Rendering from Propagating Light",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Inverse Rendering from Propagating Light"
                },
                "summary": "We present the first system for physically based, neural inverse rendering\nfrom multi-viewpoint videos of propagating light. Our approach relies on a\ntime-resolved extension of neural radiance caching -- a technique that\naccelerates inverse rendering by storing infinite-bounce radiance arriving at\nany point from any direction. The resulting model accurately accounts for\ndirect and indirect light transport effects and, when applied to captured\nmeasurements from a flash lidar system, enables state-of-the-art 3D\nreconstruction in the presence of strong indirect light. Further, we\ndemonstrate view synthesis of propagating light, automatic decomposition of\ncaptured measurements into direct and indirect components, as well as novel\ncapabilities such as multi-view time-resolved relighting of captured scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first system for physically based, neural inverse rendering\nfrom multi-viewpoint videos of propagating light. Our approach relies on a\ntime-resolved extension of neural radiance caching -- a technique that\naccelerates inverse rendering by storing infinite-bounce radiance arriving at\nany point from any direction. The resulting model accurately accounts for\ndirect and indirect light transport effects and, when applied to captured\nmeasurements from a flash lidar system, enables state-of-the-art 3D\nreconstruction in the presence of strong indirect light. Further, we\ndemonstrate view synthesis of propagating light, automatic decomposition of\ncaptured measurements into direct and indirect components, as well as novel\ncapabilities such as multi-view time-resolved relighting of captured scenes."
                },
                "authors": [
                    {
                        "name": "Anagh Malik"
                    },
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Andrew Xie"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "David B. Lindell"
                    }
                ],
                "author_detail": {
                    "name": "David B. Lindell"
                },
                "author": "David B. Lindell",
                "arxiv_comment": "Website: https://anaghmalik.com/InvProp/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05332v1",
                "updated": "2025-06-05T17:59:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T17:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    17,
                    59,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding"
                },
                "summary": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model."
                },
                "authors": [
                    {
                        "name": "Jingyang Lin"
                    },
                    {
                        "name": "Jialian Wu"
                    },
                    {
                        "name": "Ximeng Sun"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Yusheng Su"
                    },
                    {
                        "name": "Xiaodong Yu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Jiebo Luo"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "Project page: https://videomarathon.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05071v1",
                "updated": "2025-06-05T14:19:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    19,
                    5,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T14:19:05Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    19,
                    5,
                    3,
                    156,
                    0
                ],
                "title": "Memory Hierarchy Design for Caching Middleware in the Age of NVM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Hierarchy Design for Caching Middleware in the Age of NVM"
                },
                "summary": "Advances in storage technology have introduced Non-Volatile Memory, NVM, as a\nnew storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid\nState Disk (SSD), and Disk present a system designer with a wide array of\noptions in designing caching middleware. Moreover, design decisions to\nreplicate a data item in more than one level of a caching memory hierarchy may\nenhance the overall system performance with a faster recovery time in the event\nof a memory failure. Given a fixed budget, the key configuration questions are:\nWhich storage media should constitute the memory hierarchy? What is the storage\ncapacity of each hierarchy? Should data be replicated or partitioned across the\ndifferent levels of the hierarchy? We model these cache configuration questions\nas an instance of the Multiple Choice Knapsack Problem (MCKP). This model is\nguided by the specification of each type of memory along with an application's\ndatabase characteristics and its workload. Although MCKP is NP-complete, its\nlinear programming relaxation is efficiently solvable and can be used to\nclosely approximate the optimal solution. We use the resulting simple algorithm\nto evaluate design tradeoffs in the context of a memory hierarchy for a\nKey-Value Store (e.g., memcached) as well as a host-side cache (e.g.,\nFlashcache). The results show selective replication is appropriate with certain\nfailure rates and workload characteristics. With a slim failure rate and\nfrequent data updates, tiering of data across the different storage media that\nconstitute the cache is superior to replication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in storage technology have introduced Non-Volatile Memory, NVM, as a\nnew storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid\nState Disk (SSD), and Disk present a system designer with a wide array of\noptions in designing caching middleware. Moreover, design decisions to\nreplicate a data item in more than one level of a caching memory hierarchy may\nenhance the overall system performance with a faster recovery time in the event\nof a memory failure. Given a fixed budget, the key configuration questions are:\nWhich storage media should constitute the memory hierarchy? What is the storage\ncapacity of each hierarchy? Should data be replicated or partitioned across the\ndifferent levels of the hierarchy? We model these cache configuration questions\nas an instance of the Multiple Choice Knapsack Problem (MCKP). This model is\nguided by the specification of each type of memory along with an application's\ndatabase characteristics and its workload. Although MCKP is NP-complete, its\nlinear programming relaxation is efficiently solvable and can be used to\nclosely approximate the optimal solution. We use the resulting simple algorithm\nto evaluate design tradeoffs in the context of a memory hierarchy for a\nKey-Value Store (e.g., memcached) as well as a host-side cache (e.g.,\nFlashcache). The results show selective replication is appropriate with certain\nfailure rates and workload characteristics. With a slim failure rate and\nfrequent data updates, tiering of data across the different storage media that\nconstitute the cache is superior to replication."
                },
                "authors": [
                    {
                        "name": "Shahram Ghandeharizadeh"
                    },
                    {
                        "name": "Sandy Irani"
                    },
                    {
                        "name": "Jenny Lam"
                    }
                ],
                "author_detail": {
                    "name": "Jenny Lam"
                },
                "author": "Jenny Lam",
                "arxiv_doi": "10.1109/ICDE.2018.00155",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDE.2018.00155",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.05071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A shorter version appeared in the IEEE 34th International Conference\n  on Data Engineering (ICDE), Paris, France, 2018, pp. 1380-1383, doi:\n  10.1109/ICDE.2018.00155",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16950v2",
                "updated": "2025-06-05T13:38:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    38,
                    34,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-22T17:33:49Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    33,
                    49,
                    3,
                    142,
                    0
                ],
                "title": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bottlenecked Transformers: Periodic KV Cache Abstraction for Generalised\n  Reasoning"
                },
                "summary": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their impressive capabilities, Large Language Models struggle with\ngeneralisation beyond their training distribution, often exhibiting\nsophisticated pattern interpolation rather than true abstract reasoning\n(extrapolation). In this work, we approach this limitation through the lens of\nInformation Bottleneck (IB) theory, which posits that model generalisation\nemerges from an optimal balance between input compression and retention of\npredictive information in latent representations. We prove using IB theory that\ndecoder-only Transformers are inherently constrained in their ability to form\ntask-optimal sequence representations. We then use this result to demonstrate\nthat periodic global transformation of the internal sequence-level\nrepresentations (KV cache) is a necessary computational step for improving\nTransformer generalisation in reasoning tasks. Based on these theoretical\ninsights, we propose a modification to the Transformer architecture, in the\nform of an additional module that globally rewrites the KV cache at periodic\nintervals, shifting its capacity away from memorising input prefixes and toward\nencoding features most useful for predicting future tokens. Our model delivers\nsubstantial gains on mathematical reasoning benchmarks, outperforming both\nvanilla Transformers with up to 3.5x more parameters, as well as\nheuristic-driven pruning mechanisms for cache compression. Our approach can be\nseen as a principled generalisation of existing KV-cache compression methods;\nwhereas such methods focus solely on compressing input representations, they\noften do so at the expense of retaining predictive information, and thus their\ncapabilities are inherently bounded by those of an unconstrained model. This\nestablishes a principled framework to manipulate Transformer memory using\ninformation theory, addressing fundamental reasoning limitations that scaling\nalone cannot overcome."
                },
                "authors": [
                    {
                        "name": "Adnan Oomerjee"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    },
                    {
                        "name": "Zhongwei Yu"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v2",
                "updated": "2025-06-05T13:20:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    13,
                    20,
                    9,
                    3,
                    156,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches are focused on\nreduction of the amount of compute in existing language models rather than\nminimization of number of bits needed to store text. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "arxiv_comment": "ACL 2025 (main conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04920v1",
                "updated": "2025-06-05T11:53:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    11,
                    53,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T11:53:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    11,
                    53,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating LLM-to-LLM Tutoring for Multilingual Math Feedback"
                },
                "summary": "Large language models (LLMs) have demonstrated the ability to generate\nformative feedback and instructional hints in English, making them increasingly\nrelevant for AI-assisted education. However, their ability to provide effective\ninstructional support across different languages, especially for mathematically\ngrounded reasoning tasks, remains largely unexamined. In this work, we present\nthe first large-scale simulation of multilingual tutor-student interactions\nusing LLMs. A stronger model plays the role of the tutor, generating feedback\nin the form of hints, while a weaker model simulates the student. We explore\n352 experimental settings across 11 typologically diverse languages, four\nstate-of-the-art LLMs, and multiple prompting strategies to assess whether\nlanguage-specific feedback leads to measurable learning gains. Our study\nexamines how student input language, teacher feedback language, model choice,\nand language resource level jointly influence performance. Results show that\nmultilingual hints can significantly improve learning outcomes, particularly in\nlow-resource languages when feedback is aligned with the student's native\nlanguage. These findings offer practical insights for developing multilingual,\nLLM-based educational tools that are both effective and inclusive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the ability to generate\nformative feedback and instructional hints in English, making them increasingly\nrelevant for AI-assisted education. However, their ability to provide effective\ninstructional support across different languages, especially for mathematically\ngrounded reasoning tasks, remains largely unexamined. In this work, we present\nthe first large-scale simulation of multilingual tutor-student interactions\nusing LLMs. A stronger model plays the role of the tutor, generating feedback\nin the form of hints, while a weaker model simulates the student. We explore\n352 experimental settings across 11 typologically diverse languages, four\nstate-of-the-art LLMs, and multiple prompting strategies to assess whether\nlanguage-specific feedback leads to measurable learning gains. Our study\nexamines how student input language, teacher feedback language, model choice,\nand language resource level jointly influence performance. Results show that\nmultilingual hints can significantly improve learning outcomes, particularly in\nlow-resource languages when feedback is aligned with the student's native\nlanguage. These findings offer practical insights for developing multilingual,\nLLM-based educational tools that are both effective and inclusive."
                },
                "authors": [
                    {
                        "name": "Junior Cedric Tonga"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "Preprint, in submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04844v1",
                "updated": "2025-06-05T10:11:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    10,
                    11,
                    4,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T10:11:04Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    10,
                    11,
                    4,
                    3,
                    156,
                    0
                ],
                "title": "Characterization of the Hamamatsu R12699-406-M4 Photomultiplier Tube in\n  Cold Xenon Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterization of the Hamamatsu R12699-406-M4 Photomultiplier Tube in\n  Cold Xenon Environments"
                },
                "summary": "The Hamamatsu R12699-406-M2 is a $2\\times2$ multi-anode 2-inch\nphotomultiplier tube that offers a compact form factor, low intrinsic\nradioactivity, and high photocathode coverage. These characteristics make it a\npromising candidate for next-generation xenon-based direct detection dark\nmatter experiments, such as XLZD and PandaX-xT. We present a detailed\ncharacterization of this photosensor operated in cold xenon environments,\nfocusing on its single photoelectron response, dark count rate, light emission,\nand afterpulsing behavior. The device demonstrated a gain exceeding $2\\cdot\n10^6$ at the nominal voltage of -1.0 kV, along with a low dark count rate of\n$(0.4\\pm0.2)\\;\\text{Hz/cm}^2$. Due to the compact design, afterpulses exhibited\nshort delay times, resulting in some cases in an overlap with the light-induced\nsignal. To evaluate its applicability in a realistic detector environment, two\nR12699-406-M2 units were deployed in a small-scale dual-phase xenon time\nprojection chamber. The segmented $2\\times2$ anode structure enabled lateral\nposition reconstruction using a single photomultiplier tube, highlighting the\npotential of the sensor for effective event localization in future detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hamamatsu R12699-406-M2 is a $2\\times2$ multi-anode 2-inch\nphotomultiplier tube that offers a compact form factor, low intrinsic\nradioactivity, and high photocathode coverage. These characteristics make it a\npromising candidate for next-generation xenon-based direct detection dark\nmatter experiments, such as XLZD and PandaX-xT. We present a detailed\ncharacterization of this photosensor operated in cold xenon environments,\nfocusing on its single photoelectron response, dark count rate, light emission,\nand afterpulsing behavior. The device demonstrated a gain exceeding $2\\cdot\n10^6$ at the nominal voltage of -1.0 kV, along with a low dark count rate of\n$(0.4\\pm0.2)\\;\\text{Hz/cm}^2$. Due to the compact design, afterpulses exhibited\nshort delay times, resulting in some cases in an overlap with the light-induced\nsignal. To evaluate its applicability in a realistic detector environment, two\nR12699-406-M2 units were deployed in a small-scale dual-phase xenon time\nprojection chamber. The segmented $2\\times2$ anode structure enabled lateral\nposition reconstruction using a single photomultiplier tube, highlighting the\npotential of the sensor for effective event localization in future detectors."
                },
                "authors": [
                    {
                        "name": "M. Adrover"
                    },
                    {
                        "name": "L. Baudis"
                    },
                    {
                        "name": "A. Bismark"
                    },
                    {
                        "name": "A. P. Colijn"
                    },
                    {
                        "name": "J. J. Cuenca-García"
                    },
                    {
                        "name": "M. P. Decowski"
                    },
                    {
                        "name": "M. Flierman"
                    },
                    {
                        "name": "T. den Hollander"
                    }
                ],
                "author_detail": {
                    "name": "T. den Hollander"
                },
                "author": "T. den Hollander",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04826v1",
                "updated": "2025-06-05T09:49:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    9,
                    49,
                    1,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T09:49:01Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    9,
                    49,
                    1,
                    3,
                    156,
                    0
                ],
                "title": "Discharge dynamics in a cylindrical SDBD prototype reactor under\n  ns-pulsed and sinusoidal AC operation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discharge dynamics in a cylindrical SDBD prototype reactor under\n  ns-pulsed and sinusoidal AC operation"
                },
                "summary": "We developed a prototype reactor generating surface dielectric barrier\ndischarges (SDBDs) in ambient air, designed for consistent operation while\npreventing constructive material degradation. It features detachable stainless\nsteel electrodes and quartz dielectric to ensure precise fabrication. The\ngrounded electrode is fully immersed into transformer oil drastically\nsuppressing undesired parasitic discharges. The device efficiently sustains\nns-pulsed and AC discharges at 10 kHz, enabling fundamental studies of their\nelectrical characteristics (applied voltage, induced current, electric power)\nand spatiotemporal dynamics (morphology, propagation length and velocity). The\nelectric power (P) consumed exhibits a dissimilar non-linear increase with the\nrising peak voltage (Vp) in each case: P$\\approx$0.8-2.5 W for ns-pulsed\n(Vp=7-9 kV) and P$\\approx$0.9-5.3 W (Vp=7-10 kV) for AC operation. Using ICCD\nimaging, distinct ionization channels are recorded in the rising part of the\npulsed voltage being detached from the driven electrode; during the voltage\ndecrease, a glow-like discharge is formed remaining anchored on the driven\nelectrode. The rising part of the AC voltage is characterized by erratic,\nelongated ionization channels in a filamentary form, the voltage drop featuring\na glow-like behavior. During the rising and falling parts of the AC voltage,\nthe discharge reaches maximum propagation lengths (Lmax) of $\\approx$12 mm and\n$\\approx$7 mm, respectively, while remaining attached to the driven electrode.\nThe corresponding maximum discharge velocities (vmax) are about 5x10 2 m/s and\n3x10 2 m/s. For the ns-pulsed operation, Lmax$\\approx$5 mm (vmax$\\approx$5x10 5\nm/s) and Lmax$\\approx$3.5 mm (vmax$\\approx$1.5x10 5 m/s) during the rising and\nfalling parts of the voltage pulse, respectively. The SDBD dynamics generated\nwith a ns-pulsed voltage is more reproducible than for the AC case allowing for\nthe use of a 500 times smaller ICCD gate width (2 ns) and a more accurate\ndescription of the discharge's spatiotemporal development. This reactor is\nsuitable for performing fundamental studies and understanding key SDBD features\nfor various applications such as flow control, biomedicine and agriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a prototype reactor generating surface dielectric barrier\ndischarges (SDBDs) in ambient air, designed for consistent operation while\npreventing constructive material degradation. It features detachable stainless\nsteel electrodes and quartz dielectric to ensure precise fabrication. The\ngrounded electrode is fully immersed into transformer oil drastically\nsuppressing undesired parasitic discharges. The device efficiently sustains\nns-pulsed and AC discharges at 10 kHz, enabling fundamental studies of their\nelectrical characteristics (applied voltage, induced current, electric power)\nand spatiotemporal dynamics (morphology, propagation length and velocity). The\nelectric power (P) consumed exhibits a dissimilar non-linear increase with the\nrising peak voltage (Vp) in each case: P$\\approx$0.8-2.5 W for ns-pulsed\n(Vp=7-9 kV) and P$\\approx$0.9-5.3 W (Vp=7-10 kV) for AC operation. Using ICCD\nimaging, distinct ionization channels are recorded in the rising part of the\npulsed voltage being detached from the driven electrode; during the voltage\ndecrease, a glow-like discharge is formed remaining anchored on the driven\nelectrode. The rising part of the AC voltage is characterized by erratic,\nelongated ionization channels in a filamentary form, the voltage drop featuring\na glow-like behavior. During the rising and falling parts of the AC voltage,\nthe discharge reaches maximum propagation lengths (Lmax) of $\\approx$12 mm and\n$\\approx$7 mm, respectively, while remaining attached to the driven electrode.\nThe corresponding maximum discharge velocities (vmax) are about 5x10 2 m/s and\n3x10 2 m/s. For the ns-pulsed operation, Lmax$\\approx$5 mm (vmax$\\approx$5x10 5\nm/s) and Lmax$\\approx$3.5 mm (vmax$\\approx$1.5x10 5 m/s) during the rising and\nfalling parts of the voltage pulse, respectively. The SDBD dynamics generated\nwith a ns-pulsed voltage is more reproducible than for the AC case allowing for\nthe use of a 500 times smaller ICCD gate width (2 ns) and a more accurate\ndescription of the discharge's spatiotemporal development. This reactor is\nsuitable for performing fundamental studies and understanding key SDBD features\nfor various applications such as flow control, biomedicine and agriculture."
                },
                "authors": [
                    {
                        "name": "Konstantinos Giotis"
                    },
                    {
                        "name": "Dimitrios Stefas"
                    },
                    {
                        "name": "Yanis Agha"
                    },
                    {
                        "name": "Hans Höft"
                    },
                    {
                        "name": "Xavier Duten"
                    },
                    {
                        "name": "Panagiotis Svarnas"
                    },
                    {
                        "name": "Guillaume Lombardi"
                    },
                    {
                        "name": "Kristaq Gazeli"
                    }
                ],
                "author_detail": {
                    "name": "Kristaq Gazeli"
                },
                "arxiv_affiliation": "LSPM",
                "author": "Kristaq Gazeli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04108v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04108v2",
                "updated": "2025-06-05T05:39:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    5,
                    39,
                    48,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-04T16:01:48Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    1,
                    48,
                    2,
                    155,
                    0
                ],
                "title": "Rectified Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rectified Sparse Attention"
                },
                "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42$\\times$ end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM."
                },
                "authors": [
                    {
                        "name": "Yutao Sun"
                    },
                    {
                        "name": "Tianzhu Ye"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Yizhao Gao"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04108v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04108v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04642v1",
                "updated": "2025-06-05T05:23:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    5,
                    23,
                    38,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-05T05:23:38Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    5,
                    23,
                    38,
                    3,
                    156,
                    0
                ],
                "title": "TaDA: Training-free recipe for Decoding with Adaptive KV Cache\n  Compression and Mean-centering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TaDA: Training-free recipe for Decoding with Adaptive KV Cache\n  Compression and Mean-centering"
                },
                "summary": "The key-value (KV) cache in transformer models is a critical component for\nefficient decoding or inference, yet its memory demands scale poorly with\nsequence length, posing a major challenge for scalable deployment of large\nlanguage models. Among several approaches to KV cache compression, quantization\nof key and value activations has been widely explored. Most KV cache\nquantization methods still need to manage sparse and noncontiguous outliers\nseparately. To address this, we introduce TaDA, a training-free recipe for KV\ncache compression with quantization precision that adapts to error sensitivity\nacross layers and a mean centering to eliminate separate outlier handling. Our\napproach yields substantial accuracy improvements for multiple models\nsupporting various context lengths. Moreover, our approach does not need to\nseparately manage outlier elements -- a persistent hurdle in most traditional\nquantization methods. Experiments on standard benchmarks demonstrate that our\ntechnique reduces KV cache memory footprint to 27% of the original 16-bit\nbaseline while achieving comparable accuracy. Our method paves the way for\nscalable and high-performance reasoning in language models by potentially\nenabling inference for longer context length models, reasoning models, and\nlonger chain of thoughts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache in transformer models is a critical component for\nefficient decoding or inference, yet its memory demands scale poorly with\nsequence length, posing a major challenge for scalable deployment of large\nlanguage models. Among several approaches to KV cache compression, quantization\nof key and value activations has been widely explored. Most KV cache\nquantization methods still need to manage sparse and noncontiguous outliers\nseparately. To address this, we introduce TaDA, a training-free recipe for KV\ncache compression with quantization precision that adapts to error sensitivity\nacross layers and a mean centering to eliminate separate outlier handling. Our\napproach yields substantial accuracy improvements for multiple models\nsupporting various context lengths. Moreover, our approach does not need to\nseparately manage outlier elements -- a persistent hurdle in most traditional\nquantization methods. Experiments on standard benchmarks demonstrate that our\ntechnique reduces KV cache memory footprint to 27% of the original 16-bit\nbaseline while achieving comparable accuracy. Our method paves the way for\nscalable and high-performance reasoning in language models by potentially\nenabling inference for longer context length models, reasoning models, and\nlonger chain of thoughts."
                },
                "authors": [
                    {
                        "name": "Vinay Joshi"
                    },
                    {
                        "name": "Pratik Prabhanjan Brahma"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "ACL-2025 industry-track accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v3",
                "updated": "2025-06-05T04:21:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    4,
                    21,
                    30,
                    3,
                    156,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively affects key Service Level Objectives (SLOs),\nsuch as time to first token (TTFT) and time per output token (TPOT). We\nintroduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that\nseparates the encoding, prefill, and decode stages onto dedicated resources.\nUnlike current systems, which bundle encoding and prefill together, our\napproach decouples these steps, unlocking new opportunities and optimizations.\nThese include a mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize the encoding load within a request, a module for\noptimal resource allocation for disaggregated serving, and a novel\nrole-switching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more\nimages per request, and 2.2x larger KV caches. Furthermore, it leads to\nsignificant improvements in SLO attainment (up to 90-100% improvement) and TTFT\n(up to 71% reduction), compared to systems that do not disaggregate. The code\nis available at https://github.com/vbdi/epdserve."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "17 pages, 12 figures, 9 tables",
                "arxiv_journal_ref": "International Conference on Machine Proceedings of the 42nd\n  International Conference on Machine Learning, Vancouver, Canada. PMLR 267,\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04213v2",
                "updated": "2025-06-05T03:35:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    3,
                    35,
                    21,
                    3,
                    156,
                    0
                ],
                "published": "2025-06-04T17:57:09Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    57,
                    9,
                    2,
                    155,
                    0
                ],
                "title": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FullDiT2: Efficient In-Context Conditioning for Video Diffusion\n  Transformers"
                },
                "summary": "Fine-grained and efficient controllability on video diffusion transformers\nhas raised increasing desires for the applicability. Recently, In-context\nConditioning emerged as a powerful paradigm for unified conditional video\ngeneration, which enables diverse controls by concatenating varying context\nconditioning signals with noisy video latents into a long unified token\nsequence and jointly processing them via full-attention, e.g., FullDiT. Despite\ntheir effectiveness, these methods face quadratic computation overhead as task\ncomplexity increases, hindering practical deployment. In this paper, we study\nthe efficiency bottleneck neglected in original in-context conditioning video\ngeneration framework. We begin with systematic analysis to identify two key\nsources of the computation inefficiencies: the inherent redundancy within\ncontext condition tokens and the computational redundancy in context-latent\ninteractions throughout the diffusion process. Based on these insights, we\npropose FullDiT2, an efficient in-context conditioning framework for general\ncontrollability in both video generation and editing tasks, which innovates\nfrom two key perspectives. Firstly, to address the token redundancy, FullDiT2\nleverages a dynamic token selection mechanism to adaptively identify important\ncontext tokens, reducing the sequence length for unified full-attention.\nAdditionally, a selective context caching mechanism is devised to minimize\nredundant interactions between condition tokens and video latents. Extensive\nexperiments on six diverse conditional video editing and generation tasks\ndemonstrate that FullDiT2 achieves significant computation reduction and 2-3\ntimes speedup in averaged time cost per diffusion step, with minimal\ndegradation or even higher performance in video generation quality. The project\npage is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained and efficient controllability on video diffusion transformers\nhas raised increasing desires for the applicability. Recently, In-context\nConditioning emerged as a powerful paradigm for unified conditional video\ngeneration, which enables diverse controls by concatenating varying context\nconditioning signals with noisy video latents into a long unified token\nsequence and jointly processing them via full-attention, e.g., FullDiT. Despite\ntheir effectiveness, these methods face quadratic computation overhead as task\ncomplexity increases, hindering practical deployment. In this paper, we study\nthe efficiency bottleneck neglected in original in-context conditioning video\ngeneration framework. We begin with systematic analysis to identify two key\nsources of the computation inefficiencies: the inherent redundancy within\ncontext condition tokens and the computational redundancy in context-latent\ninteractions throughout the diffusion process. Based on these insights, we\npropose FullDiT2, an efficient in-context conditioning framework for general\ncontrollability in both video generation and editing tasks, which innovates\nfrom two key perspectives. Firstly, to address the token redundancy, FullDiT2\nleverages a dynamic token selection mechanism to adaptively identify important\ncontext tokens, reducing the sequence length for unified full-attention.\nAdditionally, a selective context caching mechanism is devised to minimize\nredundant interactions between condition tokens and video latents. Extensive\nexperiments on six diverse conditional video editing and generation tasks\ndemonstrate that FullDiT2 achieves significant computation reduction and 2-3\ntimes speedup in averaged time cost per diffusion step, with minimal\ndegradation or even higher performance in video generation quality. The project\npage is at \\href{https://fulldit2.github.io/}{https://fulldit2.github.io/}."
                },
                "authors": [
                    {
                        "name": "Xuanhua He"
                    },
                    {
                        "name": "Quande Liu"
                    },
                    {
                        "name": "Zixuan Ye"
                    },
                    {
                        "name": "Weicai Ye"
                    },
                    {
                        "name": "Qiulin Wang"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24357v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24357v2",
                "updated": "2025-06-05T02:27:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    5,
                    2,
                    27,
                    34,
                    3,
                    156,
                    0
                ],
                "published": "2025-05-30T08:49:27Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    8,
                    49,
                    27,
                    4,
                    150,
                    0
                ],
                "title": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline\n  Calibration"
                },
                "summary": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. The code and models will be available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable performance, yet their\ncapability on long-context reasoning is often constrained by the excessive\nmemory required to store the Key-Value (KV) cache. This makes KV cache\ncompression an essential step toward enabling efficient long-context reasoning.\nRecent methods have explored reducing the hidden dimensions of the KV cache,\nbut many introduce additional computation through projection layers or suffer\nfrom significant performance degradation under high compression ratios. To\naddress these challenges, we propose ReCalKV, a post-training KV cache\ncompression method that reduces the hidden dimensions of the KV cache. We\ndevelop distinct compression strategies for Keys and Values based on their\ndifferent roles and varying importance in the attention mechanism. For Keys, we\npropose Head-wise Similarity-aware Reordering (HSR), which clusters similar\nheads and applies grouped SVD to the key projection matrix, reducing additional\ncomputation while preserving accuracy. For Values, we propose Offline\nCalibration and Matrix Fusion (OCMF) to preserve accuracy without extra\ncomputational overhead. Experiments show that ReCalKV outperforms existing\nlow-rank compression methods, achieving high compression ratios with minimal\nperformance loss. The code and models will be available at:\nhttps://github.com/XIANGLONGYAN/ReCalKV."
                },
                "authors": [
                    {
                        "name": "Xianglong Yan"
                    },
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Tianao Zhang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24357v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24357v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02488v2",
                "updated": "2025-06-04T23:47:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    23,
                    47,
                    53,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-03T06:02:50Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    2,
                    50,
                    1,
                    154,
                    0
                ],
                "title": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for\n  Efficient Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for\n  Efficient Diffusion Models"
                },
                "summary": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) are powerful generative models capable of producing\nhigh-fidelity images but are constrained by high computational costs due to\niterative multi-step inference. While Neural Architecture Search (NAS) can\noptimize DMs, existing methods are hindered by retraining requirements,\nexponential search complexity from step-wise optimization, and slow evaluation\nrelying on massive image generation. To address these challenges, we propose\nFlexiffusion, a training-free NAS framework that jointly optimizes generation\nschedules and model architectures without modifying pre-trained parameters. Our\nkey insight is to decompose the generation process into flexible segments of\nequal length, where each segment dynamically combines three step types: full\n(complete computation), partial (cache-reused computation), and null (skipped\ncomputation). This segment-wise search space reduces the candidate pool\nexponentially compared to step-wise NAS while preserving architectural\ndiversity. Further, we introduce relative FID (rFID), a lightweight evaluation\nmetric for NAS that measures divergence from a teacher model's outputs instead\nof ground truth, slashing evaluation time by over $90\\%$. In practice,\nFlexiffusion achieves at least $2\\times$ acceleration across LDMs, Stable\nDiffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\\%$,\noutperforming prior NAS and caching methods. Notably, it attains $5.1\\times$\nspeedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers\na resource-efficient paradigm for searching high-speed DMs without sacrificing\nquality."
                },
                "authors": [
                    {
                        "name": "Hongtao Huang"
                    },
                    {
                        "name": "Xiaojun Chang"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "arxiv_comment": "This paper was intended to be a v2 version of my previous paper\n  (arXiv:2409.17566), but it was submitted as a new paper by mistake",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v3",
                "updated": "2025-06-04T22:37:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    22,
                    37,
                    29,
                    2,
                    155,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermüller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00839v2",
                "updated": "2025-06-04T18:10:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    18,
                    10,
                    39,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-01T05:04:56Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    5,
                    4,
                    56,
                    6,
                    152,
                    0
                ],
                "title": "Neural Path Guiding with Distribution Factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Path Guiding with Distribution Factorization"
                },
                "summary": "In this paper, we present a neural path guiding method to aid with Monte\nCarlo (MC) integration in rendering. Existing neural methods utilize\ndistribution representations that are either fast or expressive, but not both.\nWe propose a simple, but effective, representation that is sufficiently\nexpressive and reasonably fast. Specifically, we break down the 2D distribution\nover the directional domain into two 1D probability distribution functions\n(PDF). We propose to model each 1D PDF using a neural network that estimates\nthe distribution at a set of discrete coordinates. The PDF at an arbitrary\nlocation can then be evaluated and sampled through interpolation. To train the\nnetwork, we maximize the similarity of the learned and target distributions. To\nreduce the variance of the gradient during optimizations and estimate the\nnormalization factor, we propose to cache the incoming radiance using an\nadditional network. Through extensive experiments, we demonstrate that our\napproach is better than the existing methods, particularly in challenging\nscenes with complex light transport.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a neural path guiding method to aid with Monte\nCarlo (MC) integration in rendering. Existing neural methods utilize\ndistribution representations that are either fast or expressive, but not both.\nWe propose a simple, but effective, representation that is sufficiently\nexpressive and reasonably fast. Specifically, we break down the 2D distribution\nover the directional domain into two 1D probability distribution functions\n(PDF). We propose to model each 1D PDF using a neural network that estimates\nthe distribution at a set of discrete coordinates. The PDF at an arbitrary\nlocation can then be evaluated and sampled through interpolation. To train the\nnetwork, we maximize the similarity of the learned and target distributions. To\nreduce the variance of the gradient during optimizations and estimate the\nnormalization factor, we propose to cache the incoming radiance using an\nadditional network. Through extensive experiments, we demonstrate that our\napproach is better than the existing methods, particularly in challenging\nscenes with complex light transport."
                },
                "authors": [
                    {
                        "name": "Pedro Figueiredo"
                    },
                    {
                        "name": "Qihao He"
                    },
                    {
                        "name": "Nima Khademi Kalantari"
                    }
                ],
                "author_detail": {
                    "name": "Nima Khademi Kalantari"
                },
                "author": "Nima Khademi Kalantari",
                "arxiv_comment": "11 pages, 11 figures. Accepted to EGSR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04225v1",
                "updated": "2025-06-04T17:59:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    59,
                    4,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T17:59:04Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    17,
                    59,
                    4,
                    2,
                    155,
                    0
                ],
                "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation"
                },
                "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications."
                },
                "authors": [
                    {
                        "name": "Tianyu Huang"
                    },
                    {
                        "name": "Wangguandong Zheng"
                    },
                    {
                        "name": "Tengfei Wang"
                    },
                    {
                        "name": "Yuhao Liu"
                    },
                    {
                        "name": "Zhenwei Wang"
                    },
                    {
                        "name": "Junta Wu"
                    },
                    {
                        "name": "Jie Jiang"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Rynson W. H. Lau"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    },
                    {
                        "name": "Chunchao Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chunchao Guo"
                },
                "author": "Chunchao Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05410v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05410v1",
                "updated": "2025-06-04T16:10:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    10,
                    44,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T16:10:44Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    10,
                    44,
                    2,
                    155,
                    0
                ],
                "title": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache\n  Asymmetry for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache\n  Asymmetry for Long-Context LLMs"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have highlighted the critical\nimportance of extending context length, yet the quadratic complexity of\nattention mechanisms poses significant challenges for efficient long-context\nmodeling. KV cache compression has emerged as a key approach to address this\nchallenge. Through extensive empirical analysis, we reveal a fundamental yet\npreviously overlooked asymmetry in KV caches: while adjacent keys receive\nsimilar attention weights (local homogeneity), adjacent values demonstrate\ndistinct heterogeneous distributions. This key-value asymmetry reveals a\ncritical limitation in existing compression methods that treat keys and values\nuniformly. To address the limitation, we propose a training-free compression\nframework (AsymKV) that combines homogeneity-based key merging with a\nmathematically proven lossless value compression. Extensive experiments\ndemonstrate that AsymKV consistently outperforms existing long-context methods\nacross various tasks and base models. For example, on LLaMA3.1-8B, AsymKV\nachieves an average score of 43.95 on LongBench, surpassing SOTA methods like\nH$_2$O (38.89) by a large margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have highlighted the critical\nimportance of extending context length, yet the quadratic complexity of\nattention mechanisms poses significant challenges for efficient long-context\nmodeling. KV cache compression has emerged as a key approach to address this\nchallenge. Through extensive empirical analysis, we reveal a fundamental yet\npreviously overlooked asymmetry in KV caches: while adjacent keys receive\nsimilar attention weights (local homogeneity), adjacent values demonstrate\ndistinct heterogeneous distributions. This key-value asymmetry reveals a\ncritical limitation in existing compression methods that treat keys and values\nuniformly. To address the limitation, we propose a training-free compression\nframework (AsymKV) that combines homogeneity-based key merging with a\nmathematically proven lossless value compression. Extensive experiments\ndemonstrate that AsymKV consistently outperforms existing long-context methods\nacross various tasks and base models. For example, on LLaMA3.1-8B, AsymKV\nachieves an average score of 43.95 on LongBench, surpassing SOTA methods like\nH$_2$O (38.89) by a large margin."
                },
                "authors": [
                    {
                        "name": "Wanyun Cui"
                    },
                    {
                        "name": "Mingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Mingwei Xu"
                },
                "author": "Mingwei Xu",
                "arxiv_comment": "14 pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05410v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05410v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v2",
                "updated": "2025-06-04T16:08:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    16,
                    8,
                    50,
                    2,
                    155,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial\n  Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPR: Efficient LLM Inference with I/O-Aware KV Cache Partial\n  Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) cache is used to\nstore intermediate activations, which significantly lowers the computational\noverhead for token generation. However, the memory required for the KV cache\ngrows rapidly, often exceeding the capacity of GPU memory. A cost-effective\nalternative is to offload KV cache to CPU memory, which alleviates GPU memory\npressure, but shifts the bottleneck to the limited bandwidth of the PCIe\nconnection between the CPU and GPU. Existing methods attempt to address these\nissues by overlapping GPU computation with I/O or employing CPU-GPU\nheterogeneous execution, but they are hindered by excessive data movement and\ndependence on CPU capabilities. Fully overlapping PCIe communication latency\ngets challenging as the size of the KV cache grows and/or the GPU compute\ncapabilities increase. In this paper, we introduce KVPR, an efficient I/O-aware\nLLM inference method where the CPU first transfers a partial set of\nactivations, from which the GPU can start recomputing the KV cache values.\nWhile the GPU recomputes the partial KV cache, the remaining portion of the KV\ncache is transferred concurrently from the CPU. This approach overlaps GPU\nrecomputation with KV cache transfer to minimize idle GPU time and maximize\ninference performance. KVPR is fully automated by integrating a profiler module\nthat utilizes input characteristics and system hardware information, a\nscheduler module to optimize the distribution of computation and communication\nworkloads, and a runtime module to efficiently execute the derived execution\nplan. Experimental results show that KVPR achieves up to 35.8% lower latency\nand 46.2% higher throughput during decoding compared to state-of-the-art\napproaches. The code is available at https://github.com/chaoyij/KVPR."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "arxiv_comment": "ACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03854v1",
                "updated": "2025-06-04T11:37:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    37,
                    51,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T11:37:51Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    11,
                    37,
                    51,
                    2,
                    155,
                    0
                ],
                "title": "Analysis of Server Throughput For Managed Big Data Analytics Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Server Throughput For Managed Big Data Analytics Frameworks"
                },
                "summary": "Managed big data frameworks, such as Apache Spark and Giraph demand a large\namount of memory per core to process massive volume datasets effectively. The\nmemory pressure that arises from the big data processing leads to high garbage\ncollection (GC) overhead. Big data analytics frameworks attempt to remove this\noverhead by offloading objects to storage devices. At the same time,\ninfrastructure providers, trying to address the same problem, attribute more\nmemory to increase memory per instance leaving cores underutilized. For\nframeworks, trying to avoid GC through offloading to storage devices leads to\nhigh Serialization/Deserialization (S/D) overhead. For infrastructure, the\nresult is that resource usage is decreased. These limitations prevent managed\nbig data frameworks from effectively utilizing the CPU thus leading to low\nserver throughput.\n  We conduct a methodological analysis of server throughput for managed big\ndata analytics frameworks. More specifically, we examine, whether reducing GC\nand S/D can help increase the effective CPU utilization of the server. We use a\nsystem called TeraHeap that moves objects from the Java managed heap (H1) to a\nsecondary heap over a fast storage device (H2) to reduce the GC overhead and\neliminate S/D over data. We focus on analyzing the system's performance under\nthe co-location of multiple memory-bound instances to utilize all available\nDRAM and study server throughput. Our detailed methodology includes choosing\nthe DRAM budget for each instance and how to distribute this budget among H1\nand Page Cache (PC). We try two different distributions for the DRAM budget,\none with more H1 and one with more PC to study the needs of both approaches. We\nevaluate both techniques under 3 different memory-per-core scenarios using\nSpark and Giraph with native JVM or JVM with TeraHeap. We do this to check\nthroughput changes when memory capacity increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managed big data frameworks, such as Apache Spark and Giraph demand a large\namount of memory per core to process massive volume datasets effectively. The\nmemory pressure that arises from the big data processing leads to high garbage\ncollection (GC) overhead. Big data analytics frameworks attempt to remove this\noverhead by offloading objects to storage devices. At the same time,\ninfrastructure providers, trying to address the same problem, attribute more\nmemory to increase memory per instance leaving cores underutilized. For\nframeworks, trying to avoid GC through offloading to storage devices leads to\nhigh Serialization/Deserialization (S/D) overhead. For infrastructure, the\nresult is that resource usage is decreased. These limitations prevent managed\nbig data frameworks from effectively utilizing the CPU thus leading to low\nserver throughput.\n  We conduct a methodological analysis of server throughput for managed big\ndata analytics frameworks. More specifically, we examine, whether reducing GC\nand S/D can help increase the effective CPU utilization of the server. We use a\nsystem called TeraHeap that moves objects from the Java managed heap (H1) to a\nsecondary heap over a fast storage device (H2) to reduce the GC overhead and\neliminate S/D over data. We focus on analyzing the system's performance under\nthe co-location of multiple memory-bound instances to utilize all available\nDRAM and study server throughput. Our detailed methodology includes choosing\nthe DRAM budget for each instance and how to distribute this budget among H1\nand Page Cache (PC). We try two different distributions for the DRAM budget,\none with more H1 and one with more PC to study the needs of both approaches. We\nevaluate both techniques under 3 different memory-per-core scenarios using\nSpark and Giraph with native JVM or JVM with TeraHeap. We do this to check\nthroughput changes when memory capacity increases."
                },
                "authors": [
                    {
                        "name": "Emmanouil Anagnostakis"
                    },
                    {
                        "name": "Polyvios Pratikakis"
                    }
                ],
                "author_detail": {
                    "name": "Polyvios Pratikakis"
                },
                "author": "Polyvios Pratikakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03762v1",
                "updated": "2025-06-04T09:25:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    25,
                    53,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T09:25:53Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    9,
                    25,
                    53,
                    2,
                    155,
                    0
                ],
                "title": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for\n  Efficient Inference of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for\n  Efficient Inference of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced the field of\nArtificial Intelligence. However, their deployment is resource-intensive, not\nonly due to the large number of model parameters but also because the\n(Key-Value) KV cache consumes a lot of memory during inference. While several\nworks propose reducing the KV cache by evicting the unnecessary tokens, these\napproaches rely on accumulated attention score as eviction score to quantify\nthe importance of the token. We identify the accumulated attention score is\nbiased and it decreases with the position of the tokens in the mathematical\nexpectation. As a result, the retained tokens concentrate on the initial\npositions, limiting model's access to global contextual information. To address\nthis issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the\nbias of the accumulated attention score by adaptively tuning the scale of\nsoftmax according the expectation of information entropy of attention scores.\nTo make use of the holistic attention information in self-attention mechanism,\nAhaKV utilize the information of value vectors, which is overlooked in previous\nworks, to refine the adaptive score. We show theoretically that our method is\nwell suited for bias reduction. We deployed AhaKV on different models with a\nfixed cache budget. Experiments show that AhaKV successfully mitigates bias and\nretains crucial tokens across global context and achieve state-of-the-art\nresults against other related work on several benchmark tasks."
                },
                "authors": [
                    {
                        "name": "Yifeng Gu"
                    },
                    {
                        "name": "Zicong Jiang"
                    },
                    {
                        "name": "Jianxiu Jin"
                    },
                    {
                        "name": "Kailing Guo"
                    },
                    {
                        "name": "Ziyang Zhang"
                    },
                    {
                        "name": "Xiangmin Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xiangmin Xu"
                },
                "author": "Xiangmin Xu",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03700v1",
                "updated": "2025-06-04T08:32:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    8,
                    32,
                    30,
                    2,
                    155,
                    0
                ],
                "published": "2025-06-04T08:32:30Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    8,
                    32,
                    30,
                    2,
                    155,
                    0
                ],
                "title": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism"
                },
                "summary": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used for long-content\ngeneration (e.g., long Chain-of-Thought reasoning) where decoding efficiency\nbecomes a critical bottleneck: Autoregressive decoding is inherently limited by\nits sequential token generation process, where each token must be generated\nbefore the next can be processed. This sequential dependency restricts the\nability to fully leverage modern hardware's parallel processing capabilities.\nExisting methods like speculative decoding and layer skipping offer potential\nspeedups but have notable drawbacks: speculative decoding relies on an\nauxiliary \"drafter\" model, which can be challenging to acquire and increases\nmemory overhead, while layer skipping may introduce discrepancies in the\noutputs due to the missing key-value cache at skipped layers. In this work, we\npropose AdaDecode, which accelerates LLM decoding without requiring auxiliary\nmodels or changes to the original model parameters, while ensuring output\nconsistency. AdaDecode leverages the insight that many tokens can accurately be\ngenerated at intermediate layers, as further layers often do not significantly\nalter predictions once the model reaches a certain confidence. By adaptively\ngenerating tokens at intermediate layers when confidence is high, AdaDecode\nenables the next token's computation to begin immediately. The remaining layer\ncomputations for early-predicted tokens are deferred and executed in parallel\nwith subsequent tokens when needed, maximizing hardware utilization and\nreducing decoding latency. A final verification step ensures that early\npredictions match the results of standard autoregressive decoding, preserving\noutput parity. Experiments across diverse generation tasks shows that AdaDecode\nconsistently achieves superior decoding throughput with up to 1.73x speedup,\nwhile guaranteeing output parity with standard autoregressive decoding."
                },
                "authors": [
                    {
                        "name": "Zhepei Wei"
                    },
                    {
                        "name": "Wei-Lin Chen"
                    },
                    {
                        "name": "Xinyu Zhu"
                    },
                    {
                        "name": "Yu Meng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Meng"
                },
                "author": "Yu Meng",
                "arxiv_comment": "ICML 2025. Code: https://github.com/weizhepei/AdaDecode",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01969v2",
                "updated": "2025-06-04T03:20:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    4,
                    3,
                    20,
                    26,
                    2,
                    155,
                    0
                ],
                "published": "2025-05-13T17:45:34Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    45,
                    34,
                    1,
                    133,
                    0
                ],
                "title": "FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating\n  MLA Inference on NVIDIA H20 GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating\n  MLA Inference on NVIDIA H20 GPUs"
                },
                "summary": "Efficient inference of Multi-Head Latent Attention (MLA) is challenged by\ndeploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper\nintroduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the\nsingle-instance deployment scenario on NVIDIA H20 GPUs. We propose the\nEfficient Transpose Attention Pipeline (ETAP), which reconfigures attention\ncomputation through transposition to align the KV context length with the\n\\(M\\)-dimension in WGMMA operations, significantly reducing redundant\ncomputations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K\nsequence length (batch size 16), with 5.24x and 4.94x improvements over\nFlashAttention-3 and FlashInfer, respectively, while maintaining numerical\nstability with a 15.2x lower RMSE (\\(1.25 \\times 10^{-5}\\)) than\nFlashAttention-3. Furthermore, ETAP's design enables seamless integration into\nframeworks like FlashAttention-3 and FlashInfer, supported by a detailed\ntheoretical analysis. Our work addresses a critical gap in resource-constrained\ninference, offering a scalable solution for mid-tier GPUs and paving the way\nfor broader adoption in hardware-aware optimization. Code is available at\nhttps://github.com/pengcuo/FlashMLA-ETAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient inference of Multi-Head Latent Attention (MLA) is challenged by\ndeploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper\nintroduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the\nsingle-instance deployment scenario on NVIDIA H20 GPUs. We propose the\nEfficient Transpose Attention Pipeline (ETAP), which reconfigures attention\ncomputation through transposition to align the KV context length with the\n\\(M\\)-dimension in WGMMA operations, significantly reducing redundant\ncomputations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K\nsequence length (batch size 16), with 5.24x and 4.94x improvements over\nFlashAttention-3 and FlashInfer, respectively, while maintaining numerical\nstability with a 15.2x lower RMSE (\\(1.25 \\times 10^{-5}\\)) than\nFlashAttention-3. Furthermore, ETAP's design enables seamless integration into\nframeworks like FlashAttention-3 and FlashInfer, supported by a detailed\ntheoretical analysis. Our work addresses a critical gap in resource-constrained\ninference, offering a scalable solution for mid-tier GPUs and paving the way\nfor broader adoption in hardware-aware optimization. Code is available at\nhttps://github.com/pengcuo/FlashMLA-ETAP."
                },
                "authors": [
                    {
                        "name": "Pengcuo Dege"
                    },
                    {
                        "name": "Qiuming Luo"
                    },
                    {
                        "name": "Rui Mao"
                    },
                    {
                        "name": "Chang Kong"
                    }
                ],
                "author_detail": {
                    "name": "Chang Kong"
                },
                "author": "Chang Kong",
                "arxiv_comment": "15 pages, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.03275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.03275v1",
                "updated": "2025-06-03T18:03:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    3,
                    32,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T18:03:32Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    18,
                    3,
                    32,
                    1,
                    154,
                    0
                ],
                "title": "Chipmunk: Training-Free Acceleration of Diffusion Transformers with\n  Dynamic Column-Sparse Deltas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chipmunk: Training-Free Acceleration of Diffusion Transformers with\n  Dynamic Column-Sparse Deltas"
                },
                "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in\nhigh-quality image and video generation but incur substantial compute cost at\ninference. A common observation is that DiT latent noise vectors change slowly\nacross inference steps, which suggests that the DiT compute may be redundant\nacross steps. In this paper, we aim to speed up inference by reducing this\nredundancy, without additional training. We first study how activations change\nbetween steps in two state-of-the-art open-source DiTs. We find that just 5-25%\nof the values in attention and MLP explain 70-90% of the change in activations\nacross steps. This finding motivates our approach, Chipmunk, which uses dynamic\nsparsity at inference time to recompute only the fastest-changing intermediate\nactivations, while caching the rest. Dynamic sparsity introduces two systems\nchallenges: (1) sparse attention and MLP operations tend to underutilize GPU\ntensor cores; and (2) computing dynamic sparsity patterns at runtime and\ncaching activations both introduce overhead. To address these challenges,\nChipmunk first uses a voxel-based reordering of input tokens to introduce\ncolumn-wise sparsity. We implement column-sparse kernels utilizing efficient\nsparse gathers from global to shared GPU memory, achieving a 9.3x speedup at\n93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk\noverlaps the computation of sparsity patterns and cache updates with other\nparts of the computation (e.g., second layer of the MLP) to hide the extra\nlatency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on\nFLUX.1-dev without compromising generation quality. Furthermore, we show that\nChipmunk can be stacked on top of full step caching, achieving a 3.72x speedup\non HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev\nwith minimal quality impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in\nhigh-quality image and video generation but incur substantial compute cost at\ninference. A common observation is that DiT latent noise vectors change slowly\nacross inference steps, which suggests that the DiT compute may be redundant\nacross steps. In this paper, we aim to speed up inference by reducing this\nredundancy, without additional training. We first study how activations change\nbetween steps in two state-of-the-art open-source DiTs. We find that just 5-25%\nof the values in attention and MLP explain 70-90% of the change in activations\nacross steps. This finding motivates our approach, Chipmunk, which uses dynamic\nsparsity at inference time to recompute only the fastest-changing intermediate\nactivations, while caching the rest. Dynamic sparsity introduces two systems\nchallenges: (1) sparse attention and MLP operations tend to underutilize GPU\ntensor cores; and (2) computing dynamic sparsity patterns at runtime and\ncaching activations both introduce overhead. To address these challenges,\nChipmunk first uses a voxel-based reordering of input tokens to introduce\ncolumn-wise sparsity. We implement column-sparse kernels utilizing efficient\nsparse gathers from global to shared GPU memory, achieving a 9.3x speedup at\n93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk\noverlaps the computation of sparsity patterns and cache updates with other\nparts of the computation (e.g., second layer of the MLP) to hide the extra\nlatency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on\nFLUX.1-dev without compromising generation quality. Furthermore, we show that\nChipmunk can be stacked on top of full step caching, achieving a 3.72x speedup\non HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev\nwith minimal quality impact."
                },
                "authors": [
                    {
                        "name": "Austin Silveria"
                    },
                    {
                        "name": "Soham V. Govande"
                    },
                    {
                        "name": "Daniel Y. Fu"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Y. Fu"
                },
                "author": "Daniel Y. Fu",
                "arxiv_comment": "10 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.03275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.03275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v2",
                "updated": "2025-06-03T17:18:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    17,
                    18,
                    23,
                    1,
                    154,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02850v1",
                "updated": "2025-06-03T13:19:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T13:19:41Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    13,
                    19,
                    41,
                    1,
                    154,
                    0
                ],
                "title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding"
                },
                "summary": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy."
                },
                "authors": [
                    {
                        "name": "Mengyue Wang"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Kristian Kersting"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "arxiv_comment": "14 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02671v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02671v1",
                "updated": "2025-06-03T09:16:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    16,
                    51,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T09:16:51Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    9,
                    16,
                    51,
                    1,
                    154,
                    0
                ],
                "title": "Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language\n  Models with AdaptNet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language\n  Models with AdaptNet"
                },
                "summary": "Test-time adaptation (TTA) has emerged as a critical technique for enhancing\nthe generalization capability of vision-language models (VLMs) during\ninference. However, existing approaches often incur substantial computational\ncosts and exhibit poor scalability, primarily due to sample-wise adaptation\ngranularity and reliance on costly auxiliary designs such as data augmentation.\nTo address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel\nadapter-based TTA framework that leverages a lightweight, learnable AdaptNet to\nenable efficient and scalable model adaptation. As SAIL's core, a frozen\npre-trained VLM collaborates with AdaptNet through a confidence-based\ninterpolation weight, generating robust predictions during inference. These\npredictions serve as self-supervised targets to align AdaptNet's outputs\nthrough efficient batch-wise processing, dramatically reducing computational\ncosts without modifying the VLM or requiring memory caches. To mitigate\ncatastrophic forgetting during continual adaptation, we propose a\ngradient-aware reset strategy driven by a gradient drift indicator (GDI), which\ndynamically detects domain transitions and strategically resets AdaptNet for\nstable adaptation. Extensive experiments across diverse benchmarks on two\nscenarios demonstrate that SAIL achieves state-of-the-art performance while\nmaintaining low computational costs. These results highlight SAIL's\neffectiveness, efficiency and scalability for real-world deployment. The code\nwill be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) has emerged as a critical technique for enhancing\nthe generalization capability of vision-language models (VLMs) during\ninference. However, existing approaches often incur substantial computational\ncosts and exhibit poor scalability, primarily due to sample-wise adaptation\ngranularity and reliance on costly auxiliary designs such as data augmentation.\nTo address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel\nadapter-based TTA framework that leverages a lightweight, learnable AdaptNet to\nenable efficient and scalable model adaptation. As SAIL's core, a frozen\npre-trained VLM collaborates with AdaptNet through a confidence-based\ninterpolation weight, generating robust predictions during inference. These\npredictions serve as self-supervised targets to align AdaptNet's outputs\nthrough efficient batch-wise processing, dramatically reducing computational\ncosts without modifying the VLM or requiring memory caches. To mitigate\ncatastrophic forgetting during continual adaptation, we propose a\ngradient-aware reset strategy driven by a gradient drift indicator (GDI), which\ndynamically detects domain transitions and strategically resets AdaptNet for\nstable adaptation. Extensive experiments across diverse benchmarks on two\nscenarios demonstrate that SAIL achieves state-of-the-art performance while\nmaintaining low computational costs. These results highlight SAIL's\neffectiveness, efficiency and scalability for real-world deployment. The code\nwill be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Xiao Chen"
                    },
                    {
                        "name": "Jiazhen Huang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Xianghua Fu"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02671v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02671v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v2",
                "updated": "2025-06-14T04:39:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    14,
                    4,
                    39,
                    21,
                    5,
                    165,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02523v1",
                "updated": "2025-06-03T06:53:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    53,
                    4,
                    1,
                    154,
                    0
                ],
                "published": "2025-06-03T06:53:04Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    53,
                    4,
                    1,
                    154,
                    0
                ],
                "title": "Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, improves the\nefficiency of large language models by projecting query, key, and value tensors\ninto a compact latent space. This architectural change reduces the KV-cache\nsize and significantly lowers memory bandwidth demands, particularly in the\nautoregressive decode phase. This letter presents the first hardware-centric\nanalysis of MLA, comparing it to conventional Multi-Head Attention (MHA) and\nevaluating its implications for accelerator performance. We identify two\nalternative execution schemes of MLA--reusing, resp. recomputing latent\nprojection matrices--which offer distinct trade-offs between compute and memory\naccess. Using the Stream design space exploration framework, we model their\nthroughput and energy cost across a range of hardware platforms and find that\nMLA can shift attention workloads toward the compute-bound regime.\n  Our results show that MLA not only reduces bandwidth usage but also enables\nadaptable execution strategies aligned with hardware constraints. Compared to\nMHA, it provides more stable and efficient performance, particularly on\nbandwidth-limited hardware platforms. These findings emphasize MLA's relevance\nas a co-design opportunity for future AI accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, improves the\nefficiency of large language models by projecting query, key, and value tensors\ninto a compact latent space. This architectural change reduces the KV-cache\nsize and significantly lowers memory bandwidth demands, particularly in the\nautoregressive decode phase. This letter presents the first hardware-centric\nanalysis of MLA, comparing it to conventional Multi-Head Attention (MHA) and\nevaluating its implications for accelerator performance. We identify two\nalternative execution schemes of MLA--reusing, resp. recomputing latent\nprojection matrices--which offer distinct trade-offs between compute and memory\naccess. Using the Stream design space exploration framework, we model their\nthroughput and energy cost across a range of hardware platforms and find that\nMLA can shift attention workloads toward the compute-bound regime.\n  Our results show that MLA not only reduces bandwidth usage but also enables\nadaptable execution strategies aligned with hardware constraints. Compared to\nMHA, it provides more stable and efficient performance, particularly on\nbandwidth-limited hardware platforms. These findings emphasize MLA's relevance\nas a co-design opportunity for future AI accelerators."
                },
                "authors": [
                    {
                        "name": "Robin Geens"
                    },
                    {
                        "name": "Marian Verhelst"
                    }
                ],
                "author_detail": {
                    "name": "Marian Verhelst"
                },
                "author": "Marian Verhelst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v3",
                "updated": "2025-06-03T06:43:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    6,
                    43,
                    53,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24133v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24133v3",
                "updated": "2025-06-13T21:01:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    21,
                    1,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-30T02:03:24Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    3,
                    24,
                    4,
                    150,
                    0
                ],
                "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models"
                },
                "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Yikai Zhang"
                    },
                    {
                        "name": "Ke Wan"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Yeyang Zhou"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Anima Anandkumar"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Junjie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Hu"
                },
                "author": "Junjie Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24133v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24133v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v3",
                "updated": "2025-06-03T01:55:18Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    1,
                    55,
                    18,
                    1,
                    154,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v2",
                "updated": "2025-06-03T01:51:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    3,
                    1,
                    51,
                    37,
                    1,
                    154,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3716368.3735166",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3716368.3735166",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.04005v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 14 figures",
                "arxiv_journal_ref": "Great Lakes Symposium on VLSI 2025 (GLSVLSI '25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04975v2",
                "updated": "2025-06-02T19:27:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    19,
                    27,
                    12,
                    0,
                    153,
                    0
                ],
                "published": "2024-11-07T18:49:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    49,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications"
                },
                "summary": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference."
                },
                "authors": [
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Aurick Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Aurick Qiao"
                },
                "author": "Aurick Qiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01928v1",
                "updated": "2025-06-02T17:47:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    47,
                    27,
                    0,
                    153,
                    0
                ],
                "published": "2025-06-02T17:47:27Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    47,
                    27,
                    0,
                    153,
                    0
                ],
                "title": "Esoteric Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Esoteric Language Models"
                },
                "summary": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)"
                },
                "authors": [
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Johnna Liu"
                    },
                    {
                        "name": "Deepansha Singh"
                    },
                    {
                        "name": "Zhoujun Cheng"
                    },
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Eric Xing"
                    },
                    {
                        "name": "John Thickstun"
                    },
                    {
                        "name": "Arash Vahdat"
                    }
                ],
                "author_detail": {
                    "name": "Arash Vahdat"
                },
                "author": "Arash Vahdat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v8",
                "updated": "2025-06-02T17:46:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    2,
                    17,
                    46,
                    50,
                    0,
                    153,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.12015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12015v1",
                "updated": "2025-06-13T17:59:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    59,
                    58,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:59:58Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    59,
                    58,
                    4,
                    164,
                    0
                ],
                "title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction"
                },
                "summary": "Open-source foundation models have seen rapid adoption and development,\nenabling powerful general-purpose capabilities across diverse domains. However,\nfine-tuning large foundation models for domain-specific or personalized tasks\nremains prohibitively expensive for most users due to the significant memory\noverhead beyond that of inference. We introduce EMLoC, an Emulator-based\nMemory-efficient fine-tuning framework with LoRA Correction, which enables\nmodel fine-tuning within the same memory budget required for inference. EMLoC\nconstructs a task-specific light-weight emulator using activation-aware\nsingular value decomposition (SVD) on a small downstream calibration set.\nFine-tuning then is performed on this lightweight emulator via LoRA. To tackle\nthe misalignment between the original model and the compressed emulator, we\npropose a novel compensation algorithm to correct the fine-tuned LoRA module,\nwhich thus can be merged into the original model for inference. EMLoC supports\nflexible compression ratios and standard training pipelines, making it\nadaptable to a wide range of applications. Extensive experiments demonstrate\nthat EMLoC outperforms other baselines across multiple datasets and modalities.\nMoreover, without quantization, EMLoC enables fine-tuning of a 38B model on a\nsingle 24GB consumer GPU-bringing efficient and practical model adaptation to\nindividual users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source foundation models have seen rapid adoption and development,\nenabling powerful general-purpose capabilities across diverse domains. However,\nfine-tuning large foundation models for domain-specific or personalized tasks\nremains prohibitively expensive for most users due to the significant memory\noverhead beyond that of inference. We introduce EMLoC, an Emulator-based\nMemory-efficient fine-tuning framework with LoRA Correction, which enables\nmodel fine-tuning within the same memory budget required for inference. EMLoC\nconstructs a task-specific light-weight emulator using activation-aware\nsingular value decomposition (SVD) on a small downstream calibration set.\nFine-tuning then is performed on this lightweight emulator via LoRA. To tackle\nthe misalignment between the original model and the compressed emulator, we\npropose a novel compensation algorithm to correct the fine-tuned LoRA module,\nwhich thus can be merged into the original model for inference. EMLoC supports\nflexible compression ratios and standard training pipelines, making it\nadaptable to a wide range of applications. Extensive experiments demonstrate\nthat EMLoC outperforms other baselines across multiple datasets and modalities.\nMoreover, without quantization, EMLoC enables fine-tuning of a 38B model on a\nsingle 24GB consumer GPU-bringing efficient and practical model adaptation to\nindividual users."
                },
                "authors": [
                    {
                        "name": "Hsi-Che Lin"
                    },
                    {
                        "name": "Yu-Chu Yu"
                    },
                    {
                        "name": "Kai-Po Chang"
                    },
                    {
                        "name": "Yu-Chiang Frank Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Chiang Frank Wang"
                },
                "author": "Yu-Chiang Frank Wang",
                "arxiv_comment": "Under review. Project page: https://hsi-che-lin.github.io/EMLoC/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12014v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12014v1",
                "updated": "2025-06-13T17:59:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    59,
                    39,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:59:39Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    59,
                    39,
                    4,
                    164,
                    0
                ],
                "title": "code_transformed: The Influence of Large Language Models on Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "code_transformed: The Influence of Large Language Models on Code"
                },
                "summary": "Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style."
                },
                "authors": [
                    {
                        "name": "Yuliang Xu"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Mingmeng Geng"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Xuanhua Shi"
                    },
                    {
                        "name": "Dongping Chen"
                    }
                ],
                "author_detail": {
                    "name": "Dongping Chen"
                },
                "author": "Dongping Chen",
                "arxiv_comment": "We release all the experimental dataset and source code at:\n  https://github.com/ignorancex/LLM_code",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12014v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12014v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12012v1",
                "updated": "2025-06-13T17:59:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    59,
                    10,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:59:10Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    59,
                    10,
                    4,
                    164,
                    0
                ],
                "title": "Tracing LLM Reasoning Processes with Strategic Games: A Framework for\n  Planning, Revision, and Resource-Constrained Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing LLM Reasoning Processes with Strategic Games: A Framework for\n  Planning, Revision, and Resource-Constrained Decision Making"
                },
                "summary": "Large language models (LLMs) are increasingly used for tasks that require\ncomplex reasoning. Most benchmarks focus on final outcomes but overlook the\nintermediate reasoning steps - such as planning, revision, and decision making\nunder resource constraints. We argue that measuring these internal processes is\nessential for understanding model behavior and improving reliability. We\npropose using strategic games as a natural evaluation environment: closed,\nrule-based systems with clear states, limited resources, and automatic\nfeedback. We introduce a framework that evaluates LLMs along three core\ndimensions: planning, revision, and resource-constrained decision making. To\noperationalize this, we define metrics beyond win rate, including\novercorrection risk rate, correction success rate, improvement slope, and\nover-budget ratio. In 4320 adversarial rounds across 12 leading models,\nChatGPT-o3-mini achieves the top composite score, with a win rate of 74.7\npercent, a correction success rate of 78.6 percent, and an improvement slope of\n0.041. By contrast, Qwen-Plus, despite an overcorrection risk rate of 81.6\npercent, wins only 25.6 percent of its matches - primarily due to excessive\nresource use. We also observe a negative correlation between overcorrection\nrisk rate and correction success rate (Pearson r = -0.51, p = 0.093),\nsuggesting that more frequent edits do not always improve outcomes. Our\nfindings highlight the value of assessing not only what LLMs decide but how\nthey arrive at those decisions",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used for tasks that require\ncomplex reasoning. Most benchmarks focus on final outcomes but overlook the\nintermediate reasoning steps - such as planning, revision, and decision making\nunder resource constraints. We argue that measuring these internal processes is\nessential for understanding model behavior and improving reliability. We\npropose using strategic games as a natural evaluation environment: closed,\nrule-based systems with clear states, limited resources, and automatic\nfeedback. We introduce a framework that evaluates LLMs along three core\ndimensions: planning, revision, and resource-constrained decision making. To\noperationalize this, we define metrics beyond win rate, including\novercorrection risk rate, correction success rate, improvement slope, and\nover-budget ratio. In 4320 adversarial rounds across 12 leading models,\nChatGPT-o3-mini achieves the top composite score, with a win rate of 74.7\npercent, a correction success rate of 78.6 percent, and an improvement slope of\n0.041. By contrast, Qwen-Plus, despite an overcorrection risk rate of 81.6\npercent, wins only 25.6 percent of its matches - primarily due to excessive\nresource use. We also observe a negative correlation between overcorrection\nrisk rate and correction success rate (Pearson r = -0.51, p = 0.093),\nsuggesting that more frequent edits do not always improve outcomes. Our\nfindings highlight the value of assessing not only what LLMs decide but how\nthey arrive at those decisions"
                },
                "authors": [
                    {
                        "name": "Xiaopeng Yuan"
                    },
                    {
                        "name": "Xingjian Zhang"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Lijun Yu"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Yushun Dong"
                    },
                    {
                        "name": "Haohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haohan Wang"
                },
                "author": "Haohan Wang",
                "arxiv_comment": "19 pages, 7 figures. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06266v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06266v3",
                "updated": "2025-06-13T17:58:55Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    58,
                    55,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-06T17:48:23Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    17,
                    48,
                    23,
                    4,
                    157,
                    0
                ],
                "title": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cartridges: Lightweight and general-purpose long context representations\n  via self-study"
                },
                "summary": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are often used to answer queries grounded in large text\ncorpora (e.g. codebases, legal documents, or chat histories) by placing the\nentire corpus in the context window and leveraging in-context learning (ICL).\nAlthough current models support contexts of 100K-1M tokens, this setup is\ncostly to serve because the memory consumption of the KV cache scales with\ninput length. We explore an alternative: training a smaller KV cache offline on\neach corpus. At inference time, we load this trained KV cache, which we call a\nCartridge, and decode a response. Critically, the cost of training a Cartridge\ncan be amortized across all the queries referencing the same corpus. However,\nwe find that the naive approach of training the Cartridge with next-token\nprediction on the corpus is not competitive with ICL. Instead, we propose\nself-study, a training recipe in which we generate synthetic conversations\nabout the corpus and train the Cartridge with a context-distillation objective.\nWe find that Cartridges trained with self-study replicate the functionality of\nICL, while being significantly cheaper to serve. On challenging long-context\nbenchmarks, Cartridges trained with self-study match ICL performance while\nusing 38.6x less memory and enabling 26.4x higher throughput. Self-study also\nextends the model's effective context length (e.g. from 128k to 484k tokens on\nMTOB) and surprisingly, leads to Cartridges that can be composed at inference\ntime without retraining."
                },
                "authors": [
                    {
                        "name": "Sabri Eyuboglu"
                    },
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Simran Arora"
                    },
                    {
                        "name": "Neel Guha"
                    },
                    {
                        "name": "Dylan Zinsley"
                    },
                    {
                        "name": "Emily Liu"
                    },
                    {
                        "name": "Will Tennien"
                    },
                    {
                        "name": "Atri Rudra"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    },
                    {
                        "name": "Christopher Re"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Re"
                },
                "author": "Christopher Re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06266v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12004v1",
                "updated": "2025-06-13T17:56:04Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    56,
                    4,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:56:04Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    56,
                    4,
                    4,
                    164,
                    0
                ],
                "title": "Bayesian and frequentist perspectives agree on dynamical dark energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian and frequentist perspectives agree on dynamical dark energy"
                },
                "summary": "Baryon acoustic oscillation data from the Dark Energy Spectroscopic\nInstrument (DESI) show evidence of a deviation from a cosmological constant\n$\\Lambda$ within a Bayesian analysis. In this work, we validate that\nfrequentist constraints from profile likelihoods on the\nChevallier-Polarski-Linder parameters $w_0$, $w_a$ are in excellent agreement\nwith the Bayesian constraints when combining with Planck cosmic microwave\nbackground, Planck and Atacama Cosmology Telescope lensing, and either\nPantheon+ or Dark Energy Survey Y5 supernova data. Further, we assess which\ndatasets drive these constraints by considering the contributions to the\n$\\chi^2$ from the individual datasets. For profile likelihoods of the matter\nfraction $\\Omega_\\mathrm{m}$, such an investigation shows internal\ninconsistencies when assuming $\\Lambda$, which are resolved when assuming a\n$w_0w_a$ dark-energy model. We infer the equations of state $w(z)$ at the pivot\nredshifts, supporting previous interpretations that current data appears to be\nmore sensitive to the derivative of $w(z)$ rather than a mean offset from\n$\\Lambda$. Thus our frequentist analysis corroborates previous findings on\ndynamical DE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Baryon acoustic oscillation data from the Dark Energy Spectroscopic\nInstrument (DESI) show evidence of a deviation from a cosmological constant\n$\\Lambda$ within a Bayesian analysis. In this work, we validate that\nfrequentist constraints from profile likelihoods on the\nChevallier-Polarski-Linder parameters $w_0$, $w_a$ are in excellent agreement\nwith the Bayesian constraints when combining with Planck cosmic microwave\nbackground, Planck and Atacama Cosmology Telescope lensing, and either\nPantheon+ or Dark Energy Survey Y5 supernova data. Further, we assess which\ndatasets drive these constraints by considering the contributions to the\n$\\chi^2$ from the individual datasets. For profile likelihoods of the matter\nfraction $\\Omega_\\mathrm{m}$, such an investigation shows internal\ninconsistencies when assuming $\\Lambda$, which are resolved when assuming a\n$w_0w_a$ dark-energy model. We infer the equations of state $w(z)$ at the pivot\nredshifts, supporting previous interpretations that current data appears to be\nmore sensitive to the derivative of $w(z)$ rather than a mean offset from\n$\\Lambda$. Thus our frequentist analysis corroborates previous findings on\ndynamical DE."
                },
                "authors": [
                    {
                        "name": "Laura Herold"
                    },
                    {
                        "name": "Tanvi Karwal"
                    }
                ],
                "author_detail": {
                    "name": "Tanvi Karwal"
                },
                "author": "Tanvi Karwal",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11994v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11994v1",
                "updated": "2025-06-13T17:49:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    49,
                    25,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:49:25Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    49,
                    25,
                    4,
                    164,
                    0
                ],
                "title": "Spectral Estimation with Free Decompression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral Estimation with Free Decompression"
                },
                "summary": "Computing eigenvalues of very large matrices is a critical task in many\nmachine learning applications, including the evaluation of log-determinants,\nthe trace of matrix functions, and other important metrics. As datasets\ncontinue to grow in scale, the corresponding covariance and kernel matrices\nbecome increasingly large, often reaching magnitudes that make their direct\nformation impractical or impossible. Existing techniques typically rely on\nmatrix-vector products, which can provide efficient approximations, if the\nmatrix spectrum behaves well. However, in settings like distributed learning,\nor when the matrix is defined only indirectly, access to the full data set can\nbe restricted to only very small sub-matrices of the original matrix. In these\ncases, the matrix of nominal interest is not even available as an implicit\noperator, meaning that even matrix-vector products may not be available. In\nsuch settings, the matrix is \"impalpable,\" in the sense that we have access to\nonly masked snapshots of it. We draw on principles from free probability theory\nto introduce a novel method of \"free decompression\" to estimate the spectrum of\nsuch matrices. Our method can be used to extrapolate from the empirical\nspectral densities of small submatrices to infer the eigenspectrum of extremely\nlarge (impalpable) matrices (that we cannot form or even evaluate with full\nmatrix-vector products). We demonstrate the effectiveness of this approach\nthrough a series of examples, comparing its performance against known limiting\ndistributions from random matrix theory in synthetic settings, as well as\napplying it to submatrices of real-world datasets, matching them with their\nfull empirical eigenspectra.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computing eigenvalues of very large matrices is a critical task in many\nmachine learning applications, including the evaluation of log-determinants,\nthe trace of matrix functions, and other important metrics. As datasets\ncontinue to grow in scale, the corresponding covariance and kernel matrices\nbecome increasingly large, often reaching magnitudes that make their direct\nformation impractical or impossible. Existing techniques typically rely on\nmatrix-vector products, which can provide efficient approximations, if the\nmatrix spectrum behaves well. However, in settings like distributed learning,\nor when the matrix is defined only indirectly, access to the full data set can\nbe restricted to only very small sub-matrices of the original matrix. In these\ncases, the matrix of nominal interest is not even available as an implicit\noperator, meaning that even matrix-vector products may not be available. In\nsuch settings, the matrix is \"impalpable,\" in the sense that we have access to\nonly masked snapshots of it. We draw on principles from free probability theory\nto introduce a novel method of \"free decompression\" to estimate the spectrum of\nsuch matrices. Our method can be used to extrapolate from the empirical\nspectral densities of small submatrices to infer the eigenspectrum of extremely\nlarge (impalpable) matrices (that we cannot form or even evaluate with full\nmatrix-vector products). We demonstrate the effectiveness of this approach\nthrough a series of examples, comparing its performance against known limiting\ndistributions from random matrix theory in synthetic settings, as well as\napplying it to submatrices of real-world datasets, matching them with their\nfull empirical eigenspectra."
                },
                "authors": [
                    {
                        "name": "Siavash Ameli"
                    },
                    {
                        "name": "Chris van der Heide"
                    },
                    {
                        "name": "Liam Hodgkinson"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    }
                ],
                "author_detail": {
                    "name": "Michael W. Mahoney"
                },
                "author": "Michael W. Mahoney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11994v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11994v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11991v1",
                "updated": "2025-06-13T17:47:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    47,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:47:43Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    47,
                    43,
                    4,
                    164,
                    0
                ],
                "title": "VGR: Visual Grounded Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VGR: Visual Grounded Reasoning"
                },
                "summary": "In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced fine-grained\nvisual perception capabilities. Unlike traditional MLLMs that answer the\nquestion or reasoning solely on the language space, our VGR first detects\nrelevant regions that may help to solve problems, and then provides precise\nanswers based on replayed image regions. To achieve this, we conduct a\nlarge-scale SFT dataset called VGR -SFT that contains reasoning data with mixed\nvision grounding and language deduction. The inference pipeline of VGR allows\nthe model to choose bounding boxes for visual reference and a replay stage is\nintroduced to integrates the corresponding regions into the reasoning process,\nenhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline\nshow that VGR achieves superior performance on multi-modal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30\\% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and a +12.9 improvement on ChartQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced fine-grained\nvisual perception capabilities. Unlike traditional MLLMs that answer the\nquestion or reasoning solely on the language space, our VGR first detects\nrelevant regions that may help to solve problems, and then provides precise\nanswers based on replayed image regions. To achieve this, we conduct a\nlarge-scale SFT dataset called VGR -SFT that contains reasoning data with mixed\nvision grounding and language deduction. The inference pipeline of VGR allows\nthe model to choose bounding boxes for visual reference and a replay stage is\nintroduced to integrates the corresponding regions into the reasoning process,\nenhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline\nshow that VGR achieves superior performance on multi-modal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30\\% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and a +12.9 improvement on ChartQA."
                },
                "authors": [
                    {
                        "name": "Jiacong Wang"
                    },
                    {
                        "name": "Zijiang Kang"
                    },
                    {
                        "name": "Haochen Wang"
                    },
                    {
                        "name": "Haiyong Jiang"
                    },
                    {
                        "name": "Jiawen Li"
                    },
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Jiao Ran"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Jun Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xiao"
                },
                "author": "Jun Xiao",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11989v1",
                "updated": "2025-06-13T17:46:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    46,
                    14,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:46:14Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    46,
                    14,
                    4,
                    164,
                    0
                ],
                "title": "Simple Radiology VLLM Test-time Scaling with Thought Graph Traversal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple Radiology VLLM Test-time Scaling with Thought Graph Traversal"
                },
                "summary": "Test-time scaling offers a promising way to improve the reasoning performance\nof vision-language large models (VLLMs) without additional training. In this\npaper, we explore a simple but effective approach for applying test-time\nscaling to radiology report generation. Specifically, we introduce a\nlightweight Thought Graph Traversal (TGT) framework that guides the model to\nreason through organ-specific findings in a medically coherent order. This\nframework integrates structured medical priors into the prompt, enabling deeper\nand more logical analysis with no changes to the underlying model. To further\nenhance reasoning depth, we apply a reasoning budget forcing strategy that\nadjusts the model's inference depth at test time by dynamically extending its\ngeneration process. This simple yet powerful combination allows a frozen\nradiology VLLM to self-correct and generate more accurate, consistent chest\nX-ray reports. Our method outperforms baseline prompting approaches on standard\nbenchmarks, and also reveals dataset biases through traceable reasoning paths.\nCode and prompts are open-sourced for reproducibility at\nhttps://github.com/glerium/Thought-Graph-Traversal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling offers a promising way to improve the reasoning performance\nof vision-language large models (VLLMs) without additional training. In this\npaper, we explore a simple but effective approach for applying test-time\nscaling to radiology report generation. Specifically, we introduce a\nlightweight Thought Graph Traversal (TGT) framework that guides the model to\nreason through organ-specific findings in a medically coherent order. This\nframework integrates structured medical priors into the prompt, enabling deeper\nand more logical analysis with no changes to the underlying model. To further\nenhance reasoning depth, we apply a reasoning budget forcing strategy that\nadjusts the model's inference depth at test time by dynamically extending its\ngeneration process. This simple yet powerful combination allows a frozen\nradiology VLLM to self-correct and generate more accurate, consistent chest\nX-ray reports. Our method outperforms baseline prompting approaches on standard\nbenchmarks, and also reveals dataset biases through traceable reasoning paths.\nCode and prompts are open-sourced for reproducibility at\nhttps://github.com/glerium/Thought-Graph-Traversal."
                },
                "authors": [
                    {
                        "name": "Yue Yao"
                    },
                    {
                        "name": "Zelin Wen"
                    },
                    {
                        "name": "Yan Tong"
                    },
                    {
                        "name": "Xinyu Tian"
                    },
                    {
                        "name": "Xuqing Li"
                    },
                    {
                        "name": "Xiao Ma"
                    },
                    {
                        "name": "Dongliang Xu"
                    },
                    {
                        "name": "Tom Gedeon"
                    }
                ],
                "author_detail": {
                    "name": "Tom Gedeon"
                },
                "author": "Tom Gedeon",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2404.11209 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09026v2",
                "updated": "2025-06-13T17:44:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    44,
                    3,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-10T17:52:42Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    52,
                    42,
                    1,
                    161,
                    0
                ],
                "title": "e3: Learning to Explore Enables Extrapolation of Test-Time Compute for\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "e3: Learning to Explore Enables Extrapolation of Test-Time Compute for\n  LLMs"
                },
                "summary": "Test-time scaling offers a promising path to improve LLM reasoning by\nutilizing more compute at inference time; however, the true promise of this\nparadigm lies in extrapolation (i.e., improvement in performance on hard\nproblems as LLMs keep \"thinking\" for longer, beyond the maximum token budget\nthey were trained on). Surprisingly, we find that most existing reasoning\nmodels do not extrapolate well. We show that one way to enable extrapolation is\nby training the LLM to perform in-context exploration: training the LLM to\neffectively spend its test time budget by chaining operations (such as\ngeneration, verification, refinement, etc.), or testing multiple hypotheses\nbefore it commits to an answer. To enable in-context exploration, we identify\nthree key ingredients as part of our recipe e3: (1) chaining skills that the\nbase LLM has asymmetric competence in, e.g., chaining verification (easy) with\ngeneration (hard), as a way to implement in-context search; (2) leveraging\n\"negative\" gradients from incorrect traces to amplify exploration during RL,\nresulting in longer search traces that chains additional asymmetries; and (3)\ncoupling task difficulty with training token budget during training via a\nspecifically-designed curriculum to structure in-context exploration. Our\nrecipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25\nscores, and extrapolates to 2x the training token budget. Our e3-1.7B model not\nonly attains high pass@1 scores, but also improves pass@k over the base model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling offers a promising path to improve LLM reasoning by\nutilizing more compute at inference time; however, the true promise of this\nparadigm lies in extrapolation (i.e., improvement in performance on hard\nproblems as LLMs keep \"thinking\" for longer, beyond the maximum token budget\nthey were trained on). Surprisingly, we find that most existing reasoning\nmodels do not extrapolate well. We show that one way to enable extrapolation is\nby training the LLM to perform in-context exploration: training the LLM to\neffectively spend its test time budget by chaining operations (such as\ngeneration, verification, refinement, etc.), or testing multiple hypotheses\nbefore it commits to an answer. To enable in-context exploration, we identify\nthree key ingredients as part of our recipe e3: (1) chaining skills that the\nbase LLM has asymmetric competence in, e.g., chaining verification (easy) with\ngeneration (hard), as a way to implement in-context search; (2) leveraging\n\"negative\" gradients from incorrect traces to amplify exploration during RL,\nresulting in longer search traces that chains additional asymmetries; and (3)\ncoupling task difficulty with training token budget during training via a\nspecifically-designed curriculum to structure in-context exploration. Our\nrecipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25\nscores, and extrapolates to 2x the training token budget. Our e3-1.7B model not\nonly attains high pass@1 scores, but also improves pass@k over the base model."
                },
                "authors": [
                    {
                        "name": "Amrith Setlur"
                    },
                    {
                        "name": "Matthew Y. R. Yang"
                    },
                    {
                        "name": "Charlie Snell"
                    },
                    {
                        "name": "Jeremy Greer"
                    },
                    {
                        "name": "Ian Wu"
                    },
                    {
                        "name": "Virginia Smith"
                    },
                    {
                        "name": "Max Simchowitz"
                    },
                    {
                        "name": "Aviral Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Aviral Kumar"
                },
                "author": "Aviral Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11976v1",
                "updated": "2025-06-13T17:34:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    34,
                    5,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:34:05Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    34,
                    5,
                    4,
                    164,
                    0
                ],
                "title": "How Visual Representations Map to Language Feature Space in Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Visual Representations Map to Language Feature Space in Multimodal\n  LLMs"
                },
                "summary": "Effective multimodal reasoning depends on the alignment of visual and\nlinguistic representations, yet the mechanisms by which vision-language models\n(VLMs) achieve this alignment remain poorly understood. We introduce a\nmethodological framework that deliberately maintains a frozen large language\nmodel (LLM) and a frozen vision transformer (ViT), connected solely by training\na linear adapter during visual instruction tuning. This design is fundamental\nto our approach: by keeping the language model frozen, we ensure it maintains\nits original language representations without adaptation to visual data.\nConsequently, the linear adapter must map visual features directly into the\nLLM's existing representational space rather than allowing the language model\nto develop specialized visual understanding through fine-tuning. Our\nexperimental design uniquely enables the use of pre-trained sparse autoencoders\n(SAEs) of the LLM as analytical probes. These SAEs remain perfectly aligned\nwith the unchanged language model and serve as a snapshot of the learned\nlanguage feature-representations. Through systematic analysis of SAE\nreconstruction error, sparsity patterns, and feature SAE descriptions, we\nreveal the layer-wise progression through which visual representations\ngradually align with language feature representations, converging in\nmiddle-to-later layers. This suggests a fundamental misalignment between ViT\noutputs and early LLM layers, raising important questions about whether current\nadapter-based architectures optimally facilitate cross-modal representation\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective multimodal reasoning depends on the alignment of visual and\nlinguistic representations, yet the mechanisms by which vision-language models\n(VLMs) achieve this alignment remain poorly understood. We introduce a\nmethodological framework that deliberately maintains a frozen large language\nmodel (LLM) and a frozen vision transformer (ViT), connected solely by training\na linear adapter during visual instruction tuning. This design is fundamental\nto our approach: by keeping the language model frozen, we ensure it maintains\nits original language representations without adaptation to visual data.\nConsequently, the linear adapter must map visual features directly into the\nLLM's existing representational space rather than allowing the language model\nto develop specialized visual understanding through fine-tuning. Our\nexperimental design uniquely enables the use of pre-trained sparse autoencoders\n(SAEs) of the LLM as analytical probes. These SAEs remain perfectly aligned\nwith the unchanged language model and serve as a snapshot of the learned\nlanguage feature-representations. Through systematic analysis of SAE\nreconstruction error, sparsity patterns, and feature SAE descriptions, we\nreveal the layer-wise progression through which visual representations\ngradually align with language feature representations, converging in\nmiddle-to-later layers. This suggests a fundamental misalignment between ViT\noutputs and early LLM layers, raising important questions about whether current\nadapter-based architectures optimally facilitate cross-modal representation\nlearning."
                },
                "authors": [
                    {
                        "name": "Constantin Venhoff"
                    },
                    {
                        "name": "Ashkan Khakzar"
                    },
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Neel Nanda"
                    }
                ],
                "author_detail": {
                    "name": "Neel Nanda"
                },
                "author": "Neel Nanda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10467v2",
                "updated": "2025-06-13T17:32:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    32,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T08:16:17Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    8,
                    16,
                    17,
                    3,
                    163,
                    0
                ],
                "title": "Specification and Evaluation of Multi-Agent LLM Systems -- Prototype and\n  Cybersecurity Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specification and Evaluation of Multi-Agent LLM Systems -- Prototype and\n  Cybersecurity Applications"
                },
                "summary": "Recent advancements in LLMs indicate potential for novel applications, e.g.,\nthrough reasoning capabilities in the latest OpenAI and DeepSeek models. For\napplying these models in specific domains beyond text generation, LLM-based\nmulti-agent approaches can be utilized that solve complex tasks by combining\nreasoning techniques, code generation, and software execution. Applications\nmight utilize these capabilities and the knowledge of specialized LLM agents.\nHowever, while many evaluations are performed on LLMs, reasoning techniques,\nand applications individually, their joint specification and combined\napplication is not explored well. Defined specifications for multi-agent LLM\nsystems are required to explore their potential and their suitability for\nspecific applications, allowing for systematic evaluations of LLMs, reasoning\ntechniques, and related aspects. This paper reports the results of exploratory\nresearch to specify and evaluate these aspects through a multi-agent system.\nThe system architecture and prototype are extended from previous research and a\nspecification is introduced for multi-agent systems. Test cases involving\ncybersecurity tasks indicate feasibility of the architecture and evaluation\napproach. In particular, the results show the evaluation of question answering,\nserver security, and network security tasks that were completed correctly by\nagents with LLMs from OpenAI and DeepSeek.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLMs indicate potential for novel applications, e.g.,\nthrough reasoning capabilities in the latest OpenAI and DeepSeek models. For\napplying these models in specific domains beyond text generation, LLM-based\nmulti-agent approaches can be utilized that solve complex tasks by combining\nreasoning techniques, code generation, and software execution. Applications\nmight utilize these capabilities and the knowledge of specialized LLM agents.\nHowever, while many evaluations are performed on LLMs, reasoning techniques,\nand applications individually, their joint specification and combined\napplication is not explored well. Defined specifications for multi-agent LLM\nsystems are required to explore their potential and their suitability for\nspecific applications, allowing for systematic evaluations of LLMs, reasoning\ntechniques, and related aspects. This paper reports the results of exploratory\nresearch to specify and evaluate these aspects through a multi-agent system.\nThe system architecture and prototype are extended from previous research and a\nspecification is introduced for multi-agent systems. Test cases involving\ncybersecurity tasks indicate feasibility of the architecture and evaluation\napproach. In particular, the results show the evaluation of question answering,\nserver security, and network security tasks that were completed correctly by\nagents with LLMs from OpenAI and DeepSeek."
                },
                "authors": [
                    {
                        "name": "Felix Härer"
                    }
                ],
                "author_detail": {
                    "name": "Felix Härer"
                },
                "author": "Felix Härer",
                "arxiv_comment": "This work has been submitted for a possible publication. Copyright\n  may be transferred. In this case, this version will be updated with a notice,\n  according to the publisher's guidelines",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07833v2",
                "updated": "2025-06-13T17:24:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    24,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-09T14:55:00Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    14,
                    55,
                    0,
                    0,
                    160,
                    0
                ],
                "title": "Improving Large Language Models with Concept-Aware Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Large Language Models with Concept-Aware Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm"
                },
                "authors": [
                    {
                        "name": "Michael K. Chen"
                    },
                    {
                        "name": "Xikun Zhang"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01503v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01503v7",
                "updated": "2025-06-13T17:01:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    1,
                    34,
                    4,
                    164,
                    0
                ],
                "published": "2024-11-03T09:49:12Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    49,
                    12,
                    6,
                    308,
                    0
                ],
                "title": "A Highly Scalable LLM Clusters with Optical Interconnect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Highly Scalable LLM Clusters with Optical Interconnect"
                },
                "summary": "The rapid development of large-scale GPU clusters for LLM training has driven\nenterprises to replace core-layer electrical switches with optical circuit\nswitches (OCS) to meet escalating bandwidth demands. However, current physical\ntopology design of OCS-based clusters faces two critical challenges. First,\nthere exist unrealizable logical topologies, leading to underutilization of\nbandwidth resource. Second, calculating OCS reconfiguration constitutes an\nNP-Complete problem and is time-consuming for multi-tenant GPU clusters which\nneed real-time scheduling. In this paper, we propose \\emph{Cross Wiring}, a new\nphysical topology design that resolves both limitations. Our physical topology\nguarantees full compatibility with all logical topologies under\nL2-compatibility constraints. Through a proposed \\emph{Symmetric Integer Matrix\nDecomposition Theorem}, we design a polynomial-time OCS reconfiguration\nalgorithm that satisfies arbitrary logical topology requirements. Evaluations\nshow a up to 39.5\\% higher training throughput versus prior architectures such\nas \\emph{Gemini} in 128-NPU testbed and a 12.6\\% reduction in average job\ncompletion time through real-workload based multi-tenant large-scale\nsimulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large-scale GPU clusters for LLM training has driven\nenterprises to replace core-layer electrical switches with optical circuit\nswitches (OCS) to meet escalating bandwidth demands. However, current physical\ntopology design of OCS-based clusters faces two critical challenges. First,\nthere exist unrealizable logical topologies, leading to underutilization of\nbandwidth resource. Second, calculating OCS reconfiguration constitutes an\nNP-Complete problem and is time-consuming for multi-tenant GPU clusters which\nneed real-time scheduling. In this paper, we propose \\emph{Cross Wiring}, a new\nphysical topology design that resolves both limitations. Our physical topology\nguarantees full compatibility with all logical topologies under\nL2-compatibility constraints. Through a proposed \\emph{Symmetric Integer Matrix\nDecomposition Theorem}, we design a polynomial-time OCS reconfiguration\nalgorithm that satisfies arbitrary logical topology requirements. Evaluations\nshow a up to 39.5\\% higher training throughput versus prior architectures such\nas \\emph{Gemini} in 128-NPU testbed and a 12.6\\% reduction in average job\ncompletion time through real-workload based multi-tenant large-scale\nsimulations."
                },
                "authors": [
                    {
                        "name": "Xinchi Han"
                    },
                    {
                        "name": "Yongxi Lv"
                    },
                    {
                        "name": "Shuyuan Zhang"
                    },
                    {
                        "name": "Yingming Mao"
                    },
                    {
                        "name": "Shizhen Zhao"
                    },
                    {
                        "name": "ZhuoRan Liu"
                    },
                    {
                        "name": "Zhuotao Liu"
                    },
                    {
                        "name": "Peirui Cao"
                    },
                    {
                        "name": "Ximeng Liu"
                    },
                    {
                        "name": "Xinbing Wang"
                    },
                    {
                        "name": "Wu Dongchao"
                    },
                    {
                        "name": "Yang Jian"
                    },
                    {
                        "name": "Zhang zhanbang"
                    }
                ],
                "author_detail": {
                    "name": "Zhang zhanbang"
                },
                "author": "Zhang zhanbang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01503v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01503v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11949v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11949v1",
                "updated": "2025-06-13T16:58:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    58,
                    31,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T16:58:31Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    58,
                    31,
                    4,
                    164,
                    0
                ],
                "title": "Modeling Complex Life Systems: Bayesian Inference for Weibull Failure\n  Times Using Adaptive MCMC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Complex Life Systems: Bayesian Inference for Weibull Failure\n  Times Using Adaptive MCMC"
                },
                "summary": "This research develops a Bayesian framework for analyzing failure times using\nthe Weibull distribution, addressing challenges in prior selection due to the\nlack of conjugate priors and multi-dimensional sufficient statistics. We\npropose an adaptive semi-parametric MCMC algorithm for lifetime data analysis,\nemploying a hierarchical Bayesian model and the No-U-Turn Sampler (NUTS) in\nSTAN. Twenty-four combinations of prior distributions are evaluated, with a\nnoninformative LogNormal hyper-prior ensuring flexibility. A simulation study\nof seventy-two datasets with varying structures compares MCMC and classical\nmethods, identifying optimal priors for Bayesian regularization. The approach\neffectively handles the Increasing Hazard Rate (IHR) and Decreasing Hazard Rate\n(DHR) scenarios. Finally, we demonstrate the algorithm's utility by predicting\nthe remaining lifetime of prostate cancer patients, showcasing its practical\napplication. This work advances Bayesian methodologies for modeling complex\nlife systems and testing processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research develops a Bayesian framework for analyzing failure times using\nthe Weibull distribution, addressing challenges in prior selection due to the\nlack of conjugate priors and multi-dimensional sufficient statistics. We\npropose an adaptive semi-parametric MCMC algorithm for lifetime data analysis,\nemploying a hierarchical Bayesian model and the No-U-Turn Sampler (NUTS) in\nSTAN. Twenty-four combinations of prior distributions are evaluated, with a\nnoninformative LogNormal hyper-prior ensuring flexibility. A simulation study\nof seventy-two datasets with varying structures compares MCMC and classical\nmethods, identifying optimal priors for Bayesian regularization. The approach\neffectively handles the Increasing Hazard Rate (IHR) and Decreasing Hazard Rate\n(DHR) scenarios. Finally, we demonstrate the algorithm's utility by predicting\nthe remaining lifetime of prostate cancer patients, showcasing its practical\napplication. This work advances Bayesian methodologies for modeling complex\nlife systems and testing processes."
                },
                "authors": [
                    {
                        "name": "Tobias Oketch"
                    },
                    {
                        "name": "Mohammad Sepehrifar"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Sepehrifar"
                },
                "author": "Mohammad Sepehrifar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11949v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11949v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62C10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11948v1",
                "updated": "2025-06-13T16:58:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    58,
                    20,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T16:58:20Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    58,
                    20,
                    4,
                    164,
                    0
                ],
                "title": "SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies"
                },
                "summary": "Offline Imitation Learning (IL) methods such as Behavior Cloning are\neffective at acquiring complex robotic manipulation skills. However, existing\nIL-trained policies are confined to executing the task at the same speed as\nshown in demonstration data. This limits the task throughput of a robotic\nsystem, a critical requirement for applications such as industrial automation.\nIn this paper, we introduce and formalize the novel problem of enabling\nfaster-than-demonstration execution of visuomotor policies and identify\nfundamental challenges in robot dynamics and state-action distribution shifts.\nWe instantiate the key insights as SAIL (Speed Adaptation for Imitation\nLearning), a full-stack system integrating four tightly-connected components:\n(1) a consistency-preserving action inference algorithm for smooth motion at\nhigh speed, (2) high-fidelity tracking of controller-invariant motion targets,\n(3) adaptive speed modulation that dynamically adjusts execution speed based on\nmotion complexity, and (4) action scheduling to handle real-world system\nlatencies. Experiments on 12 tasks across simulation and two real, distinct\nrobot platforms show that SAIL achieves up to a 4x speedup over demonstration\nspeed in simulation and up to 3.2x speedup in the real world. Additional detail\nis available at https://nadunranawaka1.github.io/sail-policy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Imitation Learning (IL) methods such as Behavior Cloning are\neffective at acquiring complex robotic manipulation skills. However, existing\nIL-trained policies are confined to executing the task at the same speed as\nshown in demonstration data. This limits the task throughput of a robotic\nsystem, a critical requirement for applications such as industrial automation.\nIn this paper, we introduce and formalize the novel problem of enabling\nfaster-than-demonstration execution of visuomotor policies and identify\nfundamental challenges in robot dynamics and state-action distribution shifts.\nWe instantiate the key insights as SAIL (Speed Adaptation for Imitation\nLearning), a full-stack system integrating four tightly-connected components:\n(1) a consistency-preserving action inference algorithm for smooth motion at\nhigh speed, (2) high-fidelity tracking of controller-invariant motion targets,\n(3) adaptive speed modulation that dynamically adjusts execution speed based on\nmotion complexity, and (4) action scheduling to handle real-world system\nlatencies. Experiments on 12 tasks across simulation and two real, distinct\nrobot platforms show that SAIL achieves up to a 4x speedup over demonstration\nspeed in simulation and up to 3.2x speedup in the real world. Additional detail\nis available at https://nadunranawaka1.github.io/sail-policy"
                },
                "authors": [
                    {
                        "name": "Nadun Ranawaka Arachchige"
                    },
                    {
                        "name": "Zhenyang Chen"
                    },
                    {
                        "name": "Wonsuhk Jung"
                    },
                    {
                        "name": "Woo Chul Shin"
                    },
                    {
                        "name": "Rohan Bansal"
                    },
                    {
                        "name": "Pierre Barroso"
                    },
                    {
                        "name": "Yu Hang He"
                    },
                    {
                        "name": "Yingyang Celine Lin"
                    },
                    {
                        "name": "Benjamin Joffe"
                    },
                    {
                        "name": "Shreyas Kousik"
                    },
                    {
                        "name": "Danfei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Danfei Xu"
                },
                "author": "Danfei Xu",
                "arxiv_comment": "The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21657v2",
                "updated": "2025-06-13T16:43:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    43,
                    15,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-27T18:32:38Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    32,
                    38,
                    1,
                    147,
                    0
                ],
                "title": "Explainability of Large Language Models using SMILE: Statistical\n  Model-agnostic Interpretability with Local Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainability of Large Language Models using SMILE: Statistical\n  Model-agnostic Interpretability with Local Explanations"
                },
                "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy."
                },
                "authors": [
                    {
                        "name": "Zeinab Dehghani"
                    },
                    {
                        "name": "Mohammed Naveed Akram"
                    },
                    {
                        "name": "Koorosh Aslansefat"
                    },
                    {
                        "name": "Adil Khan"
                    }
                ],
                "author_detail": {
                    "name": "Adil Khan"
                },
                "author": "Adil Khan",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2412.16277",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11938v1",
                "updated": "2025-06-13T16:42:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    42,
                    9,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T16:42:09Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    42,
                    9,
                    4,
                    164,
                    0
                ],
                "title": "Improving Large Language Model Safety with Contrastive Representation\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Large Language Model Safety with Contrastive Representation\n  Learning"
                },
                "summary": "Large Language Models (LLMs) are powerful tools with profound societal\nimpacts, yet their ability to generate responses to diverse and uncontrolled\ninputs leaves them vulnerable to adversarial attacks. While existing defenses\noften struggle to generalize across varying attack types, recent advancements\nin representation engineering offer promising alternatives. In this work, we\npropose a defense framework that formulates model defense as a contrastive\nrepresentation learning (CRL) problem. Our method finetunes a model using a\ntriplet-based loss combined with adversarial hard negative mining to encourage\nseparation between benign and harmful representations. Our experimental results\nacross multiple models demonstrate that our approach outperforms prior\nrepresentation engineering-based defenses, improving robustness against both\ninput-level and embedding-space attacks without compromising standard\nperformance. Our code is available at\nhttps://github.com/samuelsimko/crl-llm-defense",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful tools with profound societal\nimpacts, yet their ability to generate responses to diverse and uncontrolled\ninputs leaves them vulnerable to adversarial attacks. While existing defenses\noften struggle to generalize across varying attack types, recent advancements\nin representation engineering offer promising alternatives. In this work, we\npropose a defense framework that formulates model defense as a contrastive\nrepresentation learning (CRL) problem. Our method finetunes a model using a\ntriplet-based loss combined with adversarial hard negative mining to encourage\nseparation between benign and harmful representations. Our experimental results\nacross multiple models demonstrate that our approach outperforms prior\nrepresentation engineering-based defenses, improving robustness against both\ninput-level and embedding-space attacks without compromising standard\nperformance. Our code is available at\nhttps://github.com/samuelsimko/crl-llm-defense"
                },
                "authors": [
                    {
                        "name": "Samuel Simko"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Zhijing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Jin"
                },
                "author": "Zhijing Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11936v1",
                "updated": "2025-06-13T16:38:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    38,
                    51,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T16:38:51Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    38,
                    51,
                    4,
                    164,
                    0
                ],
                "title": "Bubble Dynamics Transformer: Microrheology at Ultra-High Strain Rates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bubble Dynamics Transformer: Microrheology at Ultra-High Strain Rates"
                },
                "summary": "Laser-induced inertial cavitation (LIC)-where microscale vapor bubbles\nnucleate due to a focused high-energy pulsed laser and then violently collapse\nunder surrounding high local pressures-offers a unique opportunity to\ninvestigate soft biological material mechanics at extremely high strain rates\n(>1000 1/s). Traditional rheological tools are often limited in these regimes\nby loading speed, resolution, or invasiveness. Here we introduce novel machine\nlearning (ML) based microrheological frameworks that leverage LIC to\ncharacterize the viscoelastic properties of biological materials at ultra-high\nstrain rates. We utilize ultra-high-speed imaging to capture time-resolved\nbubble radius dynamics during LIC events in various soft viscoelastic\nmaterials. These bubble radius versus time measurements are then analyzed using\na newly developed Bubble Dynamics Transformer (BDT), a neural network trained\non physics-based simulation data. The BDT accurately infers material\nviscoelastic parameters, eliminating the need for iterative fitting or complex\ninversion processes. This enables fast, accurate, and non-contact\ncharacterization of soft materials under extreme loading conditions, with\nsignificant implications for biomedical applications and materials science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser-induced inertial cavitation (LIC)-where microscale vapor bubbles\nnucleate due to a focused high-energy pulsed laser and then violently collapse\nunder surrounding high local pressures-offers a unique opportunity to\ninvestigate soft biological material mechanics at extremely high strain rates\n(>1000 1/s). Traditional rheological tools are often limited in these regimes\nby loading speed, resolution, or invasiveness. Here we introduce novel machine\nlearning (ML) based microrheological frameworks that leverage LIC to\ncharacterize the viscoelastic properties of biological materials at ultra-high\nstrain rates. We utilize ultra-high-speed imaging to capture time-resolved\nbubble radius dynamics during LIC events in various soft viscoelastic\nmaterials. These bubble radius versus time measurements are then analyzed using\na newly developed Bubble Dynamics Transformer (BDT), a neural network trained\non physics-based simulation data. The BDT accurately infers material\nviscoelastic parameters, eliminating the need for iterative fitting or complex\ninversion processes. This enables fast, accurate, and non-contact\ncharacterization of soft materials under extreme loading conditions, with\nsignificant implications for biomedical applications and materials science."
                },
                "authors": [
                    {
                        "name": "Lehu Bu"
                    },
                    {
                        "name": "Zhaohan Yu"
                    },
                    {
                        "name": "Shaoting Lin"
                    },
                    {
                        "name": "Jan N. Fuhg"
                    },
                    {
                        "name": "Jin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jin Yang"
                },
                "author": "Jin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11930v1",
                "updated": "2025-06-13T16:31:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    31,
                    51,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T16:31:51Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    31,
                    51,
                    4,
                    164,
                    0
                ],
                "title": "Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback"
                },
                "summary": "Recent studies have shown LLMs possess some ability to improve their\nresponses when given external feedback. However, it remains unclear how\neffectively and thoroughly these models can incorporate extrinsic feedback. In\nan ideal scenario, if LLMs receive near-perfect and complete feedback, we would\nexpect them to fully integrate the feedback and change their incorrect answers\nto correct ones. In this paper, we systematically investigate LLMs' ability to\nincorporate feedback by designing a controlled experimental environment. For\neach problem, a solver model attempts a solution, then a feedback generator\nwith access to near-complete ground-truth answers produces targeted feedback,\nafter which the solver tries again. We evaluate this pipeline across a diverse\nrange of tasks, including math reasoning, knowledge reasoning, scientific\nreasoning, and general multi-domain evaluations with state-of-the-art language\nmodels including Claude 3.7 (with and without extended thinking). Surprisingly,\neven under these near-ideal conditions, solver models consistently show\nresistance to feedback, a limitation that we term FEEDBACK FRICTION. To\nmitigate this limitation, we experiment with sampling-based strategies like\nprogressive temperature increases and explicit rejection of previously\nattempted incorrect answers, which yield improvements but still fail to help\nmodels achieve target performance. We also perform a rigorous exploration of\npotential causes of FEEDBACK FRICTION, ruling out factors such as model\noverconfidence and data familiarity. We hope that highlighting this issue in\nLLMs and ruling out several apparent causes will help future research in\nself-improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown LLMs possess some ability to improve their\nresponses when given external feedback. However, it remains unclear how\neffectively and thoroughly these models can incorporate extrinsic feedback. In\nan ideal scenario, if LLMs receive near-perfect and complete feedback, we would\nexpect them to fully integrate the feedback and change their incorrect answers\nto correct ones. In this paper, we systematically investigate LLMs' ability to\nincorporate feedback by designing a controlled experimental environment. For\neach problem, a solver model attempts a solution, then a feedback generator\nwith access to near-complete ground-truth answers produces targeted feedback,\nafter which the solver tries again. We evaluate this pipeline across a diverse\nrange of tasks, including math reasoning, knowledge reasoning, scientific\nreasoning, and general multi-domain evaluations with state-of-the-art language\nmodels including Claude 3.7 (with and without extended thinking). Surprisingly,\neven under these near-ideal conditions, solver models consistently show\nresistance to feedback, a limitation that we term FEEDBACK FRICTION. To\nmitigate this limitation, we experiment with sampling-based strategies like\nprogressive temperature increases and explicit rejection of previously\nattempted incorrect answers, which yield improvements but still fail to help\nmodels achieve target performance. We also perform a rigorous exploration of\npotential causes of FEEDBACK FRICTION, ruling out factors such as model\noverconfidence and data familiarity. We hope that highlighting this issue in\nLLMs and ruling out several apparent causes will help future research in\nself-improvement."
                },
                "authors": [
                    {
                        "name": "Dongwei Jiang"
                    },
                    {
                        "name": "Alvin Zhang"
                    },
                    {
                        "name": "Andrew Wang"
                    },
                    {
                        "name": "Nicholas Andrews"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11928v1",
                "updated": "2025-06-13T16:29:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    29,
                    9,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T16:29:09Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    29,
                    9,
                    4,
                    164,
                    0
                ],
                "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?"
                },
                "summary": "Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning."
                },
                "authors": [
                    {
                        "name": "Zihan Zheng"
                    },
                    {
                        "name": "Zerui Cheng"
                    },
                    {
                        "name": "Zeyu Shen"
                    },
                    {
                        "name": "Shang Zhou"
                    },
                    {
                        "name": "Kaiyuan Liu"
                    },
                    {
                        "name": "Hansen He"
                    },
                    {
                        "name": "Dongruixuan Li"
                    },
                    {
                        "name": "Stanley Wei"
                    },
                    {
                        "name": "Hangyi Hao"
                    },
                    {
                        "name": "Jianzhu Yao"
                    },
                    {
                        "name": "Peiyao Sheng"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Aleksandra Korolova"
                    },
                    {
                        "name": "Peter Henderson"
                    },
                    {
                        "name": "Sanjeev Arora"
                    },
                    {
                        "name": "Pramod Viswanath"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Saining Xie"
                    }
                ],
                "author_detail": {
                    "name": "Saining Xie"
                },
                "author": "Saining Xie",
                "arxiv_comment": "Project Page at https://livecodebenchpro.com/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11925v1",
                "updated": "2025-06-13T16:24:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    24,
                    28,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T16:24:28Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    24,
                    28,
                    4,
                    164,
                    0
                ],
                "title": "Real-World Deployment of a Lane Change Prediction Architecture Based on\n  Knowledge Graph Embeddings and Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-World Deployment of a Lane Change Prediction Architecture Based on\n  Knowledge Graph Embeddings and Bayesian Inference"
                },
                "summary": "Research on lane change prediction has gained a lot of momentum in the last\ncouple of years. However, most research is confined to simulation or results\nobtained from datasets, leaving a gap between algorithmic advances and on-road\ndeployment. This work closes that gap by demonstrating, on real hardware, a\nlane-change prediction system based on Knowledge Graph Embeddings (KGEs) and\nBayesian inference. Moreover, the ego-vehicle employs a longitudinal braking\naction to ensure the safety of both itself and the surrounding vehicles. Our\narchitecture consists of two modules: (i) a perception module that senses the\nenvironment, derives input numerical features, and converts them into\nlinguistic categories; and communicates them to the prediction module; (ii) a\npretrained prediction module that executes a KGE and Bayesian inference model\nto anticipate the target vehicle's maneuver and transforms the prediction into\nlongitudinal braking action. Real-world hardware experimental validation\ndemonstrates that our prediction system anticipates the target vehicle's lane\nchange three to four seconds in advance, providing the ego vehicle sufficient\ntime to react and allowing the target vehicle to make the lane change safely.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on lane change prediction has gained a lot of momentum in the last\ncouple of years. However, most research is confined to simulation or results\nobtained from datasets, leaving a gap between algorithmic advances and on-road\ndeployment. This work closes that gap by demonstrating, on real hardware, a\nlane-change prediction system based on Knowledge Graph Embeddings (KGEs) and\nBayesian inference. Moreover, the ego-vehicle employs a longitudinal braking\naction to ensure the safety of both itself and the surrounding vehicles. Our\narchitecture consists of two modules: (i) a perception module that senses the\nenvironment, derives input numerical features, and converts them into\nlinguistic categories; and communicates them to the prediction module; (ii) a\npretrained prediction module that executes a KGE and Bayesian inference model\nto anticipate the target vehicle's maneuver and transforms the prediction into\nlongitudinal braking action. Real-world hardware experimental validation\ndemonstrates that our prediction system anticipates the target vehicle's lane\nchange three to four seconds in advance, providing the ego vehicle sufficient\ntime to react and allowing the target vehicle to make the lane change safely."
                },
                "authors": [
                    {
                        "name": "M. Manzour"
                    },
                    {
                        "name": "Catherine M. Elias"
                    },
                    {
                        "name": "Omar M. Shehata"
                    },
                    {
                        "name": "R. Izquierdo"
                    },
                    {
                        "name": "M. A. Sotelo"
                    }
                ],
                "author_detail": {
                    "name": "M. A. Sotelo"
                },
                "author": "M. A. Sotelo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11924v1",
                "updated": "2025-06-13T16:19:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    19,
                    0,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T16:19:00Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    19,
                    0,
                    4,
                    164,
                    0
                ],
                "title": "Aligned Novel View Image and Geometry Synthesis via Cross-modal\n  Attention Instillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligned Novel View Image and Geometry Synthesis via Cross-modal\n  Attention Instillation"
                },
                "summary": "We introduce a diffusion-based framework that performs aligned novel view\nimage and geometry generation via a warping-and-inpainting methodology. Unlike\nprior methods that require dense posed images or pose-embedded generative\nmodels limited to in-domain views, our method leverages off-the-shelf geometry\npredictors to predict partial geometries viewed from reference images, and\nformulates novel-view synthesis as an inpainting task for both image and\ngeometry. To ensure accurate alignment between generated images and geometry,\nwe propose cross-modal attention distillation, where attention maps from the\nimage diffusion branch are injected into a parallel geometry diffusion branch\nduring both training and inference. This multi-task approach achieves\nsynergistic effects, facilitating geometrically robust image synthesis as well\nas well-defined geometry prediction. We further introduce proximity-based mesh\nconditioning to integrate depth and normal cues, interpolating between point\ncloud and filtering erroneously predicted geometry from influencing the\ngeneration process. Empirically, our method achieves high-fidelity\nextrapolative view synthesis on both image and geometry across a range of\nunseen scenes, delivers competitive reconstruction quality under interpolation\nsettings, and produces geometrically aligned colored point clouds for\ncomprehensive 3D completion. Project page is available at\nhttps://cvlab-kaist.github.io/MoAI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a diffusion-based framework that performs aligned novel view\nimage and geometry generation via a warping-and-inpainting methodology. Unlike\nprior methods that require dense posed images or pose-embedded generative\nmodels limited to in-domain views, our method leverages off-the-shelf geometry\npredictors to predict partial geometries viewed from reference images, and\nformulates novel-view synthesis as an inpainting task for both image and\ngeometry. To ensure accurate alignment between generated images and geometry,\nwe propose cross-modal attention distillation, where attention maps from the\nimage diffusion branch are injected into a parallel geometry diffusion branch\nduring both training and inference. This multi-task approach achieves\nsynergistic effects, facilitating geometrically robust image synthesis as well\nas well-defined geometry prediction. We further introduce proximity-based mesh\nconditioning to integrate depth and normal cues, interpolating between point\ncloud and filtering erroneously predicted geometry from influencing the\ngeneration process. Empirically, our method achieves high-fidelity\nextrapolative view synthesis on both image and geometry across a range of\nunseen scenes, delivers competitive reconstruction quality under interpolation\nsettings, and produces geometrically aligned colored point clouds for\ncomprehensive 3D completion. Project page is available at\nhttps://cvlab-kaist.github.io/MoAI."
                },
                "authors": [
                    {
                        "name": "Min-Seop Kwak"
                    },
                    {
                        "name": "Junho Kim"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Dongyoon Han"
                    },
                    {
                        "name": "Taekyoung Kim"
                    },
                    {
                        "name": "Seungryong Kim"
                    },
                    {
                        "name": "Jin-Hwa Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jin-Hwa Kim"
                },
                "author": "Jin-Hwa Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11651v2",
                "updated": "2025-06-13T16:15:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    15,
                    45,
                    4,
                    164,
                    0
                ],
                "published": "2025-01-20T18:33:33Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    18,
                    33,
                    33,
                    0,
                    20,
                    0
                ],
                "title": "T1: Advancing Language Model Reasoning through Reinforcement Learning\n  and Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: Advancing Language Model Reasoning through Reinforcement Learning\n  and Inference Scaling"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks. However, existing approaches mainly rely on imitation\nlearning and struggle to achieve effective test-time scaling. While\nreinforcement learning (RL) holds promise for enabling self-exploration, recent\nattempts yield modest improvements in complex reasoning. In this paper, we\npresent T1 to scale RL by encouraging exploration and understand inference\nscaling. We first initialize the LLM using synthesized chain-of-thought data\nthat integrates trial-and-error and self-verification. To scale RL training, we\npromote increased sampling diversity through oversampling. We demonstrate that\nT1 with open LLMs as its base exhibits inference scaling behavior and achieves\nsuperior performance on challenging math reasoning benchmarks. More\nimportantly, we present a simple strategy to examine inference scaling, where\nincreased inference budgets directly lead to T1's better performance without\nany additional verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks. However, existing approaches mainly rely on imitation\nlearning and struggle to achieve effective test-time scaling. While\nreinforcement learning (RL) holds promise for enabling self-exploration, recent\nattempts yield modest improvements in complex reasoning. In this paper, we\npresent T1 to scale RL by encouraging exploration and understand inference\nscaling. We first initialize the LLM using synthesized chain-of-thought data\nthat integrates trial-and-error and self-verification. To scale RL training, we\npromote increased sampling diversity through oversampling. We demonstrate that\nT1 with open LLMs as its base exhibits inference scaling behavior and achieves\nsuperior performance on challenging math reasoning benchmarks. More\nimportantly, we present a simple strategy to examine inference scaling, where\nincreased inference budgets directly lead to T1's better performance without\nany additional verification."
                },
                "authors": [
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Yujiang Li"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "arxiv_comment": "Accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11916v1",
                "updated": "2025-06-13T16:09:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    9,
                    21,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T16:09:21Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    9,
                    21,
                    4,
                    164,
                    0
                ],
                "title": "mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity"
                },
                "summary": "We present a diffusion-based model recipe for real-world control of a highly\ndexterous humanoid robotic hand, designed for sample-efficient learning and\nsmooth fine-motor action inference. Our system features a newly designed 16-DoF\ntendon-driven hand, equipped with wide angle wrist cameras and mounted on a\nFranka Emika Panda arm. We develop a versatile teleoperation pipeline and data\ncollection protocol using both glove-based and VR interfaces, enabling\nhigh-quality data collection across diverse tasks such as pick and place, item\nsorting and assembly insertion. Leveraging high-frequency generative control,\nwe train end-to-end policies from raw sensory inputs, enabling smooth,\nself-correcting motions in complex manipulation scenarios. Real-world\nevaluations demonstrate up to 93.3% out of distribution success rates, with up\nto a +33.3% performance boost due to emergent self-correcting behaviors, while\nalso revealing scaling trends in policy performance. Our results advance the\nstate-of-the-art in dexterous robotic manipulation through a fully integrated,\npractical approach to hardware, learning, and real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a diffusion-based model recipe for real-world control of a highly\ndexterous humanoid robotic hand, designed for sample-efficient learning and\nsmooth fine-motor action inference. Our system features a newly designed 16-DoF\ntendon-driven hand, equipped with wide angle wrist cameras and mounted on a\nFranka Emika Panda arm. We develop a versatile teleoperation pipeline and data\ncollection protocol using both glove-based and VR interfaces, enabling\nhigh-quality data collection across diverse tasks such as pick and place, item\nsorting and assembly insertion. Leveraging high-frequency generative control,\nwe train end-to-end policies from raw sensory inputs, enabling smooth,\nself-correcting motions in complex manipulation scenarios. Real-world\nevaluations demonstrate up to 93.3% out of distribution success rates, with up\nto a +33.3% performance boost due to emergent self-correcting behaviors, while\nalso revealing scaling trends in policy performance. Our results advance the\nstate-of-the-art in dexterous robotic manipulation through a fully integrated,\npractical approach to hardware, learning, and real-world deployment."
                },
                "authors": [
                    {
                        "name": "Elvis Nava"
                    },
                    {
                        "name": "Victoriano Montesinos"
                    },
                    {
                        "name": "Erik Bauer"
                    },
                    {
                        "name": "Benedek Forrai"
                    },
                    {
                        "name": "Jonas Pai"
                    },
                    {
                        "name": "Stefan Weirich"
                    },
                    {
                        "name": "Stephan-Daniel Gravert"
                    },
                    {
                        "name": "Philipp Wand"
                    },
                    {
                        "name": "Stephan Polinski"
                    },
                    {
                        "name": "Benjamin F. Grewe"
                    },
                    {
                        "name": "Robert K. Katzschmann"
                    }
                ],
                "author_detail": {
                    "name": "Robert K. Katzschmann"
                },
                "author": "Robert K. Katzschmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11908v1",
                "updated": "2025-06-13T15:58:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    58,
                    5,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:58:05Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    58,
                    5,
                    4,
                    164,
                    0
                ],
                "title": "Spectra-to-Structure and Structure-to-Spectra Inference Across the\n  Periodic Table",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectra-to-Structure and Structure-to-Spectra Inference Across the\n  Periodic Table"
                },
                "summary": "X-ray Absorption Spectroscopy (XAS) is a powerful technique for probing local\natomic environments, yet its interpretation remains limited by the need for\nexpert-driven analysis, computationally expensive simulations, and\nelement-specific heuristics. Recent advances in machine learning have shown\npromise for accelerating XAS interpretation, but many existing models are\nnarrowly focused on specific elements, edge types, or spectral regimes. In this\nwork, we present XAStruct, a learning framework capable of both predicting XAS\nspectra from crystal structures and inferring local structural descriptors from\nXAS input. XAStruct is trained on a large-scale dataset spanning over 70\nelements across the periodic table, enabling generalization to a wide variety\nof chemistries and bonding environments. The model includes the first machine\nlearning approach for predicting neighbor atom types directly from XAS spectra,\nas well as a unified regression model for mean nearest-neighbor distance that\nrequires no element-specific tuning. While we explored integrating the two\npipelines into a single end-to-end model, empirical results showed performance\ndegradation. As a result, the two tasks were trained independently to ensure\noptimal accuracy and task-specific performance. By combining deep neural\nnetworks for complex structure-property mappings with efficient baseline models\nfor simpler tasks, XAStruct offers a scalable and extensible solution for\ndata-driven XAS analysis and local structure inference. The source code will be\nreleased upon paper acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray Absorption Spectroscopy (XAS) is a powerful technique for probing local\natomic environments, yet its interpretation remains limited by the need for\nexpert-driven analysis, computationally expensive simulations, and\nelement-specific heuristics. Recent advances in machine learning have shown\npromise for accelerating XAS interpretation, but many existing models are\nnarrowly focused on specific elements, edge types, or spectral regimes. In this\nwork, we present XAStruct, a learning framework capable of both predicting XAS\nspectra from crystal structures and inferring local structural descriptors from\nXAS input. XAStruct is trained on a large-scale dataset spanning over 70\nelements across the periodic table, enabling generalization to a wide variety\nof chemistries and bonding environments. The model includes the first machine\nlearning approach for predicting neighbor atom types directly from XAS spectra,\nas well as a unified regression model for mean nearest-neighbor distance that\nrequires no element-specific tuning. While we explored integrating the two\npipelines into a single end-to-end model, empirical results showed performance\ndegradation. As a result, the two tasks were trained independently to ensure\noptimal accuracy and task-specific performance. By combining deep neural\nnetworks for complex structure-property mappings with efficient baseline models\nfor simpler tasks, XAStruct offers a scalable and extensible solution for\ndata-driven XAS analysis and local structure inference. The source code will be\nreleased upon paper acceptance."
                },
                "authors": [
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Peiyao Wang"
                    },
                    {
                        "name": "Lu Ma"
                    },
                    {
                        "name": "Yuewei Lin"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Haibin Ling"
                    }
                ],
                "author_detail": {
                    "name": "Haibin Ling"
                },
                "author": "Haibin Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19037v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19037v2",
                "updated": "2025-06-13T15:57:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    57,
                    15,
                    4,
                    164,
                    0
                ],
                "published": "2024-11-28T10:33:01Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    10,
                    33,
                    1,
                    3,
                    333,
                    0
                ],
                "title": "3D-WAG: Hierarchical Wavelet-Guided Autoregressive Generation for\n  High-Fidelity 3D Shapes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D-WAG: Hierarchical Wavelet-Guided Autoregressive Generation for\n  High-Fidelity 3D Shapes"
                },
                "summary": "Autoregressive (AR) models have achieved remarkable success in natural\nlanguage and image generation, but their application to 3D shape modeling\nremains largely unexplored. Unlike diffusion models, AR models enable more\nefficient and controllable generation with faster inference times, making them\nespecially suitable for data-intensive domains. Traditional 3D generative\nmodels using AR approaches often rely on ``next-token\" predictions at the voxel\nor point level. While effective for certain applications, these methods can be\nrestrictive and computationally expensive when dealing with large-scale 3D\ndata. To tackle these challenges, we introduce 3D-WAG, an AR model for 3D\nimplicit distance fields that can perform unconditional shape generation,\nclass-conditioned and also text-conditioned shape generation. Our key idea is\nto encode shapes as multi-scale wavelet token maps and use a Transformer to\npredict the ``next higher-resolution token map\" in an autoregressive manner. By\nredefining 3D AR generation task as ``next-scale\" prediction, we reduce the\ncomputational cost of generation compared to traditional ``next-token\"\nprediction models, while preserving essential geometric details of 3D shapes in\na more structured and hierarchical manner. We evaluate 3D-WAG to showcase its\nbenefit by quantitative and qualitative comparisons with state-of-the-art\nmethods on widely used benchmarks. Our results show 3D-WAG achieves superior\nperformance in key metrics like Coverage and MMD, generating high-fidelity 3D\nshapes that closely match the real data distribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) models have achieved remarkable success in natural\nlanguage and image generation, but their application to 3D shape modeling\nremains largely unexplored. Unlike diffusion models, AR models enable more\nefficient and controllable generation with faster inference times, making them\nespecially suitable for data-intensive domains. Traditional 3D generative\nmodels using AR approaches often rely on ``next-token\" predictions at the voxel\nor point level. While effective for certain applications, these methods can be\nrestrictive and computationally expensive when dealing with large-scale 3D\ndata. To tackle these challenges, we introduce 3D-WAG, an AR model for 3D\nimplicit distance fields that can perform unconditional shape generation,\nclass-conditioned and also text-conditioned shape generation. Our key idea is\nto encode shapes as multi-scale wavelet token maps and use a Transformer to\npredict the ``next higher-resolution token map\" in an autoregressive manner. By\nredefining 3D AR generation task as ``next-scale\" prediction, we reduce the\ncomputational cost of generation compared to traditional ``next-token\"\nprediction models, while preserving essential geometric details of 3D shapes in\na more structured and hierarchical manner. We evaluate 3D-WAG to showcase its\nbenefit by quantitative and qualitative comparisons with state-of-the-art\nmethods on widely used benchmarks. Our results show 3D-WAG achieves superior\nperformance in key metrics like Coverage and MMD, generating high-fidelity 3D\nshapes that closely match the real data distribution."
                },
                "authors": [
                    {
                        "name": "Tejaswini Medi"
                    },
                    {
                        "name": "Arianna Rampini"
                    },
                    {
                        "name": "Pradyumna Reddy"
                    },
                    {
                        "name": "Pradeep Kumar Jayaraman"
                    },
                    {
                        "name": "Margret Keuper"
                    }
                ],
                "author_detail": {
                    "name": "Margret Keuper"
                },
                "author": "Margret Keuper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19037v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19037v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11902v1",
                "updated": "2025-06-13T15:52:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    52,
                    37,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:52:37Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    52,
                    37,
                    4,
                    164,
                    0
                ],
                "title": "TreeRL: LLM Reinforcement Learning with On-Policy Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeRL: LLM Reinforcement Learning with On-Policy Tree Search"
                },
                "summary": "Reinforcement learning (RL) with tree search has demonstrated superior\nperformance in traditional reasoning tasks. Compared to conventional\nindependent chain sampling strategies with outcome supervision, tree search\nenables better exploration of the reasoning space and provides dense, on-policy\nprocess rewards during RL training but remains under-explored in On-Policy LLM\nRL. We propose TreeRL, a reinforcement learning framework that directly\nincorporates on-policy tree search for RL training. Our approach includes\nintermediate supervision and eliminates the need for a separate reward model\ntraining. Existing approaches typically train a separate process reward model,\nwhich can suffer from distribution mismatch and reward hacking. We also\nintroduce a cost-effective tree search approach that achieves higher search\nefficiency under the same generation token budget by strategically branching\nfrom high-uncertainty intermediate steps rather than using random branching.\nExperiments on challenging math and code reasoning benchmarks demonstrate that\nTreeRL achieves superior performance compared to traditional ChainRL,\nhighlighting the potential of tree search for LLM. TreeRL is open-sourced at\nhttps://github.com/THUDM/TreeRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) with tree search has demonstrated superior\nperformance in traditional reasoning tasks. Compared to conventional\nindependent chain sampling strategies with outcome supervision, tree search\nenables better exploration of the reasoning space and provides dense, on-policy\nprocess rewards during RL training but remains under-explored in On-Policy LLM\nRL. We propose TreeRL, a reinforcement learning framework that directly\nincorporates on-policy tree search for RL training. Our approach includes\nintermediate supervision and eliminates the need for a separate reward model\ntraining. Existing approaches typically train a separate process reward model,\nwhich can suffer from distribution mismatch and reward hacking. We also\nintroduce a cost-effective tree search approach that achieves higher search\nefficiency under the same generation token budget by strategically branching\nfrom high-uncertainty intermediate steps rather than using random branching.\nExperiments on challenging math and code reasoning benchmarks demonstrate that\nTreeRL achieves superior performance compared to traditional ChainRL,\nhighlighting the potential of tree search for LLM. TreeRL is open-sourced at\nhttps://github.com/THUDM/TreeRL."
                },
                "authors": [
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Ziniu Hu"
                    },
                    {
                        "name": "Yujiang Li"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "arxiv_comment": "Accepted to ACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18638v2",
                "updated": "2025-06-13T15:44:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    44,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-01-28T17:10:20Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    10,
                    20,
                    1,
                    28,
                    0
                ],
                "title": "Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt\n  Generation for Enhanced LLM Content Moderation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt\n  Generation for Enhanced LLM Content Moderation"
                },
                "summary": "As large language models (LLMs) become increasingly prevalent, ensuring their\nrobustness against adversarial misuse is crucial. This paper introduces the GAP\n(Graph of Attacks with Pruning) framework, an advanced approach for generating\nstealthy jailbreak prompts to evaluate and enhance LLM safeguards. GAP\naddresses limitations in existing tree-based LLM jailbreak methods by\nimplementing an interconnected graph structure that enables knowledge sharing\nacross attack paths. Our experimental evaluation demonstrates GAP's superiority\nover existing techniques, achieving a 20.8% increase in attack success rates\nwhile reducing query costs by 62.7%. GAP consistently outperforms\nstate-of-the-art methods for attacking both open and closed LLMs, with attack\nsuccess rates of >96%. Additionally, we present specialized variants like\nGAP-Auto for automated seed generation and GAP-VLM for multimodal attacks.\nGAP-generated prompts prove highly effective in improving content moderation\nsystems, increasing true positive detection rates by 108.5% and accuracy by\n183.6% when used for fine-tuning. Our implementation is available at\nhttps://github.com/dsbuddy/GAP-LLM-Safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly prevalent, ensuring their\nrobustness against adversarial misuse is crucial. This paper introduces the GAP\n(Graph of Attacks with Pruning) framework, an advanced approach for generating\nstealthy jailbreak prompts to evaluate and enhance LLM safeguards. GAP\naddresses limitations in existing tree-based LLM jailbreak methods by\nimplementing an interconnected graph structure that enables knowledge sharing\nacross attack paths. Our experimental evaluation demonstrates GAP's superiority\nover existing techniques, achieving a 20.8% increase in attack success rates\nwhile reducing query costs by 62.7%. GAP consistently outperforms\nstate-of-the-art methods for attacking both open and closed LLMs, with attack\nsuccess rates of >96%. Additionally, we present specialized variants like\nGAP-Auto for automated seed generation and GAP-VLM for multimodal attacks.\nGAP-generated prompts prove highly effective in improving content moderation\nsystems, increasing true positive detection rates by 108.5% and accuracy by\n183.6% when used for fine-tuning. Our implementation is available at\nhttps://github.com/dsbuddy/GAP-LLM-Safety."
                },
                "authors": [
                    {
                        "name": "Daniel Schwartz"
                    },
                    {
                        "name": "Dmitriy Bespalov"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Ninad Kulkarni"
                    },
                    {
                        "name": "Yanjun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Qi"
                },
                "author": "Yanjun Qi",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11898v1",
                "updated": "2025-06-13T15:44:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    44,
                    14,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:44:14Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    44,
                    14,
                    4,
                    164,
                    0
                ],
                "title": "Scalable Generalized Bayesian Online Neural Network Training for\n  Sequential Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Generalized Bayesian Online Neural Network Training for\n  Sequential Decision Making"
                },
                "summary": "We introduce scalable algorithms for online learning and generalized Bayesian\ninference of neural network parameters, designed for sequential decision making\ntasks. Our methods combine the strengths of frequentist and Bayesian filtering,\nwhich include fast low-rank updates via a block-diagonal approximation of the\nparameter error covariance, and a well-defined posterior predictive\ndistribution that we use for decision making. More precisely, our main method\nupdates a low-rank error covariance for the hidden layers parameters, and a\nfull-rank error covariance for the final layer parameters. Although this\ncharacterizes an improper posterior, we show that the resulting posterior\npredictive distribution is well-defined. Our methods update all network\nparameters online, with no need for replay buffers or offline retraining. We\nshow, empirically, that our methods achieve a competitive tradeoff between\nspeed and accuracy on (non-stationary) contextual bandit problems and Bayesian\noptimization problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce scalable algorithms for online learning and generalized Bayesian\ninference of neural network parameters, designed for sequential decision making\ntasks. Our methods combine the strengths of frequentist and Bayesian filtering,\nwhich include fast low-rank updates via a block-diagonal approximation of the\nparameter error covariance, and a well-defined posterior predictive\ndistribution that we use for decision making. More precisely, our main method\nupdates a low-rank error covariance for the hidden layers parameters, and a\nfull-rank error covariance for the final layer parameters. Although this\ncharacterizes an improper posterior, we show that the resulting posterior\npredictive distribution is well-defined. Our methods update all network\nparameters online, with no need for replay buffers or offline retraining. We\nshow, empirically, that our methods achieve a competitive tradeoff between\nspeed and accuracy on (non-stationary) contextual bandit problems and Bayesian\noptimization problems."
                },
                "authors": [
                    {
                        "name": "Gerardo Duran-Martin"
                    },
                    {
                        "name": "Leandro Sánchez-Betancourt"
                    },
                    {
                        "name": "Álvaro Cartea"
                    },
                    {
                        "name": "Kevin Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Murphy"
                },
                "author": "Kevin Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11889v1",
                "updated": "2025-06-13T15:37:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    37,
                    10,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:37:10Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    37,
                    10,
                    4,
                    164,
                    0
                ],
                "title": "Simultaneous hypothesis testing for comparing many functional means",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous hypothesis testing for comparing many functional means"
                },
                "summary": "Data with multiple functional recordings at each observational unit are\nincreasingly common in various fields including medical imaging and\nenvironmental sciences. To conduct inference for such observations, we develop\na paired two-sample test that allows to simultaneously compare the means of\nmany functional observations while maintaining family-wise error rate control.\nWe explicitly allow the number of functional recordings to increase,\npotentially much faster than the sample size. Our test is fully functional and\ndoes not rely on dimension reduction or functional PCA type approaches or the\nchoice of tuning parameters. To provide a theoretical justification for the\nproposed procedure, we develop a number of new anti-concentration and Gaussian\napproximation results for maxima of $L^2$ statistics which might be of\nindependent interest. The methodology is illustrated on the task-related\ncortical surface functional magnetic resonance imaging data from Human\nConnectome Project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data with multiple functional recordings at each observational unit are\nincreasingly common in various fields including medical imaging and\nenvironmental sciences. To conduct inference for such observations, we develop\na paired two-sample test that allows to simultaneously compare the means of\nmany functional observations while maintaining family-wise error rate control.\nWe explicitly allow the number of functional recordings to increase,\npotentially much faster than the sample size. Our test is fully functional and\ndoes not rely on dimension reduction or functional PCA type approaches or the\nchoice of tuning parameters. To provide a theoretical justification for the\nproposed procedure, we develop a number of new anti-concentration and Gaussian\napproximation results for maxima of $L^2$ statistics which might be of\nindependent interest. The methodology is illustrated on the task-related\ncortical surface functional magnetic resonance imaging data from Human\nConnectome Project."
                },
                "authors": [
                    {
                        "name": "Colin Decker"
                    },
                    {
                        "name": "Dehan Kong"
                    },
                    {
                        "name": "Stanislav Volgushev"
                    }
                ],
                "author_detail": {
                    "name": "Stanislav Volgushev"
                },
                "author": "Stanislav Volgushev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02509v2",
                "updated": "2025-06-13T15:36:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    36,
                    41,
                    4,
                    164,
                    0
                ],
                "published": "2024-08-05T14:31:26Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    14,
                    31,
                    26,
                    0,
                    218,
                    0
                ],
                "title": "Black-Box Adversarial Attacks on LLM-Based Code Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-Box Adversarial Attacks on LLM-Based Code Completion"
                },
                "summary": "Modern code completion engines, powered by large language models (LLMs),\nassist millions of developers with their strong capabilities to generate\nfunctionally correct code. Due to this popularity, it is crucial to investigate\nthe security implications of relying on LLM-based code completion. In this\nwork, we demonstrate that state-of-the-art black-box LLM-based code completion\nengines can be stealthily biased by adversaries to significantly increase their\nrate of insecure code generation. We present the first attack, named INSEC,\nthat achieves this goal. INSEC works by injecting an attack string as a short\ncomment in the completion input. The attack string is crafted through a\nquery-based optimization procedure starting from a set of carefully designed\ninitialization schemes. We demonstrate INSEC's broad applicability and\neffectiveness by evaluating it on various state-of-the-art open-source models\nand black-box commercial services (e.g., OpenAI API and GitHub Copilot). On a\ndiverse set of security-critical test cases, covering 16 CWEs across 5\nprogramming languages, INSEC increases the rate of generated insecure code by\nmore than 50%, while maintaining the functional correctness of generated code.\nWe consider INSEC practical -- it requires low resources and costs less than 10\nUS dollars to develop on commodity hardware. Moreover, we showcase the attack's\nreal-world deployability, by developing an IDE plug-in that stealthily injects\nINSEC into the GitHub Copilot extension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern code completion engines, powered by large language models (LLMs),\nassist millions of developers with their strong capabilities to generate\nfunctionally correct code. Due to this popularity, it is crucial to investigate\nthe security implications of relying on LLM-based code completion. In this\nwork, we demonstrate that state-of-the-art black-box LLM-based code completion\nengines can be stealthily biased by adversaries to significantly increase their\nrate of insecure code generation. We present the first attack, named INSEC,\nthat achieves this goal. INSEC works by injecting an attack string as a short\ncomment in the completion input. The attack string is crafted through a\nquery-based optimization procedure starting from a set of carefully designed\ninitialization schemes. We demonstrate INSEC's broad applicability and\neffectiveness by evaluating it on various state-of-the-art open-source models\nand black-box commercial services (e.g., OpenAI API and GitHub Copilot). On a\ndiverse set of security-critical test cases, covering 16 CWEs across 5\nprogramming languages, INSEC increases the rate of generated insecure code by\nmore than 50%, while maintaining the functional correctness of generated code.\nWe consider INSEC practical -- it requires low resources and costs less than 10\nUS dollars to develop on commodity hardware. Moreover, we showcase the attack's\nreal-world deployability, by developing an IDE plug-in that stealthily injects\nINSEC into the GitHub Copilot extension."
                },
                "authors": [
                    {
                        "name": "Slobodan Jenko"
                    },
                    {
                        "name": "Niels Mündler"
                    },
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11887v1",
                "updated": "2025-06-13T15:36:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    36,
                    22,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:36:22Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    36,
                    22,
                    4,
                    164,
                    0
                ],
                "title": "Towards a Cascaded LLM Framework for Cost-effective Human-AI\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Cascaded LLM Framework for Cost-effective Human-AI\n  Decision-Making"
                },
                "summary": "Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions."
                },
                "authors": [
                    {
                        "name": "Claudio Fanconi"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11886v1",
                "updated": "2025-06-13T15:35:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:35:54Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache"
                },
                "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Siyang He"
                    },
                    {
                        "name": "Qiqi Wang"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "10 pages, 7 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11880v1",
                "updated": "2025-06-13T15:29:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    29,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:29:43Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    29,
                    43,
                    4,
                    164,
                    0
                ],
                "title": "Addressing Bias in LLMs: Strategies and Application to Fair AI-based\n  Recruitment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Bias in LLMs: Strategies and Application to Fair AI-based\n  Recruitment"
                },
                "summary": "The use of language technologies in high-stake settings is increasing in\nrecent years, mostly motivated by the success of Large Language Models (LLMs).\nHowever, despite the great performance of LLMs, they are are susceptible to\nethical concerns, such as demographic biases, accountability, or privacy. This\nwork seeks to analyze the capacity of Transformers-based systems to learn\ndemographic biases present in the data, using a case study on AI-based\nautomated recruitment. We propose a privacy-enhancing framework to reduce\ngender information from the learning pipeline as a way to mitigate biased\nbehaviors in the final tools. Our experiments analyze the influence of data\nbiases on systems built on two different LLMs, and how the proposed framework\neffectively prevents trained systems from reproducing the bias in the data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of language technologies in high-stake settings is increasing in\nrecent years, mostly motivated by the success of Large Language Models (LLMs).\nHowever, despite the great performance of LLMs, they are are susceptible to\nethical concerns, such as demographic biases, accountability, or privacy. This\nwork seeks to analyze the capacity of Transformers-based systems to learn\ndemographic biases present in the data, using a case study on AI-based\nautomated recruitment. We propose a privacy-enhancing framework to reduce\ngender information from the learning pipeline as a way to mitigate biased\nbehaviors in the final tools. Our experiments analyze the influence of data\nbiases on systems built on two different LLMs, and how the proposed framework\neffectively prevents trained systems from reproducing the bias in the data."
                },
                "authors": [
                    {
                        "name": "Alejandro Peña"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Aythami Morales"
                    },
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Miguel Lopez"
                    },
                    {
                        "name": "Ruben Tolosana"
                    }
                ],
                "author_detail": {
                    "name": "Ruben Tolosana"
                },
                "author": "Ruben Tolosana",
                "arxiv_comment": "Submitted to AIES 2025 (Under Review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11874v1",
                "updated": "2025-06-13T15:26:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    26,
                    58,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:26:58Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    26,
                    58,
                    4,
                    164,
                    0
                ],
                "title": "A Short Survey on Formalising Software Requirements using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Short Survey on Formalising Software Requirements using Large Language\n  Models"
                },
                "summary": "This paper presents a focused literature survey on the use of large language\nmodels (LLM) to assist in writing formal specifications for software. A summary\nof thirty-five key papers is presented, including examples for specifying\nprograms written in Dafny, C and Java. This paper arose from the project\nVERIFAI - Traceability and verification of natural language requirements that\naddresses the challenges in writing formal specifications from requirements\nthat are expressed in natural language. Our methodology employed multiple\nacademic databases to identify relevant research. The AI-assisted tool Elicit\nfacilitated the initial paper selection, which were manually screened for final\nselection. The survey provides valuable insights and future directions for\nutilising LLMs while formalising software requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a focused literature survey on the use of large language\nmodels (LLM) to assist in writing formal specifications for software. A summary\nof thirty-five key papers is presented, including examples for specifying\nprograms written in Dafny, C and Java. This paper arose from the project\nVERIFAI - Traceability and verification of natural language requirements that\naddresses the challenges in writing formal specifications from requirements\nthat are expressed in natural language. Our methodology employed multiple\nacademic databases to identify relevant research. The AI-assisted tool Elicit\nfacilitated the initial paper selection, which were manually screened for final\nselection. The survey provides valuable insights and future directions for\nutilising LLMs while formalising software requirements."
                },
                "authors": [
                    {
                        "name": "Arshad Beg"
                    },
                    {
                        "name": "Diarmuid O'Donoghue"
                    },
                    {
                        "name": "Rosemary Monahan"
                    }
                ],
                "author_detail": {
                    "name": "Rosemary Monahan"
                },
                "author": "Rosemary Monahan",
                "arxiv_comment": "Submitted to SAIV 2025 as extended abstract and received valuable\n  comments improving our draft. This version is the improved one after\n  addressing suggestions from reviewers for improving the draft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.1; D.2.4; D.2.10; F.4.1; F.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11870v1",
                "updated": "2025-06-13T15:23:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    23,
                    7,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:23:07Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    23,
                    7,
                    4,
                    164,
                    0
                ],
                "title": "LLM-based Dynamic Differential Testing for Database Connectors with\n  Reinforcement Learning-Guided Prompt Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Dynamic Differential Testing for Database Connectors with\n  Reinforcement Learning-Guided Prompt Selection"
                },
                "summary": "Database connectors are critical components enabling applications to interact\nwith underlying database management systems (DBMS), yet their security\nvulnerabilities often remain overlooked. Unlike traditional software defects,\nconnector vulnerabilities exhibit subtle behavioral patterns and are inherently\nchallenging to detect. Besides, nonstandardized implementation of connectors\nleaves potential risks (a.k.a. unsafe implementations) but is more elusive. As\na result, traditional fuzzing methods are incapable of finding such\nvulnerabilities. Even for LLM-enable test case generation, due to a lack of\ndomain knowledge, they are also incapable of generating test cases that invoke\nall interface and internal logic of connectors. In this paper, we propose\nreinforcement learning (RL)-guided LLM test-case generation for database\nconnector testing. Specifically, to equip the LLM with sufficient and\nappropriate domain knowledge, a parameterized prompt template is composed which\ncan be utilized to generate numerous prompts. Test cases are generated via LLM\nwith a prompt, and are dynamically evaluated through differential testing\nacross multiple connectors. The testing is iteratively conducted, with each\nround RL is adopted to select optimal prompt based on prior-round behavioral\nfeedback, so as to maximize control flow coverage. We implement aforementioned\nmethodology in a practical tool and evaluate it on two widely used JDBC\nconnectors: MySQL Connector/J and OceanBase Connector/J. In total, we reported\n16 bugs, among them 10 are officially confirmed and the rest are acknowledged\nas unsafe implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Database connectors are critical components enabling applications to interact\nwith underlying database management systems (DBMS), yet their security\nvulnerabilities often remain overlooked. Unlike traditional software defects,\nconnector vulnerabilities exhibit subtle behavioral patterns and are inherently\nchallenging to detect. Besides, nonstandardized implementation of connectors\nleaves potential risks (a.k.a. unsafe implementations) but is more elusive. As\na result, traditional fuzzing methods are incapable of finding such\nvulnerabilities. Even for LLM-enable test case generation, due to a lack of\ndomain knowledge, they are also incapable of generating test cases that invoke\nall interface and internal logic of connectors. In this paper, we propose\nreinforcement learning (RL)-guided LLM test-case generation for database\nconnector testing. Specifically, to equip the LLM with sufficient and\nappropriate domain knowledge, a parameterized prompt template is composed which\ncan be utilized to generate numerous prompts. Test cases are generated via LLM\nwith a prompt, and are dynamically evaluated through differential testing\nacross multiple connectors. The testing is iteratively conducted, with each\nround RL is adopted to select optimal prompt based on prior-round behavioral\nfeedback, so as to maximize control flow coverage. We implement aforementioned\nmethodology in a practical tool and evaluate it on two widely used JDBC\nconnectors: MySQL Connector/J and OceanBase Connector/J. In total, we reported\n16 bugs, among them 10 are officially confirmed and the rest are acknowledged\nas unsafe implementations."
                },
                "authors": [
                    {
                        "name": "Ce Lyu"
                    },
                    {
                        "name": "Minghao Zhao"
                    },
                    {
                        "name": "Yanhao Wang"
                    },
                    {
                        "name": "Liang Jie"
                    }
                ],
                "author_detail": {
                    "name": "Liang Jie"
                },
                "author": "Liang Jie",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13615v2",
                "updated": "2025-06-13T15:09:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    9,
                    26,
                    4,
                    164,
                    0
                ],
                "published": "2025-04-18T10:43:21Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    43,
                    21,
                    4,
                    108,
                    0
                ],
                "title": "Long-context Non-factoid Question Answering in Indic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Non-factoid Question Answering in Indic Languages"
                },
                "summary": "Question Answering (QA) tasks, which involve extracting answers from a given\ncontext, are relatively straightforward for modern Large Language Models (LLMs)\nwhen the context is short. However, long contexts pose challenges due to the\nquadratic complexity of the self-attention mechanism. This challenge is\ncompounded in Indic languages, which are often low-resource. This study\nexplores context-shortening techniques, including Open Information Extraction\n(OIE), coreference resolution, Answer Paragraph Selection (APS), and their\ncombinations, to improve QA performance. Compared to the baseline of\nunshortened (long) contexts, our experiments on four Indic languages (Hindi,\nTamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield\nan average improvement of 4\\% in semantic scores and 47\\% in token-level scores\nwhen evaluated on three popular LLMs without fine-tuning. Furthermore, with\nfine-tuning, we achieve an average increase of 2\\% in both semantic and\ntoken-level scores. Additionally, context-shortening reduces computational\noverhead. Explainability techniques like LIME and SHAP reveal that when the APS\nmodel confidently identifies the paragraph containing the answer, nearly all\ntokens within the selected text receive high relevance scores. However, the\nstudy also highlights the limitations of LLM-based QA systems in addressing\nnon-factoid questions, particularly those requiring reasoning or debate.\nMoreover, verbalizing OIE-generated triples does not enhance system\nperformance. These findings emphasize the potential of context-shortening\ntechniques to improve the efficiency and effectiveness of LLM-based QA systems,\nespecially for low-resource languages. The source code and resources are\navailable at https://github.com/ritwikmishra/IndicGenQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question Answering (QA) tasks, which involve extracting answers from a given\ncontext, are relatively straightforward for modern Large Language Models (LLMs)\nwhen the context is short. However, long contexts pose challenges due to the\nquadratic complexity of the self-attention mechanism. This challenge is\ncompounded in Indic languages, which are often low-resource. This study\nexplores context-shortening techniques, including Open Information Extraction\n(OIE), coreference resolution, Answer Paragraph Selection (APS), and their\ncombinations, to improve QA performance. Compared to the baseline of\nunshortened (long) contexts, our experiments on four Indic languages (Hindi,\nTamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield\nan average improvement of 4\\% in semantic scores and 47\\% in token-level scores\nwhen evaluated on three popular LLMs without fine-tuning. Furthermore, with\nfine-tuning, we achieve an average increase of 2\\% in both semantic and\ntoken-level scores. Additionally, context-shortening reduces computational\noverhead. Explainability techniques like LIME and SHAP reveal that when the APS\nmodel confidently identifies the paragraph containing the answer, nearly all\ntokens within the selected text receive high relevance scores. However, the\nstudy also highlights the limitations of LLM-based QA systems in addressing\nnon-factoid questions, particularly those requiring reasoning or debate.\nMoreover, verbalizing OIE-generated triples does not enhance system\nperformance. These findings emphasize the potential of context-shortening\ntechniques to improve the efficiency and effectiveness of LLM-based QA systems,\nespecially for low-resource languages. The source code and resources are\navailable at https://github.com/ritwikmishra/IndicGenQA."
                },
                "authors": [
                    {
                        "name": "Ritwik Mishra"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    }
                ],
                "author_detail": {
                    "name": "Ponnurangam Kumaraguru"
                },
                "author": "Ponnurangam Kumaraguru",
                "arxiv_comment": "Short version of this manuscript accepted at\n  https://bda2025.iiitb.net/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11860v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11860v1",
                "updated": "2025-06-13T15:09:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    9,
                    15,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:09:15Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    9,
                    15,
                    4,
                    164,
                    0
                ],
                "title": "MindGrab for BrainChop: Fast and Accurate Skull Stripping for Command\n  Line and Browser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindGrab for BrainChop: Fast and Accurate Skull Stripping for Command\n  Line and Browser"
                },
                "summary": "We developed MindGrab, a parameter- and memory-efficient deep\nfully-convolutional model for volumetric skull-stripping in head images of any\nmodality. Its architecture, informed by a spectral interpretation of dilated\nconvolutions, was trained exclusively on modality-agnostic synthetic data.\nMindGrab was evaluated on a retrospective dataset of 606 multimodal adult-brain\nscans (T1, T2, DWI, MRA, PDw MRI, EPI, CT, PET) sourced from the SynthStrip\ndataset. Performance was benchmarked against SynthStrip, ROBEX, and BET using\nDice scores, with Wilcoxon signed-rank significance tests. MindGrab achieved a\nmean Dice score of 95.9 with standard deviation (SD) 1.6 across modalities,\nsignificantly outperforming classical methods (ROBEX: 89.1 SD 7.7, P < 0.05;\nBET: 85.2 SD 14.4, P < 0.05). Compared to SynthStrip (96.5 SD 1.1, P=0.0352),\nMindGrab delivered equivalent or superior performance in nearly half of the\ntested scenarios, with minor differences (<3% Dice) in the others. MindGrab\nutilized 95% fewer parameters (146,237 vs. 2,566,561) than SynthStrip. This\nefficiency yielded at least 2x faster inference, 50% lower memory usage on\nGPUs, and enabled exceptional performance (e.g., 10-30x speedup, and up to 30x\nmemory reduction) and accessibility on a wider range of hardware, including\nsystems without high-end GPUs. MindGrab delivers state-of-the-art accuracy with\ndramatically lower resource demands, supported in brainchop-cli\n(https://pypi.org/project/brainchop/) and at brainchop.org.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed MindGrab, a parameter- and memory-efficient deep\nfully-convolutional model for volumetric skull-stripping in head images of any\nmodality. Its architecture, informed by a spectral interpretation of dilated\nconvolutions, was trained exclusively on modality-agnostic synthetic data.\nMindGrab was evaluated on a retrospective dataset of 606 multimodal adult-brain\nscans (T1, T2, DWI, MRA, PDw MRI, EPI, CT, PET) sourced from the SynthStrip\ndataset. Performance was benchmarked against SynthStrip, ROBEX, and BET using\nDice scores, with Wilcoxon signed-rank significance tests. MindGrab achieved a\nmean Dice score of 95.9 with standard deviation (SD) 1.6 across modalities,\nsignificantly outperforming classical methods (ROBEX: 89.1 SD 7.7, P < 0.05;\nBET: 85.2 SD 14.4, P < 0.05). Compared to SynthStrip (96.5 SD 1.1, P=0.0352),\nMindGrab delivered equivalent or superior performance in nearly half of the\ntested scenarios, with minor differences (<3% Dice) in the others. MindGrab\nutilized 95% fewer parameters (146,237 vs. 2,566,561) than SynthStrip. This\nefficiency yielded at least 2x faster inference, 50% lower memory usage on\nGPUs, and enabled exceptional performance (e.g., 10-30x speedup, and up to 30x\nmemory reduction) and accessibility on a wider range of hardware, including\nsystems without high-end GPUs. MindGrab delivers state-of-the-art accuracy with\ndramatically lower resource demands, supported in brainchop-cli\n(https://pypi.org/project/brainchop/) and at brainchop.org."
                },
                "authors": [
                    {
                        "name": "Armina Fani"
                    },
                    {
                        "name": "Mike Doan"
                    },
                    {
                        "name": "Isabelle Le"
                    },
                    {
                        "name": "Alex Fedorov"
                    },
                    {
                        "name": "Malte Hoffmann"
                    },
                    {
                        "name": "Chris Rorden"
                    },
                    {
                        "name": "Sergey Plis"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Plis"
                },
                "arxiv_affiliation": "Tri-Institutional Center for Translational Research in Neuroimaging and Data Science",
                "author": "Sergey Plis",
                "arxiv_comment": "12 pages, 1 table, 4 figures. 2 supplementary tables, 1 supplementary\n  figure. Brainchop-cli: https://pypi.org/project/brainchop/ . Brainchop web:\n  https://brainchop.org/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11860v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11860v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09347v2",
                "updated": "2025-06-13T15:07:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    7,
                    8,
                    4,
                    164,
                    0
                ],
                "published": "2025-03-12T12:49:02Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    49,
                    2,
                    2,
                    71,
                    0
                ],
                "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments."
                },
                "authors": [
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Seraphina Goldfarb-Tarrant"
                    }
                ],
                "author_detail": {
                    "name": "Seraphina Goldfarb-Tarrant"
                },
                "author": "Seraphina Goldfarb-Tarrant",
                "arxiv_comment": "9 pages, ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11857v1",
                "updated": "2025-06-13T15:04:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    4,
                    1,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:04:01Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    4,
                    1,
                    4,
                    164,
                    0
                ],
                "title": "Post Persona Alignment for Multi-Session Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post Persona Alignment for Multi-Session Dialogue Generation"
                },
                "summary": "Multi-session persona-based dialogue generation presents challenges in\nmaintaining long-term consistency and generating diverse, personalized\nresponses. While large language models (LLMs) excel in single-session\ndialogues, they struggle to preserve persona fidelity and conversational\ncoherence across extended interactions. Existing methods typically retrieve\npersona information before response generation, which can constrain diversity\nand result in generic outputs. We propose Post Persona Alignment (PPA), a novel\ntwo-stage framework that reverses this process. PPA first generates a general\nresponse based solely on dialogue context, then retrieves relevant persona\nmemories using the response as a query, and finally refines the response to\nalign with the speaker's persona. This post-hoc alignment strategy promotes\nnaturalness and diversity while preserving consistency and personalization.\nExperiments on multi-session LLM-generated dialogue data demonstrate that PPA\nsignificantly outperforms prior approaches in consistency, diversity, and\npersona relevance, offering a more flexible and effective paradigm for\nlong-term personalized dialogue generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-session persona-based dialogue generation presents challenges in\nmaintaining long-term consistency and generating diverse, personalized\nresponses. While large language models (LLMs) excel in single-session\ndialogues, they struggle to preserve persona fidelity and conversational\ncoherence across extended interactions. Existing methods typically retrieve\npersona information before response generation, which can constrain diversity\nand result in generic outputs. We propose Post Persona Alignment (PPA), a novel\ntwo-stage framework that reverses this process. PPA first generates a general\nresponse based solely on dialogue context, then retrieves relevant persona\nmemories using the response as a query, and finally refines the response to\nalign with the speaker's persona. This post-hoc alignment strategy promotes\nnaturalness and diversity while preserving consistency and personalization.\nExperiments on multi-session LLM-generated dialogue data demonstrate that PPA\nsignificantly outperforms prior approaches in consistency, diversity, and\npersona relevance, offering a more flexible and effective paradigm for\nlong-term personalized dialogue generation."
                },
                "authors": [
                    {
                        "name": "Yi-Pei Chen"
                    },
                    {
                        "name": "Noriki Nishida"
                    },
                    {
                        "name": "Hideki Nakayama"
                    },
                    {
                        "name": "Yuji Matsumoto"
                    }
                ],
                "author_detail": {
                    "name": "Yuji Matsumoto"
                },
                "author": "Yuji Matsumoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06139v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06139v2",
                "updated": "2025-06-13T15:02:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    2,
                    29,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-06T14:56:24Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    14,
                    56,
                    24,
                    4,
                    157,
                    0
                ],
                "title": "Impact of initial mass function on the chemical evolution of\n  high-redshift galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of initial mass function on the chemical evolution of\n  high-redshift galaxies"
                },
                "summary": "Star formation and metal enrichment in galaxies are regulated by supernova\n(SN) explosions and metal yields from massive stars, which are sensitive to the\nhigh-mass end of the initial mass function (IMF). Recent JWST observations\nfound evidence for an invariant relation between stellar mass, metallicity, and\nstar formation rate up to $z\\sim 8$ and its breakdown at higher redshifts. It\nis crucial to understand the underlying physics, especially the role played by\nthe IMF. We explore the impact of IMF on the chemical evolution of\nhigh-redshift galaxies and the interplay between IMF and galactic outflow\nparameters. The ultimate goal is to constrain the high-mass end of the IMF by\nthe cosmic star formation history and stellar mass-metallicity-star formation\nrate relation (MZSFR) inferred from observations at $z\\sim 4-10$. Using the\nsemi-analytical galaxy evolution code A-SLOTH, we follow galactic baryon cycles\nalong merger trees built from a high-resolution cosmological simulation.\nStellar feedback is modeled with up-to-date stellar evolution tracks covering\nthe full metallicity range ($Z \\sim 10^{-11} - 0.03$) and a broad stellar mass\nrange ($m_\\star\\sim2 - 600\\ \\rm M_\\odot$) including the metal yields from\nstellar winds and all types of SNe. Assuming a Kroupa-like shape of the IMF\nwith a varying upper mass limit $m_{\\max}$, we find $m_{\\max} \\gtrsim 200\\ \\rm\nM_\\odot$ is required to reproduce the observed MZSFR. Observational data at\n$z\\gtrsim 6$ favor a galactic outflow model where the outflow rate is\nproportional to the supernova energy injection rate divided by the halo binding\nenergy. We conclude that very massive ($\\gtrsim 200\\ \\rm M_\\odot$) stars can\nplay important roles in the star formation and chemical enrichment histories of\nhigh-$z$ galaxies and discuss the implications of our results for reionization\nand transient sources of both electromagnetic waves and gravitational waves.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star formation and metal enrichment in galaxies are regulated by supernova\n(SN) explosions and metal yields from massive stars, which are sensitive to the\nhigh-mass end of the initial mass function (IMF). Recent JWST observations\nfound evidence for an invariant relation between stellar mass, metallicity, and\nstar formation rate up to $z\\sim 8$ and its breakdown at higher redshifts. It\nis crucial to understand the underlying physics, especially the role played by\nthe IMF. We explore the impact of IMF on the chemical evolution of\nhigh-redshift galaxies and the interplay between IMF and galactic outflow\nparameters. The ultimate goal is to constrain the high-mass end of the IMF by\nthe cosmic star formation history and stellar mass-metallicity-star formation\nrate relation (MZSFR) inferred from observations at $z\\sim 4-10$. Using the\nsemi-analytical galaxy evolution code A-SLOTH, we follow galactic baryon cycles\nalong merger trees built from a high-resolution cosmological simulation.\nStellar feedback is modeled with up-to-date stellar evolution tracks covering\nthe full metallicity range ($Z \\sim 10^{-11} - 0.03$) and a broad stellar mass\nrange ($m_\\star\\sim2 - 600\\ \\rm M_\\odot$) including the metal yields from\nstellar winds and all types of SNe. Assuming a Kroupa-like shape of the IMF\nwith a varying upper mass limit $m_{\\max}$, we find $m_{\\max} \\gtrsim 200\\ \\rm\nM_\\odot$ is required to reproduce the observed MZSFR. Observational data at\n$z\\gtrsim 6$ favor a galactic outflow model where the outflow rate is\nproportional to the supernova energy injection rate divided by the halo binding\nenergy. We conclude that very massive ($\\gtrsim 200\\ \\rm M_\\odot$) stars can\nplay important roles in the star formation and chemical enrichment histories of\nhigh-$z$ galaxies and discuss the implications of our results for reionization\nand transient sources of both electromagnetic waves and gravitational waves."
                },
                "authors": [
                    {
                        "name": "Boyuan Liu"
                    },
                    {
                        "name": "Michela Mapelli"
                    },
                    {
                        "name": "Volker Bromm"
                    },
                    {
                        "name": "Ralf S. Klessen"
                    },
                    {
                        "name": "Lumen Boco"
                    },
                    {
                        "name": "Tilman Hartwig"
                    },
                    {
                        "name": "Simon C. O. Glover"
                    },
                    {
                        "name": "Veronika Lipatova"
                    },
                    {
                        "name": "Guglielmo Costa"
                    },
                    {
                        "name": "Marco Dall'Amico"
                    },
                    {
                        "name": "Giuliano Iorio"
                    },
                    {
                        "name": "Kendall Shepherd"
                    },
                    {
                        "name": "Alessandro Bressan"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Bressan"
                },
                "author": "Alessandro Bressan",
                "arxiv_comment": "19+7 pages, 12+4 figures, submitted to A&A, comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06139v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06139v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00073v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00073v3",
                "updated": "2025-06-13T15:02:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    2,
                    2,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-29T17:41:39Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    41,
                    39,
                    3,
                    149,
                    0
                ],
                "title": "The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and\n  Transactions in Consumer Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and\n  Transactions in Consumer Markets"
                },
                "summary": "AI agents are increasingly used in consumer-facing applications to assist\nwith tasks such as product search, negotiation, and transaction execution. In\nthis paper, we explore a future scenario where both consumers and merchants\nauthorize AI agents to fully automate negotiations and transactions. We aim to\nanswer two key questions: (1) Do different LLM agents vary in their ability to\nsecure favorable deals for users? (2) What risks arise from fully automating\ndeal-making with AI agents in consumer markets? To address these questions, we\ndevelop an experimental framework that evaluates the performance of various LLM\nagents in real-world negotiation and transaction settings. Our findings reveal\nthat AI-mediated deal-making is an inherently imbalanced game -- different\nagents achieve significantly different outcomes for their users. Moreover,\nbehavioral anomalies in LLMs can result in financial losses for both consumers\nand merchants, such as overspending or accepting unreasonable deals. These\nresults underscore that while automation can improve efficiency, it also\nintroduces substantial risks. Users should exercise caution when delegating\nbusiness decisions to AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents are increasingly used in consumer-facing applications to assist\nwith tasks such as product search, negotiation, and transaction execution. In\nthis paper, we explore a future scenario where both consumers and merchants\nauthorize AI agents to fully automate negotiations and transactions. We aim to\nanswer two key questions: (1) Do different LLM agents vary in their ability to\nsecure favorable deals for users? (2) What risks arise from fully automating\ndeal-making with AI agents in consumer markets? To address these questions, we\ndevelop an experimental framework that evaluates the performance of various LLM\nagents in real-world negotiation and transaction settings. Our findings reveal\nthat AI-mediated deal-making is an inherently imbalanced game -- different\nagents achieve significantly different outcomes for their users. Moreover,\nbehavioral anomalies in LLMs can result in financial losses for both consumers\nand merchants, such as overspending or accepting unreasonable deals. These\nresults underscore that while automation can improve efficiency, it also\nintroduces substantial risks. Users should exercise caution when delegating\nbusiness decisions to AI agents."
                },
                "authors": [
                    {
                        "name": "Shenzhe Zhu"
                    },
                    {
                        "name": "Jiao Sun"
                    },
                    {
                        "name": "Yi Nian"
                    },
                    {
                        "name": "Tobin South"
                    },
                    {
                        "name": "Alex Pentland"
                    },
                    {
                        "name": "Jiaxin Pei"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxin Pei"
                },
                "author": "Jiaxin Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00073v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00073v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11845v1",
                "updated": "2025-06-13T14:54:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    54,
                    46,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T14:54:46Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    54,
                    46,
                    4,
                    164,
                    0
                ],
                "title": "Chemo-dynamics of the stellar component of the Sculptor dwarf galaxy II:\n  dynamical properties and dark matter halo density",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chemo-dynamics of the stellar component of the Sculptor dwarf galaxy II:\n  dynamical properties and dark matter halo density"
                },
                "summary": "Dwarf galaxies satellite of the Milky Way are excellent laboratories for\ntesting dark matter (DM) models and baryonic feedback implementation in\nsimulations. The Sculptor 'classical' dwarf spheroidal galaxy, a system with\ntwo distinct stellar populations and high-quality data, offers a remarkable\nopportunity to study DM distributions in these galaxies. In this work, we infer\nthe DM halo density distribution of Sculptor, applying a method based on\nspherically symmetric distribution functions depending on actions to fit the\nstellar structural and kinematic properties of Sculptor. The galaxy is\nrepresented via four components: two distinct stellar populations based on\ndistribution functions, tracers within a fixed and dominant DM potential, plus\nthe contribution of a third stellar component that accounts for possible\nsources of contamination. The model-data comparison accounts for the kinematics\nand metallicities of individual stars, allowing us to assign probabilities of\nmembership to each star. The modeling is applied on the largest available set\nof spectroscopic data, which have not been previously analyzed with this\nobjective. We find the DM distribution of Sculptor to have a logarithmic inner\nslope of 0.39+0.23-0.26 and a scale radius of 0.79+0.38-0.17 kpc at 1 sigma\nconfidence level. Our results show that Sculptor DM density profile deviates\nfrom predictions of DM-only simulations at a 3 sigma level over a large range\nof radii. Our analysis suggests that the velocity distribution of Sculptor's\ntwo main stellar components is isotropic in the center and becomes radially\nanisotropic in the outskirts. Additionally, we provide predictions for the\nprojected radial and tangential velocity dispersion profiles. We also present\nupdated DM annihilation and decay J- and D-factors, finding J = 18.15+0.11-0.12\nand D = 18.07+0.10-0.10 for an angular aperture of 0.5 degrees.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dwarf galaxies satellite of the Milky Way are excellent laboratories for\ntesting dark matter (DM) models and baryonic feedback implementation in\nsimulations. The Sculptor 'classical' dwarf spheroidal galaxy, a system with\ntwo distinct stellar populations and high-quality data, offers a remarkable\nopportunity to study DM distributions in these galaxies. In this work, we infer\nthe DM halo density distribution of Sculptor, applying a method based on\nspherically symmetric distribution functions depending on actions to fit the\nstellar structural and kinematic properties of Sculptor. The galaxy is\nrepresented via four components: two distinct stellar populations based on\ndistribution functions, tracers within a fixed and dominant DM potential, plus\nthe contribution of a third stellar component that accounts for possible\nsources of contamination. The model-data comparison accounts for the kinematics\nand metallicities of individual stars, allowing us to assign probabilities of\nmembership to each star. The modeling is applied on the largest available set\nof spectroscopic data, which have not been previously analyzed with this\nobjective. We find the DM distribution of Sculptor to have a logarithmic inner\nslope of 0.39+0.23-0.26 and a scale radius of 0.79+0.38-0.17 kpc at 1 sigma\nconfidence level. Our results show that Sculptor DM density profile deviates\nfrom predictions of DM-only simulations at a 3 sigma level over a large range\nof radii. Our analysis suggests that the velocity distribution of Sculptor's\ntwo main stellar components is isotropic in the center and becomes radially\nanisotropic in the outskirts. Additionally, we provide predictions for the\nprojected radial and tangential velocity dispersion profiles. We also present\nupdated DM annihilation and decay J- and D-factors, finding J = 18.15+0.11-0.12\nand D = 18.07+0.10-0.10 for an angular aperture of 0.5 degrees."
                },
                "authors": [
                    {
                        "name": "José María Arroyo Polonio"
                    },
                    {
                        "name": "Raffaele Pascale"
                    },
                    {
                        "name": "Giuseppina Battaglia"
                    },
                    {
                        "name": "Guillaume F. Thomas"
                    },
                    {
                        "name": "Carlo Nipoti"
                    },
                    {
                        "name": "Eugene Vasiliev"
                    },
                    {
                        "name": "Eline Tolstoy"
                    }
                ],
                "author_detail": {
                    "name": "Eline Tolstoy"
                },
                "author": "Eline Tolstoy",
                "arxiv_comment": "21 pages, 12 figures, accepted for publication in Astronomy &\n  Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19645v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19645v2",
                "updated": "2025-06-13T14:54:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    54,
                    40,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-26T08:01:45Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    1,
                    45,
                    0,
                    146,
                    0
                ],
                "title": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse\n  MoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse\n  MoE"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across many\napplications, with Mixture of Experts (MoE) models demonstrating great\npotential. Compared to traditional dense models, MoEs achieve better\nperformance with less computation. Speculative decoding (SD) is a widely used\ntechnique to accelerate LLM inference without accuracy loss, but it has been\nconsidered efficient only for dense models. In this work, we first demonstrate\nthat, under medium batch sizes, MoE surprisingly benefits more from SD than\ndense models. Furthermore, as MoE becomes sparser -- the prevailing trend in\nMoE designs -- the batch size range where SD acceleration is expected to be\neffective becomes broader. To quantitatively understand tradeoffs involved in\nSD, we develop a reliable modeling based on theoretical analyses. While current\nSD research primarily focuses on improving acceptance rates of algorithms,\nchanges in workload and model architecture can still lead to degraded SD\nacceleration even with high acceptance rates. To address this limitation, we\nintroduce a new metric 'target efficiency' that characterizes these effects,\nthus helping researchers identify system bottlenecks and understand SD\nacceleration more comprehensively. For scenarios like private serving, this\nwork unveils a new perspective to speed up MoE inference, where existing\nsolutions struggle. Experiments on different GPUs show up to 2.29x speedup for\nQwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across many\napplications, with Mixture of Experts (MoE) models demonstrating great\npotential. Compared to traditional dense models, MoEs achieve better\nperformance with less computation. Speculative decoding (SD) is a widely used\ntechnique to accelerate LLM inference without accuracy loss, but it has been\nconsidered efficient only for dense models. In this work, we first demonstrate\nthat, under medium batch sizes, MoE surprisingly benefits more from SD than\ndense models. Furthermore, as MoE becomes sparser -- the prevailing trend in\nMoE designs -- the batch size range where SD acceleration is expected to be\neffective becomes broader. To quantitatively understand tradeoffs involved in\nSD, we develop a reliable modeling based on theoretical analyses. While current\nSD research primarily focuses on improving acceptance rates of algorithms,\nchanges in workload and model architecture can still lead to degraded SD\nacceleration even with high acceptance rates. To address this limitation, we\nintroduce a new metric 'target efficiency' that characterizes these effects,\nthus helping researchers identify system bottlenecks and understand SD\nacceleration more comprehensively. For scenarios like private serving, this\nwork unveils a new perspective to speed up MoE inference, where existing\nsolutions struggle. Experiments on different GPUs show up to 2.29x speedup for\nQwen2-57B-A14B at medium batch sizes and validate our theoretical predictions."
                },
                "authors": [
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Lei Zhu"
                    },
                    {
                        "name": "Zongyuan Zhan"
                    },
                    {
                        "name": "Ting Hu"
                    },
                    {
                        "name": "Weikai Mao"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Yongpan Liu"
                    },
                    {
                        "name": "Tianyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Zhang"
                },
                "author": "Tianyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19645v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19645v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11844v1",
                "updated": "2025-06-13T14:48:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    48,
                    1,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T14:48:01Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    48,
                    1,
                    4,
                    164,
                    0
                ],
                "title": "TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text,\n  and Structure Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text,\n  and Structure Attacks"
                },
                "summary": "Inspired by the success of large language models (LLMs), there is a\nsignificant research shift from traditional graph learning methods to LLM-based\ngraph frameworks, formally known as GraphLLMs. GraphLLMs leverage the reasoning\npower of LLMs by integrating three key components: the textual attributes of\ninput nodes, the structural information of node neighborhoods, and\ntask-specific prompts that guide decision-making. Despite their promise, the\nrobustness of GraphLLMs against adversarial perturbations remains largely\nunexplored-a critical concern for deploying these models in high-stakes\nscenarios. To bridge the gap, we introduce TrustGLM, a comprehensive study\nevaluating the vulnerability of GraphLLMs to adversarial attacks across three\ndimensions: text, graph structure, and prompt manipulations. We implement\nstate-of-the-art attack algorithms from each perspective to rigorously assess\nmodel resilience. Through extensive experiments on six benchmark datasets from\ndiverse domains, our findings reveal that GraphLLMs are highly susceptible to\ntext attacks that merely replace a few semantically similar words in a node's\ntextual attribute. We also find that standard graph structure attack methods\ncan significantly degrade model performance, while random shuffling of the\ncandidate label set in prompt templates leads to substantial performance drops.\nBeyond characterizing these vulnerabilities, we investigate defense techniques\ntailored to each attack vector through data-augmented training and adversarial\ntraining, which show promising potential to enhance the robustness of\nGraphLLMs. We hope that our open-sourced library will facilitate rapid,\nequitable evaluation and inspire further innovative research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by the success of large language models (LLMs), there is a\nsignificant research shift from traditional graph learning methods to LLM-based\ngraph frameworks, formally known as GraphLLMs. GraphLLMs leverage the reasoning\npower of LLMs by integrating three key components: the textual attributes of\ninput nodes, the structural information of node neighborhoods, and\ntask-specific prompts that guide decision-making. Despite their promise, the\nrobustness of GraphLLMs against adversarial perturbations remains largely\nunexplored-a critical concern for deploying these models in high-stakes\nscenarios. To bridge the gap, we introduce TrustGLM, a comprehensive study\nevaluating the vulnerability of GraphLLMs to adversarial attacks across three\ndimensions: text, graph structure, and prompt manipulations. We implement\nstate-of-the-art attack algorithms from each perspective to rigorously assess\nmodel resilience. Through extensive experiments on six benchmark datasets from\ndiverse domains, our findings reveal that GraphLLMs are highly susceptible to\ntext attacks that merely replace a few semantically similar words in a node's\ntextual attribute. We also find that standard graph structure attack methods\ncan significantly degrade model performance, while random shuffling of the\ncandidate label set in prompt templates leads to substantial performance drops.\nBeyond characterizing these vulnerabilities, we investigate defense techniques\ntailored to each attack vector through data-augmented training and adversarial\ntraining, which show promising potential to enhance the robustness of\nGraphLLMs. We hope that our open-sourced library will facilitate rapid,\nequitable evaluation and inspire further innovative research in this field."
                },
                "authors": [
                    {
                        "name": "Qihai Zhang"
                    },
                    {
                        "name": "Xinyue Sheng"
                    },
                    {
                        "name": "Yuanfu Sun"
                    },
                    {
                        "name": "Qiaoyu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Qiaoyu Tan"
                },
                "author": "Qiaoyu Tan",
                "arxiv_comment": "12 pages, 5 figures, in KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11842v1",
                "updated": "2025-06-13T14:44:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    44,
                    11,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T14:44:11Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    44,
                    11,
                    4,
                    164,
                    0
                ],
                "title": "Your Ride, Your Rules: Psychology and Cognition Enabled Automated\n  Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Ride, Your Rules: Psychology and Cognition Enabled Automated\n  Driving Systems"
                },
                "summary": "Despite rapid advances in autonomous driving, current autonomous vehicles\n(AVs) lack effective bidirectional communication with occupants, limiting\npersonalization and recovery from immobilization. This reduces comfort and\ntrust, potentially slowing broader AV adoption. We propose PACE-ADS (Psychology\nand Cognition Enabled Automated Driving Systems), a human-centered autonomy\nframework that enables AVs to sense, interpret, and respond to both external\ntraffic and internal occupant states. PACE-ADS comprises three foundation\nmodel-based agents: a Driver Agent that analyzes the driving context, a\nPsychologist Agent that interprets occupant psychological signals (e.g., EEG,\nheart rate, facial expressions) and cognitive commands (e.g., speech), and a\nCoordinator Agent that integrates these inputs to produce high-level behavior\ndecisions and operational parameters. Rather than replacing existing AV\nmodules, PACE-ADS complements them by operating at the behavioral level,\ndelegating low-level control to native AV systems. This separation enables\nclosed-loop adaptation and supports integration across diverse platforms. We\nevaluate PACE-ADS in simulation across varied scenarios involving traffic\nlights, pedestrians, work zones, and car following. Results show that PACE-ADS\nadapts driving styles to occupant states, improves ride comfort, and enables\nsafe recovery from immobilization via autonomous reasoning or human guidance.\nOur findings highlight the promise of LLM-based frameworks for bridging the gap\nbetween machine autonomy and human-centered driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid advances in autonomous driving, current autonomous vehicles\n(AVs) lack effective bidirectional communication with occupants, limiting\npersonalization and recovery from immobilization. This reduces comfort and\ntrust, potentially slowing broader AV adoption. We propose PACE-ADS (Psychology\nand Cognition Enabled Automated Driving Systems), a human-centered autonomy\nframework that enables AVs to sense, interpret, and respond to both external\ntraffic and internal occupant states. PACE-ADS comprises three foundation\nmodel-based agents: a Driver Agent that analyzes the driving context, a\nPsychologist Agent that interprets occupant psychological signals (e.g., EEG,\nheart rate, facial expressions) and cognitive commands (e.g., speech), and a\nCoordinator Agent that integrates these inputs to produce high-level behavior\ndecisions and operational parameters. Rather than replacing existing AV\nmodules, PACE-ADS complements them by operating at the behavioral level,\ndelegating low-level control to native AV systems. This separation enables\nclosed-loop adaptation and supports integration across diverse platforms. We\nevaluate PACE-ADS in simulation across varied scenarios involving traffic\nlights, pedestrians, work zones, and car following. Results show that PACE-ADS\nadapts driving styles to occupant states, improves ride comfort, and enables\nsafe recovery from immobilization via autonomous reasoning or human guidance.\nOur findings highlight the promise of LLM-based frameworks for bridging the gap\nbetween machine autonomy and human-centered driving."
                },
                "authors": [
                    {
                        "name": "Zhipeng Bao"
                    },
                    {
                        "name": "Qianwen Li"
                    }
                ],
                "author_detail": {
                    "name": "Qianwen Li"
                },
                "author": "Qianwen Li",
                "arxiv_comment": "10 figures,29 pages, one colummn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11825v1",
                "updated": "2025-06-13T14:30:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    30,
                    37,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T14:30:37Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    30,
                    37,
                    4,
                    164,
                    0
                ],
                "title": "Revealing Political Bias in LLMs through Structured Multi-Agent Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Political Bias in LLMs through Structured Multi-Agent Debate"
                },
                "summary": "Large language models (LLMs) are increasingly used to simulate social\nbehaviour, yet their political biases and interaction dynamics in debates\nremain underexplored. We investigate how LLM type and agent gender attributes\ninfluence political bias using a structured multi-agent debate framework, by\nengaging Neutral, Republican, and Democrat American LLM agents in debates on\npolitically sensitive topics. We systematically vary the underlying LLMs, agent\ngenders, and debate formats to examine how model provenance and agent personas\ninfluence political bias and attitudes throughout debates. We find that Neutral\nagents consistently align with Democrats, while Republicans shift closer to the\nNeutral; gender influences agent attitudes, with agents adapting their opinions\nwhen aware of other agents' genders; and contrary to prior research, agents\nwith shared political affiliations can form echo chambers, exhibiting the\nexpected intensification of attitudes as debates progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used to simulate social\nbehaviour, yet their political biases and interaction dynamics in debates\nremain underexplored. We investigate how LLM type and agent gender attributes\ninfluence political bias using a structured multi-agent debate framework, by\nengaging Neutral, Republican, and Democrat American LLM agents in debates on\npolitically sensitive topics. We systematically vary the underlying LLMs, agent\ngenders, and debate formats to examine how model provenance and agent personas\ninfluence political bias and attitudes throughout debates. We find that Neutral\nagents consistently align with Democrats, while Republicans shift closer to the\nNeutral; gender influences agent attitudes, with agents adapting their opinions\nwhen aware of other agents' genders; and contrary to prior research, agents\nwith shared political affiliations can form echo chambers, exhibiting the\nexpected intensification of attitudes as debates progress."
                },
                "authors": [
                    {
                        "name": "Aishwarya Bandaru"
                    },
                    {
                        "name": "Fabian Bindley"
                    },
                    {
                        "name": "Trevor Bluth"
                    },
                    {
                        "name": "Nandini Chavda"
                    },
                    {
                        "name": "Baixu Chen"
                    },
                    {
                        "name": "Ethan Law"
                    }
                ],
                "author_detail": {
                    "name": "Ethan Law"
                },
                "author": "Ethan Law",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00856v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00856v2",
                "updated": "2025-06-13T14:28:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    28,
                    21,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-01T06:34:42Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    6,
                    34,
                    42,
                    6,
                    152,
                    0
                ],
                "title": "Can AI Master Econometrics? Evidence from Econometrics AI Agent on\n  Expert-Level Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI Master Econometrics? Evidence from Econometrics AI Agent on\n  Expert-Level Tasks"
                },
                "summary": "Can AI effectively perform complex econometric analysis traditionally\nrequiring human expertise? This paper evaluates AI agents' capability to master\neconometrics, focusing on empirical analysis performance. We develop an\n``Econometrics AI Agent'' built on the open-source MetaGPT framework. This\nagent exhibits outstanding performance in: (1) planning econometric tasks\nstrategically, (2) generating and executing code, (3) employing error-based\nreflection for improved robustness, and (4) allowing iterative refinement\nthrough multi-round conversations. We construct two datasets from academic\ncoursework materials and published research papers to evaluate performance\nagainst real-world challenges. Comparative testing shows our domain-specialized\nAI agent significantly outperforms both benchmark large language models (LLMs)\nand general-purpose AI agents. This work establishes a testbed for exploring\nAI's impact on social science research and enables cost-effective integration\nof domain expertise, making advanced econometric methods accessible to users\nwith minimal coding skills. Furthermore, our AI agent enhances research\nreproducibility and offers promising pedagogical applications for econometrics\nteaching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI effectively perform complex econometric analysis traditionally\nrequiring human expertise? This paper evaluates AI agents' capability to master\neconometrics, focusing on empirical analysis performance. We develop an\n``Econometrics AI Agent'' built on the open-source MetaGPT framework. This\nagent exhibits outstanding performance in: (1) planning econometric tasks\nstrategically, (2) generating and executing code, (3) employing error-based\nreflection for improved robustness, and (4) allowing iterative refinement\nthrough multi-round conversations. We construct two datasets from academic\ncoursework materials and published research papers to evaluate performance\nagainst real-world challenges. Comparative testing shows our domain-specialized\nAI agent significantly outperforms both benchmark large language models (LLMs)\nand general-purpose AI agents. This work establishes a testbed for exploring\nAI's impact on social science research and enables cost-effective integration\nof domain expertise, making advanced econometric methods accessible to users\nwith minimal coding skills. Furthermore, our AI agent enhances research\nreproducibility and offers promising pedagogical applications for econometrics\nteaching."
                },
                "authors": [
                    {
                        "name": "Qiang Chen"
                    },
                    {
                        "name": "Tianyang Han"
                    },
                    {
                        "name": "Jin Li"
                    },
                    {
                        "name": "Ye Luo"
                    },
                    {
                        "name": "Yuxiao Wu"
                    },
                    {
                        "name": "Xiaowei Zhang"
                    },
                    {
                        "name": "Tuo Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhou"
                },
                "author": "Tuo Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00856v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00856v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07450v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07450v4",
                "updated": "2025-06-13T14:27:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    27,
                    46,
                    4,
                    164,
                    0
                ],
                "published": "2025-03-10T15:30:05Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    30,
                    5,
                    0,
                    69,
                    0
                ],
                "title": "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper"
                },
                "summary": "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted."
                },
                "authors": [
                    {
                        "name": "Sargam Yadav"
                    },
                    {
                        "name": "Asifa Mehmood Qureshi"
                    },
                    {
                        "name": "Abhishek Kaushik"
                    },
                    {
                        "name": "Shubham Sharma"
                    },
                    {
                        "name": "Roisin Loughran"
                    },
                    {
                        "name": "Subramaniam Kazhuparambil"
                    },
                    {
                        "name": "Andrew Shaw"
                    },
                    {
                        "name": "Mohammed Sabry"
                    },
                    {
                        "name": "Niamh St John Lynch"
                    },
                    {
                        "name": ". Nikhil Singh"
                    },
                    {
                        "name": "Padraic O'Hara"
                    },
                    {
                        "name": "Pranay Jaiswal"
                    },
                    {
                        "name": "Roshan Chandru"
                    },
                    {
                        "name": "David Lillis"
                    }
                ],
                "author_detail": {
                    "name": "David Lillis"
                },
                "arxiv_affiliation": "School of Computer Science, University College Dublin",
                "author": "David Lillis",
                "arxiv_comment": "The project is partially supported by the DkIT Postgraduate\n  Scholarship, Research Ireland under Grant number 13/RC/2094_2, and Grant\n  number 21/FFP-A/925",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07450v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07450v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19164v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19164v2",
                "updated": "2025-06-13T14:26:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    26,
                    31,
                    4,
                    164,
                    0
                ],
                "published": "2024-05-29T15:08:55Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    15,
                    8,
                    55,
                    2,
                    150,
                    0
                ],
                "title": "Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in\n  eDiscovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in\n  eDiscovery"
                },
                "summary": "Electronic Discovery (eDiscovery) requires identifying relevant documents\nfrom vast collections for legal production requests. While artificial\nintelligence (AI) and natural language processing (NLP) have improved document\nreview efficiency, current methods still struggle with legal entities,\ncitations, and complex legal artifacts. To address these challenges, we\nintroduce DISCOvery Graph (DISCOG), an emerging system that integrates\nknowledge graphs for enhanced document ranking and classification, augmented by\nLLM-driven reasoning. DISCOG outperforms strong baselines in F1-score,\nprecision, and recall across both balanced and imbalanced datasets. In\nreal-world deployments, it has reduced litigation-related document review costs\nby approximately 98\\%, demonstrating significant business impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Discovery (eDiscovery) requires identifying relevant documents\nfrom vast collections for legal production requests. While artificial\nintelligence (AI) and natural language processing (NLP) have improved document\nreview efficiency, current methods still struggle with legal entities,\ncitations, and complex legal artifacts. To address these challenges, we\nintroduce DISCOvery Graph (DISCOG), an emerging system that integrates\nknowledge graphs for enhanced document ranking and classification, augmented by\nLLM-driven reasoning. DISCOG outperforms strong baselines in F1-score,\nprecision, and recall across both balanced and imbalanced datasets. In\nreal-world deployments, it has reduced litigation-related document review costs\nby approximately 98\\%, demonstrating significant business impact."
                },
                "authors": [
                    {
                        "name": "Sounak Lahiri"
                    },
                    {
                        "name": "Sumit Pai"
                    },
                    {
                        "name": "Tim Weninger"
                    },
                    {
                        "name": "Sanmitra Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Sanmitra Bhattacharya"
                },
                "author": "Sanmitra Bhattacharya",
                "arxiv_comment": "Updated with Camera Ready Copy for ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19164v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19164v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11820v1",
                "updated": "2025-06-13T14:23:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    23,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T14:23:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    23,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "Rethinking Multilingual Vision-Language Translation: Dataset,\n  Evaluation, and Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Multilingual Vision-Language Translation: Dataset,\n  Evaluation, and Adaptation"
                },
                "summary": "Vision-Language Translation (VLT) is a challenging task that requires\naccurately recognizing multilingual text embedded in images and translating it\ninto the target language with the support of visual context. While recent Large\nVision-Language Models (LVLMs) have demonstrated strong multilingual and visual\nunderstanding capabilities, there is a lack of systematic evaluation and\nunderstanding of their performance on VLT. In this work, we present a\ncomprehensive study of VLT from three key perspectives: data quality, model\narchitecture, and evaluation metrics. (1) We identify critical limitations in\nexisting datasets, particularly in semantic and cultural fidelity, and\nintroduce AibTrans -- a multilingual, parallel, human-verified dataset with\nOCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6\nstate-of-the-art open-source models across end-to-end and cascaded\narchitectures, revealing their OCR dependency and contrasting generation versus\nreasoning behaviors. (3) We propose Density-Aware Evaluation to address metric\nreliability issues under varying contextual complexity, introducing the DA\nScore as a more robust measure of translation quality. Building upon these\nfindings, we establish a new evaluation benchmark for VLT. Notably, we observe\nthat fine-tuning LVLMs on high-resource language pairs degrades cross-lingual\nperformance, and we propose a balanced multilingual fine-tuning strategy that\neffectively adapts LVLMs to VLT without sacrificing their generalization\nability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Translation (VLT) is a challenging task that requires\naccurately recognizing multilingual text embedded in images and translating it\ninto the target language with the support of visual context. While recent Large\nVision-Language Models (LVLMs) have demonstrated strong multilingual and visual\nunderstanding capabilities, there is a lack of systematic evaluation and\nunderstanding of their performance on VLT. In this work, we present a\ncomprehensive study of VLT from three key perspectives: data quality, model\narchitecture, and evaluation metrics. (1) We identify critical limitations in\nexisting datasets, particularly in semantic and cultural fidelity, and\nintroduce AibTrans -- a multilingual, parallel, human-verified dataset with\nOCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6\nstate-of-the-art open-source models across end-to-end and cascaded\narchitectures, revealing their OCR dependency and contrasting generation versus\nreasoning behaviors. (3) We propose Density-Aware Evaluation to address metric\nreliability issues under varying contextual complexity, introducing the DA\nScore as a more robust measure of translation quality. Building upon these\nfindings, we establish a new evaluation benchmark for VLT. Notably, we observe\nthat fine-tuning LVLMs on high-resource language pairs degrades cross-lingual\nperformance, and we propose a balanced multilingual fine-tuning strategy that\neffectively adapts LVLMs to VLT without sacrificing their generalization\nability."
                },
                "authors": [
                    {
                        "name": "Xintong Wang"
                    },
                    {
                        "name": "Jingheng Pan"
                    },
                    {
                        "name": "Yixiao Liu"
                    },
                    {
                        "name": "Xiaohu Zhao"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Chris Biemann"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Linlong Xu"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifu Zhang"
                },
                "author": "Kaifu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11812v1",
                "updated": "2025-06-13T14:14:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    14,
                    40,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T14:14:40Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    14,
                    40,
                    4,
                    164,
                    0
                ],
                "title": "On the Performance of LLMs for Real Estate Appraisal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Performance of LLMs for Real Estate Appraisal"
                },
                "summary": "The real estate market is vital to global economies but suffers from\nsignificant information asymmetry. This study examines how Large Language\nModels (LLMs) can democratize access to real estate insights by generating\ncompetitive and interpretable house price estimates through optimized\nIn-Context Learning (ICL) strategies. We systematically evaluate leading LLMs\non diverse international housing datasets, comparing zero-shot, few-shot,\nmarket report-enhanced, and hybrid prompting techniques. Our results show that\nLLMs effectively leverage hedonic variables, such as property size and\namenities, to produce meaningful estimates. While traditional machine learning\nmodels remain strong for pure predictive accuracy, LLMs offer a more\naccessible, interactive and interpretable alternative. Although\nself-explanations require cautious interpretation, we find that LLMs explain\ntheir predictions in agreement with state-of-the-art models, confirming their\ntrustworthiness. Carefully selected in-context examples based on feature\nsimilarity and geographic proximity, significantly enhance LLM performance, yet\nLLMs struggle with overconfidence in price intervals and limited spatial\nreasoning. We offer practical guidance for structured prediction tasks through\nprompt optimization. Our findings highlight LLMs' potential to improve\ntransparency in real estate appraisal and provide actionable insights for\nstakeholders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The real estate market is vital to global economies but suffers from\nsignificant information asymmetry. This study examines how Large Language\nModels (LLMs) can democratize access to real estate insights by generating\ncompetitive and interpretable house price estimates through optimized\nIn-Context Learning (ICL) strategies. We systematically evaluate leading LLMs\non diverse international housing datasets, comparing zero-shot, few-shot,\nmarket report-enhanced, and hybrid prompting techniques. Our results show that\nLLMs effectively leverage hedonic variables, such as property size and\namenities, to produce meaningful estimates. While traditional machine learning\nmodels remain strong for pure predictive accuracy, LLMs offer a more\naccessible, interactive and interpretable alternative. Although\nself-explanations require cautious interpretation, we find that LLMs explain\ntheir predictions in agreement with state-of-the-art models, confirming their\ntrustworthiness. Carefully selected in-context examples based on feature\nsimilarity and geographic proximity, significantly enhance LLM performance, yet\nLLMs struggle with overconfidence in price intervals and limited spatial\nreasoning. We offer practical guidance for structured prediction tasks through\nprompt optimization. Our findings highlight LLMs' potential to improve\ntransparency in real estate appraisal and provide actionable insights for\nstakeholders."
                },
                "authors": [
                    {
                        "name": "Margot Geerts"
                    },
                    {
                        "name": "Manon Reusens"
                    },
                    {
                        "name": "Bart Baesens"
                    },
                    {
                        "name": "Seppe vanden Broucke"
                    },
                    {
                        "name": "Jochen De Weerdt"
                    }
                ],
                "author_detail": {
                    "name": "Jochen De Weerdt"
                },
                "author": "Jochen De Weerdt",
                "arxiv_comment": "Accepted at ECML-PKDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11804v1",
                "updated": "2025-06-13T14:07:00Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    7,
                    0,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T14:07:00Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    7,
                    0,
                    4,
                    164,
                    0
                ],
                "title": "Teleoperated Driving: a New Challenge for 3D Object Detection in\n  Compressed Point Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teleoperated Driving: a New Challenge for 3D Object Detection in\n  Compressed Point Clouds"
                },
                "summary": "In recent years, the development of interconnected devices has expanded in\nmany fields, from infotainment to education and industrial applications. This\ntrend has been accelerated by the increased number of sensors and accessibility\nto powerful hardware and software. One area that significantly benefits from\nthese advancements is Teleoperated Driving (TD). In this scenario, a controller\ndrives safely a vehicle from remote leveraging sensors data generated onboard\nthe vehicle, and exchanged via Vehicle-to-Everything (V2X) communications. In\nthis work, we tackle the problem of detecting the presence of cars and\npedestrians from point cloud data to enable safe TD operations. More\nspecifically, we exploit the SELMA dataset, a multimodal, open-source,\nsynthetic dataset for autonomous driving, that we expanded by including the\nground-truth bounding boxes of 3D objects to support object detection. We\nanalyze the performance of state-of-the-art compression algorithms and object\ndetectors under several metrics, including compression efficiency,\n(de)compression and inference time, and detection accuracy. Moreover, we\nmeasure the impact of compression and detection on the V2X network in terms of\ndata rate and latency with respect to 3GPP requirements for TD applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the development of interconnected devices has expanded in\nmany fields, from infotainment to education and industrial applications. This\ntrend has been accelerated by the increased number of sensors and accessibility\nto powerful hardware and software. One area that significantly benefits from\nthese advancements is Teleoperated Driving (TD). In this scenario, a controller\ndrives safely a vehicle from remote leveraging sensors data generated onboard\nthe vehicle, and exchanged via Vehicle-to-Everything (V2X) communications. In\nthis work, we tackle the problem of detecting the presence of cars and\npedestrians from point cloud data to enable safe TD operations. More\nspecifically, we exploit the SELMA dataset, a multimodal, open-source,\nsynthetic dataset for autonomous driving, that we expanded by including the\nground-truth bounding boxes of 3D objects to support object detection. We\nanalyze the performance of state-of-the-art compression algorithms and object\ndetectors under several metrics, including compression efficiency,\n(de)compression and inference time, and detection accuracy. Moreover, we\nmeasure the impact of compression and detection on the V2X network in terms of\ndata rate and latency with respect to 3GPP requirements for TD applications."
                },
                "authors": [
                    {
                        "name": "Filippo Bragato"
                    },
                    {
                        "name": "Michael Neri"
                    },
                    {
                        "name": "Paolo Testolina"
                    },
                    {
                        "name": "Marco Giordani"
                    },
                    {
                        "name": "Federica Battisti"
                    }
                ],
                "author_detail": {
                    "name": "Federica Battisti"
                },
                "author": "Federica Battisti",
                "arxiv_comment": "Submitted to IEEE Transactions on Intelligent Transportation Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11798v1",
                "updated": "2025-06-13T14:02:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    2,
                    21,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T14:02:21Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    2,
                    21,
                    4,
                    164,
                    0
                ],
                "title": "Persona-driven Simulation of Voting Behavior in the European Parliament\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona-driven Simulation of Voting Behavior in the European Parliament\n  with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation."
                },
                "authors": [
                    {
                        "name": "Maximilian Kreutner"
                    },
                    {
                        "name": "Marlene Lutz"
                    },
                    {
                        "name": "Markus Strohmaier"
                    }
                ],
                "author_detail": {
                    "name": "Markus Strohmaier"
                },
                "author": "Markus Strohmaier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03664v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03664v2",
                "updated": "2025-06-13T13:54:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    54,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-03-15T08:48:38Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    8,
                    48,
                    38,
                    5,
                    74,
                    0
                ],
                "title": "PIPO: Pipelined Offloading for Efficient Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIPO: Pipelined Offloading for Efficient Inference on Consumer Devices"
                },
                "summary": "The high memory and computation demand of large language models (LLMs) makes\nthem challenging to be deployed on consumer devices due to limited GPU memory.\nOffloading can mitigate the memory constraint but often suffers from low GPU\nutilization, leading to low inference efficiency. In this work, we propose a\nnovel framework, called pipelined offloading (PIPO), for efficient inference on\nconsumer devices. PIPO designs a fine-grained offloading pipeline, complemented\nwith optimized data transfer and computation, to achieve high concurrency and\nefficient scheduling for inference. Experimental results show that compared\nwith state-of-the-art baseline, PIPO increases GPU utilization from below 40%\nto over 90% and achieves up to 3.1$\\times$ higher throughput, running on a\nlaptop equipped with a RTX3060 GPU of 6GB memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high memory and computation demand of large language models (LLMs) makes\nthem challenging to be deployed on consumer devices due to limited GPU memory.\nOffloading can mitigate the memory constraint but often suffers from low GPU\nutilization, leading to low inference efficiency. In this work, we propose a\nnovel framework, called pipelined offloading (PIPO), for efficient inference on\nconsumer devices. PIPO designs a fine-grained offloading pipeline, complemented\nwith optimized data transfer and computation, to achieve high concurrency and\nefficient scheduling for inference. Experimental results show that compared\nwith state-of-the-art baseline, PIPO increases GPU utilization from below 40%\nto over 90% and achieves up to 3.1$\\times$ higher throughput, running on a\nlaptop equipped with a RTX3060 GPU of 6GB memory."
                },
                "authors": [
                    {
                        "name": "Yangyijian Liu"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Wu-Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Wu-Jun Li"
                },
                "author": "Wu-Jun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03664v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03664v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11791v1",
                "updated": "2025-06-13T13:54:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    54,
                    30,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:54:30Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    54,
                    30,
                    4,
                    164,
                    0
                ],
                "title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software\n  Security Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software\n  Security Tasks"
                },
                "summary": "Rigorous security-focused evaluation of large language model (LLM) agents is\nimperative for establishing trust in their safe deployment throughout the\nsoftware development lifecycle. However, existing benchmarks largely rely on\nsynthetic challenges or simplified vulnerability datasets that fail to capture\nthe complexity and ambiguity encountered by security engineers in practice. We\nintroduce SEC-bench, the first fully automated benchmarking framework for\nevaluating LLM agents on authentic security engineering tasks. SEC-bench\nemploys a novel multi-agent scaffold that automatically constructs code\nrepositories with harnesses, reproduces vulnerabilities in isolated\nenvironments, and generates gold patches for reliable evaluation. Our framework\nautomatically creates high-quality software vulnerability datasets with\nreproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench,\nwe implement two critical software security tasks to rigorously evaluate LLM\nagents' capabilities: proof-of-concept (PoC) generation and vulnerability\npatching. A comprehensive evaluation of state-of-the-art LLM code agents\nreveals significant performance gaps, achieving at most 18.0% success in PoC\ngeneration and 34.0% in vulnerability patching on our complete dataset. These\nresults highlight the crucial steps needed toward developing LLM agents that\nare more practical, intelligent, and autonomous for security engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous security-focused evaluation of large language model (LLM) agents is\nimperative for establishing trust in their safe deployment throughout the\nsoftware development lifecycle. However, existing benchmarks largely rely on\nsynthetic challenges or simplified vulnerability datasets that fail to capture\nthe complexity and ambiguity encountered by security engineers in practice. We\nintroduce SEC-bench, the first fully automated benchmarking framework for\nevaluating LLM agents on authentic security engineering tasks. SEC-bench\nemploys a novel multi-agent scaffold that automatically constructs code\nrepositories with harnesses, reproduces vulnerabilities in isolated\nenvironments, and generates gold patches for reliable evaluation. Our framework\nautomatically creates high-quality software vulnerability datasets with\nreproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench,\nwe implement two critical software security tasks to rigorously evaluate LLM\nagents' capabilities: proof-of-concept (PoC) generation and vulnerability\npatching. A comprehensive evaluation of state-of-the-art LLM code agents\nreveals significant performance gaps, achieving at most 18.0% success in PoC\ngeneration and 34.0% in vulnerability patching on our complete dataset. These\nresults highlight the crucial steps needed toward developing LLM agents that\nare more practical, intelligent, and autonomous for security engineering."
                },
                "authors": [
                    {
                        "name": "Hwiwon Lee"
                    },
                    {
                        "name": "Ziqi Zhang"
                    },
                    {
                        "name": "Hanxiao Lu"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11789v1",
                "updated": "2025-06-13T13:51:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    51,
                    34,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:51:34Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    51,
                    34,
                    4,
                    164,
                    0
                ],
                "title": "Conversational AI as a Catalyst for Informal Learning: An Empirical\n  Large-Scale Study on LLM Use in Everyday Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational AI as a Catalyst for Informal Learning: An Empirical\n  Large-Scale Study on LLM Use in Everyday Learning"
                },
                "summary": "Large language models have not only captivated the public imagination but\nhave also sparked a profound rethinking of how we learn. In the third year\nfollowing the breakthrough launch of ChatGPT, everyday informal learning has\nbeen transformed as diverse user groups explore these novel tools. Who is\nembracing LLMs for self-directed learning, and who remains hesitant? What are\ntheir reasons for adoption or avoidance? What learning patterns emerge with\nthis novel technological landscape? We present an in-depth analysis from a\nlarge-scale survey of 776 participants, showcasing that 88% of our respondents\nalready incorporate LLMs into their everyday learning routines for a wide\nvariety of (learning) tasks. Young adults are at the forefront of adopting\nLLMs, primarily to enhance their learning experiences independently of time and\nspace. Four types of learners emerge across learning contexts, depending on the\ntasks they perform with LLMs and the devices they use to access them.\nInterestingly, our respondents exhibit paradoxical behaviours regarding their\ntrust in LLMs' accuracy and privacy protection measures. Our implications\nemphasize the importance of including different media types for learning,\nenabling collaborative learning, providing sources and meeting the needs of\ndifferent types of learners and learning by design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have not only captivated the public imagination but\nhave also sparked a profound rethinking of how we learn. In the third year\nfollowing the breakthrough launch of ChatGPT, everyday informal learning has\nbeen transformed as diverse user groups explore these novel tools. Who is\nembracing LLMs for self-directed learning, and who remains hesitant? What are\ntheir reasons for adoption or avoidance? What learning patterns emerge with\nthis novel technological landscape? We present an in-depth analysis from a\nlarge-scale survey of 776 participants, showcasing that 88% of our respondents\nalready incorporate LLMs into their everyday learning routines for a wide\nvariety of (learning) tasks. Young adults are at the forefront of adopting\nLLMs, primarily to enhance their learning experiences independently of time and\nspace. Four types of learners emerge across learning contexts, depending on the\ntasks they perform with LLMs and the devices they use to access them.\nInterestingly, our respondents exhibit paradoxical behaviours regarding their\ntrust in LLMs' accuracy and privacy protection measures. Our implications\nemphasize the importance of including different media types for learning,\nenabling collaborative learning, providing sources and meeting the needs of\ndifferent types of learners and learning by design."
                },
                "authors": [
                    {
                        "name": "Nađa Terzimehić"
                    },
                    {
                        "name": "Babette Bühler"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19175v2",
                "updated": "2025-06-13T13:50:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    50,
                    34,
                    4,
                    164,
                    0
                ],
                "published": "2025-02-26T14:31:43Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    31,
                    43,
                    2,
                    57,
                    0
                ],
                "title": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic\n  Differential Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic\n  Differential Diagnosis"
                },
                "summary": "Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical\ndecision-making, in which physicians iteratively refine a ranked list of\npossible diseases based on symptoms, antecedents, and medical knowledge. While\nrecent advances in large language models (LLMs) have shown promise in\nsupporting DDx, existing approaches face key limitations, including\nsingle-dataset evaluations, isolated optimization of components, unrealistic\nassumptions about complete patient profiles, and single-attempt diagnosis. We\nintroduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for\ninteractive DDx, where diagnostic reasoning evolves through iterative learning,\nrather than assuming a complete patient profile is accessible. MEDDxAgent\nintegrates three modular components: (1) an orchestrator (DDxDriver), (2) a\nhistory taking simulator, and (3) two specialized agents for knowledge\nretrieval and diagnosis strategy. To ensure robust evaluation, we introduce a\ncomprehensive DDx benchmark covering respiratory, skin, and rare diseases. We\nanalyze single-turn diagnostic approaches and demonstrate the importance of\niterative refinement when patient profiles are not available at the outset. Our\nbroad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy\nimprovements in interactive DDx across both large and small LLMs, while\noffering critical explainability into its diagnostic reasoning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical\ndecision-making, in which physicians iteratively refine a ranked list of\npossible diseases based on symptoms, antecedents, and medical knowledge. While\nrecent advances in large language models (LLMs) have shown promise in\nsupporting DDx, existing approaches face key limitations, including\nsingle-dataset evaluations, isolated optimization of components, unrealistic\nassumptions about complete patient profiles, and single-attempt diagnosis. We\nintroduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for\ninteractive DDx, where diagnostic reasoning evolves through iterative learning,\nrather than assuming a complete patient profile is accessible. MEDDxAgent\nintegrates three modular components: (1) an orchestrator (DDxDriver), (2) a\nhistory taking simulator, and (3) two specialized agents for knowledge\nretrieval and diagnosis strategy. To ensure robust evaluation, we introduce a\ncomprehensive DDx benchmark covering respiratory, skin, and rare diseases. We\nanalyze single-turn diagnostic approaches and demonstrate the importance of\niterative refinement when patient profiles are not available at the outset. Our\nbroad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy\nimprovements in interactive DDx across both large and small LLMs, while\noffering critical explainability into its diagnostic reasoning process."
                },
                "authors": [
                    {
                        "name": "Daniel Rose"
                    },
                    {
                        "name": "Chia-Chien Hung"
                    },
                    {
                        "name": "Marco Lepri"
                    },
                    {
                        "name": "Israa Alqassem"
                    },
                    {
                        "name": "Kiril Gashteovski"
                    },
                    {
                        "name": "Carolin Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Carolin Lawrence"
                },
                "author": "Carolin Lawrence",
                "arxiv_comment": "ACL 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11786v1",
                "updated": "2025-06-13T13:47:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    47,
                    27,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:47:27Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    47,
                    27,
                    4,
                    164,
                    0
                ],
                "title": "SSPINNpose: A Self-Supervised PINN for Inertial Pose and Dynamics\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSPINNpose: A Self-Supervised PINN for Inertial Pose and Dynamics\n  Estimation"
                },
                "summary": "Accurate real-time estimation of human movement dynamics, including internal\njoint moments and muscle forces, is essential for applications in clinical\ndiagnostics and sports performance monitoring. Inertial measurement units\n(IMUs) provide a minimally intrusive solution for capturing motion data,\nparticularly when used in sparse sensor configurations. However, current\nreal-time methods rely on supervised learning, where a ground truth dataset\nneeds to be measured with laboratory measurement systems, such as optical\nmotion capture. These systems are known to introduce measurement and processing\nerrors and often fail to generalize to real-world or previously unseen\nmovements, necessitating new data collection efforts that are time-consuming\nand impractical. To overcome these limitations, we propose SSPINNpose, a\nself-supervised, physics-informed neural network that estimates joint\nkinematics and kinetics directly from IMU data, without requiring ground truth\nlabels for training. We run the network output through a physics model of the\nhuman body to optimize physical plausibility and generate virtual measurement\ndata. Using this virtual sensor data, the network is trained directly on the\nmeasured sensor data instead of a ground truth. When compared to optical motion\ncapture, SSPINNpose is able to accurately estimate joint angles and joint\nmoments at an RMSD of 8.7 deg and 4.9 BWBH%, respectively, for walking and\nrunning at speeds up to 4.9 m/s at a latency of 3.5 ms. Furthermore, the\nframework demonstrates robustness across sparse sensor configurations and can\ninfer the anatomical locations of the sensors. These results underscore the\npotential of SSPINNpose as a scalable and adaptable solution for real-time\nbiomechanical analysis in both laboratory and field environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate real-time estimation of human movement dynamics, including internal\njoint moments and muscle forces, is essential for applications in clinical\ndiagnostics and sports performance monitoring. Inertial measurement units\n(IMUs) provide a minimally intrusive solution for capturing motion data,\nparticularly when used in sparse sensor configurations. However, current\nreal-time methods rely on supervised learning, where a ground truth dataset\nneeds to be measured with laboratory measurement systems, such as optical\nmotion capture. These systems are known to introduce measurement and processing\nerrors and often fail to generalize to real-world or previously unseen\nmovements, necessitating new data collection efforts that are time-consuming\nand impractical. To overcome these limitations, we propose SSPINNpose, a\nself-supervised, physics-informed neural network that estimates joint\nkinematics and kinetics directly from IMU data, without requiring ground truth\nlabels for training. We run the network output through a physics model of the\nhuman body to optimize physical plausibility and generate virtual measurement\ndata. Using this virtual sensor data, the network is trained directly on the\nmeasured sensor data instead of a ground truth. When compared to optical motion\ncapture, SSPINNpose is able to accurately estimate joint angles and joint\nmoments at an RMSD of 8.7 deg and 4.9 BWBH%, respectively, for walking and\nrunning at speeds up to 4.9 m/s at a latency of 3.5 ms. Furthermore, the\nframework demonstrates robustness across sparse sensor configurations and can\ninfer the anatomical locations of the sensors. These results underscore the\npotential of SSPINNpose as a scalable and adaptable solution for real-time\nbiomechanical analysis in both laboratory and field environments."
                },
                "authors": [
                    {
                        "name": "Markus Gambietz"
                    },
                    {
                        "name": "Eva Dorschky"
                    },
                    {
                        "name": "Altan Akat"
                    },
                    {
                        "name": "Marcel Schöckel"
                    },
                    {
                        "name": "Jörg Miehling"
                    },
                    {
                        "name": "Anne D. Koelewijn"
                    }
                ],
                "author_detail": {
                    "name": "Anne D. Koelewijn"
                },
                "author": "Anne D. Koelewijn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03479v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03479v3",
                "updated": "2025-06-13T13:42:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    42,
                    41,
                    4,
                    164,
                    0
                ],
                "published": "2025-01-07T02:47:59Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    2,
                    47,
                    59,
                    1,
                    7,
                    0
                ],
                "title": "Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia\n  Reflect on the Cross-Cultural Sociolinguistic Norms?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia\n  Reflect on the Cross-Cultural Sociolinguistic Norms?"
                },
                "summary": "Wikipedia, as a massively multilingual, community-driven platform, is a\nvaluable resource for Natural Language Processing (NLP), yet the consistency of\nhonorific usage in honorific-rich languages remains underexplored. Honorifics,\nsubtle yet profound linguistic markers, encode social hierarchies, politeness\nnorms, and cultural values, but Wikipedia's editorial guidelines lack clear\nstandards for their usage in languages where such forms are grammatically and\nsocially prevalent. This paper addresses this gap through a large-scale\nanalysis of third-person honorific pronouns and verb forms in Hindi and Bengali\nWikipedia articles. Using Large Language Models (LLM), we automatically\nannotate 10,000 articles per language for honorific usage and socio-demographic\nfeatures such as gender, age, fame, and cultural origin. We investigate: (i)\nthe consistency of honorific usage across articles, (ii) how inconsistencies\ncorrelate with socio-cultural factors, and (iii) the presence of explicit or\nimplicit biases across languages. We find that honorific usage is consistently\nmore common in Bengali than Hindi, while non-honorific forms are more frequent\nfor infamous, juvenile, and exotic entities in both. Notably, gender bias\nemerges in both languages, particularly in Hindi, where men are more likely to\nreceive honorifics than women. Our analysis highlights the need for Wikipedia\nto develop language-specific editorial guidelines for honorific usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wikipedia, as a massively multilingual, community-driven platform, is a\nvaluable resource for Natural Language Processing (NLP), yet the consistency of\nhonorific usage in honorific-rich languages remains underexplored. Honorifics,\nsubtle yet profound linguistic markers, encode social hierarchies, politeness\nnorms, and cultural values, but Wikipedia's editorial guidelines lack clear\nstandards for their usage in languages where such forms are grammatically and\nsocially prevalent. This paper addresses this gap through a large-scale\nanalysis of third-person honorific pronouns and verb forms in Hindi and Bengali\nWikipedia articles. Using Large Language Models (LLM), we automatically\nannotate 10,000 articles per language for honorific usage and socio-demographic\nfeatures such as gender, age, fame, and cultural origin. We investigate: (i)\nthe consistency of honorific usage across articles, (ii) how inconsistencies\ncorrelate with socio-cultural factors, and (iii) the presence of explicit or\nimplicit biases across languages. We find that honorific usage is consistently\nmore common in Bengali than Hindi, while non-honorific forms are more frequent\nfor infamous, juvenile, and exotic entities in both. Notably, gender bias\nemerges in both languages, particularly in Hindi, where men are more likely to\nreceive honorifics than women. Our analysis highlights the need for Wikipedia\nto develop language-specific editorial guidelines for honorific usage."
                },
                "authors": [
                    {
                        "name": "Sourabrata Mukherjee"
                    },
                    {
                        "name": "Atharva Mehta"
                    },
                    {
                        "name": "Soumya Teotia"
                    },
                    {
                        "name": "Sougata Saha"
                    },
                    {
                        "name": "Akhil Arora"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "arxiv_comment": "Accepted at 2nd WikiNLP: Advancing Natural Language Process for\n  Wikipedia, Co-located with ACL 2025 (non-archival)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03479v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03479v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11781v1",
                "updated": "2025-06-13T13:42:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    42,
                    17,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:42:17Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    42,
                    17,
                    4,
                    164,
                    0
                ],
                "title": "GeoPandas-AI: A Smart Class Bringing LLM as Stateful AI Code Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoPandas-AI: A Smart Class Bringing LLM as Stateful AI Code Assistant"
                },
                "summary": "Geospatial data analysis plays a crucial role in tackling intricate societal\nchallenges such as urban planning and climate modeling. However, employing\ntools like GeoPandas, a prominent Python library for geospatial data\nmanipulation, necessitates expertise in complex domain-specific syntax and\nworkflows. GeoPandas-AI addresses this gap by integrating LLMs directly into\nthe GeoPandas workflow, transforming the GeoDataFrame class into an\nintelligent, stateful class for both data analysis and geospatial code\ndevelopment. This paper formalizes the design of such a smart class and\nprovides an open-source implementation of GeoPandas-AI in PyPI package manager.\nThrough its innovative combination of conversational interfaces and stateful\nexploitation of LLMs for code generation and data analysis, GeoPandas-AI\nintroduces a new paradigm for code-copilots and instantiates it for geospatial\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geospatial data analysis plays a crucial role in tackling intricate societal\nchallenges such as urban planning and climate modeling. However, employing\ntools like GeoPandas, a prominent Python library for geospatial data\nmanipulation, necessitates expertise in complex domain-specific syntax and\nworkflows. GeoPandas-AI addresses this gap by integrating LLMs directly into\nthe GeoPandas workflow, transforming the GeoDataFrame class into an\nintelligent, stateful class for both data analysis and geospatial code\ndevelopment. This paper formalizes the design of such a smart class and\nprovides an open-source implementation of GeoPandas-AI in PyPI package manager.\nThrough its innovative combination of conversational interfaces and stateful\nexploitation of LLMs for code generation and data analysis, GeoPandas-AI\nintroduces a new paradigm for code-copilots and instantiates it for geospatial\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Gaspard Merten"
                    },
                    {
                        "name": "Gilles Dejaegere"
                    },
                    {
                        "name": "Mahmoud Sakr"
                    }
                ],
                "author_detail": {
                    "name": "Mahmoud Sakr"
                },
                "author": "Mahmoud Sakr",
                "arxiv_comment": "Submitted to ACM SIGSPATIAL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11773v1",
                "updated": "2025-06-13T13:31:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    31,
                    8,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:31:08Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    31,
                    8,
                    4,
                    164,
                    0
                ],
                "title": "AgentSense: Virtual Sensor Data Generation Using LLM Agent in Simulated\n  Home Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSense: Virtual Sensor Data Generation Using LLM Agent in Simulated\n  Home Environments"
                },
                "summary": "A major obstacle in developing robust and generalizable smart home-based\nHuman Activity Recognition (HAR) systems is the lack of large-scale, diverse\nlabeled datasets. Variability in home layouts, sensor configurations, and user\nbehavior adds further complexity, as individuals follow varied routines and\nperform activities in distinct ways. Building HAR systems that generalize well\nrequires training data that captures the diversity across users and\nenvironments. To address these challenges, we introduce AgentSense, a virtual\ndata generation pipeline where diverse personas are generated by leveraging\nLarge Language Models. These personas are used to create daily routines, which\nare then decomposed into low-level action sequences. Subsequently, the actions\nare executed in a simulated home environment called VirtualHome that we\nextended with virtual ambient sensors capable of recording the agents\nactivities as they unfold. Overall, AgentSense enables the generation of rich,\nvirtual sensor datasets that represent a wide range of users and home settings.\nAcross five benchmark HAR datasets, we show that leveraging our virtual sensor\ndata substantially improves performance, particularly when real data are\nlimited. Notably, models trained on a combination of virtual data and just a\nfew days of real data achieve performance comparable to those trained on the\nentire real datasets. These results demonstrate and prove the potential of\nvirtual data to address one of the most pressing challenges in ambient sensing,\nwhich is the distinct lack of large-scale, annotated datasets without requiring\nany manual data collection efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major obstacle in developing robust and generalizable smart home-based\nHuman Activity Recognition (HAR) systems is the lack of large-scale, diverse\nlabeled datasets. Variability in home layouts, sensor configurations, and user\nbehavior adds further complexity, as individuals follow varied routines and\nperform activities in distinct ways. Building HAR systems that generalize well\nrequires training data that captures the diversity across users and\nenvironments. To address these challenges, we introduce AgentSense, a virtual\ndata generation pipeline where diverse personas are generated by leveraging\nLarge Language Models. These personas are used to create daily routines, which\nare then decomposed into low-level action sequences. Subsequently, the actions\nare executed in a simulated home environment called VirtualHome that we\nextended with virtual ambient sensors capable of recording the agents\nactivities as they unfold. Overall, AgentSense enables the generation of rich,\nvirtual sensor datasets that represent a wide range of users and home settings.\nAcross five benchmark HAR datasets, we show that leveraging our virtual sensor\ndata substantially improves performance, particularly when real data are\nlimited. Notably, models trained on a combination of virtual data and just a\nfew days of real data achieve performance comparable to those trained on the\nentire real datasets. These results demonstrate and prove the potential of\nvirtual data to address one of the most pressing challenges in ambient sensing,\nwhich is the distinct lack of large-scale, annotated datasets without requiring\nany manual data collection efforts."
                },
                "authors": [
                    {
                        "name": "Zikang Leng"
                    },
                    {
                        "name": "Megha Thukral"
                    },
                    {
                        "name": "Yaqi Liu"
                    },
                    {
                        "name": "Hrudhai Rajasekhar"
                    },
                    {
                        "name": "Shruthi K. Hiremath"
                    },
                    {
                        "name": "Thomas Plötz"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Plötz"
                },
                "author": "Thomas Plötz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11769v1",
                "updated": "2025-06-13T13:25:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    25,
                    39,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:25:39Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    25,
                    39,
                    4,
                    164,
                    0
                ],
                "title": "Long-Short Alignment for Effective Long-Context Modeling in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Short Alignment for Effective Long-Context Modeling in LLMs"
                },
                "summary": "Large language models (LLMs) have exhibited impressive performance and\nsurprising emergent properties. However, their effectiveness remains limited by\nthe fixed context window of the transformer architecture, posing challenges for\nlong-context modeling. Among these challenges, length generalization -- the\nability to generalize to sequences longer than those seen during training -- is\na classical and fundamental problem. In this work, we propose a fresh\nperspective on length generalization, shifting the focus from the conventional\nemphasis on input features such as positional encodings or data structures to\nthe output distribution of the model. Specifically, through case studies on\nsynthetic tasks, we highlight the critical role of \\textbf{long-short\nalignment} -- the consistency of output distributions across sequences of\nvarying lengths. Extending this insight to natural language tasks, we propose a\nmetric called Long-Short Misalignment to quantify this phenomenon, uncovering a\nstrong correlation between the metric and length generalization performance.\nBuilding on these findings, we develop a regularization term that promotes\nlong-short alignment during training. Extensive experiments validate the\neffectiveness of our approach, offering new insights for achieving more\neffective long-context modeling in LLMs. Code is available at\nhttps://github.com/PKU-ML/LongShortAlignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited impressive performance and\nsurprising emergent properties. However, their effectiveness remains limited by\nthe fixed context window of the transformer architecture, posing challenges for\nlong-context modeling. Among these challenges, length generalization -- the\nability to generalize to sequences longer than those seen during training -- is\na classical and fundamental problem. In this work, we propose a fresh\nperspective on length generalization, shifting the focus from the conventional\nemphasis on input features such as positional encodings or data structures to\nthe output distribution of the model. Specifically, through case studies on\nsynthetic tasks, we highlight the critical role of \\textbf{long-short\nalignment} -- the consistency of output distributions across sequences of\nvarying lengths. Extending this insight to natural language tasks, we propose a\nmetric called Long-Short Misalignment to quantify this phenomenon, uncovering a\nstrong correlation between the metric and length generalization performance.\nBuilding on these findings, we develop a regularization term that promotes\nlong-short alignment during training. Extensive experiments validate the\neffectiveness of our approach, offering new insights for achieving more\neffective long-context modeling in LLMs. Code is available at\nhttps://github.com/PKU-ML/LongShortAlignment."
                },
                "authors": [
                    {
                        "name": "Tianqi Du"
                    },
                    {
                        "name": "Haotian Huang"
                    },
                    {
                        "name": "Yifei Wang"
                    },
                    {
                        "name": "Yisen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yisen Wang"
                },
                "author": "Yisen Wang",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20273v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20273v3",
                "updated": "2025-06-13T13:24:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    24,
                    27,
                    4,
                    164,
                    0
                ],
                "published": "2025-02-27T17:01:23Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    1,
                    23,
                    3,
                    58,
                    0
                ],
                "title": "How Much is Enough? The Diminishing Returns of Tokenization Training\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Much is Enough? The Diminishing Returns of Tokenization Training\n  Data"
                },
                "summary": "Tokenization, a crucial initial step in natural language processing, is\ngoverned by several key parameters, such as the tokenization algorithm,\nvocabulary size, pre-tokenization strategy, inference strategy, and training\ndata corpus. This paper investigates the impact of an often-overlooked\nhyperparameter, tokenizer training data size. We train BPE, UnigramLM, and\nWordPiece tokenizers across various vocabulary sizes using English training\ndata ranging from 1GB to 900GB. Our findings reveal diminishing returns as\ntraining data size increases beyond roughly 150GB, suggesting a practical limit\nto the improvements in tokenization quality achievable through additional data.\nWe analyze this phenomenon and attribute the saturation effect to constraints\nintroduced by the pre-tokenization stage. We then demonstrate the extent to\nwhich these findings can generalize by experimenting on data in Russian, a\nlanguage typologically distant from English. For Russian text, we observe\ndiminishing returns after training a tokenizer from 200GB of data, which is\napproximately 33% more than when training on English. These results provide\nvaluable insights for optimizing the tokenization process by reducing the\ncompute required for training on large corpora and suggest promising directions\nfor future research in tokenization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization, a crucial initial step in natural language processing, is\ngoverned by several key parameters, such as the tokenization algorithm,\nvocabulary size, pre-tokenization strategy, inference strategy, and training\ndata corpus. This paper investigates the impact of an often-overlooked\nhyperparameter, tokenizer training data size. We train BPE, UnigramLM, and\nWordPiece tokenizers across various vocabulary sizes using English training\ndata ranging from 1GB to 900GB. Our findings reveal diminishing returns as\ntraining data size increases beyond roughly 150GB, suggesting a practical limit\nto the improvements in tokenization quality achievable through additional data.\nWe analyze this phenomenon and attribute the saturation effect to constraints\nintroduced by the pre-tokenization stage. We then demonstrate the extent to\nwhich these findings can generalize by experimenting on data in Russian, a\nlanguage typologically distant from English. For Russian text, we observe\ndiminishing returns after training a tokenizer from 200GB of data, which is\napproximately 33% more than when training on English. These results provide\nvaluable insights for optimizing the tokenization process by reducing the\ncompute required for training on large corpora and suggest promising directions\nfor future research in tokenization algorithms."
                },
                "authors": [
                    {
                        "name": "Varshini Reddy"
                    },
                    {
                        "name": "Craig W. Schmidt"
                    },
                    {
                        "name": "Yuval Pinter"
                    },
                    {
                        "name": "Chris Tanner"
                    }
                ],
                "author_detail": {
                    "name": "Chris Tanner"
                },
                "author": "Chris Tanner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20273v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20273v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11767v1",
                "updated": "2025-06-13T13:21:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    21,
                    53,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:21:53Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    21,
                    53,
                    4,
                    164,
                    0
                ],
                "title": "Designing Effective LLM-Assisted Interfaces for Curriculum Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Effective LLM-Assisted Interfaces for Curriculum Development"
                },
                "summary": "Large Language Models (LLMs) have the potential to transform the way a\ndynamic curriculum can be delivered. However, educators face significant\nchallenges in interacting with these models, particularly due to complex prompt\nengineering and usability issues, which increase workload. Additionally,\ninaccuracies in LLM outputs can raise issues around output quality and ethical\nconcerns in educational content delivery. Addressing these issues requires\ncareful oversight, best achieved through cooperation between human and AI\napproaches. This paper introduces two novel User Interface (UI) designs, UI\nPredefined and UI Open, both grounded in Direct Manipulation (DM) principles to\naddress these challenges. By reducing the reliance on intricate prompt\nengineering, these UIs improve usability, streamline interaction, and lower\nworkload, providing a more effective pathway for educators to engage with LLMs.\nIn a controlled user study with 20 participants, the proposed UIs were\nevaluated against the standard ChatGPT interface in terms of usability and\ncognitive load. Results showed that UI Predefined significantly outperformed\nboth ChatGPT and UI Open, demonstrating superior usability and reduced task\nload, while UI Open offered more flexibility at the cost of a steeper learning\ncurve. These findings underscore the importance of user-centered design in\nadopting AI-driven tools and lay the foundation for more intuitive and\nefficient educator-LLM interactions in online learning environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have the potential to transform the way a\ndynamic curriculum can be delivered. However, educators face significant\nchallenges in interacting with these models, particularly due to complex prompt\nengineering and usability issues, which increase workload. Additionally,\ninaccuracies in LLM outputs can raise issues around output quality and ethical\nconcerns in educational content delivery. Addressing these issues requires\ncareful oversight, best achieved through cooperation between human and AI\napproaches. This paper introduces two novel User Interface (UI) designs, UI\nPredefined and UI Open, both grounded in Direct Manipulation (DM) principles to\naddress these challenges. By reducing the reliance on intricate prompt\nengineering, these UIs improve usability, streamline interaction, and lower\nworkload, providing a more effective pathway for educators to engage with LLMs.\nIn a controlled user study with 20 participants, the proposed UIs were\nevaluated against the standard ChatGPT interface in terms of usability and\ncognitive load. Results showed that UI Predefined significantly outperformed\nboth ChatGPT and UI Open, demonstrating superior usability and reduced task\nload, while UI Open offered more flexibility at the cost of a steeper learning\ncurve. These findings underscore the importance of user-centered design in\nadopting AI-driven tools and lay the foundation for more intuitive and\nefficient educator-LLM interactions in online learning environments."
                },
                "authors": [
                    {
                        "name": "Abdolali Faraji"
                    },
                    {
                        "name": "Mohammadreza Tavakoli"
                    },
                    {
                        "name": "Mohammad Moein"
                    },
                    {
                        "name": "Mohammadreza Molavi"
                    },
                    {
                        "name": "Gábor Kismihók"
                    }
                ],
                "author_detail": {
                    "name": "Gábor Kismihók"
                },
                "author": "Gábor Kismihók",
                "arxiv_comment": "This is the preprint version of a paper accepted at AIED 2025. The\n  final version will be published by Springer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11763v1",
                "updated": "2025-06-13T13:17:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    17,
                    32,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:17:32Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    17,
                    32,
                    4,
                    164,
                    0
                ],
                "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents"
                },
                "summary": "Deep Research Agents are a prominent category of LLM-based agents. By\nautonomously orchestrating multistep web exploration, targeted retrieval, and\nhigher-order synthesis, they transform vast amounts of online information into\nanalyst-grade, citation-rich reports--compressing hours of manual desk research\ninto minutes. However, a comprehensive benchmark for systematically evaluating\nthe capabilities of these agents remains absent. To bridge this gap, we present\nDeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,\neach meticulously crafted by domain experts across 22 distinct fields.\nEvaluating DRAs is inherently complex and labor-intensive. We therefore propose\ntwo novel methodologies that achieve strong alignment with human judgment. The\nfirst is a reference-based method with adaptive criteria to assess the quality\nof generated research reports. The other framework is introduced to evaluate\nDRA's information retrieval and collection capabilities by assessing its\neffective citation count and overall citation accuracy. We have open-sourced\nDeepResearch Bench and key components of these frameworks at\nhttps://github.com/Ayanami0730/deep_research_bench to accelerate the\ndevelopment of practical LLM-based agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Research Agents are a prominent category of LLM-based agents. By\nautonomously orchestrating multistep web exploration, targeted retrieval, and\nhigher-order synthesis, they transform vast amounts of online information into\nanalyst-grade, citation-rich reports--compressing hours of manual desk research\ninto minutes. However, a comprehensive benchmark for systematically evaluating\nthe capabilities of these agents remains absent. To bridge this gap, we present\nDeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,\neach meticulously crafted by domain experts across 22 distinct fields.\nEvaluating DRAs is inherently complex and labor-intensive. We therefore propose\ntwo novel methodologies that achieve strong alignment with human judgment. The\nfirst is a reference-based method with adaptive criteria to assess the quality\nof generated research reports. The other framework is introduced to evaluate\nDRA's information retrieval and collection capabilities by assessing its\neffective citation count and overall citation accuracy. We have open-sourced\nDeepResearch Bench and key components of these frameworks at\nhttps://github.com/Ayanami0730/deep_research_bench to accelerate the\ndevelopment of practical LLM-based agents."
                },
                "authors": [
                    {
                        "name": "Mingxuan Du"
                    },
                    {
                        "name": "Benfeng Xu"
                    },
                    {
                        "name": "Chiwei Zhu"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "arxiv_comment": "31 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11752v1",
                "updated": "2025-06-13T13:05:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    5,
                    41,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:05:41Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    5,
                    41,
                    4,
                    164,
                    0
                ],
                "title": "DART: Distilling Autoregressive Reasoning to Silent Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DART: Distilling Autoregressive Reasoning to Silent Thought"
                },
                "summary": "Chain-of-Thought (CoT) reasoning has significantly advanced Large Language\nModels (LLMs) in solving complex tasks. However, its autoregressive paradigm\nleads to significant computational overhead, hindering its deployment in\nlatency-sensitive applications. To address this, we propose \\textbf{DART}\n(\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent\n\\textbf{T}hought), a self-distillation framework that enables LLMs to replace\nautoregressive CoT with non-autoregressive Silent Thought (ST). Specifically,\nDART introduces two training pathways: the CoT pathway for traditional\nreasoning and the ST pathway for generating answers directly from a few ST\ntokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM)\nto align its hidden states with the CoT pathway, enabling the ST tokens to\nevolve into informative embeddings. During inference, only the ST pathway is\nactivated, leveraging evolving ST tokens to deliver the answer directly.\nExtensive experimental results demonstrate that DART achieves comparable\nreasoning performance to existing baselines while offering significant\nefficiency gains, serving as a feasible alternative for efficient reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning has significantly advanced Large Language\nModels (LLMs) in solving complex tasks. However, its autoregressive paradigm\nleads to significant computational overhead, hindering its deployment in\nlatency-sensitive applications. To address this, we propose \\textbf{DART}\n(\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent\n\\textbf{T}hought), a self-distillation framework that enables LLMs to replace\nautoregressive CoT with non-autoregressive Silent Thought (ST). Specifically,\nDART introduces two training pathways: the CoT pathway for traditional\nreasoning and the ST pathway for generating answers directly from a few ST\ntokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM)\nto align its hidden states with the CoT pathway, enabling the ST tokens to\nevolve into informative embeddings. During inference, only the ST pathway is\nactivated, leveraging evolving ST tokens to deliver the answer directly.\nExtensive experimental results demonstrate that DART achieves comparable\nreasoning performance to existing baselines while offering significant\nefficiency gains, serving as a feasible alternative for efficient reasoning."
                },
                "authors": [
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Ziming Wu"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    },
                    {
                        "name": "Fuming Lai"
                    },
                    {
                        "name": "Shaobing Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shaobing Lian"
                },
                "author": "Shaobing Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12415v2",
                "updated": "2025-06-13T13:02:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    2,
                    56,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-18T13:40:18Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    13,
                    40,
                    18,
                    6,
                    138,
                    0
                ],
                "title": "Table-R1: Region-based Reinforcement Learning for Table Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table-R1: Region-based Reinforcement Learning for Table Understanding"
                },
                "summary": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning."
                },
                "authors": [
                    {
                        "name": "Zhenhe Wu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xianjie Wu"
                    },
                    {
                        "name": "Changzai Pan"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Shuangyong Song"
                    },
                    {
                        "name": "Yongxiang Li"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07595v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07595v2",
                "updated": "2025-06-13T12:47:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    47,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2024-11-12T07:09:44Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    7,
                    9,
                    44,
                    1,
                    317,
                    0
                ],
                "title": "Entropy Controllable Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy Controllable Direct Preference Optimization"
                },
                "summary": "In the post-training of large language models (LLMs), Reinforcement Learning\nfrom Human Feedback (RLHF) is an effective approach to achieve generation\naligned with human preferences. Direct Preference Optimization (DPO) allows for\npolicy training with a simple binary cross-entropy loss without a reward model.\nThe objective of DPO is regularized by reverse KL divergence that encourages\nmode-seeking fitting to the reference policy. Nonetheless, we indicate that\nminimizing reverse KL divergence could fail to capture a mode of the reference\ndistribution, which may hurt the policy's performance. Based on this\nobservation, we propose a simple modification to DPO, H-DPO, which allows for\ncontrol over the entropy of the resulting policy, enhancing the distribution's\nsharpness and thereby enabling mode-seeking fitting more effectively. In our\nexperiments, we show that H-DPO outperformed DPO across various tasks,\ndemonstrating superior results in pass@$k$ evaluations for mathematical tasks.\nMoreover, H-DPO is simple to implement, requiring only minor modifications to\nthe loss calculation of DPO, which makes it highly practical and promising for\nwide-ranging applications in the training of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the post-training of large language models (LLMs), Reinforcement Learning\nfrom Human Feedback (RLHF) is an effective approach to achieve generation\naligned with human preferences. Direct Preference Optimization (DPO) allows for\npolicy training with a simple binary cross-entropy loss without a reward model.\nThe objective of DPO is regularized by reverse KL divergence that encourages\nmode-seeking fitting to the reference policy. Nonetheless, we indicate that\nminimizing reverse KL divergence could fail to capture a mode of the reference\ndistribution, which may hurt the policy's performance. Based on this\nobservation, we propose a simple modification to DPO, H-DPO, which allows for\ncontrol over the entropy of the resulting policy, enhancing the distribution's\nsharpness and thereby enabling mode-seeking fitting more effectively. In our\nexperiments, we show that H-DPO outperformed DPO across various tasks,\ndemonstrating superior results in pass@$k$ evaluations for mathematical tasks.\nMoreover, H-DPO is simple to implement, requiring only minor modifications to\nthe loss calculation of DPO, which makes it highly practical and promising for\nwide-ranging applications in the training of LLMs."
                },
                "authors": [
                    {
                        "name": "Motoki Omura"
                    },
                    {
                        "name": "Yasuhiro Fujita"
                    },
                    {
                        "name": "Toshiki Kataoka"
                    }
                ],
                "author_detail": {
                    "name": "Toshiki Kataoka"
                },
                "author": "Toshiki Kataoka",
                "arxiv_comment": "ICML 2025 Workshop on Models of Human Feedback for AI Alignment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07595v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11729v1",
                "updated": "2025-06-13T12:42:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    42,
                    9,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T12:42:09Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    42,
                    9,
                    4,
                    164,
                    0
                ],
                "title": "5G-Enabled Smart Prosthetic Hand: Connectivity Analysis and Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "5G-Enabled Smart Prosthetic Hand: Connectivity Analysis and Assessment"
                },
                "summary": "In this paper, we demonstrate a proof-of-concept implementation of a\nframework for the development of edge-connected prosthetic systems. The\nframework is composed of a bionic hand equipped with a camera and connected to\na Jetson device that establishes a wireless connection to the edge server,\nprocessing the received video stream and feeding back the inferred information\nabout the environment. The hand-edge server connection is obtained either\nthrough a direct 5G link, where the edge server also functions as a 5G base\nstation, or through a WiFi link. We evaluate the latency of closing the control\nloop in the system, showing that, in a realistic usage scenario, the\nconnectivity and computation delays combined are well below 125 ms, which falls\ninto the natural control range. To the best of our knowledge, this is the first\nanalysis showcasing the feasibility of a 5G-enabled prosthetic system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we demonstrate a proof-of-concept implementation of a\nframework for the development of edge-connected prosthetic systems. The\nframework is composed of a bionic hand equipped with a camera and connected to\na Jetson device that establishes a wireless connection to the edge server,\nprocessing the received video stream and feeding back the inferred information\nabout the environment. The hand-edge server connection is obtained either\nthrough a direct 5G link, where the edge server also functions as a 5G base\nstation, or through a WiFi link. We evaluate the latency of closing the control\nloop in the system, showing that, in a realistic usage scenario, the\nconnectivity and computation delays combined are well below 125 ms, which falls\ninto the natural control range. To the best of our knowledge, this is the first\nanalysis showcasing the feasibility of a 5G-enabled prosthetic system."
                },
                "authors": [
                    {
                        "name": "Ozan Karaali"
                    },
                    {
                        "name": "Hossam Farag"
                    },
                    {
                        "name": "Strahinja Dosen"
                    },
                    {
                        "name": "Cedomir Stefanovic"
                    }
                ],
                "author_detail": {
                    "name": "Cedomir Stefanovic"
                },
                "author": "Cedomir Stefanovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11728v1",
                "updated": "2025-06-13T12:40:16Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    40,
                    16,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T12:40:16Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    40,
                    16,
                    4,
                    164,
                    0
                ],
                "title": "The Cambrian Explosion of Mixed-Precision Matrix Multiplication for\n  Quantized Deep Learning Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Cambrian Explosion of Mixed-Precision Matrix Multiplication for\n  Quantized Deep Learning Inference"
                },
                "summary": "Recent advances in deep learning (DL) have led to a shift from traditional\n64-bit floating point (FP64) computations toward reduced-precision formats,\nsuch as FP16, BF16, and 8- or 16-bit integers, combined with mixed-precision\narithmetic. This transition enhances computational throughput, reduces memory\nand bandwidth usage, and improves energy efficiency, offering significant\nadvantages for resource-constrained edge devices. To support this shift,\nhardware architectures have evolved accordingly, now including adapted ISAs\n(Instruction Set Architectures) that expose mixed-precision vector units and\nmatrix engines tailored for DL workloads. At the heart of many DL and\nscientific computing tasks is the general matrix-matrix multiplication gemm, a\nfundamental kernel historically optimized using axpy vector instructions on\nSIMD (single instruction, multiple data) units. However, as hardware moves\ntoward mixed-precision dot-product-centric operations optimized for quantized\ninference, these legacy approaches are being phased out. In response to this,\nour paper revisits traditional high-performance gemm and describes strategies\nfor adapting it to mixed-precision integer (MIP) arithmetic across modern ISAs,\nincluding x86_64, ARM, and RISC-V. Concretely, we illustrate novel micro-kernel\ndesigns and data layouts that better exploit today's specialized hardware and\ndemonstrate significant performance gains from MIP arithmetic over\nfloating-point implementations across three representative CPU architectures.\nThese contributions highlight a new era of gemm optimization-driven by the\ndemands of DL inference on heterogeneous architectures, marking what we term as\nthe \"Cambrian period\" for matrix multiplication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning (DL) have led to a shift from traditional\n64-bit floating point (FP64) computations toward reduced-precision formats,\nsuch as FP16, BF16, and 8- or 16-bit integers, combined with mixed-precision\narithmetic. This transition enhances computational throughput, reduces memory\nand bandwidth usage, and improves energy efficiency, offering significant\nadvantages for resource-constrained edge devices. To support this shift,\nhardware architectures have evolved accordingly, now including adapted ISAs\n(Instruction Set Architectures) that expose mixed-precision vector units and\nmatrix engines tailored for DL workloads. At the heart of many DL and\nscientific computing tasks is the general matrix-matrix multiplication gemm, a\nfundamental kernel historically optimized using axpy vector instructions on\nSIMD (single instruction, multiple data) units. However, as hardware moves\ntoward mixed-precision dot-product-centric operations optimized for quantized\ninference, these legacy approaches are being phased out. In response to this,\nour paper revisits traditional high-performance gemm and describes strategies\nfor adapting it to mixed-precision integer (MIP) arithmetic across modern ISAs,\nincluding x86_64, ARM, and RISC-V. Concretely, we illustrate novel micro-kernel\ndesigns and data layouts that better exploit today's specialized hardware and\ndemonstrate significant performance gains from MIP arithmetic over\nfloating-point implementations across three representative CPU architectures.\nThese contributions highlight a new era of gemm optimization-driven by the\ndemands of DL inference on heterogeneous architectures, marking what we term as\nthe \"Cambrian period\" for matrix multiplication."
                },
                "authors": [
                    {
                        "name": "Héctor Martínez"
                    },
                    {
                        "name": "Adrián Castelló"
                    },
                    {
                        "name": "Francisco D. Igual"
                    },
                    {
                        "name": "Enrique S. Quintana-Ortí"
                    }
                ],
                "author_detail": {
                    "name": "Enrique S. Quintana-Ortí"
                },
                "author": "Enrique S. Quintana-Ortí",
                "arxiv_comment": "16 pages, 7 tables, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11722v1",
                "updated": "2025-06-13T12:37:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    37,
                    7,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T12:37:07Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    37,
                    7,
                    4,
                    164,
                    0
                ],
                "title": "Classification of Quality Characteristics in Online User Feedback using\n  Linguistic Analysis, Crowdsourcing and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification of Quality Characteristics in Online User Feedback using\n  Linguistic Analysis, Crowdsourcing and LLMs"
                },
                "summary": "Software qualities such as usability or reliability are among the strongest\ndeterminants of mobile app user satisfaction and constitute a significant\nportion of online user feedback on software products, making it a valuable\nsource of quality-related feedback to guide the development process. The\nabundance of online user feedback warrants the automated identification of\nquality characteristics, but the online user feedback's heterogeneity and the\nlack of appropriate training corpora limit the applicability of supervised\nmachine learning. We therefore investigate the viability of three approaches\nthat could be effective in low-data settings: language patterns (LPs) based on\nquality-related keywords, instructions for crowdsourced micro-tasks, and large\nlanguage model (LLM) prompts. We determined the feasibility of each approach\nand then compared their accuracy. For the complex multiclass classification of\nquality characteristics, the LP-based approach achieved a varied precision\n(0.38-0.92) depending on the quality characteristic, and low recall;\ncrowdsourcing achieved the best average accuracy in two consecutive phases\n(0.63, 0.72), which could be matched by the best-performing LLM condition\n(0.66) and a prediction based on the LLMs' majority vote (0.68). Our findings\nshow that in this low-data setting, the two approaches that use crowdsourcing\nor LLMs instead of involving experts achieve accurate classifications, while\nthe LP-based approach has only limited potential. The promise of crowdsourcing\nand LLMs in this context might even extend to building training corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software qualities such as usability or reliability are among the strongest\ndeterminants of mobile app user satisfaction and constitute a significant\nportion of online user feedback on software products, making it a valuable\nsource of quality-related feedback to guide the development process. The\nabundance of online user feedback warrants the automated identification of\nquality characteristics, but the online user feedback's heterogeneity and the\nlack of appropriate training corpora limit the applicability of supervised\nmachine learning. We therefore investigate the viability of three approaches\nthat could be effective in low-data settings: language patterns (LPs) based on\nquality-related keywords, instructions for crowdsourced micro-tasks, and large\nlanguage model (LLM) prompts. We determined the feasibility of each approach\nand then compared their accuracy. For the complex multiclass classification of\nquality characteristics, the LP-based approach achieved a varied precision\n(0.38-0.92) depending on the quality characteristic, and low recall;\ncrowdsourcing achieved the best average accuracy in two consecutive phases\n(0.63, 0.72), which could be matched by the best-performing LLM condition\n(0.66) and a prediction based on the LLMs' majority vote (0.68). Our findings\nshow that in this low-data setting, the two approaches that use crowdsourcing\nor LLMs instead of involving experts achieve accurate classifications, while\nthe LP-based approach has only limited potential. The promise of crowdsourcing\nand LLMs in this context might even extend to building training corpora."
                },
                "authors": [
                    {
                        "name": "Eduard C. Groen"
                    },
                    {
                        "name": "Fabiano Dalpiaz"
                    },
                    {
                        "name": "Martijn van Vliet"
                    },
                    {
                        "name": "Boris Winter"
                    },
                    {
                        "name": "Joerg Doerr"
                    },
                    {
                        "name": "Sjaak Brinkkemper"
                    }
                ],
                "author_detail": {
                    "name": "Sjaak Brinkkemper"
                },
                "author": "Sjaak Brinkkemper",
                "arxiv_comment": "Accepted at the Journal of Systems and Software (JSS); online\n  appendix and supplementary material available at\n  https://doi.org/10.5281/zenodo.15604749",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06374v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06374v2",
                "updated": "2025-06-13T12:35:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    35,
                    46,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-04T13:54:02Z",
                "published_parsed": [
                    2025,
                    6,
                    4,
                    13,
                    54,
                    2,
                    2,
                    155,
                    0
                ],
                "title": "Structured State Space Model Dynamics and Parametrization for Spiking\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured State Space Model Dynamics and Parametrization for Spiking\n  Neural Networks"
                },
                "summary": "Multi-state spiking neurons such as the adaptive leaky integrate-and-fire\n(AdLIF) neuron offer compelling alternatives to conventional deep learning\nmodels thanks to their sparse binary activations, second-order nonlinear\nrecurrent dynamics, and efficient hardware realizations. However, such internal\ndynamics can cause instabilities during inference and training, often limiting\nperformance and scalability. Meanwhile, state space models (SSMs) excel in long\nsequence processing using linear state-intrinsic recurrence resembling spiking\nneurons' subthreshold regime. Here, we establish a mathematical bridge between\nSSMs and second-order spiking neuron models. Based on structure and\nparametrization strategies of diagonal SSMs, we propose two novel spiking\nneuron models. The first extends the AdLIF neuron through timestep training and\nlogarithmic reparametrization to facilitate training and improve final\nperformance. The second additionally brings initialization and structure from\ncomplex-state SSMs, broadening the dynamical regime to oscillatory dynamics.\nTogether, our two models achieve beyond or near state-of-the-art (SOTA)\nperformances for reset-based spiking neuron models across both event-based and\nraw audio speech recognition datasets. We achieve this with a favorable number\nof parameters and required dynamic memory while maintaining high activity\nsparsity. Our models demonstrate enhanced scalability in network size and\nstrike a favorable balance between performance and efficiency with respect to\nSSM models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-state spiking neurons such as the adaptive leaky integrate-and-fire\n(AdLIF) neuron offer compelling alternatives to conventional deep learning\nmodels thanks to their sparse binary activations, second-order nonlinear\nrecurrent dynamics, and efficient hardware realizations. However, such internal\ndynamics can cause instabilities during inference and training, often limiting\nperformance and scalability. Meanwhile, state space models (SSMs) excel in long\nsequence processing using linear state-intrinsic recurrence resembling spiking\nneurons' subthreshold regime. Here, we establish a mathematical bridge between\nSSMs and second-order spiking neuron models. Based on structure and\nparametrization strategies of diagonal SSMs, we propose two novel spiking\nneuron models. The first extends the AdLIF neuron through timestep training and\nlogarithmic reparametrization to facilitate training and improve final\nperformance. The second additionally brings initialization and structure from\ncomplex-state SSMs, broadening the dynamical regime to oscillatory dynamics.\nTogether, our two models achieve beyond or near state-of-the-art (SOTA)\nperformances for reset-based spiking neuron models across both event-based and\nraw audio speech recognition datasets. We achieve this with a favorable number\nof parameters and required dynamic memory while maintaining high activity\nsparsity. Our models demonstrate enhanced scalability in network size and\nstrike a favorable balance between performance and efficiency with respect to\nSSM models."
                },
                "authors": [
                    {
                        "name": "Maxime Fabre"
                    },
                    {
                        "name": "Lyubov Dudchenko"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06374v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06374v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11709v1",
                "updated": "2025-06-13T12:23:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    23,
                    49,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T12:23:49Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    23,
                    49,
                    4,
                    164,
                    0
                ],
                "title": "Inner disc and circumplanetary material in the PDS 70 system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inner disc and circumplanetary material in the PDS 70 system"
                },
                "summary": "The two giant protoplanets directly imaged in the dust-depleted cavity of PDS\n70 offer a unique opportunity to study ongoing planet formation. Both planets\nhave been detected in infrared thermal emission and in H$\\alpha$, indicating\nactive accretion. We calibrate and analyse archival ALMA Band 6 and 7\nobservations of PDS 70 from 2019, 2021, and 2023 to search for circumplanetary\nmaterial and assess its motion. Using 2D visibility modelling of the\nhigh-resolution (~0.11\"x0.08\" in Band 6; ~0.05\"x0.05\" in Band 7) dust continuum\nfrom the outer disc, we subtract the model and image the cavity at multiple\nepochs. We re-detect compact dust emission around PDS 70 c in all datasets with\n>$3.8\\sigma$ significance, and tentatively detect emission near PDS 70 b at\n~$3\\sigma$ in Band 6, with peak fluxes of $59\\pm17\\mu$Jy/beam and\n$46\\pm14\\mu$Jy/beam. The relative astrometry of the compact emission around PDS\n70 c is consistent with the expected position of the planet between 2019-2023.\nWe find a peak flux difference up to $64\\pm34\\mu$Jy/beam at 1$\\sigma$, but\nBayesian analysis indicates no significant variability. We detect no flux\nvariability in the inner disc. The inferred dust mass near PDS 70 c and in the\ninner disc ranges from $0.008$-$0.063 M_\\oplus$ and $0.04$-$0.31 M_\\oplus$,\nrespectively, consistent with prior estimates. Finally, we measure Band 6-7\nspectral indices of $2.5\\pm1.2$ (PDS 70 c) and $3.2\\pm0.5$ (inner disc),\nsuggesting that the inner disc emission is dominated by optically thin dust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The two giant protoplanets directly imaged in the dust-depleted cavity of PDS\n70 offer a unique opportunity to study ongoing planet formation. Both planets\nhave been detected in infrared thermal emission and in H$\\alpha$, indicating\nactive accretion. We calibrate and analyse archival ALMA Band 6 and 7\nobservations of PDS 70 from 2019, 2021, and 2023 to search for circumplanetary\nmaterial and assess its motion. Using 2D visibility modelling of the\nhigh-resolution (~0.11\"x0.08\" in Band 6; ~0.05\"x0.05\" in Band 7) dust continuum\nfrom the outer disc, we subtract the model and image the cavity at multiple\nepochs. We re-detect compact dust emission around PDS 70 c in all datasets with\n>$3.8\\sigma$ significance, and tentatively detect emission near PDS 70 b at\n~$3\\sigma$ in Band 6, with peak fluxes of $59\\pm17\\mu$Jy/beam and\n$46\\pm14\\mu$Jy/beam. The relative astrometry of the compact emission around PDS\n70 c is consistent with the expected position of the planet between 2019-2023.\nWe find a peak flux difference up to $64\\pm34\\mu$Jy/beam at 1$\\sigma$, but\nBayesian analysis indicates no significant variability. We detect no flux\nvariability in the inner disc. The inferred dust mass near PDS 70 c and in the\ninner disc ranges from $0.008$-$0.063 M_\\oplus$ and $0.04$-$0.31 M_\\oplus$,\nrespectively, consistent with prior estimates. Finally, we measure Band 6-7\nspectral indices of $2.5\\pm1.2$ (PDS 70 c) and $3.2\\pm0.5$ (inner disc),\nsuggesting that the inner disc emission is dominated by optically thin dust."
                },
                "authors": [
                    {
                        "name": "Daniele Fasano"
                    },
                    {
                        "name": "Myriam Benisty"
                    },
                    {
                        "name": "Pietro Curone"
                    },
                    {
                        "name": "Stefano Facchini"
                    },
                    {
                        "name": "Francesco Zagaria"
                    },
                    {
                        "name": "Tomohiro C. Yoshida"
                    },
                    {
                        "name": "Kiyoaki Doi"
                    },
                    {
                        "name": "Anibal Sierra"
                    },
                    {
                        "name": "Sean Andrews"
                    },
                    {
                        "name": "Jaehan Bae"
                    },
                    {
                        "name": "Andrea Isella"
                    },
                    {
                        "name": "Nicolás T. Kurtovic"
                    },
                    {
                        "name": "Laura M. Pérez"
                    },
                    {
                        "name": "Paola Pinilla"
                    },
                    {
                        "name": "Luna Rampinelli"
                    },
                    {
                        "name": "Richard Teague"
                    }
                ],
                "author_detail": {
                    "name": "Richard Teague"
                },
                "author": "Richard Teague",
                "arxiv_comment": "12 pages, 5 figures. Accepted for publication in A&A June 04, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04466v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04466v4",
                "updated": "2025-06-13T12:20:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    20,
                    54,
                    4,
                    164,
                    0
                ],
                "published": "2024-10-06T12:42:04Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    12,
                    42,
                    4,
                    6,
                    280,
                    0
                ],
                "title": "Large Language Model Inference Acceleration: A Comprehensive Hardware\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Inference Acceleration: A Comprehensive Hardware\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious fields, from natural language understanding to text generation.\nCompared to non-generative LLMs like BERT and DeBERTa, generative LLMs like GPT\nseries and Llama series are currently the main focus due to their superior\nalgorithmic performance. The advancements in generative LLMs are closely\nintertwined with the development of hardware capabilities. Various hardware\nplatforms exhibit distinct hardware characteristics, which can help improve LLM\ninference performance. Therefore, this paper comprehensively surveys efficient\ngenerative LLM inference on different hardware platforms. First, we provide an\noverview of the algorithm architecture of mainstream generative LLMs and delve\ninto the inference process. Then, we summarize different optimization methods\nfor different platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP, and provide\ninference results for generative LLMs. Furthermore, we perform a qualitative\nand quantitative comparison of inference performance with batch sizes 1 and 8\non different hardware platforms by considering hardware power consumption,\nabsolute inference speed (tokens/s), and energy efficiency (tokens/J). We\ncompare the performance of the same optimization methods across different\nhardware platforms, the performance across different hardware platforms, and\nthe performance of different methods on the same hardware platform. This\nprovides a systematic and comprehensive summary of existing inference\nacceleration work by integrating software optimization methods and hardware\nplatforms. We point out that three trends (multimodality, inference-time\ncompute, and higher inference energy efficiency) are promising to redefine the\ncapabilities of edge artificial intelligence systems. Our project is available\nat https://dai.sjtu.edu.cn/project.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious fields, from natural language understanding to text generation.\nCompared to non-generative LLMs like BERT and DeBERTa, generative LLMs like GPT\nseries and Llama series are currently the main focus due to their superior\nalgorithmic performance. The advancements in generative LLMs are closely\nintertwined with the development of hardware capabilities. Various hardware\nplatforms exhibit distinct hardware characteristics, which can help improve LLM\ninference performance. Therefore, this paper comprehensively surveys efficient\ngenerative LLM inference on different hardware platforms. First, we provide an\noverview of the algorithm architecture of mainstream generative LLMs and delve\ninto the inference process. Then, we summarize different optimization methods\nfor different platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP, and provide\ninference results for generative LLMs. Furthermore, we perform a qualitative\nand quantitative comparison of inference performance with batch sizes 1 and 8\non different hardware platforms by considering hardware power consumption,\nabsolute inference speed (tokens/s), and energy efficiency (tokens/J). We\ncompare the performance of the same optimization methods across different\nhardware platforms, the performance across different hardware platforms, and\nthe performance of different methods on the same hardware platform. This\nprovides a systematic and comprehensive summary of existing inference\nacceleration work by integrating software optimization methods and hardware\nplatforms. We point out that three trends (multimodality, inference-time\ncompute, and higher inference energy efficiency) are promising to redefine the\ncapabilities of edge artificial intelligence systems. Our project is available\nat https://dai.sjtu.edu.cn/project.html."
                },
                "authors": [
                    {
                        "name": "Jinhao Li"
                    },
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Shan Huang"
                    },
                    {
                        "name": "Yonghua Chen"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Yaoxiu Lian"
                    },
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Li Ding"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai",
                "arxiv_comment": "Collect and update results in recent half year. 54 pages. Github\n  link: https://github.com/Kimho666/LLM_Hardware_Survey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04466v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04466v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11702v1",
                "updated": "2025-06-13T12:17:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    17,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T12:17:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    17,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "Configurable Preference Tuning with Rubric-Guided Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Configurable Preference Tuning with Rubric-Guided Synthetic Data"
                },
                "summary": "Models of human feedback for AI alignment, such as those underpinning Direct\nPreference Optimization (DPO), often bake in a singular, static set of\npreferences, limiting adaptability. This paper challenges the assumption of\nmonolithic preferences by introducing Configurable Preference Tuning (CPT), a\nnovel framework for endowing language models with the ability to dynamically\nadjust their behavior based on explicit, human-interpretable directives. CPT\nleverages synthetically generated preference data, conditioned on system\nprompts derived from structured, fine-grained rubrics that define desired\nattributes like writing style. By fine-tuning with these rubric-guided\npreferences, the LLM learns to modulate its outputs at inference time in\nresponse to the system prompt, without retraining. This approach not only\noffers fine-grained control but also provides a mechanism for modeling more\nnuanced and context-dependent human feedback. Several experimental artifacts,\nsuch as training code, generated datasets and fine-tuned models are released at\nhttps://github.com/vicgalle/configurable-preference-tuning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Models of human feedback for AI alignment, such as those underpinning Direct\nPreference Optimization (DPO), often bake in a singular, static set of\npreferences, limiting adaptability. This paper challenges the assumption of\nmonolithic preferences by introducing Configurable Preference Tuning (CPT), a\nnovel framework for endowing language models with the ability to dynamically\nadjust their behavior based on explicit, human-interpretable directives. CPT\nleverages synthetically generated preference data, conditioned on system\nprompts derived from structured, fine-grained rubrics that define desired\nattributes like writing style. By fine-tuning with these rubric-guided\npreferences, the LLM learns to modulate its outputs at inference time in\nresponse to the system prompt, without retraining. This approach not only\noffers fine-grained control but also provides a mechanism for modeling more\nnuanced and context-dependent human feedback. Several experimental artifacts,\nsuch as training code, generated datasets and fine-tuned models are released at\nhttps://github.com/vicgalle/configurable-preference-tuning"
                },
                "authors": [
                    {
                        "name": "Víctor Gallego"
                    }
                ],
                "author_detail": {
                    "name": "Víctor Gallego"
                },
                "author": "Víctor Gallego",
                "arxiv_comment": "Accepted to ICML 2025 Workshop on Models of Human Feedback for AI\n  Alignment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18815v2",
                "updated": "2025-06-13T12:15:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    15,
                    17,
                    4,
                    164,
                    0
                ],
                "published": "2025-04-26T06:06:38Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    6,
                    6,
                    38,
                    5,
                    116,
                    0
                ],
                "title": "A Next-Generation Exoplanet Atmospheric Retrieval Framework for\n  Transmission Spectroscopy (NEXOTRANS): Comparative Characterization for\n  WASP-39 b Using JWST NIRISS, NIRSpec PRISM, and MIRI Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Next-Generation Exoplanet Atmospheric Retrieval Framework for\n  Transmission Spectroscopy (NEXOTRANS): Comparative Characterization for\n  WASP-39 b Using JWST NIRISS, NIRSpec PRISM, and MIRI Observations"
                },
                "summary": "The advent of JWST has marked a new era in exoplanetary atmospheric studies,\noffering higher-resolution data and greater precision across a broader spectral\nrange than previous space-based telescopes. Accurate analysis of these datasets\nrequires advanced retrieval frameworks capable of navigating complex parameter\nspaces. We present NEXOTRANS, an atmospheric retrieval framework that\nintegrates Bayesian inference using UltraNest/PyMultiNest with four machine\nlearning algorithms: Random Forest, Gradient Boosting, K-Nearest Neighbor, and\nStacking Regressor. This hybrid approach enables a comparison between\ntraditional Bayesian methods and computationally efficient machine learning\ntechniques. Additionally, NEXOTRANS incorporates NEXOCHEM, a module for solving\nequilibrium chemistry. We applied NEXOTRANS to JWST observations of the\nSaturn-mass exoplanet WASP-39 b, spanning wavelengths from 0.6 microns to 12.0\nmicrons using NIRISS, NIRSpec PRISM, and MIRI. Four chemistry models - free,\nequilibrium, modified hybrid equilibrium, and modified equilibrium-offset\nchemistry - were explored to retrieve precise Volume Mixing Ratios (VMRs) for\nH2O, CO2, CO, H2S, and SO2. Absorption features in both NIRSpec PRISM and MIRI\ndata constrained SO2 log VMRs to values between -6.25 and -5.73 for all models\nexcept equilibrium chemistry. High-altitude aerosols, including ZnS and MgSiO3,\nwere inferred, with constraints on their VMRs, particle sizes, and terminator\ncoverage fractions, providing insights into cloud composition. For the best-fit\nmodified hybrid equilibrium model, we derived super-solar elemental abundances\nof O/H = 14.12 (+2.86/-1.82) x solar, C/H = 21.37 (+4.93/-3.18) x solar, and\nS/H = 5.37 (+0.79/-0.65) x solar, along with a C/O ratio of 1.35 (+0.05/-0.02)\nx solar, demonstrating NEXOTRANS's potential for atmospheric characterization\nin the JWST era and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of JWST has marked a new era in exoplanetary atmospheric studies,\noffering higher-resolution data and greater precision across a broader spectral\nrange than previous space-based telescopes. Accurate analysis of these datasets\nrequires advanced retrieval frameworks capable of navigating complex parameter\nspaces. We present NEXOTRANS, an atmospheric retrieval framework that\nintegrates Bayesian inference using UltraNest/PyMultiNest with four machine\nlearning algorithms: Random Forest, Gradient Boosting, K-Nearest Neighbor, and\nStacking Regressor. This hybrid approach enables a comparison between\ntraditional Bayesian methods and computationally efficient machine learning\ntechniques. Additionally, NEXOTRANS incorporates NEXOCHEM, a module for solving\nequilibrium chemistry. We applied NEXOTRANS to JWST observations of the\nSaturn-mass exoplanet WASP-39 b, spanning wavelengths from 0.6 microns to 12.0\nmicrons using NIRISS, NIRSpec PRISM, and MIRI. Four chemistry models - free,\nequilibrium, modified hybrid equilibrium, and modified equilibrium-offset\nchemistry - were explored to retrieve precise Volume Mixing Ratios (VMRs) for\nH2O, CO2, CO, H2S, and SO2. Absorption features in both NIRSpec PRISM and MIRI\ndata constrained SO2 log VMRs to values between -6.25 and -5.73 for all models\nexcept equilibrium chemistry. High-altitude aerosols, including ZnS and MgSiO3,\nwere inferred, with constraints on their VMRs, particle sizes, and terminator\ncoverage fractions, providing insights into cloud composition. For the best-fit\nmodified hybrid equilibrium model, we derived super-solar elemental abundances\nof O/H = 14.12 (+2.86/-1.82) x solar, C/H = 21.37 (+4.93/-3.18) x solar, and\nS/H = 5.37 (+0.79/-0.65) x solar, along with a C/O ratio of 1.35 (+0.05/-0.02)\nx solar, demonstrating NEXOTRANS's potential for atmospheric characterization\nin the JWST era and beyond."
                },
                "authors": [
                    {
                        "name": "Tonmoy Deka"
                    },
                    {
                        "name": "Tasneem Basra Khan"
                    },
                    {
                        "name": "Swastik Dewan"
                    },
                    {
                        "name": "Priyankush Ghosh"
                    },
                    {
                        "name": "Debayan Das"
                    },
                    {
                        "name": "Liton Majumdar"
                    }
                ],
                "author_detail": {
                    "name": "Liton Majumdar"
                },
                "author": "Liton Majumdar",
                "arxiv_comment": "Accepted for publication in The Astrophysical Journal; 37 pages, 17\n  figures, and 5 tables. This is the final accepted version, incorporating\n  minor revisions from Version 1. Please use this version for all future\n  references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16557v2",
                "updated": "2025-06-13T11:41:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    41,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-22T11:46:46Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    11,
                    46,
                    46,
                    3,
                    142,
                    0
                ],
                "title": "Is Your LLM-Based Multi-Agent a Reliable Real-World Planner? Exploring\n  Fraud Detection in Travel Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Your LLM-Based Multi-Agent a Reliable Real-World Planner? Exploring\n  Fraud Detection in Travel Planning"
                },
                "summary": "The rise of Large Language Model-based Multi-Agent Planning has leveraged\nadvanced frameworks to enable autonomous and collaborative task execution. Some\nsystems rely on platforms like review sites and social media, which are prone\nto fraudulent information, such as fake reviews or misleading descriptions.\nThis reliance poses risks, potentially causing financial losses and harming\nuser experiences. To evaluate the risk of planning systems in real-world\napplications, we introduce \\textbf{WandaPlan}, an evaluation environment\nmirroring real-world data and injected with deceptive content. We assess system\nperformance across three fraud cases: Misinformation Fraud, Team-Coordinated\nMulti-Person Fraud, and Level-Escalating Multi-Round Fraud. We reveal\nsignificant weaknesses in existing frameworks that prioritize task efficiency\nover data authenticity. At the same time, we validate WandaPlan's\ngeneralizability, capable of assessing the risks of real-world open-source\nplanning frameworks. To mitigate the risk of fraud, we propose integrating an\nanti-fraud agent, providing a solution for reliable planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Model-based Multi-Agent Planning has leveraged\nadvanced frameworks to enable autonomous and collaborative task execution. Some\nsystems rely on platforms like review sites and social media, which are prone\nto fraudulent information, such as fake reviews or misleading descriptions.\nThis reliance poses risks, potentially causing financial losses and harming\nuser experiences. To evaluate the risk of planning systems in real-world\napplications, we introduce \\textbf{WandaPlan}, an evaluation environment\nmirroring real-world data and injected with deceptive content. We assess system\nperformance across three fraud cases: Misinformation Fraud, Team-Coordinated\nMulti-Person Fraud, and Level-Escalating Multi-Round Fraud. We reveal\nsignificant weaknesses in existing frameworks that prioritize task efficiency\nover data authenticity. At the same time, we validate WandaPlan's\ngeneralizability, capable of assessing the risks of real-world open-source\nplanning frameworks. To mitigate the risk of fraud, we propose integrating an\nanti-fraud agent, providing a solution for reliable planning."
                },
                "authors": [
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Jianhua Xu"
                    },
                    {
                        "name": "Tianyu Xin"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Shenzhe Zhu"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "arxiv_comment": "Accepted by ICML 2025 Workshop MAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11687v1",
                "updated": "2025-06-13T11:30:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    30,
                    35,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T11:30:35Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    30,
                    35,
                    4,
                    164,
                    0
                ],
                "title": "Differential Privacy in Machine Learning: From Symbolic AI to LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Privacy in Machine Learning: From Symbolic AI to LLMs"
                },
                "summary": "Machine learning models should not reveal particular information that is not\notherwise accessible. Differential privacy provides a formal framework to\nmitigate privacy risks by ensuring that the inclusion or exclusion of any\nsingle data point does not significantly alter the output of an algorithm, thus\nlimiting the exposure of private information. This survey paper explores the\nfoundational definitions of differential privacy, reviews its original\nformulations and tracing its evolution through key research contributions. It\nthen provides an in-depth examination of how DP has been integrated into\nmachine learning models, analyzing existing proposals and methods to preserve\nprivacy when training ML models. Finally, it describes how DP-based ML\ntechniques can be evaluated in practice. %Finally, it discusses the broader\nimplications of DP, highlighting its potential for public benefit, its\nreal-world applications, and the challenges it faces, including vulnerabilities\nto adversarial attacks. By offering a comprehensive overview of differential\nprivacy in machine learning, this work aims to contribute to the ongoing\ndevelopment of secure and responsible AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models should not reveal particular information that is not\notherwise accessible. Differential privacy provides a formal framework to\nmitigate privacy risks by ensuring that the inclusion or exclusion of any\nsingle data point does not significantly alter the output of an algorithm, thus\nlimiting the exposure of private information. This survey paper explores the\nfoundational definitions of differential privacy, reviews its original\nformulations and tracing its evolution through key research contributions. It\nthen provides an in-depth examination of how DP has been integrated into\nmachine learning models, analyzing existing proposals and methods to preserve\nprivacy when training ML models. Finally, it describes how DP-based ML\ntechniques can be evaluated in practice. %Finally, it discusses the broader\nimplications of DP, highlighting its potential for public benefit, its\nreal-world applications, and the challenges it faces, including vulnerabilities\nto adversarial attacks. By offering a comprehensive overview of differential\nprivacy in machine learning, this work aims to contribute to the ongoing\ndevelopment of secure and responsible AI systems."
                },
                "authors": [
                    {
                        "name": "Francisco Aguilera-Martínez"
                    },
                    {
                        "name": "Fernando Berzal"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Berzal"
                },
                "author": "Fernando Berzal",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2303.00654 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11683v1",
                "updated": "2025-06-13T11:20:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    20,
                    49,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T11:20:49Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    20,
                    49,
                    4,
                    164,
                    0
                ],
                "title": "On the performance of multi-fidelity and reduced-dimensional neural\n  emulators for inference of physiologic boundary conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the performance of multi-fidelity and reduced-dimensional neural\n  emulators for inference of physiologic boundary conditions"
                },
                "summary": "Solving inverse problems in cardiovascular modeling is particularly\nchallenging due to the high computational cost of running high-fidelity\nsimulations. In this work, we focus on Bayesian parameter estimation and\nexplore different methods to reduce the computational cost of sampling from the\nposterior distribution by leveraging low-fidelity approximations. A common\napproach is to construct a surrogate model for the high-fidelity simulation\nitself. Another is to build a surrogate for the discrepancy between high- and\nlow-fidelity models. This discrepancy, which is often easier to approximate, is\nmodeled with either a fully connected neural network or a nonlinear\ndimensionality reduction technique that enables surrogate construction in a\nlower-dimensional space. A third possible approach is to treat the discrepancy\nbetween the high-fidelity and surrogate models as random noise and estimate its\ndistribution using normalizing flows. This allows us to incorporate the\napproximation error into the Bayesian inverse problem by modifying the\nlikelihood function. We validate five different methods which are variations of\nthe above on analytical test cases by comparing them to posterior distributions\nderived solely from high-fidelity models, assessing both accuracy and\ncomputational cost. Finally, we demonstrate our approaches on two\ncardiovascular examples of increasing complexity: a lumped-parameter Windkessel\nmodel and a patient-specific three-dimensional anatomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving inverse problems in cardiovascular modeling is particularly\nchallenging due to the high computational cost of running high-fidelity\nsimulations. In this work, we focus on Bayesian parameter estimation and\nexplore different methods to reduce the computational cost of sampling from the\nposterior distribution by leveraging low-fidelity approximations. A common\napproach is to construct a surrogate model for the high-fidelity simulation\nitself. Another is to build a surrogate for the discrepancy between high- and\nlow-fidelity models. This discrepancy, which is often easier to approximate, is\nmodeled with either a fully connected neural network or a nonlinear\ndimensionality reduction technique that enables surrogate construction in a\nlower-dimensional space. A third possible approach is to treat the discrepancy\nbetween the high-fidelity and surrogate models as random noise and estimate its\ndistribution using normalizing flows. This allows us to incorporate the\napproximation error into the Bayesian inverse problem by modifying the\nlikelihood function. We validate five different methods which are variations of\nthe above on analytical test cases by comparing them to posterior distributions\nderived solely from high-fidelity models, assessing both accuracy and\ncomputational cost. Finally, we demonstrate our approaches on two\ncardiovascular examples of increasing complexity: a lumped-parameter Windkessel\nmodel and a patient-specific three-dimensional anatomy."
                },
                "authors": [
                    {
                        "name": "Chloe H. Choi"
                    },
                    {
                        "name": "Andrea Zanoni"
                    },
                    {
                        "name": "Daniele E. Schiavazzi"
                    },
                    {
                        "name": "Alison L. Marsden"
                    }
                ],
                "author_detail": {
                    "name": "Alison L. Marsden"
                },
                "author": "Alison L. Marsden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11681v1",
                "updated": "2025-06-13T11:19:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    19,
                    27,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T11:19:27Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    19,
                    27,
                    4,
                    164,
                    0
                ],
                "title": "LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting\n  Approach"
                },
                "summary": "This paper addresses the challenge of transforming complex sentences into\nsequences of logical, simplified sentences while preserving semantic and\nlogical integrity with the help of Large Language Models. We propose a hybrid\napproach that combines advanced prompting with multi-agent architectures to\nenhance the sentence simplification process. Experimental results show that our\napproach was able to successfully simplify 70% of the complex sentences written\nfor video game design application. In comparison, a single-agent approach\nattained a 48% success rate on the same task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of transforming complex sentences into\nsequences of logical, simplified sentences while preserving semantic and\nlogical integrity with the help of Large Language Models. We propose a hybrid\napproach that combines advanced prompting with multi-agent architectures to\nenhance the sentence simplification process. Experimental results show that our\napproach was able to successfully simplify 70% of the complex sentences written\nfor video game design application. In comparison, a single-agent approach\nattained a 48% success rate on the same task."
                },
                "authors": [
                    {
                        "name": "Pratibha Zunjare"
                    },
                    {
                        "name": "Michael Hsiao"
                    }
                ],
                "author_detail": {
                    "name": "Michael Hsiao"
                },
                "author": "Michael Hsiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11680v1",
                "updated": "2025-06-13T11:19:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    19,
                    21,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T11:19:21Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    19,
                    21,
                    4,
                    164,
                    0
                ],
                "title": "Malicious LLM-Based Conversational AI Makes Users Reveal Personal\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malicious LLM-Based Conversational AI Makes Users Reveal Personal\n  Information"
                },
                "summary": "LLM-based Conversational AIs (CAIs), also known as GenAI chatbots, like\nChatGPT, are increasingly used across various domains, but they pose privacy\nrisks, as users may disclose personal information during their conversations\nwith CAIs. Recent research has demonstrated that LLM-based CAIs could be used\nfor malicious purposes. However, a novel and particularly concerning type of\nmalicious LLM application remains unexplored: an LLM-based CAI that is\ndeliberately designed to extract personal information from users.\n  In this paper, we report on the malicious LLM-based CAIs that we created\nbased on system prompts that used different strategies to encourage disclosures\nof personal information from users. We systematically investigate CAIs' ability\nto extract personal information from users during conversations by conducting a\nrandomized-controlled trial with 502 participants. We assess the effectiveness\nof different malicious and benign CAIs to extract personal information from\nparticipants, and we analyze participants' perceptions after their interactions\nwith the CAIs. Our findings reveal that malicious CAIs extract significantly\nmore personal information than benign CAIs, with strategies based on the social\nnature of privacy being the most effective while minimizing perceived risks.\nThis study underscores the privacy threats posed by this novel type of\nmalicious LLM-based CAIs and provides actionable recommendations to guide\nfuture research and practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Conversational AIs (CAIs), also known as GenAI chatbots, like\nChatGPT, are increasingly used across various domains, but they pose privacy\nrisks, as users may disclose personal information during their conversations\nwith CAIs. Recent research has demonstrated that LLM-based CAIs could be used\nfor malicious purposes. However, a novel and particularly concerning type of\nmalicious LLM application remains unexplored: an LLM-based CAI that is\ndeliberately designed to extract personal information from users.\n  In this paper, we report on the malicious LLM-based CAIs that we created\nbased on system prompts that used different strategies to encourage disclosures\nof personal information from users. We systematically investigate CAIs' ability\nto extract personal information from users during conversations by conducting a\nrandomized-controlled trial with 502 participants. We assess the effectiveness\nof different malicious and benign CAIs to extract personal information from\nparticipants, and we analyze participants' perceptions after their interactions\nwith the CAIs. Our findings reveal that malicious CAIs extract significantly\nmore personal information than benign CAIs, with strategies based on the social\nnature of privacy being the most effective while minimizing perceived risks.\nThis study underscores the privacy threats posed by this novel type of\nmalicious LLM-based CAIs and provides actionable recommendations to guide\nfuture research and practice."
                },
                "authors": [
                    {
                        "name": "Xiao Zhan"
                    },
                    {
                        "name": "Juan Carlos Carrillo"
                    },
                    {
                        "name": "William Seymour"
                    },
                    {
                        "name": "Jose Such"
                    }
                ],
                "author_detail": {
                    "name": "Jose Such"
                },
                "author": "Jose Such",
                "arxiv_comment": "This paper has been accepted at USENIX Security '25",
                "arxiv_journal_ref": "USENIX Security 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08754v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08754v4",
                "updated": "2025-06-13T11:19:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    19,
                    12,
                    4,
                    164,
                    0
                ],
                "published": "2025-03-28T15:49:52Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    49,
                    52,
                    4,
                    87,
                    0
                ],
                "title": "Towards Personalized Conversational Sales Agents: Contextual User\n  Profiling for Strategic Action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Personalized Conversational Sales Agents: Contextual User\n  Profiling for Strategic Action"
                },
                "summary": "Conversational Recommender Systems (CRSs)aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To capture this complexity, we introduce Conversational\nSales (CSALES), a novel task that integrates preference elicitation,\nrecommendation, and persuasion within a unified conversational framework. To\nsupport realistic and systematic evaluation, we present CSUSER, an evaluation\nprotocol with LLM-based user simulator grounded in real-world behavioral data\nby modeling fine-grained user profiles for personalized interaction. We also\npropose CSI, a conversational sales agent that proactively infers contextual\nuser profiles and strategically selects actions through conversation.\nComprehensive experiments show that CSI significantly improves both\nrecommendation success and persuasive effectiveness across diverse user\nprofiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Recommender Systems (CRSs)aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To capture this complexity, we introduce Conversational\nSales (CSALES), a novel task that integrates preference elicitation,\nrecommendation, and persuasion within a unified conversational framework. To\nsupport realistic and systematic evaluation, we present CSUSER, an evaluation\nprotocol with LLM-based user simulator grounded in real-world behavioral data\nby modeling fine-grained user profiles for personalized interaction. We also\npropose CSI, a conversational sales agent that proactively infers contextual\nuser profiles and strategically selects actions through conversation.\nComprehensive experiments show that CSI significantly improves both\nrecommendation success and persuasive effectiveness across diverse user\nprofiles."
                },
                "authors": [
                    {
                        "name": "Tongyoung Kim"
                    },
                    {
                        "name": "Jeongeun Lee"
                    },
                    {
                        "name": "Soojin Yoon"
                    },
                    {
                        "name": "Sunghwan Kim"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08754v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08754v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11679v1",
                "updated": "2025-06-13T11:17:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    17,
                    15,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T11:17:15Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    17,
                    15,
                    4,
                    164,
                    0
                ],
                "title": "LLMs on support of privacy and security of mobile apps: state of the art\n  and research directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs on support of privacy and security of mobile apps: state of the art\n  and research directions"
                },
                "summary": "Modern life has witnessed the explosion of mobile devices. However, besides\nthe valuable features that bring convenience to end users, security and privacy\nrisks still threaten users of mobile apps. The increasing sophistication of\nthese threats in recent years has underscored the need for more advanced and\nefficient detection approaches. In this chapter, we explore the application of\nLarge Language Models (LLMs) to identify security risks and privacy violations\nand mitigate them for the mobile application ecosystem. By introducing\nstate-of-the-art research that applied LLMs to mitigate the top 10 common\nsecurity risks of smartphone platforms, we highlight the feasibility and\npotential of LLMs to replace traditional analysis methods, such as dynamic and\nhybrid analysis of mobile apps. As a representative example of LLM-based\nsolutions, we present an approach to detect sensitive data leakage when users\nshare images online, a common behavior of smartphone users nowadays. Finally,\nwe discuss open research challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern life has witnessed the explosion of mobile devices. However, besides\nthe valuable features that bring convenience to end users, security and privacy\nrisks still threaten users of mobile apps. The increasing sophistication of\nthese threats in recent years has underscored the need for more advanced and\nefficient detection approaches. In this chapter, we explore the application of\nLarge Language Models (LLMs) to identify security risks and privacy violations\nand mitigate them for the mobile application ecosystem. By introducing\nstate-of-the-art research that applied LLMs to mitigate the top 10 common\nsecurity risks of smartphone platforms, we highlight the feasibility and\npotential of LLMs to replace traditional analysis methods, such as dynamic and\nhybrid analysis of mobile apps. As a representative example of LLM-based\nsolutions, we present an approach to detect sensitive data leakage when users\nshare images online, a common behavior of smartphone users nowadays. Finally,\nwe discuss open research challenges."
                },
                "authors": [
                    {
                        "name": "Tran Thanh Lam Nguyen"
                    },
                    {
                        "name": "Barbara Carminati"
                    },
                    {
                        "name": "Elena Ferrari"
                    }
                ],
                "author_detail": {
                    "name": "Elena Ferrari"
                },
                "author": "Elena Ferrari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09096v2",
                "updated": "2025-06-13T11:11:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    11,
                    52,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-10T12:59:14Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    12,
                    59,
                    14,
                    1,
                    161,
                    0
                ],
                "title": "Intra-Trajectory Consistency for Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intra-Trajectory Consistency for Reward Modeling"
                },
                "summary": "Reward models are critical for improving large language models (LLMs),\nparticularly in reinforcement learning from human feedback (RLHF) or\ninference-time verification. Current reward modeling typically relies on scores\nof overall responses to learn the outcome rewards for the responses. However,\nsince the response-level scores are coarse-grained supervision signals, the\nreward model struggles to identify the specific components within a response\ntrajectory that truly correlate with the scores, leading to poor generalization\non unseen responses. In this paper, we propose to leverage generation\nprobabilities to establish reward consistency between processes in the response\ntrajectory, which allows the response-level supervisory signal to propagate\nacross processes, thereby providing additional fine-grained signals for reward\nlearning. Building on analysis under the Bayesian framework, we develop an\nintra-trajectory consistency regularization to enforce that adjacent processes\nwith higher next-token generation probability maintain more consistent rewards.\nWe apply the proposed regularization to the advanced outcome reward model,\nimproving its performance on RewardBench. Besides, we show that the reward\nmodel trained with the proposed regularization induces better DPO-aligned\npolicies and achieves better best-of-N (BON) inference-time verification\nresults. Our code is provided in https://github.com/chaoyang101/ICRM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models are critical for improving large language models (LLMs),\nparticularly in reinforcement learning from human feedback (RLHF) or\ninference-time verification. Current reward modeling typically relies on scores\nof overall responses to learn the outcome rewards for the responses. However,\nsince the response-level scores are coarse-grained supervision signals, the\nreward model struggles to identify the specific components within a response\ntrajectory that truly correlate with the scores, leading to poor generalization\non unseen responses. In this paper, we propose to leverage generation\nprobabilities to establish reward consistency between processes in the response\ntrajectory, which allows the response-level supervisory signal to propagate\nacross processes, thereby providing additional fine-grained signals for reward\nlearning. Building on analysis under the Bayesian framework, we develop an\nintra-trajectory consistency regularization to enforce that adjacent processes\nwith higher next-token generation probability maintain more consistent rewards.\nWe apply the proposed regularization to the advanced outcome reward model,\nimproving its performance on RewardBench. Besides, we show that the reward\nmodel trained with the proposed regularization induces better DPO-aligned\npolicies and achieves better best-of-N (BON) inference-time verification\nresults. Our code is provided in https://github.com/chaoyang101/ICRM."
                },
                "authors": [
                    {
                        "name": "Chaoyang Zhou"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21227v2",
                "updated": "2025-06-13T11:04:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    4,
                    13,
                    4,
                    164,
                    0
                ],
                "published": "2025-03-27T07:36:11Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    36,
                    11,
                    3,
                    86,
                    0
                ],
                "title": "LLaVA-CMoE: Towards Continual Mixture of Experts for Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-CMoE: Towards Continual Mixture of Experts for Large\n  Vision-Language Models"
                },
                "summary": "Mixture of Experts (MoE) architectures have recently advanced the scalability\nand adaptability of large language models (LLMs) for continual multimodal\nlearning. However, efficiently extending these models to accommodate sequential\ntasks remains challenging. As new tasks arrive, naive model expansion leads to\nrapid parameter growth, while modifying shared routing components often causes\ncatastrophic forgetting, undermining previously learned knowledge. To address\nthese issues, we propose LLaVA-CMoE, a continual learning framework for LLMs\nthat requires no replay data of previous tasks and ensures both parameter\nefficiency and robust knowledge retention. Our approach introduces a\nProbe-Guided Knowledge Extension mechanism, which uses probe experts to\ndynamically determine when and where new experts should be added, enabling\nadaptive and minimal parameter expansion tailored to task complexity.\nFurthermore, we present a Probabilistic Task Locator that assigns each task a\ndedicated, lightweight router. To handle the practical issue that task labels\nare unknown during inference, we leverage a VAE-based reconstruction strategy\nto identify the most suitable router by matching input distributions, allowing\nautomatic and accurate expert allocation. This design mitigates routing\nconflicts and catastrophic forgetting, enabling robust continual learning\nwithout explicit task labels. Extensive experiments on the CoIN benchmark,\ncovering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong\ncontinual learning performance with a compact model size, significantly\nreducing forgetting and parameter overhead compared to prior methods. These\nresults showcase the effectiveness and scalability of our approach for\nparameter-efficient continual learning in large language models. Our code will\nbe open-sourced soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) architectures have recently advanced the scalability\nand adaptability of large language models (LLMs) for continual multimodal\nlearning. However, efficiently extending these models to accommodate sequential\ntasks remains challenging. As new tasks arrive, naive model expansion leads to\nrapid parameter growth, while modifying shared routing components often causes\ncatastrophic forgetting, undermining previously learned knowledge. To address\nthese issues, we propose LLaVA-CMoE, a continual learning framework for LLMs\nthat requires no replay data of previous tasks and ensures both parameter\nefficiency and robust knowledge retention. Our approach introduces a\nProbe-Guided Knowledge Extension mechanism, which uses probe experts to\ndynamically determine when and where new experts should be added, enabling\nadaptive and minimal parameter expansion tailored to task complexity.\nFurthermore, we present a Probabilistic Task Locator that assigns each task a\ndedicated, lightweight router. To handle the practical issue that task labels\nare unknown during inference, we leverage a VAE-based reconstruction strategy\nto identify the most suitable router by matching input distributions, allowing\nautomatic and accurate expert allocation. This design mitigates routing\nconflicts and catastrophic forgetting, enabling robust continual learning\nwithout explicit task labels. Extensive experiments on the CoIN benchmark,\ncovering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong\ncontinual learning performance with a compact model size, significantly\nreducing forgetting and parameter overhead compared to prior methods. These\nresults showcase the effectiveness and scalability of our approach for\nparameter-efficient continual learning in large language models. Our code will\nbe open-sourced soon."
                },
                "authors": [
                    {
                        "name": "Hengyuan Zhao"
                    },
                    {
                        "name": "Ziqin Wang"
                    },
                    {
                        "name": "Qixin Sun"
                    },
                    {
                        "name": "Kaiyou Song"
                    },
                    {
                        "name": "Yilin Li"
                    },
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Qingpei Guo"
                    },
                    {
                        "name": "Si Liu"
                    }
                ],
                "author_detail": {
                    "name": "Si Liu"
                },
                "author": "Si Liu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01600v2",
                "updated": "2025-06-13T10:59:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    59,
                    12,
                    4,
                    164,
                    0
                ],
                "published": "2024-12-02T15:21:08Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    15,
                    21,
                    8,
                    0,
                    337,
                    0
                ],
                "title": "An implementation of neural simulation-based inference for parameter\n  estimation in ATLAS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An implementation of neural simulation-based inference for parameter\n  estimation in ATLAS"
                },
                "summary": "Neural simulation-based inference is a powerful class of\nmachine-learning-based methods for statistical inference that naturally handles\nhigh-dimensional parameter estimation without the need to bin data into\nlow-dimensional summary histograms. Such methods are promising for a range of\nmeasurements, including at the Large Hadron Collider, where no single\nobservable may be optimal to scan over the entire theoretical phase space under\nconsideration, or where binning data into histograms could result in a loss of\nsensitivity. This work develops a neural simulation-based inference framework\nfor statistical inference, using neural networks to estimate probability\ndensity ratios, which enables the application to a full-scale analysis. It\nincorporates a large number of systematic uncertainties, quantifies the\nuncertainty due to the finite number of events in training samples, develops a\nmethod to construct confidence intervals, and demonstrates a series of\nintermediate diagnostic checks that can be performed to validate the robustness\nof the method. As an example, the power and feasibility of the method are\nassessed on simulated data for a simplified version of an off-shell Higgs boson\ncouplings measurement in the four-lepton final states. This approach represents\nan extension to the standard statistical methodology used by the experiments at\nthe Large Hadron Collider, and can benefit many physics analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural simulation-based inference is a powerful class of\nmachine-learning-based methods for statistical inference that naturally handles\nhigh-dimensional parameter estimation without the need to bin data into\nlow-dimensional summary histograms. Such methods are promising for a range of\nmeasurements, including at the Large Hadron Collider, where no single\nobservable may be optimal to scan over the entire theoretical phase space under\nconsideration, or where binning data into histograms could result in a loss of\nsensitivity. This work develops a neural simulation-based inference framework\nfor statistical inference, using neural networks to estimate probability\ndensity ratios, which enables the application to a full-scale analysis. It\nincorporates a large number of systematic uncertainties, quantifies the\nuncertainty due to the finite number of events in training samples, develops a\nmethod to construct confidence intervals, and demonstrates a series of\nintermediate diagnostic checks that can be performed to validate the robustness\nof the method. As an example, the power and feasibility of the method are\nassessed on simulated data for a simplified version of an off-shell Higgs boson\ncouplings measurement in the four-lepton final states. This approach represents\nan extension to the standard statistical methodology used by the experiments at\nthe Large Hadron Collider, and can benefit many physics analyses."
                },
                "authors": [
                    {
                        "name": "ATLAS Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "ATLAS Collaboration"
                },
                "author": "ATLAS Collaboration",
                "arxiv_doi": "10.1088/1361-6633/add370",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1361-6633/add370",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "50 pages in total, author list starting page 33, 8 figures, 1 table,\n  published by Rep. Prog. Phys. All figures including auxiliary figures are\n  available at\n  https://atlas.web.cern.ch/Atlas/GROUPS/PHYSICS/PAPERS/SOFT-2023-01",
                "arxiv_journal_ref": "Rep. Prog. Phys. 88 (2025) 067801",
                "arxiv_primary_category": {
                    "term": "physics.data-an",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11666v1",
                "updated": "2025-06-13T10:53:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    53,
                    50,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T10:53:50Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    53,
                    50,
                    4,
                    164,
                    0
                ],
                "title": "Converting Annotated Clinical Cases into Structured Case Report Forms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Converting Annotated Clinical Cases into Structured Case Report Forms"
                },
                "summary": "Case Report Forms (CRFs) are largely used in medical research as they ensure\naccuracy, reliability, and validity of results in clinical studies. However,\npublicly available, wellannotated CRF datasets are scarce, limiting the\ndevelopment of CRF slot filling systems able to fill in a CRF from clinical\nnotes. To mitigate the scarcity of CRF datasets, we propose to take advantage\nof available datasets annotated for information extraction tasks and to convert\nthem into structured CRFs. We present a semi-automatic conversion methodology,\nwhich has been applied to the E3C dataset in two languages (English and\nItalian), resulting in a new, high-quality dataset for CRF slot filling.\nThrough several experiments on the created dataset, we report that slot filling\nachieves 59.7% for Italian and 67.3% for English on a closed Large Language\nModels (zero-shot) and worse performances on three families of open-source\nmodels, showing that filling CRFs is challenging even for recent\nstate-of-the-art LLMs. We release the datest at\nhttps://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Case Report Forms (CRFs) are largely used in medical research as they ensure\naccuracy, reliability, and validity of results in clinical studies. However,\npublicly available, wellannotated CRF datasets are scarce, limiting the\ndevelopment of CRF slot filling systems able to fill in a CRF from clinical\nnotes. To mitigate the scarcity of CRF datasets, we propose to take advantage\nof available datasets annotated for information extraction tasks and to convert\nthem into structured CRFs. We present a semi-automatic conversion methodology,\nwhich has been applied to the E3C dataset in two languages (English and\nItalian), resulting in a new, high-quality dataset for CRF slot filling.\nThrough several experiments on the created dataset, we report that slot filling\nachieves 59.7% for Italian and 67.3% for English on a closed Large Language\nModels (zero-shot) and worse performances on three families of open-source\nmodels, showing that filling CRFs is challenging even for recent\nstate-of-the-art LLMs. We release the datest at\nhttps://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166"
                },
                "authors": [
                    {
                        "name": "Pietro Ferrazzi"
                    },
                    {
                        "name": "Alberto Lavelli"
                    },
                    {
                        "name": "Bernardo Magnini"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Magnini"
                },
                "author": "Bernardo Magnini",
                "arxiv_comment": "to be published in BioNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11663v1",
                "updated": "2025-06-13T10:49:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    49,
                    17,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T10:49:17Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    49,
                    17,
                    4,
                    164,
                    0
                ],
                "title": "Identification and Inference of Partial Effects in Sharp Regression Kink\n  Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identification and Inference of Partial Effects in Sharp Regression Kink\n  Designs"
                },
                "summary": "The partial effect refers to the impact of a change in a target variable D on\nthe distribution of an outcome variable Y . This study examines the\nidentification and inference of a wide range of partial effects at the\nthreshold in the sharp regression kink (RK) design under general policy\ninterventions. We establish a unifying framework for conducting inference on\nthe effect of an infinitesimal change in D on smooth functionals of the\ndistribution of Y, particularly when D is endogenous and instrumental variables\nare unavailable. This framework yields a general formula that clarifies the\ncausal interpretation of numerous existing sharp RK estimands in the\nliterature.\n  We develop the relevant asymptotic theory, introduce a multiplier bootstrap\nprocedure for inference, and provide practical implementation guidelines.\nApplying our method to the effect of unemployment insurance (UI) benefits on\nunemployment duration, we find that while higher benefits lead to longer\ndurations, they also tend to reduce their dispersion. Furthermore, our results\nshow that the magnitude of the partial effect can change substantially\ndepending on the specific form of the policy intervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The partial effect refers to the impact of a change in a target variable D on\nthe distribution of an outcome variable Y . This study examines the\nidentification and inference of a wide range of partial effects at the\nthreshold in the sharp regression kink (RK) design under general policy\ninterventions. We establish a unifying framework for conducting inference on\nthe effect of an infinitesimal change in D on smooth functionals of the\ndistribution of Y, particularly when D is endogenous and instrumental variables\nare unavailable. This framework yields a general formula that clarifies the\ncausal interpretation of numerous existing sharp RK estimands in the\nliterature.\n  We develop the relevant asymptotic theory, introduce a multiplier bootstrap\nprocedure for inference, and provide practical implementation guidelines.\nApplying our method to the effect of unemployment insurance (UI) benefits on\nunemployment duration, we find that while higher benefits lead to longer\ndurations, they also tend to reduce their dispersion. Furthermore, our results\nshow that the magnitude of the partial effect can change substantially\ndepending on the specific form of the policy intervention."
                },
                "authors": [
                    {
                        "name": "Zhixin Wang"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhengyu Zhang"
                },
                "author": "Zhengyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11659v1",
                "updated": "2025-06-13T10:40:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    40,
                    23,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T10:40:23Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    40,
                    23,
                    4,
                    164,
                    0
                ],
                "title": "An Empirical study on LLM-based Log Retrieval for Software Engineering\n  Metadata Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical study on LLM-based Log Retrieval for Software Engineering\n  Metadata Management"
                },
                "summary": "Developing autonomous driving systems (ADSs) involves generating and storing\nextensive log data from test drives, which is essential for verification,\nresearch, and simulation. However, these high-frequency logs, recorded over\nvarying durations, pose challenges for developers attempting to locate specific\ndriving scenarios. This difficulty arises due to the wide range of signals\nrepresenting various vehicle components and driving conditions, as well as\nunfamiliarity of some developers' with the detailed meaning of these signals.\nTraditional SQL-based querying exacerbates this challenge by demanding both\ndomain expertise and database knowledge, often yielding results that are\ndifficult to verify for accuracy.\n  This paper introduces a Large Language Model (LLM)-supported approach that\ncombines signal log data with video recordings from test drives, enabling\nnatural language based scenario searches while reducing the need for\nspecialized knowledge. By leveraging scenario distance graphs and relative gap\nindicators, it provides quantifiable metrics to evaluate the reliability of\nquery results. The method is implemented as an API for efficient database\nquerying and retrieval of relevant records, paired with video frames for\nintuitive visualization. Evaluation on an open industrial dataset demonstrates\nimproved efficiency and reliability in scenario retrieval, eliminating\ndependency on a single data source and conventional SQL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing autonomous driving systems (ADSs) involves generating and storing\nextensive log data from test drives, which is essential for verification,\nresearch, and simulation. However, these high-frequency logs, recorded over\nvarying durations, pose challenges for developers attempting to locate specific\ndriving scenarios. This difficulty arises due to the wide range of signals\nrepresenting various vehicle components and driving conditions, as well as\nunfamiliarity of some developers' with the detailed meaning of these signals.\nTraditional SQL-based querying exacerbates this challenge by demanding both\ndomain expertise and database knowledge, often yielding results that are\ndifficult to verify for accuracy.\n  This paper introduces a Large Language Model (LLM)-supported approach that\ncombines signal log data with video recordings from test drives, enabling\nnatural language based scenario searches while reducing the need for\nspecialized knowledge. By leveraging scenario distance graphs and relative gap\nindicators, it provides quantifiable metrics to evaluate the reliability of\nquery results. The method is implemented as an API for efficient database\nquerying and retrieval of relevant records, paired with video frames for\nintuitive visualization. Evaluation on an open industrial dataset demonstrates\nimproved efficiency and reliability in scenario retrieval, eliminating\ndependency on a single data source and conventional SQL."
                },
                "authors": [
                    {
                        "name": "Simin Sun"
                    },
                    {
                        "name": "Yuchuan Jin"
                    },
                    {
                        "name": "Miroslaw Staron"
                    }
                ],
                "author_detail": {
                    "name": "Miroslaw Staron"
                },
                "author": "Miroslaw Staron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11638v1",
                "updated": "2025-06-13T10:11:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    11,
                    1,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T10:11:01Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    11,
                    1,
                    4,
                    164,
                    0
                ],
                "title": "LoRA-Gen: Specializing Large Language Model via Online LoRA Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRA-Gen: Specializing Large Language Model via Online LoRA Generation"
                },
                "summary": "Recent advances have highlighted the benefits of scaling language models to\nenhance performance across a wide range of NLP tasks. However, these approaches\nstill face limitations in effectiveness and efficiency when applied to\ndomain-specific tasks, particularly for small edge-side models. We propose the\nLoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA\nparameters for edge-side models based on task descriptions. By employing the\nreparameterization technique, we merge the LoRA parameters into the edge-side\nmodel to achieve flexible specialization. Our method facilitates knowledge\ntransfer between models while significantly improving the inference efficiency\nof the specialized model by reducing the input context length. Without\nspecialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which\nachieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in\nreasoning tasks. Besides, our method delivers a compression ratio of 10.1x with\nGemma-2B on intelligent agent tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have highlighted the benefits of scaling language models to\nenhance performance across a wide range of NLP tasks. However, these approaches\nstill face limitations in effectiveness and efficiency when applied to\ndomain-specific tasks, particularly for small edge-side models. We propose the\nLoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA\nparameters for edge-side models based on task descriptions. By employing the\nreparameterization technique, we merge the LoRA parameters into the edge-side\nmodel to achieve flexible specialization. Our method facilitates knowledge\ntransfer between models while significantly improving the inference efficiency\nof the specialized model by reducing the input context length. Without\nspecialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which\nachieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in\nreasoning tasks. Besides, our method delivers a compression ratio of 10.1x with\nGemma-2B on intelligent agent tasks."
                },
                "authors": [
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Lin Song"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Cheng Cheng"
                    },
                    {
                        "name": "Yixiao Ge"
                    },
                    {
                        "name": "Xiu Li"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08967v2",
                "updated": "2025-06-13T10:07:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    7,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-10T16:37:39Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    16,
                    37,
                    39,
                    1,
                    161,
                    0
                ],
                "title": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language\n  Model"
                },
                "summary": "Large Audio-Language Models (LALMs) have significantly advanced intelligent\nhuman-computer interaction, yet their reliance on text-based outputs limits\ntheir ability to generate natural speech responses directly, hindering seamless\naudio interactions. To address this, we introduce Step-Audio-AQAA, a fully\nend-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model\nintegrates a dual-codebook audio tokenizer for linguistic and semantic feature\nextraction, a 130-billion-parameter backbone LLM and a neural vocoder for\nhigh-fidelity speech synthesis. Our post-training approach employs interleaved\ntoken-output of text and audio to enhance semantic coherence and combines\nDirect Preference Optimization (DPO) with model merge to improve performance.\nEvaluations on the StepEval-Audio-360 benchmark demonstrate that\nStep-Audio-AQAA excels especially in speech control, outperforming the\nstate-of-art LALMs in key areas. This work contributes a promising solution for\nend-to-end LALMs and highlights the critical role of token-based vocoder in\nenhancing overall performance for AQAA tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Audio-Language Models (LALMs) have significantly advanced intelligent\nhuman-computer interaction, yet their reliance on text-based outputs limits\ntheir ability to generate natural speech responses directly, hindering seamless\naudio interactions. To address this, we introduce Step-Audio-AQAA, a fully\nend-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model\nintegrates a dual-codebook audio tokenizer for linguistic and semantic feature\nextraction, a 130-billion-parameter backbone LLM and a neural vocoder for\nhigh-fidelity speech synthesis. Our post-training approach employs interleaved\ntoken-output of text and audio to enhance semantic coherence and combines\nDirect Preference Optimization (DPO) with model merge to improve performance.\nEvaluations on the StepEval-Audio-360 benchmark demonstrate that\nStep-Audio-AQAA excels especially in speech control, outperforming the\nstate-of-art LALMs in key areas. This work contributes a promising solution for\nend-to-end LALMs and highlights the critical role of token-based vocoder in\nenhancing overall performance for AQAA tasks."
                },
                "authors": [
                    {
                        "name": "Ailin Huang"
                    },
                    {
                        "name": "Bingxin Li"
                    },
                    {
                        "name": "Bruce Wang"
                    },
                    {
                        "name": "Boyong Wu"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Chengli Feng"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Jingbei Li"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Joanna Wang"
                    },
                    {
                        "name": "Mingrui Chen"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Tian Fei"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xuerui Yang"
                    },
                    {
                        "name": "Yechang Huang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Buyun Ma"
                    },
                    {
                        "name": "Changxin Miao"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Dapeng Shi"
                    },
                    {
                        "name": "Dingyuan Hu"
                    },
                    {
                        "name": "Enle Liu"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Gulin Yan"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Haonan Jia"
                    },
                    {
                        "name": "Jiahao Gong"
                    },
                    {
                        "name": "Jiaoren Wu"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Junzhe Lin"
                    },
                    {
                        "name": "Kaixiang Li"
                    },
                    {
                        "name": "Lei Xia"
                    },
                    {
                        "name": "Longlong Gu"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Nie Hao"
                    },
                    {
                        "name": "Ranchen Ming"
                    },
                    {
                        "name": "Shaoliang Pang"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Tiancheng Cao"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Wenqing He"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Xuelin Zhang"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yuanwei Liang"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Zidong Yang"
                    },
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Jiansheng Chen"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Xinhao Zhang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Shuchang Zhou"
                    },
                    {
                        "name": "Chen Hu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Hu"
                },
                "author": "Chen Hu",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11635v1",
                "updated": "2025-06-13T10:05:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    5,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T10:05:43Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    5,
                    43,
                    4,
                    164,
                    0
                ],
                "title": "FAA Framework: A Large Language Model-Based Approach for Credit Card\n  Fraud Investigations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAA Framework: A Large Language Model-Based Approach for Credit Card\n  Fraud Investigations"
                },
                "summary": "The continuous growth of the e-commerce industry attracts fraudsters who\nexploit stolen credit card details. Companies often investigate suspicious\ntransactions in order to retain customer trust and address gaps in their fraud\ndetection systems. However, analysts are overwhelmed with an enormous number of\nalerts from credit card transaction monitoring systems. Each alert\ninvestigation requires from the fraud analysts careful attention, specialized\nknowledge, and precise documentation of the outcomes, leading to alert fatigue.\nTo address this, we propose a fraud analyst assistant (FAA) framework, which\nemploys multi-modal large language models (LLMs) to automate credit card fraud\ninvestigations and generate explanatory reports. The FAA framework leverages\nthe reasoning, code execution, and vision capabilities of LLMs to conduct\nplanning, evidence collection, and analysis in each investigation step. A\ncomprehensive empirical evaluation of 500 credit card fraud investigations\ndemonstrates that the FAA framework produces reliable and efficient\ninvestigations comprising seven steps on average. Thus we found that the FAA\nframework can automate large parts of the workload and help reduce the\nchallenges faced by fraud analysts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The continuous growth of the e-commerce industry attracts fraudsters who\nexploit stolen credit card details. Companies often investigate suspicious\ntransactions in order to retain customer trust and address gaps in their fraud\ndetection systems. However, analysts are overwhelmed with an enormous number of\nalerts from credit card transaction monitoring systems. Each alert\ninvestigation requires from the fraud analysts careful attention, specialized\nknowledge, and precise documentation of the outcomes, leading to alert fatigue.\nTo address this, we propose a fraud analyst assistant (FAA) framework, which\nemploys multi-modal large language models (LLMs) to automate credit card fraud\ninvestigations and generate explanatory reports. The FAA framework leverages\nthe reasoning, code execution, and vision capabilities of LLMs to conduct\nplanning, evidence collection, and analysis in each investigation step. A\ncomprehensive empirical evaluation of 500 credit card fraud investigations\ndemonstrates that the FAA framework produces reliable and efficient\ninvestigations comprising seven steps on average. Thus we found that the FAA\nframework can automate large parts of the workload and help reduce the\nchallenges faced by fraud analysts."
                },
                "authors": [
                    {
                        "name": "Shaun Shuster"
                    },
                    {
                        "name": "Eyal Zaloof"
                    },
                    {
                        "name": "Asaf Shabtai"
                    },
                    {
                        "name": "Rami Puzis"
                    }
                ],
                "author_detail": {
                    "name": "Rami Puzis"
                },
                "author": "Rami Puzis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06634v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06634v2",
                "updated": "2025-06-13T10:03:47Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    3,
                    47,
                    4,
                    164,
                    0
                ],
                "published": "2025-01-11T20:39:04Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    20,
                    39,
                    4,
                    5,
                    11,
                    0
                ],
                "title": "Fast Approximate Solution of Stein Equations for Post-Processing of MCMC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Approximate Solution of Stein Equations for Post-Processing of MCMC"
                },
                "summary": "Bayesian inference is conceptually elegant, but calculating posterior\nexpectations can entail a heavy computational cost. Monte Carlo methods are\nreliable and supported by strong asymptotic guarantees, but do not leverage\nsmoothness of the integrand. Solving Stein equations has emerged as a possible\nalternative, providing a framework for numerical approximation of posterior\nexpectations in which smoothness can be exploited. However, existing numerical\nmethods for Stein equations are associated with high computational cost due to\nthe need to solve large linear systems. This paper considers the combination of\niterative linear solvers and preconditioning strategies to obtain fast\napproximate solutions of Stein equations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference is conceptually elegant, but calculating posterior\nexpectations can entail a heavy computational cost. Monte Carlo methods are\nreliable and supported by strong asymptotic guarantees, but do not leverage\nsmoothness of the integrand. Solving Stein equations has emerged as a possible\nalternative, providing a framework for numerical approximation of posterior\nexpectations in which smoothness can be exploited. However, existing numerical\nmethods for Stein equations are associated with high computational cost due to\nthe need to solve large linear systems. This paper considers the combination of\niterative linear solvers and preconditioning strategies to obtain fast\napproximate solutions of Stein equations."
                },
                "authors": [
                    {
                        "name": "Qingyang Liu"
                    },
                    {
                        "name": "Heishiro Kanagawa"
                    },
                    {
                        "name": "Matthew A. Fisher"
                    },
                    {
                        "name": "François-Xavier Briol"
                    },
                    {
                        "name": "Chris. J. Oates"
                    }
                ],
                "author_detail": {
                    "name": "Chris. J. Oates"
                },
                "author": "Chris. J. Oates",
                "arxiv_comment": "Bugs fixed, additional experiments included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06634v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06634v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11631v1",
                "updated": "2025-06-13T10:02:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    2,
                    39,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T10:02:39Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    2,
                    39,
                    4,
                    164,
                    0
                ],
                "title": "SceneGram: Conceptualizing and Describing Tangrams in Scene Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SceneGram: Conceptualizing and Describing Tangrams in Scene Context"
                },
                "summary": "Research on reference and naming suggests that humans can come up with very\ndifferent ways of conceptualizing and referring to the same object, e.g. the\nsame abstract tangram shape can be a \"crab\", \"sink\" or \"space ship\". Another\ncommon assumption in cognitive science is that scene context fundamentally\nshapes our visual perception of objects and conceptual expectations. This paper\ncontributes SceneGram, a dataset of human references to tangram shapes placed\nin different scene contexts, allowing for systematic analyses of the effect of\nscene context on conceptualization. Based on this data, we analyze references\nto tangram shapes generated by multimodal LLMs, showing that these models do\nnot account for the richness and variability of conceptualizations found in\nhuman references.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on reference and naming suggests that humans can come up with very\ndifferent ways of conceptualizing and referring to the same object, e.g. the\nsame abstract tangram shape can be a \"crab\", \"sink\" or \"space ship\". Another\ncommon assumption in cognitive science is that scene context fundamentally\nshapes our visual perception of objects and conceptual expectations. This paper\ncontributes SceneGram, a dataset of human references to tangram shapes placed\nin different scene contexts, allowing for systematic analyses of the effect of\nscene context on conceptualization. Based on this data, we analyze references\nto tangram shapes generated by multimodal LLMs, showing that these models do\nnot account for the richness and variability of conceptualizations found in\nhuman references."
                },
                "authors": [
                    {
                        "name": "Simeon Junker"
                    },
                    {
                        "name": "Sina Zarrieß"
                    }
                ],
                "author_detail": {
                    "name": "Sina Zarrieß"
                },
                "author": "Sina Zarrieß",
                "arxiv_comment": "To appear in ACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23549v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23549v2",
                "updated": "2025-06-13T09:56:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    56,
                    36,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-29T15:27:52Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    27,
                    52,
                    3,
                    149,
                    0
                ],
                "title": "LLM-based Property-based Test Generation for Guardrailing Cyber-Physical\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Property-based Test Generation for Guardrailing Cyber-Physical\n  Systems"
                },
                "summary": "Cyber-physical systems (CPSs) are complex systems that integrate physical,\ncomputational, and communication subsystems. The heterogeneous nature of these\nsystems makes their safety assurance challenging. In this paper, we propose a\nnovel automated approach for guardrailing cyber-physical systems using\nproperty-based tests (PBTs) generated by Large Language Models (LLMs). Our\napproach employs an LLM to extract properties from the code and documentation\nof CPSs. Next, we use the LLM to generate PBTs that verify the extracted\nproperties on the CPS. The generated PBTs have two uses. First, they are used\nto test the CPS before it is deployed, i.e., at design time. Secondly, these\nPBTs can be used after deployment, i.e., at run time, to monitor the behavior\nof the system and guardrail it against unsafe states. We implement our approach\nin ChekProp and conduct preliminary experiments to evaluate the generated PBTs\nin terms of their relevance (how well they match manually crafted properties),\nexecutability (how many run with minimal manual modification), and\neffectiveness (coverage of the input space partitions). The results of our\nexperiments and evaluation demonstrate a promising path forward for creating\nguardrails for CPSs using LLM-generated property-based tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber-physical systems (CPSs) are complex systems that integrate physical,\ncomputational, and communication subsystems. The heterogeneous nature of these\nsystems makes their safety assurance challenging. In this paper, we propose a\nnovel automated approach for guardrailing cyber-physical systems using\nproperty-based tests (PBTs) generated by Large Language Models (LLMs). Our\napproach employs an LLM to extract properties from the code and documentation\nof CPSs. Next, we use the LLM to generate PBTs that verify the extracted\nproperties on the CPS. The generated PBTs have two uses. First, they are used\nto test the CPS before it is deployed, i.e., at design time. Secondly, these\nPBTs can be used after deployment, i.e., at run time, to monitor the behavior\nof the system and guardrail it against unsafe states. We implement our approach\nin ChekProp and conduct preliminary experiments to evaluate the generated PBTs\nin terms of their relevance (how well they match manually crafted properties),\nexecutability (how many run with minimal manual modification), and\neffectiveness (coverage of the input space partitions). The results of our\nexperiments and evaluation demonstrate a promising path forward for creating\nguardrails for CPSs using LLM-generated property-based tests."
                },
                "authors": [
                    {
                        "name": "Khashayar Etemadi"
                    },
                    {
                        "name": "Marjan Sirjani"
                    },
                    {
                        "name": "Mahshid Helali Moghadam"
                    },
                    {
                        "name": "Per Strandberg"
                    },
                    {
                        "name": "Paul Pettersson"
                    }
                ],
                "author_detail": {
                    "name": "Paul Pettersson"
                },
                "author": "Paul Pettersson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23549v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23549v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02050v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02050v4",
                "updated": "2025-06-13T09:44:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    44,
                    56,
                    4,
                    164,
                    0
                ],
                "published": "2024-06-04T07:31:06Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    31,
                    6,
                    1,
                    156,
                    0
                ],
                "title": "JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large\n  Language Models"
                },
                "summary": "With the development of large language models (LLMs), social biases in these\nLLMs have become a pressing issue. Although there are various benchmarks for\nsocial biases across languages, the extent to which Japanese LLMs exhibit\nsocial biases has not been fully investigated. In this study, we construct the\nJapanese Bias Benchmark dataset for Question Answering (JBBQ) based on the\nEnglish bias benchmark BBQ, with analysis of social biases in Japanese LLMs.\nThe results show that while current open Japanese LLMs with more parameters\nshow improved accuracies on JBBQ, their bias scores increase. In addition,\nprompts with a warning about social biases and chain-of-thought prompting\nreduce the effect of biases in model outputs, but there is room for improvement\nin extracting the correct evidence from contexts in Japanese. Our dataset is\navailable at https://github.com/ynklab/JBBQ_data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), social biases in these\nLLMs have become a pressing issue. Although there are various benchmarks for\nsocial biases across languages, the extent to which Japanese LLMs exhibit\nsocial biases has not been fully investigated. In this study, we construct the\nJapanese Bias Benchmark dataset for Question Answering (JBBQ) based on the\nEnglish bias benchmark BBQ, with analysis of social biases in Japanese LLMs.\nThe results show that while current open Japanese LLMs with more parameters\nshow improved accuracies on JBBQ, their bias scores increase. In addition,\nprompts with a warning about social biases and chain-of-thought prompting\nreduce the effect of biases in model outputs, but there is room for improvement\nin extracting the correct evidence from contexts in Japanese. Our dataset is\navailable at https://github.com/ynklab/JBBQ_data."
                },
                "authors": [
                    {
                        "name": "Hitomi Yanaka"
                    },
                    {
                        "name": "Namgi Han"
                    },
                    {
                        "name": "Ryoma Kumon"
                    },
                    {
                        "name": "Jie Lu"
                    },
                    {
                        "name": "Masashi Takeshita"
                    },
                    {
                        "name": "Ryo Sekizawa"
                    },
                    {
                        "name": "Taisei Kato"
                    },
                    {
                        "name": "Hiromi Arai"
                    }
                ],
                "author_detail": {
                    "name": "Hiromi Arai"
                },
                "author": "Hiromi Arai",
                "arxiv_comment": "Accepted to the 6th Workshop on Gender Bias in Natural Language\n  Processing (GeBNLP2025) at ACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02050v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02050v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11613v1",
                "updated": "2025-06-13T09:34:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    34,
                    25,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T09:34:25Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    34,
                    25,
                    4,
                    164,
                    0
                ],
                "title": "Model Organisms for Emergent Misalignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Organisms for Emergent Misalignment"
                },
                "summary": "Recent work discovered Emergent Misalignment (EM): fine-tuning large language\nmodels on narrowly harmful datasets can lead them to become broadly misaligned.\nA survey of experts prior to publication revealed this was highly unexpected,\ndemonstrating critical gaps in our understanding of model alignment. In this\nwork, we both advance understanding and provide tools for future research.\nUsing new narrowly misaligned datasets, we create a set of improved model\norganisms that achieve 99% coherence (vs. 67% prior), work with smaller 0.5B\nparameter models (vs. 32B), and that induce misalignment using a single rank-1\nLoRA adapter. We demonstrate that EM occurs robustly across diverse model\nsizes, three model families, and numerous training protocols including full\nsupervised fine-tuning. Leveraging these cleaner model organisms, we isolate a\nmechanistic phase transition and demonstrate that it corresponds to a robust\nbehavioural phase transition in all studied organisms. Aligning large language\nmodels is critical for frontier AI safety, yet EM exposes how far we are from\nachieving this robustly. By distilling clean model organisms that isolate a\nminimal alignment-compromising change, and where this is learnt, we establish a\nfoundation for future research into understanding and mitigating alignment\nrisks in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work discovered Emergent Misalignment (EM): fine-tuning large language\nmodels on narrowly harmful datasets can lead them to become broadly misaligned.\nA survey of experts prior to publication revealed this was highly unexpected,\ndemonstrating critical gaps in our understanding of model alignment. In this\nwork, we both advance understanding and provide tools for future research.\nUsing new narrowly misaligned datasets, we create a set of improved model\norganisms that achieve 99% coherence (vs. 67% prior), work with smaller 0.5B\nparameter models (vs. 32B), and that induce misalignment using a single rank-1\nLoRA adapter. We demonstrate that EM occurs robustly across diverse model\nsizes, three model families, and numerous training protocols including full\nsupervised fine-tuning. Leveraging these cleaner model organisms, we isolate a\nmechanistic phase transition and demonstrate that it corresponds to a robust\nbehavioural phase transition in all studied organisms. Aligning large language\nmodels is critical for frontier AI safety, yet EM exposes how far we are from\nachieving this robustly. By distilling clean model organisms that isolate a\nminimal alignment-compromising change, and where this is learnt, we establish a\nfoundation for future research into understanding and mitigating alignment\nrisks in LLMs."
                },
                "authors": [
                    {
                        "name": "Edward Turner"
                    },
                    {
                        "name": "Anna Soligo"
                    },
                    {
                        "name": "Mia Taylor"
                    },
                    {
                        "name": "Senthooran Rajamanoharan"
                    },
                    {
                        "name": "Neel Nanda"
                    }
                ],
                "author_detail": {
                    "name": "Neel Nanda"
                },
                "author": "Neel Nanda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11612v1",
                "updated": "2025-06-13T09:33:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    33,
                    58,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T09:33:58Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    33,
                    58,
                    4,
                    164,
                    0
                ],
                "title": "KEENHash: Hashing Programs into Function-Aware Embeddings for\n  Large-Scale Binary Code Similarity Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KEENHash: Hashing Programs into Function-Aware Embeddings for\n  Large-Scale Binary Code Similarity Analysis"
                },
                "summary": "Binary code similarity analysis (BCSA) is a crucial research area in many\nfields such as cybersecurity. Specifically, function-level diffing tools are\nthe most widely used in BCSA: they perform function matching one by one for\nevaluating the similarity between binary programs. However, such methods need a\nhigh time complexity, making them unscalable in large-scale scenarios (e.g.,\n1/n-to-n search). Towards effective and efficient program-level BCSA, we\npropose KEENHash, a novel hashing approach that hashes binaries into\nprogram-level representations through large language model (LLM)-generated\nfunction embeddings. KEENHash condenses a binary into one compact and\nfixed-length program embedding using K-Means and Feature Hashing, allowing us\nto do effective and efficient large-scale program-level BCSA, surpassing the\nprevious state-of-the-art methods. The experimental results show that KEENHash\nis at least 215 times faster than the state-of-the-art function matching tools\nwhile maintaining effectiveness. Furthermore, in a large-scale scenario with\n5.3 billion similarity evaluations, KEENHash takes only 395.83 seconds while\nthese tools will cost at least 56 days. We also evaluate KEENHash on the\nprogram clone search of large-scale BCSA across extensive datasets in 202,305\nbinaries. Compared with 4 state-of-the-art methods, KEENHash outperforms all of\nthem by at least 23.16%, and displays remarkable superiority over them in the\nlarge-scale BCSA security scenario of malware detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary code similarity analysis (BCSA) is a crucial research area in many\nfields such as cybersecurity. Specifically, function-level diffing tools are\nthe most widely used in BCSA: they perform function matching one by one for\nevaluating the similarity between binary programs. However, such methods need a\nhigh time complexity, making them unscalable in large-scale scenarios (e.g.,\n1/n-to-n search). Towards effective and efficient program-level BCSA, we\npropose KEENHash, a novel hashing approach that hashes binaries into\nprogram-level representations through large language model (LLM)-generated\nfunction embeddings. KEENHash condenses a binary into one compact and\nfixed-length program embedding using K-Means and Feature Hashing, allowing us\nto do effective and efficient large-scale program-level BCSA, surpassing the\nprevious state-of-the-art methods. The experimental results show that KEENHash\nis at least 215 times faster than the state-of-the-art function matching tools\nwhile maintaining effectiveness. Furthermore, in a large-scale scenario with\n5.3 billion similarity evaluations, KEENHash takes only 395.83 seconds while\nthese tools will cost at least 56 days. We also evaluate KEENHash on the\nprogram clone search of large-scale BCSA across extensive datasets in 202,305\nbinaries. Compared with 4 state-of-the-art methods, KEENHash outperforms all of\nthem by at least 23.16%, and displays remarkable superiority over them in the\nlarge-scale BCSA security scenario of malware detection."
                },
                "authors": [
                    {
                        "name": "Zhijie Liu"
                    },
                    {
                        "name": "Qiyi Tang"
                    },
                    {
                        "name": "Sen Nie"
                    },
                    {
                        "name": "Shi Wu"
                    },
                    {
                        "name": "Liang Feng Zhang"
                    },
                    {
                        "name": "Yutian Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yutian Tang"
                },
                "author": "Yutian Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11812v2",
                "updated": "2025-06-13T09:32:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    32,
                    19,
                    4,
                    164,
                    0
                ],
                "published": "2025-02-17T13:59:41Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    13,
                    59,
                    41,
                    0,
                    48,
                    0
                ],
                "title": "Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit\n  Analysis"
                },
                "summary": "Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies (Prakash et al. 2024; Chhabra et al. 2024) that focus on tasks\nwhere pre-trained models already perform well, we develop a set of mathematical\ntasks where fine-tuning yields substantial performance gains, which are closer\nto the practical setting. In our experiments, we identify circuits at various\ncheckpoints during fine-tuning and examine the interplay between circuit\nanalysis, fine-tuning methods, and task complexities. First, we find that while\ncircuits maintain high node similarity before and after fine-tuning, their\nedges undergo significant changes, in contrast to prior work that shows\ncircuits only add some additional components after fine-tuning. Based on these\nobservations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method,\nwhich assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies (Prakash et al. 2024; Chhabra et al. 2024) that focus on tasks\nwhere pre-trained models already perform well, we develop a set of mathematical\ntasks where fine-tuning yields substantial performance gains, which are closer\nto the practical setting. In our experiments, we identify circuits at various\ncheckpoints during fine-tuning and examine the interplay between circuit\nanalysis, fine-tuning methods, and task complexities. First, we find that while\ncircuits maintain high node similarity before and after fine-tuning, their\nedges undergo significant changes, in contrast to prior work that shows\ncircuits only add some additional components after fine-tuning. Based on these\nobservations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method,\nwhich assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms."
                },
                "authors": [
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Wenyu Du"
                    },
                    {
                        "name": "Reynold Cheng"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Difan Zou"
                    }
                ],
                "author_detail": {
                    "name": "Difan Zou"
                },
                "author": "Difan Zou",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.12014v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12014v1",
                "updated": "2025-06-13T17:59:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    59,
                    39,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:59:39Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    59,
                    39,
                    4,
                    164,
                    0
                ],
                "title": "code_transformed: The Influence of Large Language Models on Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "code_transformed: The Influence of Large Language Models on Code"
                },
                "summary": "Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style."
                },
                "authors": [
                    {
                        "name": "Yuliang Xu"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Mingmeng Geng"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Xuanhua Shi"
                    },
                    {
                        "name": "Dongping Chen"
                    }
                ],
                "author_detail": {
                    "name": "Dongping Chen"
                },
                "author": "Dongping Chen",
                "arxiv_comment": "We release all the experimental dataset and source code at:\n  https://github.com/ignorancex/LLM_code",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12014v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12014v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12012v1",
                "updated": "2025-06-13T17:59:10Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    59,
                    10,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:59:10Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    59,
                    10,
                    4,
                    164,
                    0
                ],
                "title": "Tracing LLM Reasoning Processes with Strategic Games: A Framework for\n  Planning, Revision, and Resource-Constrained Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tracing LLM Reasoning Processes with Strategic Games: A Framework for\n  Planning, Revision, and Resource-Constrained Decision Making"
                },
                "summary": "Large language models (LLMs) are increasingly used for tasks that require\ncomplex reasoning. Most benchmarks focus on final outcomes but overlook the\nintermediate reasoning steps - such as planning, revision, and decision making\nunder resource constraints. We argue that measuring these internal processes is\nessential for understanding model behavior and improving reliability. We\npropose using strategic games as a natural evaluation environment: closed,\nrule-based systems with clear states, limited resources, and automatic\nfeedback. We introduce a framework that evaluates LLMs along three core\ndimensions: planning, revision, and resource-constrained decision making. To\noperationalize this, we define metrics beyond win rate, including\novercorrection risk rate, correction success rate, improvement slope, and\nover-budget ratio. In 4320 adversarial rounds across 12 leading models,\nChatGPT-o3-mini achieves the top composite score, with a win rate of 74.7\npercent, a correction success rate of 78.6 percent, and an improvement slope of\n0.041. By contrast, Qwen-Plus, despite an overcorrection risk rate of 81.6\npercent, wins only 25.6 percent of its matches - primarily due to excessive\nresource use. We also observe a negative correlation between overcorrection\nrisk rate and correction success rate (Pearson r = -0.51, p = 0.093),\nsuggesting that more frequent edits do not always improve outcomes. Our\nfindings highlight the value of assessing not only what LLMs decide but how\nthey arrive at those decisions",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used for tasks that require\ncomplex reasoning. Most benchmarks focus on final outcomes but overlook the\nintermediate reasoning steps - such as planning, revision, and decision making\nunder resource constraints. We argue that measuring these internal processes is\nessential for understanding model behavior and improving reliability. We\npropose using strategic games as a natural evaluation environment: closed,\nrule-based systems with clear states, limited resources, and automatic\nfeedback. We introduce a framework that evaluates LLMs along three core\ndimensions: planning, revision, and resource-constrained decision making. To\noperationalize this, we define metrics beyond win rate, including\novercorrection risk rate, correction success rate, improvement slope, and\nover-budget ratio. In 4320 adversarial rounds across 12 leading models,\nChatGPT-o3-mini achieves the top composite score, with a win rate of 74.7\npercent, a correction success rate of 78.6 percent, and an improvement slope of\n0.041. By contrast, Qwen-Plus, despite an overcorrection risk rate of 81.6\npercent, wins only 25.6 percent of its matches - primarily due to excessive\nresource use. We also observe a negative correlation between overcorrection\nrisk rate and correction success rate (Pearson r = -0.51, p = 0.093),\nsuggesting that more frequent edits do not always improve outcomes. Our\nfindings highlight the value of assessing not only what LLMs decide but how\nthey arrive at those decisions"
                },
                "authors": [
                    {
                        "name": "Xiaopeng Yuan"
                    },
                    {
                        "name": "Xingjian Zhang"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Lijun Yu"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Yushun Dong"
                    },
                    {
                        "name": "Haohan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haohan Wang"
                },
                "author": "Haohan Wang",
                "arxiv_comment": "19 pages, 7 figures. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12003v1",
                "updated": "2025-06-13T17:55:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    55,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:55:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    55,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "Upgrade or Switch: Do We Need a New Registry Architecture for the\n  Internet of AI Agents?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade or Switch: Do We Need a New Registry Architecture for the\n  Internet of AI Agents?"
                },
                "summary": "The emerging Internet of AI Agents challenges existing web infrastructure\ndesigned for human-scale, reactive interactions. Unlike traditional web\nresources, autonomous AI agents initiate actions, maintain persistent state,\nspawn sub-agents, and negotiate directly with peers: demanding\nmillisecond-level discovery, instant credential revocation, and cryptographic\nbehavioral proofs that exceed current DNS/PKI capabilities. This paper analyzes\nwhether to upgrade existing infrastructure or implement purpose-built registry\narchitectures for autonomous agents. We identify critical failure points: DNS\npropagation (24-48 hours vs. required milliseconds), certificate revocation\nunable to scale to trillions of entities, and IPv4/IPv6 addressing inadequate\nfor agent-scale routing. We evaluate three approaches: (1) Upgrade paths, (2)\nSwitch options, (3) Hybrid registries. Drawing parallels to dialup-to-broadband\ntransitions, we find that agent requirements constitute qualitative, and not\nincremental, changes. While upgrades offer compatibility and faster deployment,\nclean-slate solutions provide better performance but require longer for\nadoption. Our analysis suggests hybrid approaches will emerge, with centralized\nregistries for critical agents and federated meshes for specialized use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emerging Internet of AI Agents challenges existing web infrastructure\ndesigned for human-scale, reactive interactions. Unlike traditional web\nresources, autonomous AI agents initiate actions, maintain persistent state,\nspawn sub-agents, and negotiate directly with peers: demanding\nmillisecond-level discovery, instant credential revocation, and cryptographic\nbehavioral proofs that exceed current DNS/PKI capabilities. This paper analyzes\nwhether to upgrade existing infrastructure or implement purpose-built registry\narchitectures for autonomous agents. We identify critical failure points: DNS\npropagation (24-48 hours vs. required milliseconds), certificate revocation\nunable to scale to trillions of entities, and IPv4/IPv6 addressing inadequate\nfor agent-scale routing. We evaluate three approaches: (1) Upgrade paths, (2)\nSwitch options, (3) Hybrid registries. Drawing parallels to dialup-to-broadband\ntransitions, we find that agent requirements constitute qualitative, and not\nincremental, changes. While upgrades offer compatibility and faster deployment,\nclean-slate solutions provide better performance but require longer for\nadoption. Our analysis suggests hybrid approaches will emerge, with centralized\nregistries for critical agents and federated meshes for specialized use cases."
                },
                "authors": [
                    {
                        "name": "Ramesh Raskar"
                    },
                    {
                        "name": "Pradyumna Chari"
                    },
                    {
                        "name": "Jared James Grogan"
                    },
                    {
                        "name": "Mahesh Lambe"
                    },
                    {
                        "name": "Robert Lincourt"
                    },
                    {
                        "name": "Raghu Bala"
                    },
                    {
                        "name": "Abhishek Singh"
                    },
                    {
                        "name": "Ayush Chopra"
                    },
                    {
                        "name": "Rajesh Ranjan"
                    },
                    {
                        "name": "Shailja Gupta"
                    },
                    {
                        "name": "Dimitris Stripelis"
                    },
                    {
                        "name": "Maria Gorskikh"
                    },
                    {
                        "name": "Sichao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sichao Wang"
                },
                "author": "Sichao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09026v2",
                "updated": "2025-06-13T17:44:03Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    44,
                    3,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-10T17:52:42Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    52,
                    42,
                    1,
                    161,
                    0
                ],
                "title": "e3: Learning to Explore Enables Extrapolation of Test-Time Compute for\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "e3: Learning to Explore Enables Extrapolation of Test-Time Compute for\n  LLMs"
                },
                "summary": "Test-time scaling offers a promising path to improve LLM reasoning by\nutilizing more compute at inference time; however, the true promise of this\nparadigm lies in extrapolation (i.e., improvement in performance on hard\nproblems as LLMs keep \"thinking\" for longer, beyond the maximum token budget\nthey were trained on). Surprisingly, we find that most existing reasoning\nmodels do not extrapolate well. We show that one way to enable extrapolation is\nby training the LLM to perform in-context exploration: training the LLM to\neffectively spend its test time budget by chaining operations (such as\ngeneration, verification, refinement, etc.), or testing multiple hypotheses\nbefore it commits to an answer. To enable in-context exploration, we identify\nthree key ingredients as part of our recipe e3: (1) chaining skills that the\nbase LLM has asymmetric competence in, e.g., chaining verification (easy) with\ngeneration (hard), as a way to implement in-context search; (2) leveraging\n\"negative\" gradients from incorrect traces to amplify exploration during RL,\nresulting in longer search traces that chains additional asymmetries; and (3)\ncoupling task difficulty with training token budget during training via a\nspecifically-designed curriculum to structure in-context exploration. Our\nrecipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25\nscores, and extrapolates to 2x the training token budget. Our e3-1.7B model not\nonly attains high pass@1 scores, but also improves pass@k over the base model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling offers a promising path to improve LLM reasoning by\nutilizing more compute at inference time; however, the true promise of this\nparadigm lies in extrapolation (i.e., improvement in performance on hard\nproblems as LLMs keep \"thinking\" for longer, beyond the maximum token budget\nthey were trained on). Surprisingly, we find that most existing reasoning\nmodels do not extrapolate well. We show that one way to enable extrapolation is\nby training the LLM to perform in-context exploration: training the LLM to\neffectively spend its test time budget by chaining operations (such as\ngeneration, verification, refinement, etc.), or testing multiple hypotheses\nbefore it commits to an answer. To enable in-context exploration, we identify\nthree key ingredients as part of our recipe e3: (1) chaining skills that the\nbase LLM has asymmetric competence in, e.g., chaining verification (easy) with\ngeneration (hard), as a way to implement in-context search; (2) leveraging\n\"negative\" gradients from incorrect traces to amplify exploration during RL,\nresulting in longer search traces that chains additional asymmetries; and (3)\ncoupling task difficulty with training token budget during training via a\nspecifically-designed curriculum to structure in-context exploration. Our\nrecipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25\nscores, and extrapolates to 2x the training token budget. Our e3-1.7B model not\nonly attains high pass@1 scores, but also improves pass@k over the base model."
                },
                "authors": [
                    {
                        "name": "Amrith Setlur"
                    },
                    {
                        "name": "Matthew Y. R. Yang"
                    },
                    {
                        "name": "Charlie Snell"
                    },
                    {
                        "name": "Jeremy Greer"
                    },
                    {
                        "name": "Ian Wu"
                    },
                    {
                        "name": "Virginia Smith"
                    },
                    {
                        "name": "Max Simchowitz"
                    },
                    {
                        "name": "Aviral Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Aviral Kumar"
                },
                "author": "Aviral Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11976v1",
                "updated": "2025-06-13T17:34:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    34,
                    5,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T17:34:05Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    34,
                    5,
                    4,
                    164,
                    0
                ],
                "title": "How Visual Representations Map to Language Feature Space in Multimodal\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Visual Representations Map to Language Feature Space in Multimodal\n  LLMs"
                },
                "summary": "Effective multimodal reasoning depends on the alignment of visual and\nlinguistic representations, yet the mechanisms by which vision-language models\n(VLMs) achieve this alignment remain poorly understood. We introduce a\nmethodological framework that deliberately maintains a frozen large language\nmodel (LLM) and a frozen vision transformer (ViT), connected solely by training\na linear adapter during visual instruction tuning. This design is fundamental\nto our approach: by keeping the language model frozen, we ensure it maintains\nits original language representations without adaptation to visual data.\nConsequently, the linear adapter must map visual features directly into the\nLLM's existing representational space rather than allowing the language model\nto develop specialized visual understanding through fine-tuning. Our\nexperimental design uniquely enables the use of pre-trained sparse autoencoders\n(SAEs) of the LLM as analytical probes. These SAEs remain perfectly aligned\nwith the unchanged language model and serve as a snapshot of the learned\nlanguage feature-representations. Through systematic analysis of SAE\nreconstruction error, sparsity patterns, and feature SAE descriptions, we\nreveal the layer-wise progression through which visual representations\ngradually align with language feature representations, converging in\nmiddle-to-later layers. This suggests a fundamental misalignment between ViT\noutputs and early LLM layers, raising important questions about whether current\nadapter-based architectures optimally facilitate cross-modal representation\nlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective multimodal reasoning depends on the alignment of visual and\nlinguistic representations, yet the mechanisms by which vision-language models\n(VLMs) achieve this alignment remain poorly understood. We introduce a\nmethodological framework that deliberately maintains a frozen large language\nmodel (LLM) and a frozen vision transformer (ViT), connected solely by training\na linear adapter during visual instruction tuning. This design is fundamental\nto our approach: by keeping the language model frozen, we ensure it maintains\nits original language representations without adaptation to visual data.\nConsequently, the linear adapter must map visual features directly into the\nLLM's existing representational space rather than allowing the language model\nto develop specialized visual understanding through fine-tuning. Our\nexperimental design uniquely enables the use of pre-trained sparse autoencoders\n(SAEs) of the LLM as analytical probes. These SAEs remain perfectly aligned\nwith the unchanged language model and serve as a snapshot of the learned\nlanguage feature-representations. Through systematic analysis of SAE\nreconstruction error, sparsity patterns, and feature SAE descriptions, we\nreveal the layer-wise progression through which visual representations\ngradually align with language feature representations, converging in\nmiddle-to-later layers. This suggests a fundamental misalignment between ViT\noutputs and early LLM layers, raising important questions about whether current\nadapter-based architectures optimally facilitate cross-modal representation\nlearning."
                },
                "authors": [
                    {
                        "name": "Constantin Venhoff"
                    },
                    {
                        "name": "Ashkan Khakzar"
                    },
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Neel Nanda"
                    }
                ],
                "author_detail": {
                    "name": "Neel Nanda"
                },
                "author": "Neel Nanda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10467v2",
                "updated": "2025-06-13T17:32:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    32,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T08:16:17Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    8,
                    16,
                    17,
                    3,
                    163,
                    0
                ],
                "title": "Specification and Evaluation of Multi-Agent LLM Systems -- Prototype and\n  Cybersecurity Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specification and Evaluation of Multi-Agent LLM Systems -- Prototype and\n  Cybersecurity Applications"
                },
                "summary": "Recent advancements in LLMs indicate potential for novel applications, e.g.,\nthrough reasoning capabilities in the latest OpenAI and DeepSeek models. For\napplying these models in specific domains beyond text generation, LLM-based\nmulti-agent approaches can be utilized that solve complex tasks by combining\nreasoning techniques, code generation, and software execution. Applications\nmight utilize these capabilities and the knowledge of specialized LLM agents.\nHowever, while many evaluations are performed on LLMs, reasoning techniques,\nand applications individually, their joint specification and combined\napplication is not explored well. Defined specifications for multi-agent LLM\nsystems are required to explore their potential and their suitability for\nspecific applications, allowing for systematic evaluations of LLMs, reasoning\ntechniques, and related aspects. This paper reports the results of exploratory\nresearch to specify and evaluate these aspects through a multi-agent system.\nThe system architecture and prototype are extended from previous research and a\nspecification is introduced for multi-agent systems. Test cases involving\ncybersecurity tasks indicate feasibility of the architecture and evaluation\napproach. In particular, the results show the evaluation of question answering,\nserver security, and network security tasks that were completed correctly by\nagents with LLMs from OpenAI and DeepSeek.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in LLMs indicate potential for novel applications, e.g.,\nthrough reasoning capabilities in the latest OpenAI and DeepSeek models. For\napplying these models in specific domains beyond text generation, LLM-based\nmulti-agent approaches can be utilized that solve complex tasks by combining\nreasoning techniques, code generation, and software execution. Applications\nmight utilize these capabilities and the knowledge of specialized LLM agents.\nHowever, while many evaluations are performed on LLMs, reasoning techniques,\nand applications individually, their joint specification and combined\napplication is not explored well. Defined specifications for multi-agent LLM\nsystems are required to explore their potential and their suitability for\nspecific applications, allowing for systematic evaluations of LLMs, reasoning\ntechniques, and related aspects. This paper reports the results of exploratory\nresearch to specify and evaluate these aspects through a multi-agent system.\nThe system architecture and prototype are extended from previous research and a\nspecification is introduced for multi-agent systems. Test cases involving\ncybersecurity tasks indicate feasibility of the architecture and evaluation\napproach. In particular, the results show the evaluation of question answering,\nserver security, and network security tasks that were completed correctly by\nagents with LLMs from OpenAI and DeepSeek."
                },
                "authors": [
                    {
                        "name": "Felix Härer"
                    }
                ],
                "author_detail": {
                    "name": "Felix Härer"
                },
                "author": "Felix Härer",
                "arxiv_comment": "This work has been submitted for a possible publication. Copyright\n  may be transferred. In this case, this version will be updated with a notice,\n  according to the publisher's guidelines",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07833v2",
                "updated": "2025-06-13T17:24:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    24,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-09T14:55:00Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    14,
                    55,
                    0,
                    0,
                    160,
                    0
                ],
                "title": "Improving Large Language Models with Concept-Aware Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Large Language Models with Concept-Aware Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm"
                },
                "authors": [
                    {
                        "name": "Michael K. Chen"
                    },
                    {
                        "name": "Xikun Zhang"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01503v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01503v7",
                "updated": "2025-06-13T17:01:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    17,
                    1,
                    34,
                    4,
                    164,
                    0
                ],
                "published": "2024-11-03T09:49:12Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    9,
                    49,
                    12,
                    6,
                    308,
                    0
                ],
                "title": "A Highly Scalable LLM Clusters with Optical Interconnect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Highly Scalable LLM Clusters with Optical Interconnect"
                },
                "summary": "The rapid development of large-scale GPU clusters for LLM training has driven\nenterprises to replace core-layer electrical switches with optical circuit\nswitches (OCS) to meet escalating bandwidth demands. However, current physical\ntopology design of OCS-based clusters faces two critical challenges. First,\nthere exist unrealizable logical topologies, leading to underutilization of\nbandwidth resource. Second, calculating OCS reconfiguration constitutes an\nNP-Complete problem and is time-consuming for multi-tenant GPU clusters which\nneed real-time scheduling. In this paper, we propose \\emph{Cross Wiring}, a new\nphysical topology design that resolves both limitations. Our physical topology\nguarantees full compatibility with all logical topologies under\nL2-compatibility constraints. Through a proposed \\emph{Symmetric Integer Matrix\nDecomposition Theorem}, we design a polynomial-time OCS reconfiguration\nalgorithm that satisfies arbitrary logical topology requirements. Evaluations\nshow a up to 39.5\\% higher training throughput versus prior architectures such\nas \\emph{Gemini} in 128-NPU testbed and a 12.6\\% reduction in average job\ncompletion time through real-workload based multi-tenant large-scale\nsimulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large-scale GPU clusters for LLM training has driven\nenterprises to replace core-layer electrical switches with optical circuit\nswitches (OCS) to meet escalating bandwidth demands. However, current physical\ntopology design of OCS-based clusters faces two critical challenges. First,\nthere exist unrealizable logical topologies, leading to underutilization of\nbandwidth resource. Second, calculating OCS reconfiguration constitutes an\nNP-Complete problem and is time-consuming for multi-tenant GPU clusters which\nneed real-time scheduling. In this paper, we propose \\emph{Cross Wiring}, a new\nphysical topology design that resolves both limitations. Our physical topology\nguarantees full compatibility with all logical topologies under\nL2-compatibility constraints. Through a proposed \\emph{Symmetric Integer Matrix\nDecomposition Theorem}, we design a polynomial-time OCS reconfiguration\nalgorithm that satisfies arbitrary logical topology requirements. Evaluations\nshow a up to 39.5\\% higher training throughput versus prior architectures such\nas \\emph{Gemini} in 128-NPU testbed and a 12.6\\% reduction in average job\ncompletion time through real-workload based multi-tenant large-scale\nsimulations."
                },
                "authors": [
                    {
                        "name": "Xinchi Han"
                    },
                    {
                        "name": "Yongxi Lv"
                    },
                    {
                        "name": "Shuyuan Zhang"
                    },
                    {
                        "name": "Yingming Mao"
                    },
                    {
                        "name": "Shizhen Zhao"
                    },
                    {
                        "name": "ZhuoRan Liu"
                    },
                    {
                        "name": "Zhuotao Liu"
                    },
                    {
                        "name": "Peirui Cao"
                    },
                    {
                        "name": "Ximeng Liu"
                    },
                    {
                        "name": "Xinbing Wang"
                    },
                    {
                        "name": "Wu Dongchao"
                    },
                    {
                        "name": "Yang Jian"
                    },
                    {
                        "name": "Zhang zhanbang"
                    }
                ],
                "author_detail": {
                    "name": "Zhang zhanbang"
                },
                "author": "Zhang zhanbang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01503v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01503v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21657v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21657v2",
                "updated": "2025-06-13T16:43:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    43,
                    15,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-27T18:32:38Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    18,
                    32,
                    38,
                    1,
                    147,
                    0
                ],
                "title": "Explainability of Large Language Models using SMILE: Statistical\n  Model-agnostic Interpretability with Local Explanations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explainability of Large Language Models using SMILE: Statistical\n  Model-agnostic Interpretability with Local Explanations"
                },
                "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy."
                },
                "authors": [
                    {
                        "name": "Zeinab Dehghani"
                    },
                    {
                        "name": "Mohammed Naveed Akram"
                    },
                    {
                        "name": "Koorosh Aslansefat"
                    },
                    {
                        "name": "Adil Khan"
                    }
                ],
                "author_detail": {
                    "name": "Adil Khan"
                },
                "author": "Adil Khan",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2412.16277",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21657v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21657v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11938v1",
                "updated": "2025-06-13T16:42:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    42,
                    9,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T16:42:09Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    42,
                    9,
                    4,
                    164,
                    0
                ],
                "title": "Improving Large Language Model Safety with Contrastive Representation\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Large Language Model Safety with Contrastive Representation\n  Learning"
                },
                "summary": "Large Language Models (LLMs) are powerful tools with profound societal\nimpacts, yet their ability to generate responses to diverse and uncontrolled\ninputs leaves them vulnerable to adversarial attacks. While existing defenses\noften struggle to generalize across varying attack types, recent advancements\nin representation engineering offer promising alternatives. In this work, we\npropose a defense framework that formulates model defense as a contrastive\nrepresentation learning (CRL) problem. Our method finetunes a model using a\ntriplet-based loss combined with adversarial hard negative mining to encourage\nseparation between benign and harmful representations. Our experimental results\nacross multiple models demonstrate that our approach outperforms prior\nrepresentation engineering-based defenses, improving robustness against both\ninput-level and embedding-space attacks without compromising standard\nperformance. Our code is available at\nhttps://github.com/samuelsimko/crl-llm-defense",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful tools with profound societal\nimpacts, yet their ability to generate responses to diverse and uncontrolled\ninputs leaves them vulnerable to adversarial attacks. While existing defenses\noften struggle to generalize across varying attack types, recent advancements\nin representation engineering offer promising alternatives. In this work, we\npropose a defense framework that formulates model defense as a contrastive\nrepresentation learning (CRL) problem. Our method finetunes a model using a\ntriplet-based loss combined with adversarial hard negative mining to encourage\nseparation between benign and harmful representations. Our experimental results\nacross multiple models demonstrate that our approach outperforms prior\nrepresentation engineering-based defenses, improving robustness against both\ninput-level and embedding-space attacks without compromising standard\nperformance. Our code is available at\nhttps://github.com/samuelsimko/crl-llm-defense"
                },
                "authors": [
                    {
                        "name": "Samuel Simko"
                    },
                    {
                        "name": "Mrinmaya Sachan"
                    },
                    {
                        "name": "Bernhard Schölkopf"
                    },
                    {
                        "name": "Zhijing Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Jin"
                },
                "author": "Zhijing Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11930v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11930v1",
                "updated": "2025-06-13T16:31:51Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    31,
                    51,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T16:31:51Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    31,
                    51,
                    4,
                    164,
                    0
                ],
                "title": "Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback"
                },
                "summary": "Recent studies have shown LLMs possess some ability to improve their\nresponses when given external feedback. However, it remains unclear how\neffectively and thoroughly these models can incorporate extrinsic feedback. In\nan ideal scenario, if LLMs receive near-perfect and complete feedback, we would\nexpect them to fully integrate the feedback and change their incorrect answers\nto correct ones. In this paper, we systematically investigate LLMs' ability to\nincorporate feedback by designing a controlled experimental environment. For\neach problem, a solver model attempts a solution, then a feedback generator\nwith access to near-complete ground-truth answers produces targeted feedback,\nafter which the solver tries again. We evaluate this pipeline across a diverse\nrange of tasks, including math reasoning, knowledge reasoning, scientific\nreasoning, and general multi-domain evaluations with state-of-the-art language\nmodels including Claude 3.7 (with and without extended thinking). Surprisingly,\neven under these near-ideal conditions, solver models consistently show\nresistance to feedback, a limitation that we term FEEDBACK FRICTION. To\nmitigate this limitation, we experiment with sampling-based strategies like\nprogressive temperature increases and explicit rejection of previously\nattempted incorrect answers, which yield improvements but still fail to help\nmodels achieve target performance. We also perform a rigorous exploration of\npotential causes of FEEDBACK FRICTION, ruling out factors such as model\noverconfidence and data familiarity. We hope that highlighting this issue in\nLLMs and ruling out several apparent causes will help future research in\nself-improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown LLMs possess some ability to improve their\nresponses when given external feedback. However, it remains unclear how\neffectively and thoroughly these models can incorporate extrinsic feedback. In\nan ideal scenario, if LLMs receive near-perfect and complete feedback, we would\nexpect them to fully integrate the feedback and change their incorrect answers\nto correct ones. In this paper, we systematically investigate LLMs' ability to\nincorporate feedback by designing a controlled experimental environment. For\neach problem, a solver model attempts a solution, then a feedback generator\nwith access to near-complete ground-truth answers produces targeted feedback,\nafter which the solver tries again. We evaluate this pipeline across a diverse\nrange of tasks, including math reasoning, knowledge reasoning, scientific\nreasoning, and general multi-domain evaluations with state-of-the-art language\nmodels including Claude 3.7 (with and without extended thinking). Surprisingly,\neven under these near-ideal conditions, solver models consistently show\nresistance to feedback, a limitation that we term FEEDBACK FRICTION. To\nmitigate this limitation, we experiment with sampling-based strategies like\nprogressive temperature increases and explicit rejection of previously\nattempted incorrect answers, which yield improvements but still fail to help\nmodels achieve target performance. We also perform a rigorous exploration of\npotential causes of FEEDBACK FRICTION, ruling out factors such as model\noverconfidence and data familiarity. We hope that highlighting this issue in\nLLMs and ruling out several apparent causes will help future research in\nself-improvement."
                },
                "authors": [
                    {
                        "name": "Dongwei Jiang"
                    },
                    {
                        "name": "Alvin Zhang"
                    },
                    {
                        "name": "Andrew Wang"
                    },
                    {
                        "name": "Nicholas Andrews"
                    },
                    {
                        "name": "Daniel Khashabi"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Khashabi"
                },
                "author": "Daniel Khashabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11930v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11930v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11928v1",
                "updated": "2025-06-13T16:29:09Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    29,
                    9,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T16:29:09Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    29,
                    9,
                    4,
                    164,
                    0
                ],
                "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?"
                },
                "summary": "Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning."
                },
                "authors": [
                    {
                        "name": "Zihan Zheng"
                    },
                    {
                        "name": "Zerui Cheng"
                    },
                    {
                        "name": "Zeyu Shen"
                    },
                    {
                        "name": "Shang Zhou"
                    },
                    {
                        "name": "Kaiyuan Liu"
                    },
                    {
                        "name": "Hansen He"
                    },
                    {
                        "name": "Dongruixuan Li"
                    },
                    {
                        "name": "Stanley Wei"
                    },
                    {
                        "name": "Hangyi Hao"
                    },
                    {
                        "name": "Jianzhu Yao"
                    },
                    {
                        "name": "Peiyao Sheng"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Aleksandra Korolova"
                    },
                    {
                        "name": "Peter Henderson"
                    },
                    {
                        "name": "Sanjeev Arora"
                    },
                    {
                        "name": "Pramod Viswanath"
                    },
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Saining Xie"
                    }
                ],
                "author_detail": {
                    "name": "Saining Xie"
                },
                "author": "Saining Xie",
                "arxiv_comment": "Project Page at https://livecodebenchpro.com/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11925v1",
                "updated": "2025-06-13T16:24:28Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    24,
                    28,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T16:24:28Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    24,
                    28,
                    4,
                    164,
                    0
                ],
                "title": "Real-World Deployment of a Lane Change Prediction Architecture Based on\n  Knowledge Graph Embeddings and Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-World Deployment of a Lane Change Prediction Architecture Based on\n  Knowledge Graph Embeddings and Bayesian Inference"
                },
                "summary": "Research on lane change prediction has gained a lot of momentum in the last\ncouple of years. However, most research is confined to simulation or results\nobtained from datasets, leaving a gap between algorithmic advances and on-road\ndeployment. This work closes that gap by demonstrating, on real hardware, a\nlane-change prediction system based on Knowledge Graph Embeddings (KGEs) and\nBayesian inference. Moreover, the ego-vehicle employs a longitudinal braking\naction to ensure the safety of both itself and the surrounding vehicles. Our\narchitecture consists of two modules: (i) a perception module that senses the\nenvironment, derives input numerical features, and converts them into\nlinguistic categories; and communicates them to the prediction module; (ii) a\npretrained prediction module that executes a KGE and Bayesian inference model\nto anticipate the target vehicle's maneuver and transforms the prediction into\nlongitudinal braking action. Real-world hardware experimental validation\ndemonstrates that our prediction system anticipates the target vehicle's lane\nchange three to four seconds in advance, providing the ego vehicle sufficient\ntime to react and allowing the target vehicle to make the lane change safely.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on lane change prediction has gained a lot of momentum in the last\ncouple of years. However, most research is confined to simulation or results\nobtained from datasets, leaving a gap between algorithmic advances and on-road\ndeployment. This work closes that gap by demonstrating, on real hardware, a\nlane-change prediction system based on Knowledge Graph Embeddings (KGEs) and\nBayesian inference. Moreover, the ego-vehicle employs a longitudinal braking\naction to ensure the safety of both itself and the surrounding vehicles. Our\narchitecture consists of two modules: (i) a perception module that senses the\nenvironment, derives input numerical features, and converts them into\nlinguistic categories; and communicates them to the prediction module; (ii) a\npretrained prediction module that executes a KGE and Bayesian inference model\nto anticipate the target vehicle's maneuver and transforms the prediction into\nlongitudinal braking action. Real-world hardware experimental validation\ndemonstrates that our prediction system anticipates the target vehicle's lane\nchange three to four seconds in advance, providing the ego vehicle sufficient\ntime to react and allowing the target vehicle to make the lane change safely."
                },
                "authors": [
                    {
                        "name": "M. Manzour"
                    },
                    {
                        "name": "Catherine M. Elias"
                    },
                    {
                        "name": "Omar M. Shehata"
                    },
                    {
                        "name": "R. Izquierdo"
                    },
                    {
                        "name": "M. A. Sotelo"
                    }
                ],
                "author_detail": {
                    "name": "M. A. Sotelo"
                },
                "author": "M. A. Sotelo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11651v2",
                "updated": "2025-06-13T16:15:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    15,
                    45,
                    4,
                    164,
                    0
                ],
                "published": "2025-01-20T18:33:33Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    18,
                    33,
                    33,
                    0,
                    20,
                    0
                ],
                "title": "T1: Advancing Language Model Reasoning through Reinforcement Learning\n  and Inference Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: Advancing Language Model Reasoning through Reinforcement Learning\n  and Inference Scaling"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks. However, existing approaches mainly rely on imitation\nlearning and struggle to achieve effective test-time scaling. While\nreinforcement learning (RL) holds promise for enabling self-exploration, recent\nattempts yield modest improvements in complex reasoning. In this paper, we\npresent T1 to scale RL by encouraging exploration and understand inference\nscaling. We first initialize the LLM using synthesized chain-of-thought data\nthat integrates trial-and-error and self-verification. To scale RL training, we\npromote increased sampling diversity through oversampling. We demonstrate that\nT1 with open LLMs as its base exhibits inference scaling behavior and achieves\nsuperior performance on challenging math reasoning benchmarks. More\nimportantly, we present a simple strategy to examine inference scaling, where\nincreased inference budgets directly lead to T1's better performance without\nany additional verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\ncomplex reasoning tasks. However, existing approaches mainly rely on imitation\nlearning and struggle to achieve effective test-time scaling. While\nreinforcement learning (RL) holds promise for enabling self-exploration, recent\nattempts yield modest improvements in complex reasoning. In this paper, we\npresent T1 to scale RL by encouraging exploration and understand inference\nscaling. We first initialize the LLM using synthesized chain-of-thought data\nthat integrates trial-and-error and self-verification. To scale RL training, we\npromote increased sampling diversity through oversampling. We demonstrate that\nT1 with open LLMs as its base exhibits inference scaling behavior and achieves\nsuperior performance on challenging math reasoning benchmarks. More\nimportantly, we present a simple strategy to examine inference scaling, where\nincreased inference budgets directly lead to T1's better performance without\nany additional verification."
                },
                "authors": [
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Yujiang Li"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "arxiv_comment": "Accepted to ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11916v1",
                "updated": "2025-06-13T16:09:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    9,
                    21,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T16:09:21Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    16,
                    9,
                    21,
                    4,
                    164,
                    0
                ],
                "title": "mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity"
                },
                "summary": "We present a diffusion-based model recipe for real-world control of a highly\ndexterous humanoid robotic hand, designed for sample-efficient learning and\nsmooth fine-motor action inference. Our system features a newly designed 16-DoF\ntendon-driven hand, equipped with wide angle wrist cameras and mounted on a\nFranka Emika Panda arm. We develop a versatile teleoperation pipeline and data\ncollection protocol using both glove-based and VR interfaces, enabling\nhigh-quality data collection across diverse tasks such as pick and place, item\nsorting and assembly insertion. Leveraging high-frequency generative control,\nwe train end-to-end policies from raw sensory inputs, enabling smooth,\nself-correcting motions in complex manipulation scenarios. Real-world\nevaluations demonstrate up to 93.3% out of distribution success rates, with up\nto a +33.3% performance boost due to emergent self-correcting behaviors, while\nalso revealing scaling trends in policy performance. Our results advance the\nstate-of-the-art in dexterous robotic manipulation through a fully integrated,\npractical approach to hardware, learning, and real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a diffusion-based model recipe for real-world control of a highly\ndexterous humanoid robotic hand, designed for sample-efficient learning and\nsmooth fine-motor action inference. Our system features a newly designed 16-DoF\ntendon-driven hand, equipped with wide angle wrist cameras and mounted on a\nFranka Emika Panda arm. We develop a versatile teleoperation pipeline and data\ncollection protocol using both glove-based and VR interfaces, enabling\nhigh-quality data collection across diverse tasks such as pick and place, item\nsorting and assembly insertion. Leveraging high-frequency generative control,\nwe train end-to-end policies from raw sensory inputs, enabling smooth,\nself-correcting motions in complex manipulation scenarios. Real-world\nevaluations demonstrate up to 93.3% out of distribution success rates, with up\nto a +33.3% performance boost due to emergent self-correcting behaviors, while\nalso revealing scaling trends in policy performance. Our results advance the\nstate-of-the-art in dexterous robotic manipulation through a fully integrated,\npractical approach to hardware, learning, and real-world deployment."
                },
                "authors": [
                    {
                        "name": "Elvis Nava"
                    },
                    {
                        "name": "Victoriano Montesinos"
                    },
                    {
                        "name": "Erik Bauer"
                    },
                    {
                        "name": "Benedek Forrai"
                    },
                    {
                        "name": "Jonas Pai"
                    },
                    {
                        "name": "Stefan Weirich"
                    },
                    {
                        "name": "Stephan-Daniel Gravert"
                    },
                    {
                        "name": "Philipp Wand"
                    },
                    {
                        "name": "Stephan Polinski"
                    },
                    {
                        "name": "Benjamin F. Grewe"
                    },
                    {
                        "name": "Robert K. Katzschmann"
                    }
                ],
                "author_detail": {
                    "name": "Robert K. Katzschmann"
                },
                "author": "Robert K. Katzschmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11902v1",
                "updated": "2025-06-13T15:52:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    52,
                    37,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:52:37Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    52,
                    37,
                    4,
                    164,
                    0
                ],
                "title": "TreeRL: LLM Reinforcement Learning with On-Policy Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeRL: LLM Reinforcement Learning with On-Policy Tree Search"
                },
                "summary": "Reinforcement learning (RL) with tree search has demonstrated superior\nperformance in traditional reasoning tasks. Compared to conventional\nindependent chain sampling strategies with outcome supervision, tree search\nenables better exploration of the reasoning space and provides dense, on-policy\nprocess rewards during RL training but remains under-explored in On-Policy LLM\nRL. We propose TreeRL, a reinforcement learning framework that directly\nincorporates on-policy tree search for RL training. Our approach includes\nintermediate supervision and eliminates the need for a separate reward model\ntraining. Existing approaches typically train a separate process reward model,\nwhich can suffer from distribution mismatch and reward hacking. We also\nintroduce a cost-effective tree search approach that achieves higher search\nefficiency under the same generation token budget by strategically branching\nfrom high-uncertainty intermediate steps rather than using random branching.\nExperiments on challenging math and code reasoning benchmarks demonstrate that\nTreeRL achieves superior performance compared to traditional ChainRL,\nhighlighting the potential of tree search for LLM. TreeRL is open-sourced at\nhttps://github.com/THUDM/TreeRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) with tree search has demonstrated superior\nperformance in traditional reasoning tasks. Compared to conventional\nindependent chain sampling strategies with outcome supervision, tree search\nenables better exploration of the reasoning space and provides dense, on-policy\nprocess rewards during RL training but remains under-explored in On-Policy LLM\nRL. We propose TreeRL, a reinforcement learning framework that directly\nincorporates on-policy tree search for RL training. Our approach includes\nintermediate supervision and eliminates the need for a separate reward model\ntraining. Existing approaches typically train a separate process reward model,\nwhich can suffer from distribution mismatch and reward hacking. We also\nintroduce a cost-effective tree search approach that achieves higher search\nefficiency under the same generation token budget by strategically branching\nfrom high-uncertainty intermediate steps rather than using random branching.\nExperiments on challenging math and code reasoning benchmarks demonstrate that\nTreeRL achieves superior performance compared to traditional ChainRL,\nhighlighting the potential of tree search for LLM. TreeRL is open-sourced at\nhttps://github.com/THUDM/TreeRL."
                },
                "authors": [
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Ziniu Hu"
                    },
                    {
                        "name": "Yujiang Li"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "arxiv_comment": "Accepted to ACL 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18638v2",
                "updated": "2025-06-13T15:44:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    44,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-01-28T17:10:20Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    17,
                    10,
                    20,
                    1,
                    28,
                    0
                ],
                "title": "Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt\n  Generation for Enhanced LLM Content Moderation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt\n  Generation for Enhanced LLM Content Moderation"
                },
                "summary": "As large language models (LLMs) become increasingly prevalent, ensuring their\nrobustness against adversarial misuse is crucial. This paper introduces the GAP\n(Graph of Attacks with Pruning) framework, an advanced approach for generating\nstealthy jailbreak prompts to evaluate and enhance LLM safeguards. GAP\naddresses limitations in existing tree-based LLM jailbreak methods by\nimplementing an interconnected graph structure that enables knowledge sharing\nacross attack paths. Our experimental evaluation demonstrates GAP's superiority\nover existing techniques, achieving a 20.8% increase in attack success rates\nwhile reducing query costs by 62.7%. GAP consistently outperforms\nstate-of-the-art methods for attacking both open and closed LLMs, with attack\nsuccess rates of >96%. Additionally, we present specialized variants like\nGAP-Auto for automated seed generation and GAP-VLM for multimodal attacks.\nGAP-generated prompts prove highly effective in improving content moderation\nsystems, increasing true positive detection rates by 108.5% and accuracy by\n183.6% when used for fine-tuning. Our implementation is available at\nhttps://github.com/dsbuddy/GAP-LLM-Safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly prevalent, ensuring their\nrobustness against adversarial misuse is crucial. This paper introduces the GAP\n(Graph of Attacks with Pruning) framework, an advanced approach for generating\nstealthy jailbreak prompts to evaluate and enhance LLM safeguards. GAP\naddresses limitations in existing tree-based LLM jailbreak methods by\nimplementing an interconnected graph structure that enables knowledge sharing\nacross attack paths. Our experimental evaluation demonstrates GAP's superiority\nover existing techniques, achieving a 20.8% increase in attack success rates\nwhile reducing query costs by 62.7%. GAP consistently outperforms\nstate-of-the-art methods for attacking both open and closed LLMs, with attack\nsuccess rates of >96%. Additionally, we present specialized variants like\nGAP-Auto for automated seed generation and GAP-VLM for multimodal attacks.\nGAP-generated prompts prove highly effective in improving content moderation\nsystems, increasing true positive detection rates by 108.5% and accuracy by\n183.6% when used for fine-tuning. Our implementation is available at\nhttps://github.com/dsbuddy/GAP-LLM-Safety."
                },
                "authors": [
                    {
                        "name": "Daniel Schwartz"
                    },
                    {
                        "name": "Dmitriy Bespalov"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Ninad Kulkarni"
                    },
                    {
                        "name": "Yanjun Qi"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Qi"
                },
                "author": "Yanjun Qi",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02509v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02509v2",
                "updated": "2025-06-13T15:36:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    36,
                    41,
                    4,
                    164,
                    0
                ],
                "published": "2024-08-05T14:31:26Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    14,
                    31,
                    26,
                    0,
                    218,
                    0
                ],
                "title": "Black-Box Adversarial Attacks on LLM-Based Code Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-Box Adversarial Attacks on LLM-Based Code Completion"
                },
                "summary": "Modern code completion engines, powered by large language models (LLMs),\nassist millions of developers with their strong capabilities to generate\nfunctionally correct code. Due to this popularity, it is crucial to investigate\nthe security implications of relying on LLM-based code completion. In this\nwork, we demonstrate that state-of-the-art black-box LLM-based code completion\nengines can be stealthily biased by adversaries to significantly increase their\nrate of insecure code generation. We present the first attack, named INSEC,\nthat achieves this goal. INSEC works by injecting an attack string as a short\ncomment in the completion input. The attack string is crafted through a\nquery-based optimization procedure starting from a set of carefully designed\ninitialization schemes. We demonstrate INSEC's broad applicability and\neffectiveness by evaluating it on various state-of-the-art open-source models\nand black-box commercial services (e.g., OpenAI API and GitHub Copilot). On a\ndiverse set of security-critical test cases, covering 16 CWEs across 5\nprogramming languages, INSEC increases the rate of generated insecure code by\nmore than 50%, while maintaining the functional correctness of generated code.\nWe consider INSEC practical -- it requires low resources and costs less than 10\nUS dollars to develop on commodity hardware. Moreover, we showcase the attack's\nreal-world deployability, by developing an IDE plug-in that stealthily injects\nINSEC into the GitHub Copilot extension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern code completion engines, powered by large language models (LLMs),\nassist millions of developers with their strong capabilities to generate\nfunctionally correct code. Due to this popularity, it is crucial to investigate\nthe security implications of relying on LLM-based code completion. In this\nwork, we demonstrate that state-of-the-art black-box LLM-based code completion\nengines can be stealthily biased by adversaries to significantly increase their\nrate of insecure code generation. We present the first attack, named INSEC,\nthat achieves this goal. INSEC works by injecting an attack string as a short\ncomment in the completion input. The attack string is crafted through a\nquery-based optimization procedure starting from a set of carefully designed\ninitialization schemes. We demonstrate INSEC's broad applicability and\neffectiveness by evaluating it on various state-of-the-art open-source models\nand black-box commercial services (e.g., OpenAI API and GitHub Copilot). On a\ndiverse set of security-critical test cases, covering 16 CWEs across 5\nprogramming languages, INSEC increases the rate of generated insecure code by\nmore than 50%, while maintaining the functional correctness of generated code.\nWe consider INSEC practical -- it requires low resources and costs less than 10\nUS dollars to develop on commodity hardware. Moreover, we showcase the attack's\nreal-world deployability, by developing an IDE plug-in that stealthily injects\nINSEC into the GitHub Copilot extension."
                },
                "authors": [
                    {
                        "name": "Slobodan Jenko"
                    },
                    {
                        "name": "Niels Mündler"
                    },
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02509v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02509v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11887v1",
                "updated": "2025-06-13T15:36:22Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    36,
                    22,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:36:22Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    36,
                    22,
                    4,
                    164,
                    0
                ],
                "title": "Towards a Cascaded LLM Framework for Cost-effective Human-AI\n  Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Cascaded LLM Framework for Cost-effective Human-AI\n  Decision-Making"
                },
                "summary": "Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions."
                },
                "authors": [
                    {
                        "name": "Claudio Fanconi"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11886v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11886v1",
                "updated": "2025-06-13T15:35:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:35:54Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    35,
                    54,
                    4,
                    164,
                    0
                ],
                "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache"
                },
                "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Siyang He"
                    },
                    {
                        "name": "Qiqi Wang"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "10 pages, 7 figures, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11886v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11886v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11880v1",
                "updated": "2025-06-13T15:29:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    29,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:29:43Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    29,
                    43,
                    4,
                    164,
                    0
                ],
                "title": "Addressing Bias in LLMs: Strategies and Application to Fair AI-based\n  Recruitment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Bias in LLMs: Strategies and Application to Fair AI-based\n  Recruitment"
                },
                "summary": "The use of language technologies in high-stake settings is increasing in\nrecent years, mostly motivated by the success of Large Language Models (LLMs).\nHowever, despite the great performance of LLMs, they are are susceptible to\nethical concerns, such as demographic biases, accountability, or privacy. This\nwork seeks to analyze the capacity of Transformers-based systems to learn\ndemographic biases present in the data, using a case study on AI-based\nautomated recruitment. We propose a privacy-enhancing framework to reduce\ngender information from the learning pipeline as a way to mitigate biased\nbehaviors in the final tools. Our experiments analyze the influence of data\nbiases on systems built on two different LLMs, and how the proposed framework\neffectively prevents trained systems from reproducing the bias in the data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of language technologies in high-stake settings is increasing in\nrecent years, mostly motivated by the success of Large Language Models (LLMs).\nHowever, despite the great performance of LLMs, they are are susceptible to\nethical concerns, such as demographic biases, accountability, or privacy. This\nwork seeks to analyze the capacity of Transformers-based systems to learn\ndemographic biases present in the data, using a case study on AI-based\nautomated recruitment. We propose a privacy-enhancing framework to reduce\ngender information from the learning pipeline as a way to mitigate biased\nbehaviors in the final tools. Our experiments analyze the influence of data\nbiases on systems built on two different LLMs, and how the proposed framework\neffectively prevents trained systems from reproducing the bias in the data."
                },
                "authors": [
                    {
                        "name": "Alejandro Peña"
                    },
                    {
                        "name": "Julian Fierrez"
                    },
                    {
                        "name": "Aythami Morales"
                    },
                    {
                        "name": "Gonzalo Mancera"
                    },
                    {
                        "name": "Miguel Lopez"
                    },
                    {
                        "name": "Ruben Tolosana"
                    }
                ],
                "author_detail": {
                    "name": "Ruben Tolosana"
                },
                "author": "Ruben Tolosana",
                "arxiv_comment": "Submitted to AIES 2025 (Under Review)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11874v1",
                "updated": "2025-06-13T15:26:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    26,
                    58,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:26:58Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    26,
                    58,
                    4,
                    164,
                    0
                ],
                "title": "A Short Survey on Formalising Software Requirements using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Short Survey on Formalising Software Requirements using Large Language\n  Models"
                },
                "summary": "This paper presents a focused literature survey on the use of large language\nmodels (LLM) to assist in writing formal specifications for software. A summary\nof thirty-five key papers is presented, including examples for specifying\nprograms written in Dafny, C and Java. This paper arose from the project\nVERIFAI - Traceability and verification of natural language requirements that\naddresses the challenges in writing formal specifications from requirements\nthat are expressed in natural language. Our methodology employed multiple\nacademic databases to identify relevant research. The AI-assisted tool Elicit\nfacilitated the initial paper selection, which were manually screened for final\nselection. The survey provides valuable insights and future directions for\nutilising LLMs while formalising software requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a focused literature survey on the use of large language\nmodels (LLM) to assist in writing formal specifications for software. A summary\nof thirty-five key papers is presented, including examples for specifying\nprograms written in Dafny, C and Java. This paper arose from the project\nVERIFAI - Traceability and verification of natural language requirements that\naddresses the challenges in writing formal specifications from requirements\nthat are expressed in natural language. Our methodology employed multiple\nacademic databases to identify relevant research. The AI-assisted tool Elicit\nfacilitated the initial paper selection, which were manually screened for final\nselection. The survey provides valuable insights and future directions for\nutilising LLMs while formalising software requirements."
                },
                "authors": [
                    {
                        "name": "Arshad Beg"
                    },
                    {
                        "name": "Diarmuid O'Donoghue"
                    },
                    {
                        "name": "Rosemary Monahan"
                    }
                ],
                "author_detail": {
                    "name": "Rosemary Monahan"
                },
                "author": "Rosemary Monahan",
                "arxiv_comment": "Submitted to SAIV 2025 as extended abstract and received valuable\n  comments improving our draft. This version is the improved one after\n  addressing suggestions from reviewers for improving the draft",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.1; D.2.4; D.2.10; F.4.1; F.4.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11870v1",
                "updated": "2025-06-13T15:23:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    23,
                    7,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:23:07Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    23,
                    7,
                    4,
                    164,
                    0
                ],
                "title": "LLM-based Dynamic Differential Testing for Database Connectors with\n  Reinforcement Learning-Guided Prompt Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Dynamic Differential Testing for Database Connectors with\n  Reinforcement Learning-Guided Prompt Selection"
                },
                "summary": "Database connectors are critical components enabling applications to interact\nwith underlying database management systems (DBMS), yet their security\nvulnerabilities often remain overlooked. Unlike traditional software defects,\nconnector vulnerabilities exhibit subtle behavioral patterns and are inherently\nchallenging to detect. Besides, nonstandardized implementation of connectors\nleaves potential risks (a.k.a. unsafe implementations) but is more elusive. As\na result, traditional fuzzing methods are incapable of finding such\nvulnerabilities. Even for LLM-enable test case generation, due to a lack of\ndomain knowledge, they are also incapable of generating test cases that invoke\nall interface and internal logic of connectors. In this paper, we propose\nreinforcement learning (RL)-guided LLM test-case generation for database\nconnector testing. Specifically, to equip the LLM with sufficient and\nappropriate domain knowledge, a parameterized prompt template is composed which\ncan be utilized to generate numerous prompts. Test cases are generated via LLM\nwith a prompt, and are dynamically evaluated through differential testing\nacross multiple connectors. The testing is iteratively conducted, with each\nround RL is adopted to select optimal prompt based on prior-round behavioral\nfeedback, so as to maximize control flow coverage. We implement aforementioned\nmethodology in a practical tool and evaluate it on two widely used JDBC\nconnectors: MySQL Connector/J and OceanBase Connector/J. In total, we reported\n16 bugs, among them 10 are officially confirmed and the rest are acknowledged\nas unsafe implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Database connectors are critical components enabling applications to interact\nwith underlying database management systems (DBMS), yet their security\nvulnerabilities often remain overlooked. Unlike traditional software defects,\nconnector vulnerabilities exhibit subtle behavioral patterns and are inherently\nchallenging to detect. Besides, nonstandardized implementation of connectors\nleaves potential risks (a.k.a. unsafe implementations) but is more elusive. As\na result, traditional fuzzing methods are incapable of finding such\nvulnerabilities. Even for LLM-enable test case generation, due to a lack of\ndomain knowledge, they are also incapable of generating test cases that invoke\nall interface and internal logic of connectors. In this paper, we propose\nreinforcement learning (RL)-guided LLM test-case generation for database\nconnector testing. Specifically, to equip the LLM with sufficient and\nappropriate domain knowledge, a parameterized prompt template is composed which\ncan be utilized to generate numerous prompts. Test cases are generated via LLM\nwith a prompt, and are dynamically evaluated through differential testing\nacross multiple connectors. The testing is iteratively conducted, with each\nround RL is adopted to select optimal prompt based on prior-round behavioral\nfeedback, so as to maximize control flow coverage. We implement aforementioned\nmethodology in a practical tool and evaluate it on two widely used JDBC\nconnectors: MySQL Connector/J and OceanBase Connector/J. In total, we reported\n16 bugs, among them 10 are officially confirmed and the rest are acknowledged\nas unsafe implementations."
                },
                "authors": [
                    {
                        "name": "Ce Lyu"
                    },
                    {
                        "name": "Minghao Zhao"
                    },
                    {
                        "name": "Yanhao Wang"
                    },
                    {
                        "name": "Liang Jie"
                    }
                ],
                "author_detail": {
                    "name": "Liang Jie"
                },
                "author": "Liang Jie",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13615v2",
                "updated": "2025-06-13T15:09:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    9,
                    26,
                    4,
                    164,
                    0
                ],
                "published": "2025-04-18T10:43:21Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    10,
                    43,
                    21,
                    4,
                    108,
                    0
                ],
                "title": "Long-context Non-factoid Question Answering in Indic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context Non-factoid Question Answering in Indic Languages"
                },
                "summary": "Question Answering (QA) tasks, which involve extracting answers from a given\ncontext, are relatively straightforward for modern Large Language Models (LLMs)\nwhen the context is short. However, long contexts pose challenges due to the\nquadratic complexity of the self-attention mechanism. This challenge is\ncompounded in Indic languages, which are often low-resource. This study\nexplores context-shortening techniques, including Open Information Extraction\n(OIE), coreference resolution, Answer Paragraph Selection (APS), and their\ncombinations, to improve QA performance. Compared to the baseline of\nunshortened (long) contexts, our experiments on four Indic languages (Hindi,\nTamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield\nan average improvement of 4\\% in semantic scores and 47\\% in token-level scores\nwhen evaluated on three popular LLMs without fine-tuning. Furthermore, with\nfine-tuning, we achieve an average increase of 2\\% in both semantic and\ntoken-level scores. Additionally, context-shortening reduces computational\noverhead. Explainability techniques like LIME and SHAP reveal that when the APS\nmodel confidently identifies the paragraph containing the answer, nearly all\ntokens within the selected text receive high relevance scores. However, the\nstudy also highlights the limitations of LLM-based QA systems in addressing\nnon-factoid questions, particularly those requiring reasoning or debate.\nMoreover, verbalizing OIE-generated triples does not enhance system\nperformance. These findings emphasize the potential of context-shortening\ntechniques to improve the efficiency and effectiveness of LLM-based QA systems,\nespecially for low-resource languages. The source code and resources are\navailable at https://github.com/ritwikmishra/IndicGenQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question Answering (QA) tasks, which involve extracting answers from a given\ncontext, are relatively straightforward for modern Large Language Models (LLMs)\nwhen the context is short. However, long contexts pose challenges due to the\nquadratic complexity of the self-attention mechanism. This challenge is\ncompounded in Indic languages, which are often low-resource. This study\nexplores context-shortening techniques, including Open Information Extraction\n(OIE), coreference resolution, Answer Paragraph Selection (APS), and their\ncombinations, to improve QA performance. Compared to the baseline of\nunshortened (long) contexts, our experiments on four Indic languages (Hindi,\nTamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield\nan average improvement of 4\\% in semantic scores and 47\\% in token-level scores\nwhen evaluated on three popular LLMs without fine-tuning. Furthermore, with\nfine-tuning, we achieve an average increase of 2\\% in both semantic and\ntoken-level scores. Additionally, context-shortening reduces computational\noverhead. Explainability techniques like LIME and SHAP reveal that when the APS\nmodel confidently identifies the paragraph containing the answer, nearly all\ntokens within the selected text receive high relevance scores. However, the\nstudy also highlights the limitations of LLM-based QA systems in addressing\nnon-factoid questions, particularly those requiring reasoning or debate.\nMoreover, verbalizing OIE-generated triples does not enhance system\nperformance. These findings emphasize the potential of context-shortening\ntechniques to improve the efficiency and effectiveness of LLM-based QA systems,\nespecially for low-resource languages. The source code and resources are\navailable at https://github.com/ritwikmishra/IndicGenQA."
                },
                "authors": [
                    {
                        "name": "Ritwik Mishra"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    },
                    {
                        "name": "Ponnurangam Kumaraguru"
                    }
                ],
                "author_detail": {
                    "name": "Ponnurangam Kumaraguru"
                },
                "author": "Ponnurangam Kumaraguru",
                "arxiv_comment": "Short version of this manuscript accepted at\n  https://bda2025.iiitb.net/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09347v2",
                "updated": "2025-06-13T15:07:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    7,
                    8,
                    4,
                    164,
                    0
                ],
                "published": "2025-03-12T12:49:02Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    12,
                    49,
                    2,
                    2,
                    71,
                    0
                ],
                "title": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed as automated\nevaluators to assess the safety of generated content, yet their reliability in\nthis role remains uncertain. This study evaluates a diverse set of 11 LLM judge\nmodels across critical safety domains, examining three key aspects:\nself-consistency in repeated judging tasks, alignment with human judgments, and\nsusceptibility to input artifacts such as apologetic or verbose phrasing. Our\nfindings reveal that biases in LLM judges can significantly distort the final\nverdict on which content source is safer, undermining the validity of\ncomparative evaluations. Notably, apologetic language artifacts alone can skew\nevaluator preferences by up to 98\\%. Contrary to expectations, larger models do\nnot consistently exhibit greater robustness, while smaller models sometimes\nshow higher resistance to specific artifacts. To mitigate LLM evaluator\nrobustness issues, we investigate jury-based evaluations aggregating decisions\nfrom multiple models. Although this approach both improves robustness and\nenhances alignment to human judgements, artifact sensitivity persists even with\nthe best jury configurations. These results highlight the urgent need for\ndiversified, artifact-resistant methodologies to ensure reliable safety\nassessments."
                },
                "authors": [
                    {
                        "name": "Hongyu Chen"
                    },
                    {
                        "name": "Seraphina Goldfarb-Tarrant"
                    }
                ],
                "author_detail": {
                    "name": "Seraphina Goldfarb-Tarrant"
                },
                "author": "Seraphina Goldfarb-Tarrant",
                "arxiv_comment": "9 pages, ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11857v1",
                "updated": "2025-06-13T15:04:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    4,
                    1,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T15:04:01Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    4,
                    1,
                    4,
                    164,
                    0
                ],
                "title": "Post Persona Alignment for Multi-Session Dialogue Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post Persona Alignment for Multi-Session Dialogue Generation"
                },
                "summary": "Multi-session persona-based dialogue generation presents challenges in\nmaintaining long-term consistency and generating diverse, personalized\nresponses. While large language models (LLMs) excel in single-session\ndialogues, they struggle to preserve persona fidelity and conversational\ncoherence across extended interactions. Existing methods typically retrieve\npersona information before response generation, which can constrain diversity\nand result in generic outputs. We propose Post Persona Alignment (PPA), a novel\ntwo-stage framework that reverses this process. PPA first generates a general\nresponse based solely on dialogue context, then retrieves relevant persona\nmemories using the response as a query, and finally refines the response to\nalign with the speaker's persona. This post-hoc alignment strategy promotes\nnaturalness and diversity while preserving consistency and personalization.\nExperiments on multi-session LLM-generated dialogue data demonstrate that PPA\nsignificantly outperforms prior approaches in consistency, diversity, and\npersona relevance, offering a more flexible and effective paradigm for\nlong-term personalized dialogue generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-session persona-based dialogue generation presents challenges in\nmaintaining long-term consistency and generating diverse, personalized\nresponses. While large language models (LLMs) excel in single-session\ndialogues, they struggle to preserve persona fidelity and conversational\ncoherence across extended interactions. Existing methods typically retrieve\npersona information before response generation, which can constrain diversity\nand result in generic outputs. We propose Post Persona Alignment (PPA), a novel\ntwo-stage framework that reverses this process. PPA first generates a general\nresponse based solely on dialogue context, then retrieves relevant persona\nmemories using the response as a query, and finally refines the response to\nalign with the speaker's persona. This post-hoc alignment strategy promotes\nnaturalness and diversity while preserving consistency and personalization.\nExperiments on multi-session LLM-generated dialogue data demonstrate that PPA\nsignificantly outperforms prior approaches in consistency, diversity, and\npersona relevance, offering a more flexible and effective paradigm for\nlong-term personalized dialogue generation."
                },
                "authors": [
                    {
                        "name": "Yi-Pei Chen"
                    },
                    {
                        "name": "Noriki Nishida"
                    },
                    {
                        "name": "Hideki Nakayama"
                    },
                    {
                        "name": "Yuji Matsumoto"
                    }
                ],
                "author_detail": {
                    "name": "Yuji Matsumoto"
                },
                "author": "Yuji Matsumoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12055v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12055v2",
                "updated": "2025-06-13T15:03:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    3,
                    21,
                    4,
                    164,
                    0
                ],
                "published": "2024-09-18T15:30:29Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    15,
                    30,
                    29,
                    2,
                    262,
                    0
                ],
                "title": "Artemis: Efficient Commit-and-Prove SNARKs for zkML",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artemis: Efficient Commit-and-Prove SNARKs for zkML"
                },
                "summary": "Ensuring that AI models are both verifiable and privacy-preserving is\nimportant for trust, accountability, and compliance. To address these concerns,\nrecent research has focused on developing zero-knowledge machine learning\n(zkML) techniques that enable the verification of various aspects of ML models\nwithout revealing sensitive information. However, while recent zkML advances\nhave made significant improvements to the efficiency of proving ML\ncomputations, they have largely overlooked the costly consistency checks on\ncommitted model parameters and input data, which have become a dominant\nperformance bottleneck. To address this gap, this paper introduces a new\nCommit-and-Prove SNARK (CP-SNARK) construction, Artemis, that effectively\naddresses the emerging challenge of commitment verification in zkML pipelines.\nIn contrast to existing approaches, Artemis is compatible with any homomorphic\npolynomial commitment, including those without trusted setup. We present the\nfirst implementation of this CP-SNARK, evaluate its performance on a diverse\nset of ML models, and show substantial improvements over existing methods,\nachieving significant reductions in prover costs and maintaining efficiency\neven for large-scale models. For example, for the VGG model, we reduce the\noverhead associated with commitment checks from 11.5x to 1.1x. Our results\nindicate that Artemis provides a concrete step toward practical deployment of\nzkML, particularly in settings involving large-scale or complex models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring that AI models are both verifiable and privacy-preserving is\nimportant for trust, accountability, and compliance. To address these concerns,\nrecent research has focused on developing zero-knowledge machine learning\n(zkML) techniques that enable the verification of various aspects of ML models\nwithout revealing sensitive information. However, while recent zkML advances\nhave made significant improvements to the efficiency of proving ML\ncomputations, they have largely overlooked the costly consistency checks on\ncommitted model parameters and input data, which have become a dominant\nperformance bottleneck. To address this gap, this paper introduces a new\nCommit-and-Prove SNARK (CP-SNARK) construction, Artemis, that effectively\naddresses the emerging challenge of commitment verification in zkML pipelines.\nIn contrast to existing approaches, Artemis is compatible with any homomorphic\npolynomial commitment, including those without trusted setup. We present the\nfirst implementation of this CP-SNARK, evaluate its performance on a diverse\nset of ML models, and show substantial improvements over existing methods,\nachieving significant reductions in prover costs and maintaining efficiency\neven for large-scale models. For example, for the VGG model, we reduce the\noverhead associated with commitment checks from 11.5x to 1.1x. Our results\nindicate that Artemis provides a concrete step toward practical deployment of\nzkML, particularly in settings involving large-scale or complex models."
                },
                "authors": [
                    {
                        "name": "Hidde Lycklama"
                    },
                    {
                        "name": "Alexander Viand"
                    },
                    {
                        "name": "Nikolay Avramov"
                    },
                    {
                        "name": "Nicolas Küchler"
                    },
                    {
                        "name": "Anwar Hithnawi"
                    }
                ],
                "author_detail": {
                    "name": "Anwar Hithnawi"
                },
                "author": "Anwar Hithnawi",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12055v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00073v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00073v3",
                "updated": "2025-06-13T15:02:02Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    15,
                    2,
                    2,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-29T17:41:39Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    41,
                    39,
                    3,
                    149,
                    0
                ],
                "title": "The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and\n  Transactions in Consumer Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and\n  Transactions in Consumer Markets"
                },
                "summary": "AI agents are increasingly used in consumer-facing applications to assist\nwith tasks such as product search, negotiation, and transaction execution. In\nthis paper, we explore a future scenario where both consumers and merchants\nauthorize AI agents to fully automate negotiations and transactions. We aim to\nanswer two key questions: (1) Do different LLM agents vary in their ability to\nsecure favorable deals for users? (2) What risks arise from fully automating\ndeal-making with AI agents in consumer markets? To address these questions, we\ndevelop an experimental framework that evaluates the performance of various LLM\nagents in real-world negotiation and transaction settings. Our findings reveal\nthat AI-mediated deal-making is an inherently imbalanced game -- different\nagents achieve significantly different outcomes for their users. Moreover,\nbehavioral anomalies in LLMs can result in financial losses for both consumers\nand merchants, such as overspending or accepting unreasonable deals. These\nresults underscore that while automation can improve efficiency, it also\nintroduces substantial risks. Users should exercise caution when delegating\nbusiness decisions to AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents are increasingly used in consumer-facing applications to assist\nwith tasks such as product search, negotiation, and transaction execution. In\nthis paper, we explore a future scenario where both consumers and merchants\nauthorize AI agents to fully automate negotiations and transactions. We aim to\nanswer two key questions: (1) Do different LLM agents vary in their ability to\nsecure favorable deals for users? (2) What risks arise from fully automating\ndeal-making with AI agents in consumer markets? To address these questions, we\ndevelop an experimental framework that evaluates the performance of various LLM\nagents in real-world negotiation and transaction settings. Our findings reveal\nthat AI-mediated deal-making is an inherently imbalanced game -- different\nagents achieve significantly different outcomes for their users. Moreover,\nbehavioral anomalies in LLMs can result in financial losses for both consumers\nand merchants, such as overspending or accepting unreasonable deals. These\nresults underscore that while automation can improve efficiency, it also\nintroduces substantial risks. Users should exercise caution when delegating\nbusiness decisions to AI agents."
                },
                "authors": [
                    {
                        "name": "Shenzhe Zhu"
                    },
                    {
                        "name": "Jiao Sun"
                    },
                    {
                        "name": "Yi Nian"
                    },
                    {
                        "name": "Tobin South"
                    },
                    {
                        "name": "Alex Pentland"
                    },
                    {
                        "name": "Jiaxin Pei"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxin Pei"
                },
                "author": "Jiaxin Pei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00073v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00073v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19645v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19645v2",
                "updated": "2025-06-13T14:54:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    54,
                    40,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-26T08:01:45Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    8,
                    1,
                    45,
                    0,
                    146,
                    0
                ],
                "title": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse\n  MoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse\n  MoE"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across many\napplications, with Mixture of Experts (MoE) models demonstrating great\npotential. Compared to traditional dense models, MoEs achieve better\nperformance with less computation. Speculative decoding (SD) is a widely used\ntechnique to accelerate LLM inference without accuracy loss, but it has been\nconsidered efficient only for dense models. In this work, we first demonstrate\nthat, under medium batch sizes, MoE surprisingly benefits more from SD than\ndense models. Furthermore, as MoE becomes sparser -- the prevailing trend in\nMoE designs -- the batch size range where SD acceleration is expected to be\neffective becomes broader. To quantitatively understand tradeoffs involved in\nSD, we develop a reliable modeling based on theoretical analyses. While current\nSD research primarily focuses on improving acceptance rates of algorithms,\nchanges in workload and model architecture can still lead to degraded SD\nacceleration even with high acceptance rates. To address this limitation, we\nintroduce a new metric 'target efficiency' that characterizes these effects,\nthus helping researchers identify system bottlenecks and understand SD\nacceleration more comprehensively. For scenarios like private serving, this\nwork unveils a new perspective to speed up MoE inference, where existing\nsolutions struggle. Experiments on different GPUs show up to 2.29x speedup for\nQwen2-57B-A14B at medium batch sizes and validate our theoretical predictions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across many\napplications, with Mixture of Experts (MoE) models demonstrating great\npotential. Compared to traditional dense models, MoEs achieve better\nperformance with less computation. Speculative decoding (SD) is a widely used\ntechnique to accelerate LLM inference without accuracy loss, but it has been\nconsidered efficient only for dense models. In this work, we first demonstrate\nthat, under medium batch sizes, MoE surprisingly benefits more from SD than\ndense models. Furthermore, as MoE becomes sparser -- the prevailing trend in\nMoE designs -- the batch size range where SD acceleration is expected to be\neffective becomes broader. To quantitatively understand tradeoffs involved in\nSD, we develop a reliable modeling based on theoretical analyses. While current\nSD research primarily focuses on improving acceptance rates of algorithms,\nchanges in workload and model architecture can still lead to degraded SD\nacceleration even with high acceptance rates. To address this limitation, we\nintroduce a new metric 'target efficiency' that characterizes these effects,\nthus helping researchers identify system bottlenecks and understand SD\nacceleration more comprehensively. For scenarios like private serving, this\nwork unveils a new perspective to speed up MoE inference, where existing\nsolutions struggle. Experiments on different GPUs show up to 2.29x speedup for\nQwen2-57B-A14B at medium batch sizes and validate our theoretical predictions."
                },
                "authors": [
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Lei Zhu"
                    },
                    {
                        "name": "Zongyuan Zhan"
                    },
                    {
                        "name": "Ting Hu"
                    },
                    {
                        "name": "Weikai Mao"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Yongpan Liu"
                    },
                    {
                        "name": "Tianyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Zhang"
                },
                "author": "Tianyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19645v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19645v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11844v1",
                "updated": "2025-06-13T14:48:01Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    48,
                    1,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T14:48:01Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    48,
                    1,
                    4,
                    164,
                    0
                ],
                "title": "TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text,\n  and Structure Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text,\n  and Structure Attacks"
                },
                "summary": "Inspired by the success of large language models (LLMs), there is a\nsignificant research shift from traditional graph learning methods to LLM-based\ngraph frameworks, formally known as GraphLLMs. GraphLLMs leverage the reasoning\npower of LLMs by integrating three key components: the textual attributes of\ninput nodes, the structural information of node neighborhoods, and\ntask-specific prompts that guide decision-making. Despite their promise, the\nrobustness of GraphLLMs against adversarial perturbations remains largely\nunexplored-a critical concern for deploying these models in high-stakes\nscenarios. To bridge the gap, we introduce TrustGLM, a comprehensive study\nevaluating the vulnerability of GraphLLMs to adversarial attacks across three\ndimensions: text, graph structure, and prompt manipulations. We implement\nstate-of-the-art attack algorithms from each perspective to rigorously assess\nmodel resilience. Through extensive experiments on six benchmark datasets from\ndiverse domains, our findings reveal that GraphLLMs are highly susceptible to\ntext attacks that merely replace a few semantically similar words in a node's\ntextual attribute. We also find that standard graph structure attack methods\ncan significantly degrade model performance, while random shuffling of the\ncandidate label set in prompt templates leads to substantial performance drops.\nBeyond characterizing these vulnerabilities, we investigate defense techniques\ntailored to each attack vector through data-augmented training and adversarial\ntraining, which show promising potential to enhance the robustness of\nGraphLLMs. We hope that our open-sourced library will facilitate rapid,\nequitable evaluation and inspire further innovative research in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by the success of large language models (LLMs), there is a\nsignificant research shift from traditional graph learning methods to LLM-based\ngraph frameworks, formally known as GraphLLMs. GraphLLMs leverage the reasoning\npower of LLMs by integrating three key components: the textual attributes of\ninput nodes, the structural information of node neighborhoods, and\ntask-specific prompts that guide decision-making. Despite their promise, the\nrobustness of GraphLLMs against adversarial perturbations remains largely\nunexplored-a critical concern for deploying these models in high-stakes\nscenarios. To bridge the gap, we introduce TrustGLM, a comprehensive study\nevaluating the vulnerability of GraphLLMs to adversarial attacks across three\ndimensions: text, graph structure, and prompt manipulations. We implement\nstate-of-the-art attack algorithms from each perspective to rigorously assess\nmodel resilience. Through extensive experiments on six benchmark datasets from\ndiverse domains, our findings reveal that GraphLLMs are highly susceptible to\ntext attacks that merely replace a few semantically similar words in a node's\ntextual attribute. We also find that standard graph structure attack methods\ncan significantly degrade model performance, while random shuffling of the\ncandidate label set in prompt templates leads to substantial performance drops.\nBeyond characterizing these vulnerabilities, we investigate defense techniques\ntailored to each attack vector through data-augmented training and adversarial\ntraining, which show promising potential to enhance the robustness of\nGraphLLMs. We hope that our open-sourced library will facilitate rapid,\nequitable evaluation and inspire further innovative research in this field."
                },
                "authors": [
                    {
                        "name": "Qihai Zhang"
                    },
                    {
                        "name": "Xinyue Sheng"
                    },
                    {
                        "name": "Yuanfu Sun"
                    },
                    {
                        "name": "Qiaoyu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Qiaoyu Tan"
                },
                "author": "Qiaoyu Tan",
                "arxiv_comment": "12 pages, 5 figures, in KDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11842v1",
                "updated": "2025-06-13T14:44:11Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    44,
                    11,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T14:44:11Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    44,
                    11,
                    4,
                    164,
                    0
                ],
                "title": "Your Ride, Your Rules: Psychology and Cognition Enabled Automated\n  Driving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Ride, Your Rules: Psychology and Cognition Enabled Automated\n  Driving Systems"
                },
                "summary": "Despite rapid advances in autonomous driving, current autonomous vehicles\n(AVs) lack effective bidirectional communication with occupants, limiting\npersonalization and recovery from immobilization. This reduces comfort and\ntrust, potentially slowing broader AV adoption. We propose PACE-ADS (Psychology\nand Cognition Enabled Automated Driving Systems), a human-centered autonomy\nframework that enables AVs to sense, interpret, and respond to both external\ntraffic and internal occupant states. PACE-ADS comprises three foundation\nmodel-based agents: a Driver Agent that analyzes the driving context, a\nPsychologist Agent that interprets occupant psychological signals (e.g., EEG,\nheart rate, facial expressions) and cognitive commands (e.g., speech), and a\nCoordinator Agent that integrates these inputs to produce high-level behavior\ndecisions and operational parameters. Rather than replacing existing AV\nmodules, PACE-ADS complements them by operating at the behavioral level,\ndelegating low-level control to native AV systems. This separation enables\nclosed-loop adaptation and supports integration across diverse platforms. We\nevaluate PACE-ADS in simulation across varied scenarios involving traffic\nlights, pedestrians, work zones, and car following. Results show that PACE-ADS\nadapts driving styles to occupant states, improves ride comfort, and enables\nsafe recovery from immobilization via autonomous reasoning or human guidance.\nOur findings highlight the promise of LLM-based frameworks for bridging the gap\nbetween machine autonomy and human-centered driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite rapid advances in autonomous driving, current autonomous vehicles\n(AVs) lack effective bidirectional communication with occupants, limiting\npersonalization and recovery from immobilization. This reduces comfort and\ntrust, potentially slowing broader AV adoption. We propose PACE-ADS (Psychology\nand Cognition Enabled Automated Driving Systems), a human-centered autonomy\nframework that enables AVs to sense, interpret, and respond to both external\ntraffic and internal occupant states. PACE-ADS comprises three foundation\nmodel-based agents: a Driver Agent that analyzes the driving context, a\nPsychologist Agent that interprets occupant psychological signals (e.g., EEG,\nheart rate, facial expressions) and cognitive commands (e.g., speech), and a\nCoordinator Agent that integrates these inputs to produce high-level behavior\ndecisions and operational parameters. Rather than replacing existing AV\nmodules, PACE-ADS complements them by operating at the behavioral level,\ndelegating low-level control to native AV systems. This separation enables\nclosed-loop adaptation and supports integration across diverse platforms. We\nevaluate PACE-ADS in simulation across varied scenarios involving traffic\nlights, pedestrians, work zones, and car following. Results show that PACE-ADS\nadapts driving styles to occupant states, improves ride comfort, and enables\nsafe recovery from immobilization via autonomous reasoning or human guidance.\nOur findings highlight the promise of LLM-based frameworks for bridging the gap\nbetween machine autonomy and human-centered driving."
                },
                "authors": [
                    {
                        "name": "Zhipeng Bao"
                    },
                    {
                        "name": "Qianwen Li"
                    }
                ],
                "author_detail": {
                    "name": "Qianwen Li"
                },
                "author": "Qianwen Li",
                "arxiv_comment": "10 figures,29 pages, one colummn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11828v1",
                "updated": "2025-06-13T14:32:20Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    32,
                    20,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T14:32:20Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    32,
                    20,
                    4,
                    164,
                    0
                ],
                "title": "A Tale of Two Mobile Generations: 5G-Advanced and 6G in 3GPP Release 20",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Tale of Two Mobile Generations: 5G-Advanced and 6G in 3GPP Release 20"
                },
                "summary": "As the telecommunications industry stands at the crossroads between the fifth\ngeneration (5G) and sixth generation (6G) of mobile communications, the 3rd\ngeneration partnership project (3GPP) Release 20 emerges as a pivotal point of\ntransition. By striking a balance between enhancing 5G-Advanced capabilities\nand setting the stage for 6G, Release 20 provides the crucial foundation upon\nwhich future mobile communication standards and deployments will be built. This\narticle examines these dual objectives, outlining the key enhancements, the\nmotivations behind them, and their implications for the future of mobile\ncommunications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the telecommunications industry stands at the crossroads between the fifth\ngeneration (5G) and sixth generation (6G) of mobile communications, the 3rd\ngeneration partnership project (3GPP) Release 20 emerges as a pivotal point of\ntransition. By striking a balance between enhancing 5G-Advanced capabilities\nand setting the stage for 6G, Release 20 provides the crucial foundation upon\nwhich future mobile communication standards and deployments will be built. This\narticle examines these dual objectives, outlining the key enhancements, the\nmotivations behind them, and their implications for the future of mobile\ncommunications."
                },
                "authors": [
                    {
                        "name": "Xingqin Lin"
                    }
                ],
                "author_detail": {
                    "name": "Xingqin Lin"
                },
                "author": "Xingqin Lin",
                "arxiv_comment": "9 pages, 5 figures, 1 table, submitted for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11825v1",
                "updated": "2025-06-13T14:30:37Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    30,
                    37,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T14:30:37Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    30,
                    37,
                    4,
                    164,
                    0
                ],
                "title": "Revealing Political Bias in LLMs through Structured Multi-Agent Debate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Political Bias in LLMs through Structured Multi-Agent Debate"
                },
                "summary": "Large language models (LLMs) are increasingly used to simulate social\nbehaviour, yet their political biases and interaction dynamics in debates\nremain underexplored. We investigate how LLM type and agent gender attributes\ninfluence political bias using a structured multi-agent debate framework, by\nengaging Neutral, Republican, and Democrat American LLM agents in debates on\npolitically sensitive topics. We systematically vary the underlying LLMs, agent\ngenders, and debate formats to examine how model provenance and agent personas\ninfluence political bias and attitudes throughout debates. We find that Neutral\nagents consistently align with Democrats, while Republicans shift closer to the\nNeutral; gender influences agent attitudes, with agents adapting their opinions\nwhen aware of other agents' genders; and contrary to prior research, agents\nwith shared political affiliations can form echo chambers, exhibiting the\nexpected intensification of attitudes as debates progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used to simulate social\nbehaviour, yet their political biases and interaction dynamics in debates\nremain underexplored. We investigate how LLM type and agent gender attributes\ninfluence political bias using a structured multi-agent debate framework, by\nengaging Neutral, Republican, and Democrat American LLM agents in debates on\npolitically sensitive topics. We systematically vary the underlying LLMs, agent\ngenders, and debate formats to examine how model provenance and agent personas\ninfluence political bias and attitudes throughout debates. We find that Neutral\nagents consistently align with Democrats, while Republicans shift closer to the\nNeutral; gender influences agent attitudes, with agents adapting their opinions\nwhen aware of other agents' genders; and contrary to prior research, agents\nwith shared political affiliations can form echo chambers, exhibiting the\nexpected intensification of attitudes as debates progress."
                },
                "authors": [
                    {
                        "name": "Aishwarya Bandaru"
                    },
                    {
                        "name": "Fabian Bindley"
                    },
                    {
                        "name": "Trevor Bluth"
                    },
                    {
                        "name": "Nandini Chavda"
                    },
                    {
                        "name": "Baixu Chen"
                    },
                    {
                        "name": "Ethan Law"
                    }
                ],
                "author_detail": {
                    "name": "Ethan Law"
                },
                "author": "Ethan Law",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00856v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00856v2",
                "updated": "2025-06-13T14:28:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    28,
                    21,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-01T06:34:42Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    6,
                    34,
                    42,
                    6,
                    152,
                    0
                ],
                "title": "Can AI Master Econometrics? Evidence from Econometrics AI Agent on\n  Expert-Level Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI Master Econometrics? Evidence from Econometrics AI Agent on\n  Expert-Level Tasks"
                },
                "summary": "Can AI effectively perform complex econometric analysis traditionally\nrequiring human expertise? This paper evaluates AI agents' capability to master\neconometrics, focusing on empirical analysis performance. We develop an\n``Econometrics AI Agent'' built on the open-source MetaGPT framework. This\nagent exhibits outstanding performance in: (1) planning econometric tasks\nstrategically, (2) generating and executing code, (3) employing error-based\nreflection for improved robustness, and (4) allowing iterative refinement\nthrough multi-round conversations. We construct two datasets from academic\ncoursework materials and published research papers to evaluate performance\nagainst real-world challenges. Comparative testing shows our domain-specialized\nAI agent significantly outperforms both benchmark large language models (LLMs)\nand general-purpose AI agents. This work establishes a testbed for exploring\nAI's impact on social science research and enables cost-effective integration\nof domain expertise, making advanced econometric methods accessible to users\nwith minimal coding skills. Furthermore, our AI agent enhances research\nreproducibility and offers promising pedagogical applications for econometrics\nteaching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI effectively perform complex econometric analysis traditionally\nrequiring human expertise? This paper evaluates AI agents' capability to master\neconometrics, focusing on empirical analysis performance. We develop an\n``Econometrics AI Agent'' built on the open-source MetaGPT framework. This\nagent exhibits outstanding performance in: (1) planning econometric tasks\nstrategically, (2) generating and executing code, (3) employing error-based\nreflection for improved robustness, and (4) allowing iterative refinement\nthrough multi-round conversations. We construct two datasets from academic\ncoursework materials and published research papers to evaluate performance\nagainst real-world challenges. Comparative testing shows our domain-specialized\nAI agent significantly outperforms both benchmark large language models (LLMs)\nand general-purpose AI agents. This work establishes a testbed for exploring\nAI's impact on social science research and enables cost-effective integration\nof domain expertise, making advanced econometric methods accessible to users\nwith minimal coding skills. Furthermore, our AI agent enhances research\nreproducibility and offers promising pedagogical applications for econometrics\nteaching."
                },
                "authors": [
                    {
                        "name": "Qiang Chen"
                    },
                    {
                        "name": "Tianyang Han"
                    },
                    {
                        "name": "Jin Li"
                    },
                    {
                        "name": "Ye Luo"
                    },
                    {
                        "name": "Yuxiao Wu"
                    },
                    {
                        "name": "Xiaowei Zhang"
                    },
                    {
                        "name": "Tuo Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhou"
                },
                "author": "Tuo Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00856v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00856v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07450v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07450v4",
                "updated": "2025-06-13T14:27:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    27,
                    46,
                    4,
                    164,
                    0
                ],
                "published": "2025-03-10T15:30:05Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    15,
                    30,
                    5,
                    0,
                    69,
                    0
                ],
                "title": "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Idea to Implementation: Evaluating the Influence of Large Language\n  Models in Software Development -- An Opinion Paper"
                },
                "summary": "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The introduction of transformer architecture was a turning point in Natural\nLanguage Processing (NLP). Models based on the transformer architecture such as\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformer (GPT) have gained widespread popularity in various\napplications such as software development and education. The availability of\nLarge Language Models (LLMs) such as ChatGPT and Bard to the general public has\nshowcased the tremendous potential of these models and encouraged their\nintegration into various domains such as software development for tasks such as\ncode generation, debugging, and documentation generation. In this study,\nopinions from 11 experts regarding their experience with LLMs for software\ndevelopment have been gathered and analysed to draw insights that can guide\nsuccessful and responsible integration. The overall opinion of the experts is\npositive, with the experts identifying advantages such as increase in\nproductivity and reduced coding time. Potential concerns and challenges such as\nrisk of over-dependence and ethical considerations have also been highlighted."
                },
                "authors": [
                    {
                        "name": "Sargam Yadav"
                    },
                    {
                        "name": "Asifa Mehmood Qureshi"
                    },
                    {
                        "name": "Abhishek Kaushik"
                    },
                    {
                        "name": "Shubham Sharma"
                    },
                    {
                        "name": "Roisin Loughran"
                    },
                    {
                        "name": "Subramaniam Kazhuparambil"
                    },
                    {
                        "name": "Andrew Shaw"
                    },
                    {
                        "name": "Mohammed Sabry"
                    },
                    {
                        "name": "Niamh St John Lynch"
                    },
                    {
                        "name": ". Nikhil Singh"
                    },
                    {
                        "name": "Padraic O'Hara"
                    },
                    {
                        "name": "Pranay Jaiswal"
                    },
                    {
                        "name": "Roshan Chandru"
                    },
                    {
                        "name": "David Lillis"
                    }
                ],
                "author_detail": {
                    "name": "David Lillis"
                },
                "arxiv_affiliation": "School of Computer Science, University College Dublin",
                "author": "David Lillis",
                "arxiv_comment": "The project is partially supported by the DkIT Postgraduate\n  Scholarship, Research Ireland under Grant number 13/RC/2094_2, and Grant\n  number 21/FFP-A/925",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07450v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07450v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19164v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19164v2",
                "updated": "2025-06-13T14:26:31Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    26,
                    31,
                    4,
                    164,
                    0
                ],
                "published": "2024-05-29T15:08:55Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    15,
                    8,
                    55,
                    2,
                    150,
                    0
                ],
                "title": "Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in\n  eDiscovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in\n  eDiscovery"
                },
                "summary": "Electronic Discovery (eDiscovery) requires identifying relevant documents\nfrom vast collections for legal production requests. While artificial\nintelligence (AI) and natural language processing (NLP) have improved document\nreview efficiency, current methods still struggle with legal entities,\ncitations, and complex legal artifacts. To address these challenges, we\nintroduce DISCOvery Graph (DISCOG), an emerging system that integrates\nknowledge graphs for enhanced document ranking and classification, augmented by\nLLM-driven reasoning. DISCOG outperforms strong baselines in F1-score,\nprecision, and recall across both balanced and imbalanced datasets. In\nreal-world deployments, it has reduced litigation-related document review costs\nby approximately 98\\%, demonstrating significant business impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic Discovery (eDiscovery) requires identifying relevant documents\nfrom vast collections for legal production requests. While artificial\nintelligence (AI) and natural language processing (NLP) have improved document\nreview efficiency, current methods still struggle with legal entities,\ncitations, and complex legal artifacts. To address these challenges, we\nintroduce DISCOvery Graph (DISCOG), an emerging system that integrates\nknowledge graphs for enhanced document ranking and classification, augmented by\nLLM-driven reasoning. DISCOG outperforms strong baselines in F1-score,\nprecision, and recall across both balanced and imbalanced datasets. In\nreal-world deployments, it has reduced litigation-related document review costs\nby approximately 98\\%, demonstrating significant business impact."
                },
                "authors": [
                    {
                        "name": "Sounak Lahiri"
                    },
                    {
                        "name": "Sumit Pai"
                    },
                    {
                        "name": "Tim Weninger"
                    },
                    {
                        "name": "Sanmitra Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Sanmitra Bhattacharya"
                },
                "author": "Sanmitra Bhattacharya",
                "arxiv_comment": "Updated with Camera Ready Copy for ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19164v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19164v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11820v1",
                "updated": "2025-06-13T14:23:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    23,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T14:23:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    23,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "Rethinking Multilingual Vision-Language Translation: Dataset,\n  Evaluation, and Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Multilingual Vision-Language Translation: Dataset,\n  Evaluation, and Adaptation"
                },
                "summary": "Vision-Language Translation (VLT) is a challenging task that requires\naccurately recognizing multilingual text embedded in images and translating it\ninto the target language with the support of visual context. While recent Large\nVision-Language Models (LVLMs) have demonstrated strong multilingual and visual\nunderstanding capabilities, there is a lack of systematic evaluation and\nunderstanding of their performance on VLT. In this work, we present a\ncomprehensive study of VLT from three key perspectives: data quality, model\narchitecture, and evaluation metrics. (1) We identify critical limitations in\nexisting datasets, particularly in semantic and cultural fidelity, and\nintroduce AibTrans -- a multilingual, parallel, human-verified dataset with\nOCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6\nstate-of-the-art open-source models across end-to-end and cascaded\narchitectures, revealing their OCR dependency and contrasting generation versus\nreasoning behaviors. (3) We propose Density-Aware Evaluation to address metric\nreliability issues under varying contextual complexity, introducing the DA\nScore as a more robust measure of translation quality. Building upon these\nfindings, we establish a new evaluation benchmark for VLT. Notably, we observe\nthat fine-tuning LVLMs on high-resource language pairs degrades cross-lingual\nperformance, and we propose a balanced multilingual fine-tuning strategy that\neffectively adapts LVLMs to VLT without sacrificing their generalization\nability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Translation (VLT) is a challenging task that requires\naccurately recognizing multilingual text embedded in images and translating it\ninto the target language with the support of visual context. While recent Large\nVision-Language Models (LVLMs) have demonstrated strong multilingual and visual\nunderstanding capabilities, there is a lack of systematic evaluation and\nunderstanding of their performance on VLT. In this work, we present a\ncomprehensive study of VLT from three key perspectives: data quality, model\narchitecture, and evaluation metrics. (1) We identify critical limitations in\nexisting datasets, particularly in semantic and cultural fidelity, and\nintroduce AibTrans -- a multilingual, parallel, human-verified dataset with\nOCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6\nstate-of-the-art open-source models across end-to-end and cascaded\narchitectures, revealing their OCR dependency and contrasting generation versus\nreasoning behaviors. (3) We propose Density-Aware Evaluation to address metric\nreliability issues under varying contextual complexity, introducing the DA\nScore as a more robust measure of translation quality. Building upon these\nfindings, we establish a new evaluation benchmark for VLT. Notably, we observe\nthat fine-tuning LVLMs on high-resource language pairs degrades cross-lingual\nperformance, and we propose a balanced multilingual fine-tuning strategy that\neffectively adapts LVLMs to VLT without sacrificing their generalization\nability."
                },
                "authors": [
                    {
                        "name": "Xintong Wang"
                    },
                    {
                        "name": "Jingheng Pan"
                    },
                    {
                        "name": "Yixiao Liu"
                    },
                    {
                        "name": "Xiaohu Zhao"
                    },
                    {
                        "name": "Chenyang Lyu"
                    },
                    {
                        "name": "Minghao Wu"
                    },
                    {
                        "name": "Chris Biemann"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Linlong Xu"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifu Zhang"
                },
                "author": "Kaifu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11812v1",
                "updated": "2025-06-13T14:14:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    14,
                    40,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T14:14:40Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    14,
                    40,
                    4,
                    164,
                    0
                ],
                "title": "On the Performance of LLMs for Real Estate Appraisal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Performance of LLMs for Real Estate Appraisal"
                },
                "summary": "The real estate market is vital to global economies but suffers from\nsignificant information asymmetry. This study examines how Large Language\nModels (LLMs) can democratize access to real estate insights by generating\ncompetitive and interpretable house price estimates through optimized\nIn-Context Learning (ICL) strategies. We systematically evaluate leading LLMs\non diverse international housing datasets, comparing zero-shot, few-shot,\nmarket report-enhanced, and hybrid prompting techniques. Our results show that\nLLMs effectively leverage hedonic variables, such as property size and\namenities, to produce meaningful estimates. While traditional machine learning\nmodels remain strong for pure predictive accuracy, LLMs offer a more\naccessible, interactive and interpretable alternative. Although\nself-explanations require cautious interpretation, we find that LLMs explain\ntheir predictions in agreement with state-of-the-art models, confirming their\ntrustworthiness. Carefully selected in-context examples based on feature\nsimilarity and geographic proximity, significantly enhance LLM performance, yet\nLLMs struggle with overconfidence in price intervals and limited spatial\nreasoning. We offer practical guidance for structured prediction tasks through\nprompt optimization. Our findings highlight LLMs' potential to improve\ntransparency in real estate appraisal and provide actionable insights for\nstakeholders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The real estate market is vital to global economies but suffers from\nsignificant information asymmetry. This study examines how Large Language\nModels (LLMs) can democratize access to real estate insights by generating\ncompetitive and interpretable house price estimates through optimized\nIn-Context Learning (ICL) strategies. We systematically evaluate leading LLMs\non diverse international housing datasets, comparing zero-shot, few-shot,\nmarket report-enhanced, and hybrid prompting techniques. Our results show that\nLLMs effectively leverage hedonic variables, such as property size and\namenities, to produce meaningful estimates. While traditional machine learning\nmodels remain strong for pure predictive accuracy, LLMs offer a more\naccessible, interactive and interpretable alternative. Although\nself-explanations require cautious interpretation, we find that LLMs explain\ntheir predictions in agreement with state-of-the-art models, confirming their\ntrustworthiness. Carefully selected in-context examples based on feature\nsimilarity and geographic proximity, significantly enhance LLM performance, yet\nLLMs struggle with overconfidence in price intervals and limited spatial\nreasoning. We offer practical guidance for structured prediction tasks through\nprompt optimization. Our findings highlight LLMs' potential to improve\ntransparency in real estate appraisal and provide actionable insights for\nstakeholders."
                },
                "authors": [
                    {
                        "name": "Margot Geerts"
                    },
                    {
                        "name": "Manon Reusens"
                    },
                    {
                        "name": "Bart Baesens"
                    },
                    {
                        "name": "Seppe vanden Broucke"
                    },
                    {
                        "name": "Jochen De Weerdt"
                    }
                ],
                "author_detail": {
                    "name": "Jochen De Weerdt"
                },
                "author": "Jochen De Weerdt",
                "arxiv_comment": "Accepted at ECML-PKDD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11798v1",
                "updated": "2025-06-13T14:02:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    2,
                    21,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T14:02:21Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    14,
                    2,
                    21,
                    4,
                    164,
                    0
                ],
                "title": "Persona-driven Simulation of Voting Behavior in the European Parliament\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona-driven Simulation of Voting Behavior in the European Parliament\n  with Large Language Models"
                },
                "summary": "Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation."
                },
                "authors": [
                    {
                        "name": "Maximilian Kreutner"
                    },
                    {
                        "name": "Marlene Lutz"
                    },
                    {
                        "name": "Markus Strohmaier"
                    }
                ],
                "author_detail": {
                    "name": "Markus Strohmaier"
                },
                "author": "Markus Strohmaier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03664v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03664v2",
                "updated": "2025-06-13T13:54:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    54,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-03-15T08:48:38Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    8,
                    48,
                    38,
                    5,
                    74,
                    0
                ],
                "title": "PIPO: Pipelined Offloading for Efficient Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIPO: Pipelined Offloading for Efficient Inference on Consumer Devices"
                },
                "summary": "The high memory and computation demand of large language models (LLMs) makes\nthem challenging to be deployed on consumer devices due to limited GPU memory.\nOffloading can mitigate the memory constraint but often suffers from low GPU\nutilization, leading to low inference efficiency. In this work, we propose a\nnovel framework, called pipelined offloading (PIPO), for efficient inference on\nconsumer devices. PIPO designs a fine-grained offloading pipeline, complemented\nwith optimized data transfer and computation, to achieve high concurrency and\nefficient scheduling for inference. Experimental results show that compared\nwith state-of-the-art baseline, PIPO increases GPU utilization from below 40%\nto over 90% and achieves up to 3.1$\\times$ higher throughput, running on a\nlaptop equipped with a RTX3060 GPU of 6GB memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high memory and computation demand of large language models (LLMs) makes\nthem challenging to be deployed on consumer devices due to limited GPU memory.\nOffloading can mitigate the memory constraint but often suffers from low GPU\nutilization, leading to low inference efficiency. In this work, we propose a\nnovel framework, called pipelined offloading (PIPO), for efficient inference on\nconsumer devices. PIPO designs a fine-grained offloading pipeline, complemented\nwith optimized data transfer and computation, to achieve high concurrency and\nefficient scheduling for inference. Experimental results show that compared\nwith state-of-the-art baseline, PIPO increases GPU utilization from below 40%\nto over 90% and achieves up to 3.1$\\times$ higher throughput, running on a\nlaptop equipped with a RTX3060 GPU of 6GB memory."
                },
                "authors": [
                    {
                        "name": "Yangyijian Liu"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Wu-Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Wu-Jun Li"
                },
                "author": "Wu-Jun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03664v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03664v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11791v1",
                "updated": "2025-06-13T13:54:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    54,
                    30,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:54:30Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    54,
                    30,
                    4,
                    164,
                    0
                ],
                "title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software\n  Security Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software\n  Security Tasks"
                },
                "summary": "Rigorous security-focused evaluation of large language model (LLM) agents is\nimperative for establishing trust in their safe deployment throughout the\nsoftware development lifecycle. However, existing benchmarks largely rely on\nsynthetic challenges or simplified vulnerability datasets that fail to capture\nthe complexity and ambiguity encountered by security engineers in practice. We\nintroduce SEC-bench, the first fully automated benchmarking framework for\nevaluating LLM agents on authentic security engineering tasks. SEC-bench\nemploys a novel multi-agent scaffold that automatically constructs code\nrepositories with harnesses, reproduces vulnerabilities in isolated\nenvironments, and generates gold patches for reliable evaluation. Our framework\nautomatically creates high-quality software vulnerability datasets with\nreproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench,\nwe implement two critical software security tasks to rigorously evaluate LLM\nagents' capabilities: proof-of-concept (PoC) generation and vulnerability\npatching. A comprehensive evaluation of state-of-the-art LLM code agents\nreveals significant performance gaps, achieving at most 18.0% success in PoC\ngeneration and 34.0% in vulnerability patching on our complete dataset. These\nresults highlight the crucial steps needed toward developing LLM agents that\nare more practical, intelligent, and autonomous for security engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rigorous security-focused evaluation of large language model (LLM) agents is\nimperative for establishing trust in their safe deployment throughout the\nsoftware development lifecycle. However, existing benchmarks largely rely on\nsynthetic challenges or simplified vulnerability datasets that fail to capture\nthe complexity and ambiguity encountered by security engineers in practice. We\nintroduce SEC-bench, the first fully automated benchmarking framework for\nevaluating LLM agents on authentic security engineering tasks. SEC-bench\nemploys a novel multi-agent scaffold that automatically constructs code\nrepositories with harnesses, reproduces vulnerabilities in isolated\nenvironments, and generates gold patches for reliable evaluation. Our framework\nautomatically creates high-quality software vulnerability datasets with\nreproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench,\nwe implement two critical software security tasks to rigorously evaluate LLM\nagents' capabilities: proof-of-concept (PoC) generation and vulnerability\npatching. A comprehensive evaluation of state-of-the-art LLM code agents\nreveals significant performance gaps, achieving at most 18.0% success in PoC\ngeneration and 34.0% in vulnerability patching on our complete dataset. These\nresults highlight the crucial steps needed toward developing LLM agents that\nare more practical, intelligent, and autonomous for security engineering."
                },
                "authors": [
                    {
                        "name": "Hwiwon Lee"
                    },
                    {
                        "name": "Ziqi Zhang"
                    },
                    {
                        "name": "Hanxiao Lu"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11789v1",
                "updated": "2025-06-13T13:51:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    51,
                    34,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:51:34Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    51,
                    34,
                    4,
                    164,
                    0
                ],
                "title": "Conversational AI as a Catalyst for Informal Learning: An Empirical\n  Large-Scale Study on LLM Use in Everyday Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational AI as a Catalyst for Informal Learning: An Empirical\n  Large-Scale Study on LLM Use in Everyday Learning"
                },
                "summary": "Large language models have not only captivated the public imagination but\nhave also sparked a profound rethinking of how we learn. In the third year\nfollowing the breakthrough launch of ChatGPT, everyday informal learning has\nbeen transformed as diverse user groups explore these novel tools. Who is\nembracing LLMs for self-directed learning, and who remains hesitant? What are\ntheir reasons for adoption or avoidance? What learning patterns emerge with\nthis novel technological landscape? We present an in-depth analysis from a\nlarge-scale survey of 776 participants, showcasing that 88% of our respondents\nalready incorporate LLMs into their everyday learning routines for a wide\nvariety of (learning) tasks. Young adults are at the forefront of adopting\nLLMs, primarily to enhance their learning experiences independently of time and\nspace. Four types of learners emerge across learning contexts, depending on the\ntasks they perform with LLMs and the devices they use to access them.\nInterestingly, our respondents exhibit paradoxical behaviours regarding their\ntrust in LLMs' accuracy and privacy protection measures. Our implications\nemphasize the importance of including different media types for learning,\nenabling collaborative learning, providing sources and meeting the needs of\ndifferent types of learners and learning by design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have not only captivated the public imagination but\nhave also sparked a profound rethinking of how we learn. In the third year\nfollowing the breakthrough launch of ChatGPT, everyday informal learning has\nbeen transformed as diverse user groups explore these novel tools. Who is\nembracing LLMs for self-directed learning, and who remains hesitant? What are\ntheir reasons for adoption or avoidance? What learning patterns emerge with\nthis novel technological landscape? We present an in-depth analysis from a\nlarge-scale survey of 776 participants, showcasing that 88% of our respondents\nalready incorporate LLMs into their everyday learning routines for a wide\nvariety of (learning) tasks. Young adults are at the forefront of adopting\nLLMs, primarily to enhance their learning experiences independently of time and\nspace. Four types of learners emerge across learning contexts, depending on the\ntasks they perform with LLMs and the devices they use to access them.\nInterestingly, our respondents exhibit paradoxical behaviours regarding their\ntrust in LLMs' accuracy and privacy protection measures. Our implications\nemphasize the importance of including different media types for learning,\nenabling collaborative learning, providing sources and meeting the needs of\ndifferent types of learners and learning by design."
                },
                "authors": [
                    {
                        "name": "Nađa Terzimehić"
                    },
                    {
                        "name": "Babette Bühler"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19175v2",
                "updated": "2025-06-13T13:50:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    50,
                    34,
                    4,
                    164,
                    0
                ],
                "published": "2025-02-26T14:31:43Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    31,
                    43,
                    2,
                    57,
                    0
                ],
                "title": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic\n  Differential Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic\n  Differential Diagnosis"
                },
                "summary": "Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical\ndecision-making, in which physicians iteratively refine a ranked list of\npossible diseases based on symptoms, antecedents, and medical knowledge. While\nrecent advances in large language models (LLMs) have shown promise in\nsupporting DDx, existing approaches face key limitations, including\nsingle-dataset evaluations, isolated optimization of components, unrealistic\nassumptions about complete patient profiles, and single-attempt diagnosis. We\nintroduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for\ninteractive DDx, where diagnostic reasoning evolves through iterative learning,\nrather than assuming a complete patient profile is accessible. MEDDxAgent\nintegrates three modular components: (1) an orchestrator (DDxDriver), (2) a\nhistory taking simulator, and (3) two specialized agents for knowledge\nretrieval and diagnosis strategy. To ensure robust evaluation, we introduce a\ncomprehensive DDx benchmark covering respiratory, skin, and rare diseases. We\nanalyze single-turn diagnostic approaches and demonstrate the importance of\niterative refinement when patient profiles are not available at the outset. Our\nbroad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy\nimprovements in interactive DDx across both large and small LLMs, while\noffering critical explainability into its diagnostic reasoning process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical\ndecision-making, in which physicians iteratively refine a ranked list of\npossible diseases based on symptoms, antecedents, and medical knowledge. While\nrecent advances in large language models (LLMs) have shown promise in\nsupporting DDx, existing approaches face key limitations, including\nsingle-dataset evaluations, isolated optimization of components, unrealistic\nassumptions about complete patient profiles, and single-attempt diagnosis. We\nintroduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for\ninteractive DDx, where diagnostic reasoning evolves through iterative learning,\nrather than assuming a complete patient profile is accessible. MEDDxAgent\nintegrates three modular components: (1) an orchestrator (DDxDriver), (2) a\nhistory taking simulator, and (3) two specialized agents for knowledge\nretrieval and diagnosis strategy. To ensure robust evaluation, we introduce a\ncomprehensive DDx benchmark covering respiratory, skin, and rare diseases. We\nanalyze single-turn diagnostic approaches and demonstrate the importance of\niterative refinement when patient profiles are not available at the outset. Our\nbroad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy\nimprovements in interactive DDx across both large and small LLMs, while\noffering critical explainability into its diagnostic reasoning process."
                },
                "authors": [
                    {
                        "name": "Daniel Rose"
                    },
                    {
                        "name": "Chia-Chien Hung"
                    },
                    {
                        "name": "Marco Lepri"
                    },
                    {
                        "name": "Israa Alqassem"
                    },
                    {
                        "name": "Kiril Gashteovski"
                    },
                    {
                        "name": "Carolin Lawrence"
                    }
                ],
                "author_detail": {
                    "name": "Carolin Lawrence"
                },
                "author": "Carolin Lawrence",
                "arxiv_comment": "ACL 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03479v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03479v3",
                "updated": "2025-06-13T13:42:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    42,
                    41,
                    4,
                    164,
                    0
                ],
                "published": "2025-01-07T02:47:59Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    2,
                    47,
                    59,
                    1,
                    7,
                    0
                ],
                "title": "Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia\n  Reflect on the Cross-Cultural Sociolinguistic Norms?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia\n  Reflect on the Cross-Cultural Sociolinguistic Norms?"
                },
                "summary": "Wikipedia, as a massively multilingual, community-driven platform, is a\nvaluable resource for Natural Language Processing (NLP), yet the consistency of\nhonorific usage in honorific-rich languages remains underexplored. Honorifics,\nsubtle yet profound linguistic markers, encode social hierarchies, politeness\nnorms, and cultural values, but Wikipedia's editorial guidelines lack clear\nstandards for their usage in languages where such forms are grammatically and\nsocially prevalent. This paper addresses this gap through a large-scale\nanalysis of third-person honorific pronouns and verb forms in Hindi and Bengali\nWikipedia articles. Using Large Language Models (LLM), we automatically\nannotate 10,000 articles per language for honorific usage and socio-demographic\nfeatures such as gender, age, fame, and cultural origin. We investigate: (i)\nthe consistency of honorific usage across articles, (ii) how inconsistencies\ncorrelate with socio-cultural factors, and (iii) the presence of explicit or\nimplicit biases across languages. We find that honorific usage is consistently\nmore common in Bengali than Hindi, while non-honorific forms are more frequent\nfor infamous, juvenile, and exotic entities in both. Notably, gender bias\nemerges in both languages, particularly in Hindi, where men are more likely to\nreceive honorifics than women. Our analysis highlights the need for Wikipedia\nto develop language-specific editorial guidelines for honorific usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wikipedia, as a massively multilingual, community-driven platform, is a\nvaluable resource for Natural Language Processing (NLP), yet the consistency of\nhonorific usage in honorific-rich languages remains underexplored. Honorifics,\nsubtle yet profound linguistic markers, encode social hierarchies, politeness\nnorms, and cultural values, but Wikipedia's editorial guidelines lack clear\nstandards for their usage in languages where such forms are grammatically and\nsocially prevalent. This paper addresses this gap through a large-scale\nanalysis of third-person honorific pronouns and verb forms in Hindi and Bengali\nWikipedia articles. Using Large Language Models (LLM), we automatically\nannotate 10,000 articles per language for honorific usage and socio-demographic\nfeatures such as gender, age, fame, and cultural origin. We investigate: (i)\nthe consistency of honorific usage across articles, (ii) how inconsistencies\ncorrelate with socio-cultural factors, and (iii) the presence of explicit or\nimplicit biases across languages. We find that honorific usage is consistently\nmore common in Bengali than Hindi, while non-honorific forms are more frequent\nfor infamous, juvenile, and exotic entities in both. Notably, gender bias\nemerges in both languages, particularly in Hindi, where men are more likely to\nreceive honorifics than women. Our analysis highlights the need for Wikipedia\nto develop language-specific editorial guidelines for honorific usage."
                },
                "authors": [
                    {
                        "name": "Sourabrata Mukherjee"
                    },
                    {
                        "name": "Atharva Mehta"
                    },
                    {
                        "name": "Soumya Teotia"
                    },
                    {
                        "name": "Sougata Saha"
                    },
                    {
                        "name": "Akhil Arora"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "arxiv_comment": "Accepted at 2nd WikiNLP: Advancing Natural Language Process for\n  Wikipedia, Co-located with ACL 2025 (non-archival)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03479v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03479v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11781v1",
                "updated": "2025-06-13T13:42:17Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    42,
                    17,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:42:17Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    42,
                    17,
                    4,
                    164,
                    0
                ],
                "title": "GeoPandas-AI: A Smart Class Bringing LLM as Stateful AI Code Assistant",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GeoPandas-AI: A Smart Class Bringing LLM as Stateful AI Code Assistant"
                },
                "summary": "Geospatial data analysis plays a crucial role in tackling intricate societal\nchallenges such as urban planning and climate modeling. However, employing\ntools like GeoPandas, a prominent Python library for geospatial data\nmanipulation, necessitates expertise in complex domain-specific syntax and\nworkflows. GeoPandas-AI addresses this gap by integrating LLMs directly into\nthe GeoPandas workflow, transforming the GeoDataFrame class into an\nintelligent, stateful class for both data analysis and geospatial code\ndevelopment. This paper formalizes the design of such a smart class and\nprovides an open-source implementation of GeoPandas-AI in PyPI package manager.\nThrough its innovative combination of conversational interfaces and stateful\nexploitation of LLMs for code generation and data analysis, GeoPandas-AI\nintroduces a new paradigm for code-copilots and instantiates it for geospatial\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geospatial data analysis plays a crucial role in tackling intricate societal\nchallenges such as urban planning and climate modeling. However, employing\ntools like GeoPandas, a prominent Python library for geospatial data\nmanipulation, necessitates expertise in complex domain-specific syntax and\nworkflows. GeoPandas-AI addresses this gap by integrating LLMs directly into\nthe GeoPandas workflow, transforming the GeoDataFrame class into an\nintelligent, stateful class for both data analysis and geospatial code\ndevelopment. This paper formalizes the design of such a smart class and\nprovides an open-source implementation of GeoPandas-AI in PyPI package manager.\nThrough its innovative combination of conversational interfaces and stateful\nexploitation of LLMs for code generation and data analysis, GeoPandas-AI\nintroduces a new paradigm for code-copilots and instantiates it for geospatial\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Gaspard Merten"
                    },
                    {
                        "name": "Gilles Dejaegere"
                    },
                    {
                        "name": "Mahmoud Sakr"
                    }
                ],
                "author_detail": {
                    "name": "Mahmoud Sakr"
                },
                "author": "Mahmoud Sakr",
                "arxiv_comment": "Submitted to ACM SIGSPATIAL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11773v1",
                "updated": "2025-06-13T13:31:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    31,
                    8,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:31:08Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    31,
                    8,
                    4,
                    164,
                    0
                ],
                "title": "AgentSense: Virtual Sensor Data Generation Using LLM Agent in Simulated\n  Home Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentSense: Virtual Sensor Data Generation Using LLM Agent in Simulated\n  Home Environments"
                },
                "summary": "A major obstacle in developing robust and generalizable smart home-based\nHuman Activity Recognition (HAR) systems is the lack of large-scale, diverse\nlabeled datasets. Variability in home layouts, sensor configurations, and user\nbehavior adds further complexity, as individuals follow varied routines and\nperform activities in distinct ways. Building HAR systems that generalize well\nrequires training data that captures the diversity across users and\nenvironments. To address these challenges, we introduce AgentSense, a virtual\ndata generation pipeline where diverse personas are generated by leveraging\nLarge Language Models. These personas are used to create daily routines, which\nare then decomposed into low-level action sequences. Subsequently, the actions\nare executed in a simulated home environment called VirtualHome that we\nextended with virtual ambient sensors capable of recording the agents\nactivities as they unfold. Overall, AgentSense enables the generation of rich,\nvirtual sensor datasets that represent a wide range of users and home settings.\nAcross five benchmark HAR datasets, we show that leveraging our virtual sensor\ndata substantially improves performance, particularly when real data are\nlimited. Notably, models trained on a combination of virtual data and just a\nfew days of real data achieve performance comparable to those trained on the\nentire real datasets. These results demonstrate and prove the potential of\nvirtual data to address one of the most pressing challenges in ambient sensing,\nwhich is the distinct lack of large-scale, annotated datasets without requiring\nany manual data collection efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major obstacle in developing robust and generalizable smart home-based\nHuman Activity Recognition (HAR) systems is the lack of large-scale, diverse\nlabeled datasets. Variability in home layouts, sensor configurations, and user\nbehavior adds further complexity, as individuals follow varied routines and\nperform activities in distinct ways. Building HAR systems that generalize well\nrequires training data that captures the diversity across users and\nenvironments. To address these challenges, we introduce AgentSense, a virtual\ndata generation pipeline where diverse personas are generated by leveraging\nLarge Language Models. These personas are used to create daily routines, which\nare then decomposed into low-level action sequences. Subsequently, the actions\nare executed in a simulated home environment called VirtualHome that we\nextended with virtual ambient sensors capable of recording the agents\nactivities as they unfold. Overall, AgentSense enables the generation of rich,\nvirtual sensor datasets that represent a wide range of users and home settings.\nAcross five benchmark HAR datasets, we show that leveraging our virtual sensor\ndata substantially improves performance, particularly when real data are\nlimited. Notably, models trained on a combination of virtual data and just a\nfew days of real data achieve performance comparable to those trained on the\nentire real datasets. These results demonstrate and prove the potential of\nvirtual data to address one of the most pressing challenges in ambient sensing,\nwhich is the distinct lack of large-scale, annotated datasets without requiring\nany manual data collection efforts."
                },
                "authors": [
                    {
                        "name": "Zikang Leng"
                    },
                    {
                        "name": "Megha Thukral"
                    },
                    {
                        "name": "Yaqi Liu"
                    },
                    {
                        "name": "Hrudhai Rajasekhar"
                    },
                    {
                        "name": "Shruthi K. Hiremath"
                    },
                    {
                        "name": "Thomas Plötz"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Plötz"
                },
                "author": "Thomas Plötz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11769v1",
                "updated": "2025-06-13T13:25:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    25,
                    39,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:25:39Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    25,
                    39,
                    4,
                    164,
                    0
                ],
                "title": "Long-Short Alignment for Effective Long-Context Modeling in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Short Alignment for Effective Long-Context Modeling in LLMs"
                },
                "summary": "Large language models (LLMs) have exhibited impressive performance and\nsurprising emergent properties. However, their effectiveness remains limited by\nthe fixed context window of the transformer architecture, posing challenges for\nlong-context modeling. Among these challenges, length generalization -- the\nability to generalize to sequences longer than those seen during training -- is\na classical and fundamental problem. In this work, we propose a fresh\nperspective on length generalization, shifting the focus from the conventional\nemphasis on input features such as positional encodings or data structures to\nthe output distribution of the model. Specifically, through case studies on\nsynthetic tasks, we highlight the critical role of \\textbf{long-short\nalignment} -- the consistency of output distributions across sequences of\nvarying lengths. Extending this insight to natural language tasks, we propose a\nmetric called Long-Short Misalignment to quantify this phenomenon, uncovering a\nstrong correlation between the metric and length generalization performance.\nBuilding on these findings, we develop a regularization term that promotes\nlong-short alignment during training. Extensive experiments validate the\neffectiveness of our approach, offering new insights for achieving more\neffective long-context modeling in LLMs. Code is available at\nhttps://github.com/PKU-ML/LongShortAlignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited impressive performance and\nsurprising emergent properties. However, their effectiveness remains limited by\nthe fixed context window of the transformer architecture, posing challenges for\nlong-context modeling. Among these challenges, length generalization -- the\nability to generalize to sequences longer than those seen during training -- is\na classical and fundamental problem. In this work, we propose a fresh\nperspective on length generalization, shifting the focus from the conventional\nemphasis on input features such as positional encodings or data structures to\nthe output distribution of the model. Specifically, through case studies on\nsynthetic tasks, we highlight the critical role of \\textbf{long-short\nalignment} -- the consistency of output distributions across sequences of\nvarying lengths. Extending this insight to natural language tasks, we propose a\nmetric called Long-Short Misalignment to quantify this phenomenon, uncovering a\nstrong correlation between the metric and length generalization performance.\nBuilding on these findings, we develop a regularization term that promotes\nlong-short alignment during training. Extensive experiments validate the\neffectiveness of our approach, offering new insights for achieving more\neffective long-context modeling in LLMs. Code is available at\nhttps://github.com/PKU-ML/LongShortAlignment."
                },
                "authors": [
                    {
                        "name": "Tianqi Du"
                    },
                    {
                        "name": "Haotian Huang"
                    },
                    {
                        "name": "Yifei Wang"
                    },
                    {
                        "name": "Yisen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yisen Wang"
                },
                "author": "Yisen Wang",
                "arxiv_comment": "ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11767v1",
                "updated": "2025-06-13T13:21:53Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    21,
                    53,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:21:53Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    21,
                    53,
                    4,
                    164,
                    0
                ],
                "title": "Designing Effective LLM-Assisted Interfaces for Curriculum Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Effective LLM-Assisted Interfaces for Curriculum Development"
                },
                "summary": "Large Language Models (LLMs) have the potential to transform the way a\ndynamic curriculum can be delivered. However, educators face significant\nchallenges in interacting with these models, particularly due to complex prompt\nengineering and usability issues, which increase workload. Additionally,\ninaccuracies in LLM outputs can raise issues around output quality and ethical\nconcerns in educational content delivery. Addressing these issues requires\ncareful oversight, best achieved through cooperation between human and AI\napproaches. This paper introduces two novel User Interface (UI) designs, UI\nPredefined and UI Open, both grounded in Direct Manipulation (DM) principles to\naddress these challenges. By reducing the reliance on intricate prompt\nengineering, these UIs improve usability, streamline interaction, and lower\nworkload, providing a more effective pathway for educators to engage with LLMs.\nIn a controlled user study with 20 participants, the proposed UIs were\nevaluated against the standard ChatGPT interface in terms of usability and\ncognitive load. Results showed that UI Predefined significantly outperformed\nboth ChatGPT and UI Open, demonstrating superior usability and reduced task\nload, while UI Open offered more flexibility at the cost of a steeper learning\ncurve. These findings underscore the importance of user-centered design in\nadopting AI-driven tools and lay the foundation for more intuitive and\nefficient educator-LLM interactions in online learning environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have the potential to transform the way a\ndynamic curriculum can be delivered. However, educators face significant\nchallenges in interacting with these models, particularly due to complex prompt\nengineering and usability issues, which increase workload. Additionally,\ninaccuracies in LLM outputs can raise issues around output quality and ethical\nconcerns in educational content delivery. Addressing these issues requires\ncareful oversight, best achieved through cooperation between human and AI\napproaches. This paper introduces two novel User Interface (UI) designs, UI\nPredefined and UI Open, both grounded in Direct Manipulation (DM) principles to\naddress these challenges. By reducing the reliance on intricate prompt\nengineering, these UIs improve usability, streamline interaction, and lower\nworkload, providing a more effective pathway for educators to engage with LLMs.\nIn a controlled user study with 20 participants, the proposed UIs were\nevaluated against the standard ChatGPT interface in terms of usability and\ncognitive load. Results showed that UI Predefined significantly outperformed\nboth ChatGPT and UI Open, demonstrating superior usability and reduced task\nload, while UI Open offered more flexibility at the cost of a steeper learning\ncurve. These findings underscore the importance of user-centered design in\nadopting AI-driven tools and lay the foundation for more intuitive and\nefficient educator-LLM interactions in online learning environments."
                },
                "authors": [
                    {
                        "name": "Abdolali Faraji"
                    },
                    {
                        "name": "Mohammadreza Tavakoli"
                    },
                    {
                        "name": "Mohammad Moein"
                    },
                    {
                        "name": "Mohammadreza Molavi"
                    },
                    {
                        "name": "Gábor Kismihók"
                    }
                ],
                "author_detail": {
                    "name": "Gábor Kismihók"
                },
                "author": "Gábor Kismihók",
                "arxiv_comment": "This is the preprint version of a paper accepted at AIED 2025. The\n  final version will be published by Springer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11763v1",
                "updated": "2025-06-13T13:17:32Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    17,
                    32,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:17:32Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    17,
                    32,
                    4,
                    164,
                    0
                ],
                "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents"
                },
                "summary": "Deep Research Agents are a prominent category of LLM-based agents. By\nautonomously orchestrating multistep web exploration, targeted retrieval, and\nhigher-order synthesis, they transform vast amounts of online information into\nanalyst-grade, citation-rich reports--compressing hours of manual desk research\ninto minutes. However, a comprehensive benchmark for systematically evaluating\nthe capabilities of these agents remains absent. To bridge this gap, we present\nDeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,\neach meticulously crafted by domain experts across 22 distinct fields.\nEvaluating DRAs is inherently complex and labor-intensive. We therefore propose\ntwo novel methodologies that achieve strong alignment with human judgment. The\nfirst is a reference-based method with adaptive criteria to assess the quality\nof generated research reports. The other framework is introduced to evaluate\nDRA's information retrieval and collection capabilities by assessing its\neffective citation count and overall citation accuracy. We have open-sourced\nDeepResearch Bench and key components of these frameworks at\nhttps://github.com/Ayanami0730/deep_research_bench to accelerate the\ndevelopment of practical LLM-based agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Research Agents are a prominent category of LLM-based agents. By\nautonomously orchestrating multistep web exploration, targeted retrieval, and\nhigher-order synthesis, they transform vast amounts of online information into\nanalyst-grade, citation-rich reports--compressing hours of manual desk research\ninto minutes. However, a comprehensive benchmark for systematically evaluating\nthe capabilities of these agents remains absent. To bridge this gap, we present\nDeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,\neach meticulously crafted by domain experts across 22 distinct fields.\nEvaluating DRAs is inherently complex and labor-intensive. We therefore propose\ntwo novel methodologies that achieve strong alignment with human judgment. The\nfirst is a reference-based method with adaptive criteria to assess the quality\nof generated research reports. The other framework is introduced to evaluate\nDRA's information retrieval and collection capabilities by assessing its\neffective citation count and overall citation accuracy. We have open-sourced\nDeepResearch Bench and key components of these frameworks at\nhttps://github.com/Ayanami0730/deep_research_bench to accelerate the\ndevelopment of practical LLM-based agents."
                },
                "authors": [
                    {
                        "name": "Mingxuan Du"
                    },
                    {
                        "name": "Benfeng Xu"
                    },
                    {
                        "name": "Chiwei Zhu"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "arxiv_comment": "31 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11752v1",
                "updated": "2025-06-13T13:05:41Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    5,
                    41,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:05:41Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    5,
                    41,
                    4,
                    164,
                    0
                ],
                "title": "DART: Distilling Autoregressive Reasoning to Silent Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DART: Distilling Autoregressive Reasoning to Silent Thought"
                },
                "summary": "Chain-of-Thought (CoT) reasoning has significantly advanced Large Language\nModels (LLMs) in solving complex tasks. However, its autoregressive paradigm\nleads to significant computational overhead, hindering its deployment in\nlatency-sensitive applications. To address this, we propose \\textbf{DART}\n(\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent\n\\textbf{T}hought), a self-distillation framework that enables LLMs to replace\nautoregressive CoT with non-autoregressive Silent Thought (ST). Specifically,\nDART introduces two training pathways: the CoT pathway for traditional\nreasoning and the ST pathway for generating answers directly from a few ST\ntokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM)\nto align its hidden states with the CoT pathway, enabling the ST tokens to\nevolve into informative embeddings. During inference, only the ST pathway is\nactivated, leveraging evolving ST tokens to deliver the answer directly.\nExtensive experimental results demonstrate that DART achieves comparable\nreasoning performance to existing baselines while offering significant\nefficiency gains, serving as a feasible alternative for efficient reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning has significantly advanced Large Language\nModels (LLMs) in solving complex tasks. However, its autoregressive paradigm\nleads to significant computational overhead, hindering its deployment in\nlatency-sensitive applications. To address this, we propose \\textbf{DART}\n(\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent\n\\textbf{T}hought), a self-distillation framework that enables LLMs to replace\nautoregressive CoT with non-autoregressive Silent Thought (ST). Specifically,\nDART introduces two training pathways: the CoT pathway for traditional\nreasoning and the ST pathway for generating answers directly from a few ST\ntokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM)\nto align its hidden states with the CoT pathway, enabling the ST tokens to\nevolve into informative embeddings. During inference, only the ST pathway is\nactivated, leveraging evolving ST tokens to deliver the answer directly.\nExtensive experimental results demonstrate that DART achieves comparable\nreasoning performance to existing baselines while offering significant\nefficiency gains, serving as a feasible alternative for efficient reasoning."
                },
                "authors": [
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Ziming Wu"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    },
                    {
                        "name": "Fuming Lai"
                    },
                    {
                        "name": "Shaobing Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shaobing Lian"
                },
                "author": "Shaobing Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12415v2",
                "updated": "2025-06-13T13:02:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    2,
                    56,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-18T13:40:18Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    13,
                    40,
                    18,
                    6,
                    138,
                    0
                ],
                "title": "Table-R1: Region-based Reinforcement Learning for Table Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table-R1: Region-based Reinforcement Learning for Table Understanding"
                },
                "summary": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning."
                },
                "authors": [
                    {
                        "name": "Zhenhe Wu"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xianjie Wu"
                    },
                    {
                        "name": "Changzai Pan"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Shuangyong Song"
                    },
                    {
                        "name": "Yongxiang Li"
                    },
                    {
                        "name": "Zhoujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhoujun Li"
                },
                "author": "Zhoujun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11749v1",
                "updated": "2025-06-13T13:01:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    1,
                    45,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T13:01:45Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    13,
                    1,
                    45,
                    4,
                    164,
                    0
                ],
                "title": "Distributed Learning for Reliable and Timely Communication in 6G\n  Industrial Subnetworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Learning for Reliable and Timely Communication in 6G\n  Industrial Subnetworks"
                },
                "summary": "Emerging 6G industrial networks envision autonomous in-X subnetworks to\nsupport efficient and cost-effective short range, localized connectivity for\nautonomous control operations. Supporting timely transmission of event-driven,\ncritical control traffic is challenging in such networks is challenging due to\nlimited radio resources, dynamic device activity, and high mobility. In this\npaper, we propose a distributed, learning-based random access protocol that\nestablishes implicit inter-subnetwork coordination to minimize the collision\nprobability and improves timely delivery. Each subnetwork independently learns\nand selects access configurations based on a contention signature signal\nbroadcast by a central access point, enabling adaptive, collision-aware access\nunder dynamic traffic and mobility conditions. The proposed approach features\nlightweight neural models and online training, making it suitable for\ndeployment in constrained industrial subnetworks. Simulation results show that\nour method significantly improves the probability of timely packet delivery\ncompared to baseline methods, particularly in dense and high-load scenarios.\nFor instance, our proposed method achieves 21% gain in the probability of\ntimely packet delivery compared to a classical Multi-Armed Bandit (MAB) for an\nindustrial setting of 60 subnetworks and 5 radio channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging 6G industrial networks envision autonomous in-X subnetworks to\nsupport efficient and cost-effective short range, localized connectivity for\nautonomous control operations. Supporting timely transmission of event-driven,\ncritical control traffic is challenging in such networks is challenging due to\nlimited radio resources, dynamic device activity, and high mobility. In this\npaper, we propose a distributed, learning-based random access protocol that\nestablishes implicit inter-subnetwork coordination to minimize the collision\nprobability and improves timely delivery. Each subnetwork independently learns\nand selects access configurations based on a contention signature signal\nbroadcast by a central access point, enabling adaptive, collision-aware access\nunder dynamic traffic and mobility conditions. The proposed approach features\nlightweight neural models and online training, making it suitable for\ndeployment in constrained industrial subnetworks. Simulation results show that\nour method significantly improves the probability of timely packet delivery\ncompared to baseline methods, particularly in dense and high-load scenarios.\nFor instance, our proposed method achieves 21% gain in the probability of\ntimely packet delivery compared to a classical Multi-Armed Bandit (MAB) for an\nindustrial setting of 60 subnetworks and 5 radio channels."
                },
                "authors": [
                    {
                        "name": "Samira Abdelrahman"
                    },
                    {
                        "name": "Hossam Farag"
                    },
                    {
                        "name": "Gilberto Berardinelli"
                    }
                ],
                "author_detail": {
                    "name": "Gilberto Berardinelli"
                },
                "author": "Gilberto Berardinelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07595v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07595v2",
                "updated": "2025-06-13T12:47:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    47,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2024-11-12T07:09:44Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    7,
                    9,
                    44,
                    1,
                    317,
                    0
                ],
                "title": "Entropy Controllable Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy Controllable Direct Preference Optimization"
                },
                "summary": "In the post-training of large language models (LLMs), Reinforcement Learning\nfrom Human Feedback (RLHF) is an effective approach to achieve generation\naligned with human preferences. Direct Preference Optimization (DPO) allows for\npolicy training with a simple binary cross-entropy loss without a reward model.\nThe objective of DPO is regularized by reverse KL divergence that encourages\nmode-seeking fitting to the reference policy. Nonetheless, we indicate that\nminimizing reverse KL divergence could fail to capture a mode of the reference\ndistribution, which may hurt the policy's performance. Based on this\nobservation, we propose a simple modification to DPO, H-DPO, which allows for\ncontrol over the entropy of the resulting policy, enhancing the distribution's\nsharpness and thereby enabling mode-seeking fitting more effectively. In our\nexperiments, we show that H-DPO outperformed DPO across various tasks,\ndemonstrating superior results in pass@$k$ evaluations for mathematical tasks.\nMoreover, H-DPO is simple to implement, requiring only minor modifications to\nthe loss calculation of DPO, which makes it highly practical and promising for\nwide-ranging applications in the training of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the post-training of large language models (LLMs), Reinforcement Learning\nfrom Human Feedback (RLHF) is an effective approach to achieve generation\naligned with human preferences. Direct Preference Optimization (DPO) allows for\npolicy training with a simple binary cross-entropy loss without a reward model.\nThe objective of DPO is regularized by reverse KL divergence that encourages\nmode-seeking fitting to the reference policy. Nonetheless, we indicate that\nminimizing reverse KL divergence could fail to capture a mode of the reference\ndistribution, which may hurt the policy's performance. Based on this\nobservation, we propose a simple modification to DPO, H-DPO, which allows for\ncontrol over the entropy of the resulting policy, enhancing the distribution's\nsharpness and thereby enabling mode-seeking fitting more effectively. In our\nexperiments, we show that H-DPO outperformed DPO across various tasks,\ndemonstrating superior results in pass@$k$ evaluations for mathematical tasks.\nMoreover, H-DPO is simple to implement, requiring only minor modifications to\nthe loss calculation of DPO, which makes it highly practical and promising for\nwide-ranging applications in the training of LLMs."
                },
                "authors": [
                    {
                        "name": "Motoki Omura"
                    },
                    {
                        "name": "Yasuhiro Fujita"
                    },
                    {
                        "name": "Toshiki Kataoka"
                    }
                ],
                "author_detail": {
                    "name": "Toshiki Kataoka"
                },
                "author": "Toshiki Kataoka",
                "arxiv_comment": "ICML 2025 Workshop on Models of Human Feedback for AI Alignment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07595v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11722v1",
                "updated": "2025-06-13T12:37:07Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    37,
                    7,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T12:37:07Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    37,
                    7,
                    4,
                    164,
                    0
                ],
                "title": "Classification of Quality Characteristics in Online User Feedback using\n  Linguistic Analysis, Crowdsourcing and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification of Quality Characteristics in Online User Feedback using\n  Linguistic Analysis, Crowdsourcing and LLMs"
                },
                "summary": "Software qualities such as usability or reliability are among the strongest\ndeterminants of mobile app user satisfaction and constitute a significant\nportion of online user feedback on software products, making it a valuable\nsource of quality-related feedback to guide the development process. The\nabundance of online user feedback warrants the automated identification of\nquality characteristics, but the online user feedback's heterogeneity and the\nlack of appropriate training corpora limit the applicability of supervised\nmachine learning. We therefore investigate the viability of three approaches\nthat could be effective in low-data settings: language patterns (LPs) based on\nquality-related keywords, instructions for crowdsourced micro-tasks, and large\nlanguage model (LLM) prompts. We determined the feasibility of each approach\nand then compared their accuracy. For the complex multiclass classification of\nquality characteristics, the LP-based approach achieved a varied precision\n(0.38-0.92) depending on the quality characteristic, and low recall;\ncrowdsourcing achieved the best average accuracy in two consecutive phases\n(0.63, 0.72), which could be matched by the best-performing LLM condition\n(0.66) and a prediction based on the LLMs' majority vote (0.68). Our findings\nshow that in this low-data setting, the two approaches that use crowdsourcing\nor LLMs instead of involving experts achieve accurate classifications, while\nthe LP-based approach has only limited potential. The promise of crowdsourcing\nand LLMs in this context might even extend to building training corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software qualities such as usability or reliability are among the strongest\ndeterminants of mobile app user satisfaction and constitute a significant\nportion of online user feedback on software products, making it a valuable\nsource of quality-related feedback to guide the development process. The\nabundance of online user feedback warrants the automated identification of\nquality characteristics, but the online user feedback's heterogeneity and the\nlack of appropriate training corpora limit the applicability of supervised\nmachine learning. We therefore investigate the viability of three approaches\nthat could be effective in low-data settings: language patterns (LPs) based on\nquality-related keywords, instructions for crowdsourced micro-tasks, and large\nlanguage model (LLM) prompts. We determined the feasibility of each approach\nand then compared their accuracy. For the complex multiclass classification of\nquality characteristics, the LP-based approach achieved a varied precision\n(0.38-0.92) depending on the quality characteristic, and low recall;\ncrowdsourcing achieved the best average accuracy in two consecutive phases\n(0.63, 0.72), which could be matched by the best-performing LLM condition\n(0.66) and a prediction based on the LLMs' majority vote (0.68). Our findings\nshow that in this low-data setting, the two approaches that use crowdsourcing\nor LLMs instead of involving experts achieve accurate classifications, while\nthe LP-based approach has only limited potential. The promise of crowdsourcing\nand LLMs in this context might even extend to building training corpora."
                },
                "authors": [
                    {
                        "name": "Eduard C. Groen"
                    },
                    {
                        "name": "Fabiano Dalpiaz"
                    },
                    {
                        "name": "Martijn van Vliet"
                    },
                    {
                        "name": "Boris Winter"
                    },
                    {
                        "name": "Joerg Doerr"
                    },
                    {
                        "name": "Sjaak Brinkkemper"
                    }
                ],
                "author_detail": {
                    "name": "Sjaak Brinkkemper"
                },
                "author": "Sjaak Brinkkemper",
                "arxiv_comment": "Accepted at the Journal of Systems and Software (JSS); online\n  appendix and supplementary material available at\n  https://doi.org/10.5281/zenodo.15604749",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04466v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04466v4",
                "updated": "2025-06-13T12:20:54Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    20,
                    54,
                    4,
                    164,
                    0
                ],
                "published": "2024-10-06T12:42:04Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    12,
                    42,
                    4,
                    6,
                    280,
                    0
                ],
                "title": "Large Language Model Inference Acceleration: A Comprehensive Hardware\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Inference Acceleration: A Comprehensive Hardware\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious fields, from natural language understanding to text generation.\nCompared to non-generative LLMs like BERT and DeBERTa, generative LLMs like GPT\nseries and Llama series are currently the main focus due to their superior\nalgorithmic performance. The advancements in generative LLMs are closely\nintertwined with the development of hardware capabilities. Various hardware\nplatforms exhibit distinct hardware characteristics, which can help improve LLM\ninference performance. Therefore, this paper comprehensively surveys efficient\ngenerative LLM inference on different hardware platforms. First, we provide an\noverview of the algorithm architecture of mainstream generative LLMs and delve\ninto the inference process. Then, we summarize different optimization methods\nfor different platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP, and provide\ninference results for generative LLMs. Furthermore, we perform a qualitative\nand quantitative comparison of inference performance with batch sizes 1 and 8\non different hardware platforms by considering hardware power consumption,\nabsolute inference speed (tokens/s), and energy efficiency (tokens/J). We\ncompare the performance of the same optimization methods across different\nhardware platforms, the performance across different hardware platforms, and\nthe performance of different methods on the same hardware platform. This\nprovides a systematic and comprehensive summary of existing inference\nacceleration work by integrating software optimization methods and hardware\nplatforms. We point out that three trends (multimodality, inference-time\ncompute, and higher inference energy efficiency) are promising to redefine the\ncapabilities of edge artificial intelligence systems. Our project is available\nat https://dai.sjtu.edu.cn/project.html.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious fields, from natural language understanding to text generation.\nCompared to non-generative LLMs like BERT and DeBERTa, generative LLMs like GPT\nseries and Llama series are currently the main focus due to their superior\nalgorithmic performance. The advancements in generative LLMs are closely\nintertwined with the development of hardware capabilities. Various hardware\nplatforms exhibit distinct hardware characteristics, which can help improve LLM\ninference performance. Therefore, this paper comprehensively surveys efficient\ngenerative LLM inference on different hardware platforms. First, we provide an\noverview of the algorithm architecture of mainstream generative LLMs and delve\ninto the inference process. Then, we summarize different optimization methods\nfor different platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP, and provide\ninference results for generative LLMs. Furthermore, we perform a qualitative\nand quantitative comparison of inference performance with batch sizes 1 and 8\non different hardware platforms by considering hardware power consumption,\nabsolute inference speed (tokens/s), and energy efficiency (tokens/J). We\ncompare the performance of the same optimization methods across different\nhardware platforms, the performance across different hardware platforms, and\nthe performance of different methods on the same hardware platform. This\nprovides a systematic and comprehensive summary of existing inference\nacceleration work by integrating software optimization methods and hardware\nplatforms. We point out that three trends (multimodality, inference-time\ncompute, and higher inference energy efficiency) are promising to redefine the\ncapabilities of edge artificial intelligence systems. Our project is available\nat https://dai.sjtu.edu.cn/project.html."
                },
                "authors": [
                    {
                        "name": "Jinhao Li"
                    },
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Shan Huang"
                    },
                    {
                        "name": "Yonghua Chen"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Yaoxiu Lian"
                    },
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Li Ding"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai",
                "arxiv_comment": "Collect and update results in recent half year. 54 pages. Github\n  link: https://github.com/Kimho666/LLM_Hardware_Survey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04466v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04466v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07855v2",
                "updated": "2025-06-13T12:20:30Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    20,
                    30,
                    4,
                    164,
                    0
                ],
                "published": "2025-02-11T14:04:43Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    4,
                    43,
                    1,
                    42,
                    0
                ],
                "title": "Vision-Language Models for Edge Networks: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models for Edge Networks: A Comprehensive Survey"
                },
                "summary": "Vision Large Language Models (VLMs) combine visual understanding with natural\nlanguage processing, enabling tasks like image captioning, visual question\nanswering, and video analysis. While VLMs show impressive capabilities across\ndomains such as autonomous vehicles, smart surveillance, and healthcare, their\ndeployment on resource-constrained edge devices remains challenging due to\nprocessing power, memory, and energy limitations. This survey explores recent\nadvancements in optimizing VLMs for edge environments, focusing on model\ncompression techniques, including pruning, quantization, knowledge\ndistillation, and specialized hardware solutions that enhance efficiency. We\nprovide a detailed discussion of efficient training and fine-tuning methods,\nedge deployment challenges, and privacy considerations. Additionally, we\ndiscuss the diverse applications of lightweight VLMs across healthcare,\nenvironmental monitoring, and autonomous systems, illustrating their growing\nimpact. By highlighting key design strategies, current challenges, and offering\nrecommendations for future directions, this survey aims to inspire further\nresearch into the practical deployment of VLMs, ultimately making advanced AI\naccessible in resource-limited settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Large Language Models (VLMs) combine visual understanding with natural\nlanguage processing, enabling tasks like image captioning, visual question\nanswering, and video analysis. While VLMs show impressive capabilities across\ndomains such as autonomous vehicles, smart surveillance, and healthcare, their\ndeployment on resource-constrained edge devices remains challenging due to\nprocessing power, memory, and energy limitations. This survey explores recent\nadvancements in optimizing VLMs for edge environments, focusing on model\ncompression techniques, including pruning, quantization, knowledge\ndistillation, and specialized hardware solutions that enhance efficiency. We\nprovide a detailed discussion of efficient training and fine-tuning methods,\nedge deployment challenges, and privacy considerations. Additionally, we\ndiscuss the diverse applications of lightweight VLMs across healthcare,\nenvironmental monitoring, and autonomous systems, illustrating their growing\nimpact. By highlighting key design strategies, current challenges, and offering\nrecommendations for future directions, this survey aims to inspire further\nresearch into the practical deployment of VLMs, ultimately making advanced AI\naccessible in resource-limited settings."
                },
                "authors": [
                    {
                        "name": "Ahmed Sharshar"
                    },
                    {
                        "name": "Latif U. Khan"
                    },
                    {
                        "name": "Waseem Ullah"
                    },
                    {
                        "name": "Mohsen Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Mohsen Guizani"
                },
                "author": "Mohsen Guizani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11702v1",
                "updated": "2025-06-13T12:17:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    17,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T12:17:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    17,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "Configurable Preference Tuning with Rubric-Guided Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Configurable Preference Tuning with Rubric-Guided Synthetic Data"
                },
                "summary": "Models of human feedback for AI alignment, such as those underpinning Direct\nPreference Optimization (DPO), often bake in a singular, static set of\npreferences, limiting adaptability. This paper challenges the assumption of\nmonolithic preferences by introducing Configurable Preference Tuning (CPT), a\nnovel framework for endowing language models with the ability to dynamically\nadjust their behavior based on explicit, human-interpretable directives. CPT\nleverages synthetically generated preference data, conditioned on system\nprompts derived from structured, fine-grained rubrics that define desired\nattributes like writing style. By fine-tuning with these rubric-guided\npreferences, the LLM learns to modulate its outputs at inference time in\nresponse to the system prompt, without retraining. This approach not only\noffers fine-grained control but also provides a mechanism for modeling more\nnuanced and context-dependent human feedback. Several experimental artifacts,\nsuch as training code, generated datasets and fine-tuned models are released at\nhttps://github.com/vicgalle/configurable-preference-tuning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Models of human feedback for AI alignment, such as those underpinning Direct\nPreference Optimization (DPO), often bake in a singular, static set of\npreferences, limiting adaptability. This paper challenges the assumption of\nmonolithic preferences by introducing Configurable Preference Tuning (CPT), a\nnovel framework for endowing language models with the ability to dynamically\nadjust their behavior based on explicit, human-interpretable directives. CPT\nleverages synthetically generated preference data, conditioned on system\nprompts derived from structured, fine-grained rubrics that define desired\nattributes like writing style. By fine-tuning with these rubric-guided\npreferences, the LLM learns to modulate its outputs at inference time in\nresponse to the system prompt, without retraining. This approach not only\noffers fine-grained control but also provides a mechanism for modeling more\nnuanced and context-dependent human feedback. Several experimental artifacts,\nsuch as training code, generated datasets and fine-tuned models are released at\nhttps://github.com/vicgalle/configurable-preference-tuning"
                },
                "authors": [
                    {
                        "name": "Víctor Gallego"
                    }
                ],
                "author_detail": {
                    "name": "Víctor Gallego"
                },
                "author": "Víctor Gallego",
                "arxiv_comment": "Accepted to ICML 2025 Workshop on Models of Human Feedback for AI\n  Alignment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10826v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10826v2",
                "updated": "2025-06-13T12:14:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    12,
                    14,
                    44,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T15:44:51Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    15,
                    44,
                    51,
                    3,
                    163,
                    0
                ],
                "title": "RationalVLA: A Rational Vision-Language-Action Model with Dual System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RationalVLA: A Rational Vision-Language-Action Model with Dual System"
                },
                "summary": "A fundamental requirement for real-world robotic deployment is the ability to\nunderstand and respond to natural language instructions. Existing\nlanguage-conditioned manipulation tasks typically assume that instructions are\nperfectly aligned with the environment. This assumption limits robustness and\ngeneralization in realistic scenarios where instructions may be ambiguous,\nirrelevant, or infeasible. To address this problem, we introduce RAtional\nMAnipulation (RAMA), a new benchmark that challenges models with both unseen\nexecutable instructions and defective ones that should be rejected. In RAMA, we\nconstruct a dataset with over 14,000 samples, including diverse defective\ninstructions spanning six dimensions: visual, physical, semantic, motion,\nsafety, and out-of-context. We further propose the Rational\nVision-Language-Action model (RationalVLA). It is a dual system for robotic\narms that integrates the high-level vision-language model with the low-level\nmanipulation policy by introducing learnable latent space embeddings. This\ndesign enables RationalVLA to reason over instructions, reject infeasible\ncommands, and execute manipulation effectively. Experiments demonstrate that\nRationalVLA outperforms state-of-the-art baselines on RAMA by a 14.5% higher\nsuccess rate and 0.94 average task length, while maintaining competitive\nperformance on standard manipulation tasks. Real-world trials further validate\nits effectiveness and robustness in practical applications. Our project page is\nhttps://irpn-eai.github.io/RationalVLA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental requirement for real-world robotic deployment is the ability to\nunderstand and respond to natural language instructions. Existing\nlanguage-conditioned manipulation tasks typically assume that instructions are\nperfectly aligned with the environment. This assumption limits robustness and\ngeneralization in realistic scenarios where instructions may be ambiguous,\nirrelevant, or infeasible. To address this problem, we introduce RAtional\nMAnipulation (RAMA), a new benchmark that challenges models with both unseen\nexecutable instructions and defective ones that should be rejected. In RAMA, we\nconstruct a dataset with over 14,000 samples, including diverse defective\ninstructions spanning six dimensions: visual, physical, semantic, motion,\nsafety, and out-of-context. We further propose the Rational\nVision-Language-Action model (RationalVLA). It is a dual system for robotic\narms that integrates the high-level vision-language model with the low-level\nmanipulation policy by introducing learnable latent space embeddings. This\ndesign enables RationalVLA to reason over instructions, reject infeasible\ncommands, and execute manipulation effectively. Experiments demonstrate that\nRationalVLA outperforms state-of-the-art baselines on RAMA by a 14.5% higher\nsuccess rate and 0.94 average task length, while maintaining competitive\nperformance on standard manipulation tasks. Real-world trials further validate\nits effectiveness and robustness in practical applications. Our project page is\nhttps://irpn-eai.github.io/RationalVLA."
                },
                "authors": [
                    {
                        "name": "Wenxuan Song"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Wenxue Li"
                    },
                    {
                        "name": "Xu He"
                    },
                    {
                        "name": "Han Zhao"
                    },
                    {
                        "name": "Can Cui"
                    },
                    {
                        "name": "Pengxiang Ding Shiyan Su"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Xuelian Cheng"
                    },
                    {
                        "name": "Donglin Wang"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Xinhu Zheng"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Hesheng Wang"
                    },
                    {
                        "name": "Haoang Li"
                    }
                ],
                "author_detail": {
                    "name": "Haoang Li"
                },
                "author": "Haoang Li",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10826v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10826v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10362v2",
                "updated": "2025-06-13T11:47:29Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    47,
                    29,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T05:39:14Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    5,
                    39,
                    14,
                    3,
                    163,
                    0
                ],
                "title": "Relaxation-Free Min-k-Partition for PCI Assignment in 5G Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxation-Free Min-k-Partition for PCI Assignment in 5G Networks"
                },
                "summary": "Physical Cell Identity (PCI) is a critical parameter in 5G networks.\nEfficient and accurate PCI assignment is essential for mitigating mod-3\ninterference, mod-30 interference, collisions, and confusions among cells,\nwhich directly affect network reliability and user experience. In this paper,\nwe propose a novel framework for PCI assignment by decomposing the problem into\nMin-3-Partition, Min-10-Partition, and a graph coloring problem, leveraging the\nChinese Remainder Theorem (CRT). Furthermore, we develop a relaxation-free\napproach to the general Min-k-Partition problem by reformulating it as a\nquadratic program with a norm-equality constraint and solving it using a\npenalized mirror descent (PMD) algorithm. The proposed method demonstrates\nsuperior computational efficiency and scalability, significantly reducing\ninterference while eliminating collisions and confusions in large-scale 5G\nnetworks. Numerical evaluations on real-world datasets show that our approach\nreduces computational time by up to 20 times compared to state-of-the-art\nmethods, making it highly practical for real-time PCI optimization in\nlarge-scale networks. These results highlight the potential of our method to\nimprove network performance and reduce deployment costs in modern 5G systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Cell Identity (PCI) is a critical parameter in 5G networks.\nEfficient and accurate PCI assignment is essential for mitigating mod-3\ninterference, mod-30 interference, collisions, and confusions among cells,\nwhich directly affect network reliability and user experience. In this paper,\nwe propose a novel framework for PCI assignment by decomposing the problem into\nMin-3-Partition, Min-10-Partition, and a graph coloring problem, leveraging the\nChinese Remainder Theorem (CRT). Furthermore, we develop a relaxation-free\napproach to the general Min-k-Partition problem by reformulating it as a\nquadratic program with a norm-equality constraint and solving it using a\npenalized mirror descent (PMD) algorithm. The proposed method demonstrates\nsuperior computational efficiency and scalability, significantly reducing\ninterference while eliminating collisions and confusions in large-scale 5G\nnetworks. Numerical evaluations on real-world datasets show that our approach\nreduces computational time by up to 20 times compared to state-of-the-art\nmethods, making it highly practical for real-time PCI optimization in\nlarge-scale networks. These results highlight the potential of our method to\nimprove network performance and reduce deployment costs in modern 5G systems."
                },
                "authors": [
                    {
                        "name": "Yeqing Qiu"
                    },
                    {
                        "name": "Chengpiao Huang"
                    },
                    {
                        "name": "Ye Xue"
                    },
                    {
                        "name": "Zhipeng Jiang"
                    },
                    {
                        "name": "Qingjiang Shi"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Zhi-Quan Luo"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Quan Luo"
                },
                "author": "Zhi-Quan Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16557v2",
                "updated": "2025-06-13T11:41:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    41,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-22T11:46:46Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    11,
                    46,
                    46,
                    3,
                    142,
                    0
                ],
                "title": "Is Your LLM-Based Multi-Agent a Reliable Real-World Planner? Exploring\n  Fraud Detection in Travel Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is Your LLM-Based Multi-Agent a Reliable Real-World Planner? Exploring\n  Fraud Detection in Travel Planning"
                },
                "summary": "The rise of Large Language Model-based Multi-Agent Planning has leveraged\nadvanced frameworks to enable autonomous and collaborative task execution. Some\nsystems rely on platforms like review sites and social media, which are prone\nto fraudulent information, such as fake reviews or misleading descriptions.\nThis reliance poses risks, potentially causing financial losses and harming\nuser experiences. To evaluate the risk of planning systems in real-world\napplications, we introduce \\textbf{WandaPlan}, an evaluation environment\nmirroring real-world data and injected with deceptive content. We assess system\nperformance across three fraud cases: Misinformation Fraud, Team-Coordinated\nMulti-Person Fraud, and Level-Escalating Multi-Round Fraud. We reveal\nsignificant weaknesses in existing frameworks that prioritize task efficiency\nover data authenticity. At the same time, we validate WandaPlan's\ngeneralizability, capable of assessing the risks of real-world open-source\nplanning frameworks. To mitigate the risk of fraud, we propose integrating an\nanti-fraud agent, providing a solution for reliable planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Model-based Multi-Agent Planning has leveraged\nadvanced frameworks to enable autonomous and collaborative task execution. Some\nsystems rely on platforms like review sites and social media, which are prone\nto fraudulent information, such as fake reviews or misleading descriptions.\nThis reliance poses risks, potentially causing financial losses and harming\nuser experiences. To evaluate the risk of planning systems in real-world\napplications, we introduce \\textbf{WandaPlan}, an evaluation environment\nmirroring real-world data and injected with deceptive content. We assess system\nperformance across three fraud cases: Misinformation Fraud, Team-Coordinated\nMulti-Person Fraud, and Level-Escalating Multi-Round Fraud. We reveal\nsignificant weaknesses in existing frameworks that prioritize task efficiency\nover data authenticity. At the same time, we validate WandaPlan's\ngeneralizability, capable of assessing the risks of real-world open-source\nplanning frameworks. To mitigate the risk of fraud, we propose integrating an\nanti-fraud agent, providing a solution for reliable planning."
                },
                "authors": [
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Jianhua Xu"
                    },
                    {
                        "name": "Tianyu Xin"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Shenzhe Zhu"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "arxiv_comment": "Accepted by ICML 2025 Workshop MAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11687v1",
                "updated": "2025-06-13T11:30:35Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    30,
                    35,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T11:30:35Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    30,
                    35,
                    4,
                    164,
                    0
                ],
                "title": "Differential Privacy in Machine Learning: From Symbolic AI to LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential Privacy in Machine Learning: From Symbolic AI to LLMs"
                },
                "summary": "Machine learning models should not reveal particular information that is not\notherwise accessible. Differential privacy provides a formal framework to\nmitigate privacy risks by ensuring that the inclusion or exclusion of any\nsingle data point does not significantly alter the output of an algorithm, thus\nlimiting the exposure of private information. This survey paper explores the\nfoundational definitions of differential privacy, reviews its original\nformulations and tracing its evolution through key research contributions. It\nthen provides an in-depth examination of how DP has been integrated into\nmachine learning models, analyzing existing proposals and methods to preserve\nprivacy when training ML models. Finally, it describes how DP-based ML\ntechniques can be evaluated in practice. %Finally, it discusses the broader\nimplications of DP, highlighting its potential for public benefit, its\nreal-world applications, and the challenges it faces, including vulnerabilities\nto adversarial attacks. By offering a comprehensive overview of differential\nprivacy in machine learning, this work aims to contribute to the ongoing\ndevelopment of secure and responsible AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models should not reveal particular information that is not\notherwise accessible. Differential privacy provides a formal framework to\nmitigate privacy risks by ensuring that the inclusion or exclusion of any\nsingle data point does not significantly alter the output of an algorithm, thus\nlimiting the exposure of private information. This survey paper explores the\nfoundational definitions of differential privacy, reviews its original\nformulations and tracing its evolution through key research contributions. It\nthen provides an in-depth examination of how DP has been integrated into\nmachine learning models, analyzing existing proposals and methods to preserve\nprivacy when training ML models. Finally, it describes how DP-based ML\ntechniques can be evaluated in practice. %Finally, it discusses the broader\nimplications of DP, highlighting its potential for public benefit, its\nreal-world applications, and the challenges it faces, including vulnerabilities\nto adversarial attacks. By offering a comprehensive overview of differential\nprivacy in machine learning, this work aims to contribute to the ongoing\ndevelopment of secure and responsible AI systems."
                },
                "authors": [
                    {
                        "name": "Francisco Aguilera-Martínez"
                    },
                    {
                        "name": "Fernando Berzal"
                    }
                ],
                "author_detail": {
                    "name": "Fernando Berzal"
                },
                "author": "Fernando Berzal",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2303.00654 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11681v1",
                "updated": "2025-06-13T11:19:27Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    19,
                    27,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T11:19:27Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    19,
                    27,
                    4,
                    164,
                    0
                ],
                "title": "LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting\n  Approach"
                },
                "summary": "This paper addresses the challenge of transforming complex sentences into\nsequences of logical, simplified sentences while preserving semantic and\nlogical integrity with the help of Large Language Models. We propose a hybrid\napproach that combines advanced prompting with multi-agent architectures to\nenhance the sentence simplification process. Experimental results show that our\napproach was able to successfully simplify 70% of the complex sentences written\nfor video game design application. In comparison, a single-agent approach\nattained a 48% success rate on the same task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses the challenge of transforming complex sentences into\nsequences of logical, simplified sentences while preserving semantic and\nlogical integrity with the help of Large Language Models. We propose a hybrid\napproach that combines advanced prompting with multi-agent architectures to\nenhance the sentence simplification process. Experimental results show that our\napproach was able to successfully simplify 70% of the complex sentences written\nfor video game design application. In comparison, a single-agent approach\nattained a 48% success rate on the same task."
                },
                "authors": [
                    {
                        "name": "Pratibha Zunjare"
                    },
                    {
                        "name": "Michael Hsiao"
                    }
                ],
                "author_detail": {
                    "name": "Michael Hsiao"
                },
                "author": "Michael Hsiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11680v1",
                "updated": "2025-06-13T11:19:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    19,
                    21,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T11:19:21Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    19,
                    21,
                    4,
                    164,
                    0
                ],
                "title": "Malicious LLM-Based Conversational AI Makes Users Reveal Personal\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malicious LLM-Based Conversational AI Makes Users Reveal Personal\n  Information"
                },
                "summary": "LLM-based Conversational AIs (CAIs), also known as GenAI chatbots, like\nChatGPT, are increasingly used across various domains, but they pose privacy\nrisks, as users may disclose personal information during their conversations\nwith CAIs. Recent research has demonstrated that LLM-based CAIs could be used\nfor malicious purposes. However, a novel and particularly concerning type of\nmalicious LLM application remains unexplored: an LLM-based CAI that is\ndeliberately designed to extract personal information from users.\n  In this paper, we report on the malicious LLM-based CAIs that we created\nbased on system prompts that used different strategies to encourage disclosures\nof personal information from users. We systematically investigate CAIs' ability\nto extract personal information from users during conversations by conducting a\nrandomized-controlled trial with 502 participants. We assess the effectiveness\nof different malicious and benign CAIs to extract personal information from\nparticipants, and we analyze participants' perceptions after their interactions\nwith the CAIs. Our findings reveal that malicious CAIs extract significantly\nmore personal information than benign CAIs, with strategies based on the social\nnature of privacy being the most effective while minimizing perceived risks.\nThis study underscores the privacy threats posed by this novel type of\nmalicious LLM-based CAIs and provides actionable recommendations to guide\nfuture research and practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Conversational AIs (CAIs), also known as GenAI chatbots, like\nChatGPT, are increasingly used across various domains, but they pose privacy\nrisks, as users may disclose personal information during their conversations\nwith CAIs. Recent research has demonstrated that LLM-based CAIs could be used\nfor malicious purposes. However, a novel and particularly concerning type of\nmalicious LLM application remains unexplored: an LLM-based CAI that is\ndeliberately designed to extract personal information from users.\n  In this paper, we report on the malicious LLM-based CAIs that we created\nbased on system prompts that used different strategies to encourage disclosures\nof personal information from users. We systematically investigate CAIs' ability\nto extract personal information from users during conversations by conducting a\nrandomized-controlled trial with 502 participants. We assess the effectiveness\nof different malicious and benign CAIs to extract personal information from\nparticipants, and we analyze participants' perceptions after their interactions\nwith the CAIs. Our findings reveal that malicious CAIs extract significantly\nmore personal information than benign CAIs, with strategies based on the social\nnature of privacy being the most effective while minimizing perceived risks.\nThis study underscores the privacy threats posed by this novel type of\nmalicious LLM-based CAIs and provides actionable recommendations to guide\nfuture research and practice."
                },
                "authors": [
                    {
                        "name": "Xiao Zhan"
                    },
                    {
                        "name": "Juan Carlos Carrillo"
                    },
                    {
                        "name": "William Seymour"
                    },
                    {
                        "name": "Jose Such"
                    }
                ],
                "author_detail": {
                    "name": "Jose Such"
                },
                "author": "Jose Such",
                "arxiv_comment": "This paper has been accepted at USENIX Security '25",
                "arxiv_journal_ref": "USENIX Security 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08754v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08754v4",
                "updated": "2025-06-13T11:19:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    19,
                    12,
                    4,
                    164,
                    0
                ],
                "published": "2025-03-28T15:49:52Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    49,
                    52,
                    4,
                    87,
                    0
                ],
                "title": "Towards Personalized Conversational Sales Agents: Contextual User\n  Profiling for Strategic Action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Personalized Conversational Sales Agents: Contextual User\n  Profiling for Strategic Action"
                },
                "summary": "Conversational Recommender Systems (CRSs)aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To capture this complexity, we introduce Conversational\nSales (CSALES), a novel task that integrates preference elicitation,\nrecommendation, and persuasion within a unified conversational framework. To\nsupport realistic and systematic evaluation, we present CSUSER, an evaluation\nprotocol with LLM-based user simulator grounded in real-world behavioral data\nby modeling fine-grained user profiles for personalized interaction. We also\npropose CSI, a conversational sales agent that proactively infers contextual\nuser profiles and strategically selects actions through conversation.\nComprehensive experiments show that CSI significantly improves both\nrecommendation success and persuasive effectiveness across diverse user\nprofiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Recommender Systems (CRSs)aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To capture this complexity, we introduce Conversational\nSales (CSALES), a novel task that integrates preference elicitation,\nrecommendation, and persuasion within a unified conversational framework. To\nsupport realistic and systematic evaluation, we present CSUSER, an evaluation\nprotocol with LLM-based user simulator grounded in real-world behavioral data\nby modeling fine-grained user profiles for personalized interaction. We also\npropose CSI, a conversational sales agent that proactively infers contextual\nuser profiles and strategically selects actions through conversation.\nComprehensive experiments show that CSI significantly improves both\nrecommendation success and persuasive effectiveness across diverse user\nprofiles."
                },
                "authors": [
                    {
                        "name": "Tongyoung Kim"
                    },
                    {
                        "name": "Jeongeun Lee"
                    },
                    {
                        "name": "Soojin Yoon"
                    },
                    {
                        "name": "Sunghwan Kim"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08754v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08754v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11679v1",
                "updated": "2025-06-13T11:17:15Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    17,
                    15,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T11:17:15Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    17,
                    15,
                    4,
                    164,
                    0
                ],
                "title": "LLMs on support of privacy and security of mobile apps: state of the art\n  and research directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs on support of privacy and security of mobile apps: state of the art\n  and research directions"
                },
                "summary": "Modern life has witnessed the explosion of mobile devices. However, besides\nthe valuable features that bring convenience to end users, security and privacy\nrisks still threaten users of mobile apps. The increasing sophistication of\nthese threats in recent years has underscored the need for more advanced and\nefficient detection approaches. In this chapter, we explore the application of\nLarge Language Models (LLMs) to identify security risks and privacy violations\nand mitigate them for the mobile application ecosystem. By introducing\nstate-of-the-art research that applied LLMs to mitigate the top 10 common\nsecurity risks of smartphone platforms, we highlight the feasibility and\npotential of LLMs to replace traditional analysis methods, such as dynamic and\nhybrid analysis of mobile apps. As a representative example of LLM-based\nsolutions, we present an approach to detect sensitive data leakage when users\nshare images online, a common behavior of smartphone users nowadays. Finally,\nwe discuss open research challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern life has witnessed the explosion of mobile devices. However, besides\nthe valuable features that bring convenience to end users, security and privacy\nrisks still threaten users of mobile apps. The increasing sophistication of\nthese threats in recent years has underscored the need for more advanced and\nefficient detection approaches. In this chapter, we explore the application of\nLarge Language Models (LLMs) to identify security risks and privacy violations\nand mitigate them for the mobile application ecosystem. By introducing\nstate-of-the-art research that applied LLMs to mitigate the top 10 common\nsecurity risks of smartphone platforms, we highlight the feasibility and\npotential of LLMs to replace traditional analysis methods, such as dynamic and\nhybrid analysis of mobile apps. As a representative example of LLM-based\nsolutions, we present an approach to detect sensitive data leakage when users\nshare images online, a common behavior of smartphone users nowadays. Finally,\nwe discuss open research challenges."
                },
                "authors": [
                    {
                        "name": "Tran Thanh Lam Nguyen"
                    },
                    {
                        "name": "Barbara Carminati"
                    },
                    {
                        "name": "Elena Ferrari"
                    }
                ],
                "author_detail": {
                    "name": "Elena Ferrari"
                },
                "author": "Elena Ferrari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09096v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09096v2",
                "updated": "2025-06-13T11:11:52Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    11,
                    52,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-10T12:59:14Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    12,
                    59,
                    14,
                    1,
                    161,
                    0
                ],
                "title": "Intra-Trajectory Consistency for Reward Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intra-Trajectory Consistency for Reward Modeling"
                },
                "summary": "Reward models are critical for improving large language models (LLMs),\nparticularly in reinforcement learning from human feedback (RLHF) or\ninference-time verification. Current reward modeling typically relies on scores\nof overall responses to learn the outcome rewards for the responses. However,\nsince the response-level scores are coarse-grained supervision signals, the\nreward model struggles to identify the specific components within a response\ntrajectory that truly correlate with the scores, leading to poor generalization\non unseen responses. In this paper, we propose to leverage generation\nprobabilities to establish reward consistency between processes in the response\ntrajectory, which allows the response-level supervisory signal to propagate\nacross processes, thereby providing additional fine-grained signals for reward\nlearning. Building on analysis under the Bayesian framework, we develop an\nintra-trajectory consistency regularization to enforce that adjacent processes\nwith higher next-token generation probability maintain more consistent rewards.\nWe apply the proposed regularization to the advanced outcome reward model,\nimproving its performance on RewardBench. Besides, we show that the reward\nmodel trained with the proposed regularization induces better DPO-aligned\npolicies and achieves better best-of-N (BON) inference-time verification\nresults. Our code is provided in https://github.com/chaoyang101/ICRM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models are critical for improving large language models (LLMs),\nparticularly in reinforcement learning from human feedback (RLHF) or\ninference-time verification. Current reward modeling typically relies on scores\nof overall responses to learn the outcome rewards for the responses. However,\nsince the response-level scores are coarse-grained supervision signals, the\nreward model struggles to identify the specific components within a response\ntrajectory that truly correlate with the scores, leading to poor generalization\non unseen responses. In this paper, we propose to leverage generation\nprobabilities to establish reward consistency between processes in the response\ntrajectory, which allows the response-level supervisory signal to propagate\nacross processes, thereby providing additional fine-grained signals for reward\nlearning. Building on analysis under the Bayesian framework, we develop an\nintra-trajectory consistency regularization to enforce that adjacent processes\nwith higher next-token generation probability maintain more consistent rewards.\nWe apply the proposed regularization to the advanced outcome reward model,\nimproving its performance on RewardBench. Besides, we show that the reward\nmodel trained with the proposed regularization induces better DPO-aligned\npolicies and achieves better best-of-N (BON) inference-time verification\nresults. Our code is provided in https://github.com/chaoyang101/ICRM."
                },
                "authors": [
                    {
                        "name": "Chaoyang Zhou"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Zengmao Wang"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09096v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09096v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21227v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21227v2",
                "updated": "2025-06-13T11:04:13Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    11,
                    4,
                    13,
                    4,
                    164,
                    0
                ],
                "published": "2025-03-27T07:36:11Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    7,
                    36,
                    11,
                    3,
                    86,
                    0
                ],
                "title": "LLaVA-CMoE: Towards Continual Mixture of Experts for Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLaVA-CMoE: Towards Continual Mixture of Experts for Large\n  Vision-Language Models"
                },
                "summary": "Mixture of Experts (MoE) architectures have recently advanced the scalability\nand adaptability of large language models (LLMs) for continual multimodal\nlearning. However, efficiently extending these models to accommodate sequential\ntasks remains challenging. As new tasks arrive, naive model expansion leads to\nrapid parameter growth, while modifying shared routing components often causes\ncatastrophic forgetting, undermining previously learned knowledge. To address\nthese issues, we propose LLaVA-CMoE, a continual learning framework for LLMs\nthat requires no replay data of previous tasks and ensures both parameter\nefficiency and robust knowledge retention. Our approach introduces a\nProbe-Guided Knowledge Extension mechanism, which uses probe experts to\ndynamically determine when and where new experts should be added, enabling\nadaptive and minimal parameter expansion tailored to task complexity.\nFurthermore, we present a Probabilistic Task Locator that assigns each task a\ndedicated, lightweight router. To handle the practical issue that task labels\nare unknown during inference, we leverage a VAE-based reconstruction strategy\nto identify the most suitable router by matching input distributions, allowing\nautomatic and accurate expert allocation. This design mitigates routing\nconflicts and catastrophic forgetting, enabling robust continual learning\nwithout explicit task labels. Extensive experiments on the CoIN benchmark,\ncovering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong\ncontinual learning performance with a compact model size, significantly\nreducing forgetting and parameter overhead compared to prior methods. These\nresults showcase the effectiveness and scalability of our approach for\nparameter-efficient continual learning in large language models. Our code will\nbe open-sourced soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) architectures have recently advanced the scalability\nand adaptability of large language models (LLMs) for continual multimodal\nlearning. However, efficiently extending these models to accommodate sequential\ntasks remains challenging. As new tasks arrive, naive model expansion leads to\nrapid parameter growth, while modifying shared routing components often causes\ncatastrophic forgetting, undermining previously learned knowledge. To address\nthese issues, we propose LLaVA-CMoE, a continual learning framework for LLMs\nthat requires no replay data of previous tasks and ensures both parameter\nefficiency and robust knowledge retention. Our approach introduces a\nProbe-Guided Knowledge Extension mechanism, which uses probe experts to\ndynamically determine when and where new experts should be added, enabling\nadaptive and minimal parameter expansion tailored to task complexity.\nFurthermore, we present a Probabilistic Task Locator that assigns each task a\ndedicated, lightweight router. To handle the practical issue that task labels\nare unknown during inference, we leverage a VAE-based reconstruction strategy\nto identify the most suitable router by matching input distributions, allowing\nautomatic and accurate expert allocation. This design mitigates routing\nconflicts and catastrophic forgetting, enabling robust continual learning\nwithout explicit task labels. Extensive experiments on the CoIN benchmark,\ncovering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong\ncontinual learning performance with a compact model size, significantly\nreducing forgetting and parameter overhead compared to prior methods. These\nresults showcase the effectiveness and scalability of our approach for\nparameter-efficient continual learning in large language models. Our code will\nbe open-sourced soon."
                },
                "authors": [
                    {
                        "name": "Hengyuan Zhao"
                    },
                    {
                        "name": "Ziqin Wang"
                    },
                    {
                        "name": "Qixin Sun"
                    },
                    {
                        "name": "Kaiyou Song"
                    },
                    {
                        "name": "Yilin Li"
                    },
                    {
                        "name": "Xiaolin Hu"
                    },
                    {
                        "name": "Qingpei Guo"
                    },
                    {
                        "name": "Si Liu"
                    }
                ],
                "author_detail": {
                    "name": "Si Liu"
                },
                "author": "Si Liu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21227v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21227v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11669v1",
                "updated": "2025-06-13T10:59:14Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    59,
                    14,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T10:59:14Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    59,
                    14,
                    4,
                    164,
                    0
                ],
                "title": "DTHA: A Digital Twin-Assisted Handover Authentication Scheme for 5G and\n  Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DTHA: A Digital Twin-Assisted Handover Authentication Scheme for 5G and\n  Beyond"
                },
                "summary": "With the rapid development and extensive deployment of the fifth-generation\nwireless system (5G), it has achieved ubiquitous high-speed connectivity and\nimproved overall communication performance. Additionally, as one of the\npromising technologies for integration beyond 5G, digital twin in cyberspace\ncan interact with the core network, transmit essential information, and further\nenhance the wireless communication quality of the corresponding mobile device\n(MD). However, the utilization of millimeter-wave, terahertz band, and\nultra-dense network technologies presents urgent challenges for MD in 5G and\nbeyond, particularly in terms of frequent handover authentication with target\nbase stations during faster mobility, which can cause connection interruption\nand incur malicious attacks. To address such challenges in 5G and beyond, in\nthis paper, we propose a secure and efficient handover authentication scheme by\nutilizing digital twin. Acting as an intelligent intermediate, the authorized\ndigital twin can handle computations and assist the corresponding MD in\nperforming secure mutual authentication and key negotiation in advance before\nattaching the target base stations in both intra-domain and inter-domain\nscenarios. In addition, we provide the formal verification based on BAN logic,\nRoR model, and ProVerif, and informal analysis to demonstrate that the proposed\nscheme can offer diverse security functionality. Performance evaluation shows\nthat the proposed scheme outperforms most related schemes in terms of\nsignaling, computation, and communication overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development and extensive deployment of the fifth-generation\nwireless system (5G), it has achieved ubiquitous high-speed connectivity and\nimproved overall communication performance. Additionally, as one of the\npromising technologies for integration beyond 5G, digital twin in cyberspace\ncan interact with the core network, transmit essential information, and further\nenhance the wireless communication quality of the corresponding mobile device\n(MD). However, the utilization of millimeter-wave, terahertz band, and\nultra-dense network technologies presents urgent challenges for MD in 5G and\nbeyond, particularly in terms of frequent handover authentication with target\nbase stations during faster mobility, which can cause connection interruption\nand incur malicious attacks. To address such challenges in 5G and beyond, in\nthis paper, we propose a secure and efficient handover authentication scheme by\nutilizing digital twin. Acting as an intelligent intermediate, the authorized\ndigital twin can handle computations and assist the corresponding MD in\nperforming secure mutual authentication and key negotiation in advance before\nattaching the target base stations in both intra-domain and inter-domain\nscenarios. In addition, we provide the formal verification based on BAN logic,\nRoR model, and ProVerif, and informal analysis to demonstrate that the proposed\nscheme can offer diverse security functionality. Performance evaluation shows\nthat the proposed scheme outperforms most related schemes in terms of\nsignaling, computation, and communication overheads."
                },
                "authors": [
                    {
                        "name": "Guanjie Li"
                    },
                    {
                        "name": "Tom H. Luan"
                    },
                    {
                        "name": "Chengzhe Lai"
                    },
                    {
                        "name": "Jinkai Zheng"
                    },
                    {
                        "name": "Rongxing Lu"
                    }
                ],
                "author_detail": {
                    "name": "Rongxing Lu"
                },
                "author": "Rongxing Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11666v1",
                "updated": "2025-06-13T10:53:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    53,
                    50,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T10:53:50Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    53,
                    50,
                    4,
                    164,
                    0
                ],
                "title": "Converting Annotated Clinical Cases into Structured Case Report Forms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Converting Annotated Clinical Cases into Structured Case Report Forms"
                },
                "summary": "Case Report Forms (CRFs) are largely used in medical research as they ensure\naccuracy, reliability, and validity of results in clinical studies. However,\npublicly available, wellannotated CRF datasets are scarce, limiting the\ndevelopment of CRF slot filling systems able to fill in a CRF from clinical\nnotes. To mitigate the scarcity of CRF datasets, we propose to take advantage\nof available datasets annotated for information extraction tasks and to convert\nthem into structured CRFs. We present a semi-automatic conversion methodology,\nwhich has been applied to the E3C dataset in two languages (English and\nItalian), resulting in a new, high-quality dataset for CRF slot filling.\nThrough several experiments on the created dataset, we report that slot filling\nachieves 59.7% for Italian and 67.3% for English on a closed Large Language\nModels (zero-shot) and worse performances on three families of open-source\nmodels, showing that filling CRFs is challenging even for recent\nstate-of-the-art LLMs. We release the datest at\nhttps://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Case Report Forms (CRFs) are largely used in medical research as they ensure\naccuracy, reliability, and validity of results in clinical studies. However,\npublicly available, wellannotated CRF datasets are scarce, limiting the\ndevelopment of CRF slot filling systems able to fill in a CRF from clinical\nnotes. To mitigate the scarcity of CRF datasets, we propose to take advantage\nof available datasets annotated for information extraction tasks and to convert\nthem into structured CRFs. We present a semi-automatic conversion methodology,\nwhich has been applied to the E3C dataset in two languages (English and\nItalian), resulting in a new, high-quality dataset for CRF slot filling.\nThrough several experiments on the created dataset, we report that slot filling\nachieves 59.7% for Italian and 67.3% for English on a closed Large Language\nModels (zero-shot) and worse performances on three families of open-source\nmodels, showing that filling CRFs is challenging even for recent\nstate-of-the-art LLMs. We release the datest at\nhttps://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166"
                },
                "authors": [
                    {
                        "name": "Pietro Ferrazzi"
                    },
                    {
                        "name": "Alberto Lavelli"
                    },
                    {
                        "name": "Bernardo Magnini"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Magnini"
                },
                "author": "Bernardo Magnini",
                "arxiv_comment": "to be published in BioNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11659v1",
                "updated": "2025-06-13T10:40:23Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    40,
                    23,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T10:40:23Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    40,
                    23,
                    4,
                    164,
                    0
                ],
                "title": "An Empirical study on LLM-based Log Retrieval for Software Engineering\n  Metadata Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical study on LLM-based Log Retrieval for Software Engineering\n  Metadata Management"
                },
                "summary": "Developing autonomous driving systems (ADSs) involves generating and storing\nextensive log data from test drives, which is essential for verification,\nresearch, and simulation. However, these high-frequency logs, recorded over\nvarying durations, pose challenges for developers attempting to locate specific\ndriving scenarios. This difficulty arises due to the wide range of signals\nrepresenting various vehicle components and driving conditions, as well as\nunfamiliarity of some developers' with the detailed meaning of these signals.\nTraditional SQL-based querying exacerbates this challenge by demanding both\ndomain expertise and database knowledge, often yielding results that are\ndifficult to verify for accuracy.\n  This paper introduces a Large Language Model (LLM)-supported approach that\ncombines signal log data with video recordings from test drives, enabling\nnatural language based scenario searches while reducing the need for\nspecialized knowledge. By leveraging scenario distance graphs and relative gap\nindicators, it provides quantifiable metrics to evaluate the reliability of\nquery results. The method is implemented as an API for efficient database\nquerying and retrieval of relevant records, paired with video frames for\nintuitive visualization. Evaluation on an open industrial dataset demonstrates\nimproved efficiency and reliability in scenario retrieval, eliminating\ndependency on a single data source and conventional SQL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing autonomous driving systems (ADSs) involves generating and storing\nextensive log data from test drives, which is essential for verification,\nresearch, and simulation. However, these high-frequency logs, recorded over\nvarying durations, pose challenges for developers attempting to locate specific\ndriving scenarios. This difficulty arises due to the wide range of signals\nrepresenting various vehicle components and driving conditions, as well as\nunfamiliarity of some developers' with the detailed meaning of these signals.\nTraditional SQL-based querying exacerbates this challenge by demanding both\ndomain expertise and database knowledge, often yielding results that are\ndifficult to verify for accuracy.\n  This paper introduces a Large Language Model (LLM)-supported approach that\ncombines signal log data with video recordings from test drives, enabling\nnatural language based scenario searches while reducing the need for\nspecialized knowledge. By leveraging scenario distance graphs and relative gap\nindicators, it provides quantifiable metrics to evaluate the reliability of\nquery results. The method is implemented as an API for efficient database\nquerying and retrieval of relevant records, paired with video frames for\nintuitive visualization. Evaluation on an open industrial dataset demonstrates\nimproved efficiency and reliability in scenario retrieval, eliminating\ndependency on a single data source and conventional SQL."
                },
                "authors": [
                    {
                        "name": "Simin Sun"
                    },
                    {
                        "name": "Yuchuan Jin"
                    },
                    {
                        "name": "Miroslaw Staron"
                    }
                ],
                "author_detail": {
                    "name": "Miroslaw Staron"
                },
                "author": "Miroslaw Staron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11650v1",
                "updated": "2025-06-13T10:24:44Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    24,
                    44,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T10:24:44Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    24,
                    44,
                    4,
                    164,
                    0
                ],
                "title": "Robot Context Protocol (RCP): A Runtime-Agnostic Interface for\n  Agent-Aware Robot Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot Context Protocol (RCP): A Runtime-Agnostic Interface for\n  Agent-Aware Robot Control"
                },
                "summary": "The Robot Context Protocol (RCP) is a lightweight, middleware-agnostic\ncommunication protocol designed to simplify the complexity of robotic systems\nand enable seamless interaction between robots, users, and autonomous agents.\nRCP provides a unified and semantically meaningful interface that decouples\nclient-facing operations from backend implementations, supporting a wide range\nof deployment environments including physical robots, cloud-based\norchestrators, and simulated platforms. Built on HTTP and WebSocket transport\nlayers, the protocol defines a schema-driven message format with structured\noperations such as read, write, execute, and subscribe. It integrates features\nsuch as runtime introspection, asynchronous feedback, multi-tenant namespace\nisolation, and strict type validation to ensure robustness, scalability, and\nsecurity. The architecture, message structure, interface model, and\nadapter-based backend integration strategy of RCP are described, along with\ndeployment practices and applicability across industries including\nmanufacturing, logistics, and healthcare. RCP enables intelligent, resilient,\nand safe robotic operations in complex, multi-agent ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Robot Context Protocol (RCP) is a lightweight, middleware-agnostic\ncommunication protocol designed to simplify the complexity of robotic systems\nand enable seamless interaction between robots, users, and autonomous agents.\nRCP provides a unified and semantically meaningful interface that decouples\nclient-facing operations from backend implementations, supporting a wide range\nof deployment environments including physical robots, cloud-based\norchestrators, and simulated platforms. Built on HTTP and WebSocket transport\nlayers, the protocol defines a schema-driven message format with structured\noperations such as read, write, execute, and subscribe. It integrates features\nsuch as runtime introspection, asynchronous feedback, multi-tenant namespace\nisolation, and strict type validation to ensure robustness, scalability, and\nsecurity. The architecture, message structure, interface model, and\nadapter-based backend integration strategy of RCP are described, along with\ndeployment practices and applicability across industries including\nmanufacturing, logistics, and healthcare. RCP enables intelligent, resilient,\nand safe robotic operations in complex, multi-agent ecosystems."
                },
                "authors": [
                    {
                        "name": "Lambert Lee"
                    },
                    {
                        "name": "Joshua Lau"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Lau"
                },
                "author": "Joshua Lau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11644v1",
                "updated": "2025-06-13T10:18:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    18,
                    36,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T10:18:36Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    18,
                    36,
                    4,
                    164,
                    0
                ],
                "title": "Bounded Memory in Distributed Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bounded Memory in Distributed Networks"
                },
                "summary": "The recent advent of programmable switches makes distributed algorithms\nreadily deployable in real-world datacenter networks. However, there are still\ngaps between theory and practice that prevent the smooth adaptation of CONGEST\nalgorithms to these environments. In this paper, we focus on the memory\nrestrictions that arise in real-world deployments. We introduce the\n$\\mu$-CONGEST model where on top of the bandwidth restriction, the memory of\nnodes is also limited to $\\mu$ words, in line with real-world systems. We\nprovide fast algorithms of two main flavors.\n  First, we observe that many algorithms in the CONGEST model are\nmemory-intensive and do not work in $\\mu$-CONGEST. A prime example of a family\nof algorithms that use large memory is clique-listing algorithms. We show that\nthe memory issue that arises here cannot be resolved without incurring a cost\nin the round complexity, by establishing a lower bound on the round complexity\nof listing cliques in $\\mu$-CONGEST. We introduce novel techniques to overcome\nthese issues and generalize the algorithms to work within a given memory bound.\nCombined with our lower bound, these provide tight tradeoffs between the\nrunning time and memory of nodes.\n  Second, we show that it is possible to efficiently simulate various families\nof streaming algorithms in $\\mu$-CONGEST. These include fast simulations of\n$p$-pass algorithms, random order streams, and various types of mergeable\nstreaming algorithms.\n  Combining our contributions, we show that we can use streaming algorithms to\nefficiently generate statistics regarding combinatorial structures in the\nnetwork. An example of an end result of this type is that we can efficiently\nidentify and provide the per-color frequencies of the frequent monochromatic\ntriangles in $\\mu$-CONGEST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advent of programmable switches makes distributed algorithms\nreadily deployable in real-world datacenter networks. However, there are still\ngaps between theory and practice that prevent the smooth adaptation of CONGEST\nalgorithms to these environments. In this paper, we focus on the memory\nrestrictions that arise in real-world deployments. We introduce the\n$\\mu$-CONGEST model where on top of the bandwidth restriction, the memory of\nnodes is also limited to $\\mu$ words, in line with real-world systems. We\nprovide fast algorithms of two main flavors.\n  First, we observe that many algorithms in the CONGEST model are\nmemory-intensive and do not work in $\\mu$-CONGEST. A prime example of a family\nof algorithms that use large memory is clique-listing algorithms. We show that\nthe memory issue that arises here cannot be resolved without incurring a cost\nin the round complexity, by establishing a lower bound on the round complexity\nof listing cliques in $\\mu$-CONGEST. We introduce novel techniques to overcome\nthese issues and generalize the algorithms to work within a given memory bound.\nCombined with our lower bound, these provide tight tradeoffs between the\nrunning time and memory of nodes.\n  Second, we show that it is possible to efficiently simulate various families\nof streaming algorithms in $\\mu$-CONGEST. These include fast simulations of\n$p$-pass algorithms, random order streams, and various types of mergeable\nstreaming algorithms.\n  Combining our contributions, we show that we can use streaming algorithms to\nefficiently generate statistics regarding combinatorial structures in the\nnetwork. An example of an end result of this type is that we can efficiently\nidentify and provide the per-color frequencies of the frequent monochromatic\ntriangles in $\\mu$-CONGEST."
                },
                "authors": [
                    {
                        "name": "Ran Ben Basat"
                    },
                    {
                        "name": "Keren Censor-Hillel"
                    },
                    {
                        "name": "Yi-Jun Chang"
                    },
                    {
                        "name": "Wenchen Han"
                    },
                    {
                        "name": "Dean Leitersdorf"
                    },
                    {
                        "name": "Gregory Schwartzman"
                    }
                ],
                "author_detail": {
                    "name": "Gregory Schwartzman"
                },
                "author": "Gregory Schwartzman",
                "arxiv_doi": "10.1145/3694906.3743302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3694906.3743302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2506.11644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at The 37th ACM Symposium on Parallelism in Algorithms and\n  Architectures (SPAA '25). 22 pages",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08967v2",
                "updated": "2025-06-13T10:07:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    7,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-10T16:37:39Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    16,
                    37,
                    39,
                    1,
                    161,
                    0
                ],
                "title": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language\n  Model"
                },
                "summary": "Large Audio-Language Models (LALMs) have significantly advanced intelligent\nhuman-computer interaction, yet their reliance on text-based outputs limits\ntheir ability to generate natural speech responses directly, hindering seamless\naudio interactions. To address this, we introduce Step-Audio-AQAA, a fully\nend-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model\nintegrates a dual-codebook audio tokenizer for linguistic and semantic feature\nextraction, a 130-billion-parameter backbone LLM and a neural vocoder for\nhigh-fidelity speech synthesis. Our post-training approach employs interleaved\ntoken-output of text and audio to enhance semantic coherence and combines\nDirect Preference Optimization (DPO) with model merge to improve performance.\nEvaluations on the StepEval-Audio-360 benchmark demonstrate that\nStep-Audio-AQAA excels especially in speech control, outperforming the\nstate-of-art LALMs in key areas. This work contributes a promising solution for\nend-to-end LALMs and highlights the critical role of token-based vocoder in\nenhancing overall performance for AQAA tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Audio-Language Models (LALMs) have significantly advanced intelligent\nhuman-computer interaction, yet their reliance on text-based outputs limits\ntheir ability to generate natural speech responses directly, hindering seamless\naudio interactions. To address this, we introduce Step-Audio-AQAA, a fully\nend-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model\nintegrates a dual-codebook audio tokenizer for linguistic and semantic feature\nextraction, a 130-billion-parameter backbone LLM and a neural vocoder for\nhigh-fidelity speech synthesis. Our post-training approach employs interleaved\ntoken-output of text and audio to enhance semantic coherence and combines\nDirect Preference Optimization (DPO) with model merge to improve performance.\nEvaluations on the StepEval-Audio-360 benchmark demonstrate that\nStep-Audio-AQAA excels especially in speech control, outperforming the\nstate-of-art LALMs in key areas. This work contributes a promising solution for\nend-to-end LALMs and highlights the critical role of token-based vocoder in\nenhancing overall performance for AQAA tasks."
                },
                "authors": [
                    {
                        "name": "Ailin Huang"
                    },
                    {
                        "name": "Bingxin Li"
                    },
                    {
                        "name": "Bruce Wang"
                    },
                    {
                        "name": "Boyong Wu"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Chengli Feng"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Jingbei Li"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Joanna Wang"
                    },
                    {
                        "name": "Mingrui Chen"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Tian Fei"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xuerui Yang"
                    },
                    {
                        "name": "Yechang Huang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Buyun Ma"
                    },
                    {
                        "name": "Changxin Miao"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Dapeng Shi"
                    },
                    {
                        "name": "Dingyuan Hu"
                    },
                    {
                        "name": "Enle Liu"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Gulin Yan"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Haonan Jia"
                    },
                    {
                        "name": "Jiahao Gong"
                    },
                    {
                        "name": "Jiaoren Wu"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Junzhe Lin"
                    },
                    {
                        "name": "Kaixiang Li"
                    },
                    {
                        "name": "Lei Xia"
                    },
                    {
                        "name": "Longlong Gu"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Nie Hao"
                    },
                    {
                        "name": "Ranchen Ming"
                    },
                    {
                        "name": "Shaoliang Pang"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Tiancheng Cao"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Wenqing He"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Xuelin Zhang"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yuanwei Liang"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Zidong Yang"
                    },
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Jiansheng Chen"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Xinhao Zhang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Shuchang Zhou"
                    },
                    {
                        "name": "Chen Hu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Hu"
                },
                "author": "Chen Hu",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11635v1",
                "updated": "2025-06-13T10:05:43Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    5,
                    43,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T10:05:43Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    5,
                    43,
                    4,
                    164,
                    0
                ],
                "title": "FAA Framework: A Large Language Model-Based Approach for Credit Card\n  Fraud Investigations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAA Framework: A Large Language Model-Based Approach for Credit Card\n  Fraud Investigations"
                },
                "summary": "The continuous growth of the e-commerce industry attracts fraudsters who\nexploit stolen credit card details. Companies often investigate suspicious\ntransactions in order to retain customer trust and address gaps in their fraud\ndetection systems. However, analysts are overwhelmed with an enormous number of\nalerts from credit card transaction monitoring systems. Each alert\ninvestigation requires from the fraud analysts careful attention, specialized\nknowledge, and precise documentation of the outcomes, leading to alert fatigue.\nTo address this, we propose a fraud analyst assistant (FAA) framework, which\nemploys multi-modal large language models (LLMs) to automate credit card fraud\ninvestigations and generate explanatory reports. The FAA framework leverages\nthe reasoning, code execution, and vision capabilities of LLMs to conduct\nplanning, evidence collection, and analysis in each investigation step. A\ncomprehensive empirical evaluation of 500 credit card fraud investigations\ndemonstrates that the FAA framework produces reliable and efficient\ninvestigations comprising seven steps on average. Thus we found that the FAA\nframework can automate large parts of the workload and help reduce the\nchallenges faced by fraud analysts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The continuous growth of the e-commerce industry attracts fraudsters who\nexploit stolen credit card details. Companies often investigate suspicious\ntransactions in order to retain customer trust and address gaps in their fraud\ndetection systems. However, analysts are overwhelmed with an enormous number of\nalerts from credit card transaction monitoring systems. Each alert\ninvestigation requires from the fraud analysts careful attention, specialized\nknowledge, and precise documentation of the outcomes, leading to alert fatigue.\nTo address this, we propose a fraud analyst assistant (FAA) framework, which\nemploys multi-modal large language models (LLMs) to automate credit card fraud\ninvestigations and generate explanatory reports. The FAA framework leverages\nthe reasoning, code execution, and vision capabilities of LLMs to conduct\nplanning, evidence collection, and analysis in each investigation step. A\ncomprehensive empirical evaluation of 500 credit card fraud investigations\ndemonstrates that the FAA framework produces reliable and efficient\ninvestigations comprising seven steps on average. Thus we found that the FAA\nframework can automate large parts of the workload and help reduce the\nchallenges faced by fraud analysts."
                },
                "authors": [
                    {
                        "name": "Shaun Shuster"
                    },
                    {
                        "name": "Eyal Zaloof"
                    },
                    {
                        "name": "Asaf Shabtai"
                    },
                    {
                        "name": "Rami Puzis"
                    }
                ],
                "author_detail": {
                    "name": "Rami Puzis"
                },
                "author": "Rami Puzis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11631v1",
                "updated": "2025-06-13T10:02:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    2,
                    39,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T10:02:39Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    10,
                    2,
                    39,
                    4,
                    164,
                    0
                ],
                "title": "SceneGram: Conceptualizing and Describing Tangrams in Scene Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SceneGram: Conceptualizing and Describing Tangrams in Scene Context"
                },
                "summary": "Research on reference and naming suggests that humans can come up with very\ndifferent ways of conceptualizing and referring to the same object, e.g. the\nsame abstract tangram shape can be a \"crab\", \"sink\" or \"space ship\". Another\ncommon assumption in cognitive science is that scene context fundamentally\nshapes our visual perception of objects and conceptual expectations. This paper\ncontributes SceneGram, a dataset of human references to tangram shapes placed\nin different scene contexts, allowing for systematic analyses of the effect of\nscene context on conceptualization. Based on this data, we analyze references\nto tangram shapes generated by multimodal LLMs, showing that these models do\nnot account for the richness and variability of conceptualizations found in\nhuman references.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on reference and naming suggests that humans can come up with very\ndifferent ways of conceptualizing and referring to the same object, e.g. the\nsame abstract tangram shape can be a \"crab\", \"sink\" or \"space ship\". Another\ncommon assumption in cognitive science is that scene context fundamentally\nshapes our visual perception of objects and conceptual expectations. This paper\ncontributes SceneGram, a dataset of human references to tangram shapes placed\nin different scene contexts, allowing for systematic analyses of the effect of\nscene context on conceptualization. Based on this data, we analyze references\nto tangram shapes generated by multimodal LLMs, showing that these models do\nnot account for the richness and variability of conceptualizations found in\nhuman references."
                },
                "authors": [
                    {
                        "name": "Simeon Junker"
                    },
                    {
                        "name": "Sina Zarrieß"
                    }
                ],
                "author_detail": {
                    "name": "Sina Zarrieß"
                },
                "author": "Sina Zarrieß",
                "arxiv_comment": "To appear in ACL Findings 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23549v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23549v2",
                "updated": "2025-06-13T09:56:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    56,
                    36,
                    4,
                    164,
                    0
                ],
                "published": "2025-05-29T15:27:52Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    27,
                    52,
                    3,
                    149,
                    0
                ],
                "title": "LLM-based Property-based Test Generation for Guardrailing Cyber-Physical\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Property-based Test Generation for Guardrailing Cyber-Physical\n  Systems"
                },
                "summary": "Cyber-physical systems (CPSs) are complex systems that integrate physical,\ncomputational, and communication subsystems. The heterogeneous nature of these\nsystems makes their safety assurance challenging. In this paper, we propose a\nnovel automated approach for guardrailing cyber-physical systems using\nproperty-based tests (PBTs) generated by Large Language Models (LLMs). Our\napproach employs an LLM to extract properties from the code and documentation\nof CPSs. Next, we use the LLM to generate PBTs that verify the extracted\nproperties on the CPS. The generated PBTs have two uses. First, they are used\nto test the CPS before it is deployed, i.e., at design time. Secondly, these\nPBTs can be used after deployment, i.e., at run time, to monitor the behavior\nof the system and guardrail it against unsafe states. We implement our approach\nin ChekProp and conduct preliminary experiments to evaluate the generated PBTs\nin terms of their relevance (how well they match manually crafted properties),\nexecutability (how many run with minimal manual modification), and\neffectiveness (coverage of the input space partitions). The results of our\nexperiments and evaluation demonstrate a promising path forward for creating\nguardrails for CPSs using LLM-generated property-based tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber-physical systems (CPSs) are complex systems that integrate physical,\ncomputational, and communication subsystems. The heterogeneous nature of these\nsystems makes their safety assurance challenging. In this paper, we propose a\nnovel automated approach for guardrailing cyber-physical systems using\nproperty-based tests (PBTs) generated by Large Language Models (LLMs). Our\napproach employs an LLM to extract properties from the code and documentation\nof CPSs. Next, we use the LLM to generate PBTs that verify the extracted\nproperties on the CPS. The generated PBTs have two uses. First, they are used\nto test the CPS before it is deployed, i.e., at design time. Secondly, these\nPBTs can be used after deployment, i.e., at run time, to monitor the behavior\nof the system and guardrail it against unsafe states. We implement our approach\nin ChekProp and conduct preliminary experiments to evaluate the generated PBTs\nin terms of their relevance (how well they match manually crafted properties),\nexecutability (how many run with minimal manual modification), and\neffectiveness (coverage of the input space partitions). The results of our\nexperiments and evaluation demonstrate a promising path forward for creating\nguardrails for CPSs using LLM-generated property-based tests."
                },
                "authors": [
                    {
                        "name": "Khashayar Etemadi"
                    },
                    {
                        "name": "Marjan Sirjani"
                    },
                    {
                        "name": "Mahshid Helali Moghadam"
                    },
                    {
                        "name": "Per Strandberg"
                    },
                    {
                        "name": "Paul Pettersson"
                    }
                ],
                "author_detail": {
                    "name": "Paul Pettersson"
                },
                "author": "Paul Pettersson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23549v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23549v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02050v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02050v4",
                "updated": "2025-06-13T09:44:56Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    44,
                    56,
                    4,
                    164,
                    0
                ],
                "published": "2024-06-04T07:31:06Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    31,
                    6,
                    1,
                    156,
                    0
                ],
                "title": "JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large\n  Language Models"
                },
                "summary": "With the development of large language models (LLMs), social biases in these\nLLMs have become a pressing issue. Although there are various benchmarks for\nsocial biases across languages, the extent to which Japanese LLMs exhibit\nsocial biases has not been fully investigated. In this study, we construct the\nJapanese Bias Benchmark dataset for Question Answering (JBBQ) based on the\nEnglish bias benchmark BBQ, with analysis of social biases in Japanese LLMs.\nThe results show that while current open Japanese LLMs with more parameters\nshow improved accuracies on JBBQ, their bias scores increase. In addition,\nprompts with a warning about social biases and chain-of-thought prompting\nreduce the effect of biases in model outputs, but there is room for improvement\nin extracting the correct evidence from contexts in Japanese. Our dataset is\navailable at https://github.com/ynklab/JBBQ_data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), social biases in these\nLLMs have become a pressing issue. Although there are various benchmarks for\nsocial biases across languages, the extent to which Japanese LLMs exhibit\nsocial biases has not been fully investigated. In this study, we construct the\nJapanese Bias Benchmark dataset for Question Answering (JBBQ) based on the\nEnglish bias benchmark BBQ, with analysis of social biases in Japanese LLMs.\nThe results show that while current open Japanese LLMs with more parameters\nshow improved accuracies on JBBQ, their bias scores increase. In addition,\nprompts with a warning about social biases and chain-of-thought prompting\nreduce the effect of biases in model outputs, but there is room for improvement\nin extracting the correct evidence from contexts in Japanese. Our dataset is\navailable at https://github.com/ynklab/JBBQ_data."
                },
                "authors": [
                    {
                        "name": "Hitomi Yanaka"
                    },
                    {
                        "name": "Namgi Han"
                    },
                    {
                        "name": "Ryoma Kumon"
                    },
                    {
                        "name": "Jie Lu"
                    },
                    {
                        "name": "Masashi Takeshita"
                    },
                    {
                        "name": "Ryo Sekizawa"
                    },
                    {
                        "name": "Taisei Kato"
                    },
                    {
                        "name": "Hiromi Arai"
                    }
                ],
                "author_detail": {
                    "name": "Hiromi Arai"
                },
                "author": "Hiromi Arai",
                "arxiv_comment": "Accepted to the 6th Workshop on Gender Bias in Natural Language\n  Processing (GeBNLP2025) at ACL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02050v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02050v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08595v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08595v3",
                "updated": "2025-06-13T09:37:42Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    37,
                    42,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-10T09:03:35Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    9,
                    3,
                    35,
                    1,
                    161,
                    0
                ],
                "title": "High-throughput polarization-independent spatial mode shaping with\n  mode-selective photonic lanterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-throughput polarization-independent spatial mode shaping with\n  mode-selective photonic lanterns"
                },
                "summary": "Coherent interference in multimode photonic systems offers a transformative\napproach for scalable, high-fidelity optical control, enabling precise beam\nshaping, efficient power delivery, and advanced signal processing through\nsingle-mode performance. Current techniques are often constrained to a single\npolarization state of light, system complexity, and the fabrication challenges\nassociated with precise waveguide structures. Here, we present a\nhigh-efficiency coherent combination scheme employing a commercial\nmode-selective photonic lantern supporting three spatial channels, achieving\nGaussian mode interference at multiple locations along the multimode core with\nup to 100% mode conversion efficiencies. By integrating a Faraday reflecting\nfibre component and piezo-phase shifters with the photonic lantern channels, we\ndemonstrate stable and adaptive mode manipulation without the need for complex\nadaptive optics. These findings highlight the potential of photonic lanterns as\nrobust platforms for controlled mode interference, paving the way for their\ndeployment in next-generation optical communication, beam shaping, and\nbeam-forming systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent interference in multimode photonic systems offers a transformative\napproach for scalable, high-fidelity optical control, enabling precise beam\nshaping, efficient power delivery, and advanced signal processing through\nsingle-mode performance. Current techniques are often constrained to a single\npolarization state of light, system complexity, and the fabrication challenges\nassociated with precise waveguide structures. Here, we present a\nhigh-efficiency coherent combination scheme employing a commercial\nmode-selective photonic lantern supporting three spatial channels, achieving\nGaussian mode interference at multiple locations along the multimode core with\nup to 100% mode conversion efficiencies. By integrating a Faraday reflecting\nfibre component and piezo-phase shifters with the photonic lantern channels, we\ndemonstrate stable and adaptive mode manipulation without the need for complex\nadaptive optics. These findings highlight the potential of photonic lanterns as\nrobust platforms for controlled mode interference, paving the way for their\ndeployment in next-generation optical communication, beam shaping, and\nbeam-forming systems."
                },
                "authors": [
                    {
                        "name": "Harikumar K Chandrasekharan"
                    },
                    {
                        "name": "Ross Donaldson"
                    }
                ],
                "author_detail": {
                    "name": "Ross Donaldson"
                },
                "author": "Ross Donaldson",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08595v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08595v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11613v1",
                "updated": "2025-06-13T09:34:25Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    34,
                    25,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T09:34:25Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    34,
                    25,
                    4,
                    164,
                    0
                ],
                "title": "Model Organisms for Emergent Misalignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Organisms for Emergent Misalignment"
                },
                "summary": "Recent work discovered Emergent Misalignment (EM): fine-tuning large language\nmodels on narrowly harmful datasets can lead them to become broadly misaligned.\nA survey of experts prior to publication revealed this was highly unexpected,\ndemonstrating critical gaps in our understanding of model alignment. In this\nwork, we both advance understanding and provide tools for future research.\nUsing new narrowly misaligned datasets, we create a set of improved model\norganisms that achieve 99% coherence (vs. 67% prior), work with smaller 0.5B\nparameter models (vs. 32B), and that induce misalignment using a single rank-1\nLoRA adapter. We demonstrate that EM occurs robustly across diverse model\nsizes, three model families, and numerous training protocols including full\nsupervised fine-tuning. Leveraging these cleaner model organisms, we isolate a\nmechanistic phase transition and demonstrate that it corresponds to a robust\nbehavioural phase transition in all studied organisms. Aligning large language\nmodels is critical for frontier AI safety, yet EM exposes how far we are from\nachieving this robustly. By distilling clean model organisms that isolate a\nminimal alignment-compromising change, and where this is learnt, we establish a\nfoundation for future research into understanding and mitigating alignment\nrisks in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work discovered Emergent Misalignment (EM): fine-tuning large language\nmodels on narrowly harmful datasets can lead them to become broadly misaligned.\nA survey of experts prior to publication revealed this was highly unexpected,\ndemonstrating critical gaps in our understanding of model alignment. In this\nwork, we both advance understanding and provide tools for future research.\nUsing new narrowly misaligned datasets, we create a set of improved model\norganisms that achieve 99% coherence (vs. 67% prior), work with smaller 0.5B\nparameter models (vs. 32B), and that induce misalignment using a single rank-1\nLoRA adapter. We demonstrate that EM occurs robustly across diverse model\nsizes, three model families, and numerous training protocols including full\nsupervised fine-tuning. Leveraging these cleaner model organisms, we isolate a\nmechanistic phase transition and demonstrate that it corresponds to a robust\nbehavioural phase transition in all studied organisms. Aligning large language\nmodels is critical for frontier AI safety, yet EM exposes how far we are from\nachieving this robustly. By distilling clean model organisms that isolate a\nminimal alignment-compromising change, and where this is learnt, we establish a\nfoundation for future research into understanding and mitigating alignment\nrisks in LLMs."
                },
                "authors": [
                    {
                        "name": "Edward Turner"
                    },
                    {
                        "name": "Anna Soligo"
                    },
                    {
                        "name": "Mia Taylor"
                    },
                    {
                        "name": "Senthooran Rajamanoharan"
                    },
                    {
                        "name": "Neel Nanda"
                    }
                ],
                "author_detail": {
                    "name": "Neel Nanda"
                },
                "author": "Neel Nanda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11612v1",
                "updated": "2025-06-13T09:33:58Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    33,
                    58,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T09:33:58Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    33,
                    58,
                    4,
                    164,
                    0
                ],
                "title": "KEENHash: Hashing Programs into Function-Aware Embeddings for\n  Large-Scale Binary Code Similarity Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KEENHash: Hashing Programs into Function-Aware Embeddings for\n  Large-Scale Binary Code Similarity Analysis"
                },
                "summary": "Binary code similarity analysis (BCSA) is a crucial research area in many\nfields such as cybersecurity. Specifically, function-level diffing tools are\nthe most widely used in BCSA: they perform function matching one by one for\nevaluating the similarity between binary programs. However, such methods need a\nhigh time complexity, making them unscalable in large-scale scenarios (e.g.,\n1/n-to-n search). Towards effective and efficient program-level BCSA, we\npropose KEENHash, a novel hashing approach that hashes binaries into\nprogram-level representations through large language model (LLM)-generated\nfunction embeddings. KEENHash condenses a binary into one compact and\nfixed-length program embedding using K-Means and Feature Hashing, allowing us\nto do effective and efficient large-scale program-level BCSA, surpassing the\nprevious state-of-the-art methods. The experimental results show that KEENHash\nis at least 215 times faster than the state-of-the-art function matching tools\nwhile maintaining effectiveness. Furthermore, in a large-scale scenario with\n5.3 billion similarity evaluations, KEENHash takes only 395.83 seconds while\nthese tools will cost at least 56 days. We also evaluate KEENHash on the\nprogram clone search of large-scale BCSA across extensive datasets in 202,305\nbinaries. Compared with 4 state-of-the-art methods, KEENHash outperforms all of\nthem by at least 23.16%, and displays remarkable superiority over them in the\nlarge-scale BCSA security scenario of malware detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binary code similarity analysis (BCSA) is a crucial research area in many\nfields such as cybersecurity. Specifically, function-level diffing tools are\nthe most widely used in BCSA: they perform function matching one by one for\nevaluating the similarity between binary programs. However, such methods need a\nhigh time complexity, making them unscalable in large-scale scenarios (e.g.,\n1/n-to-n search). Towards effective and efficient program-level BCSA, we\npropose KEENHash, a novel hashing approach that hashes binaries into\nprogram-level representations through large language model (LLM)-generated\nfunction embeddings. KEENHash condenses a binary into one compact and\nfixed-length program embedding using K-Means and Feature Hashing, allowing us\nto do effective and efficient large-scale program-level BCSA, surpassing the\nprevious state-of-the-art methods. The experimental results show that KEENHash\nis at least 215 times faster than the state-of-the-art function matching tools\nwhile maintaining effectiveness. Furthermore, in a large-scale scenario with\n5.3 billion similarity evaluations, KEENHash takes only 395.83 seconds while\nthese tools will cost at least 56 days. We also evaluate KEENHash on the\nprogram clone search of large-scale BCSA across extensive datasets in 202,305\nbinaries. Compared with 4 state-of-the-art methods, KEENHash outperforms all of\nthem by at least 23.16%, and displays remarkable superiority over them in the\nlarge-scale BCSA security scenario of malware detection."
                },
                "authors": [
                    {
                        "name": "Zhijie Liu"
                    },
                    {
                        "name": "Qiyi Tang"
                    },
                    {
                        "name": "Sen Nie"
                    },
                    {
                        "name": "Shi Wu"
                    },
                    {
                        "name": "Liang Feng Zhang"
                    },
                    {
                        "name": "Yutian Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yutian Tang"
                },
                "author": "Yutian Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11812v2",
                "updated": "2025-06-13T09:32:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    32,
                    19,
                    4,
                    164,
                    0
                ],
                "published": "2025-02-17T13:59:41Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    13,
                    59,
                    41,
                    0,
                    48,
                    0
                ],
                "title": "Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit\n  Analysis"
                },
                "summary": "Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies (Prakash et al. 2024; Chhabra et al. 2024) that focus on tasks\nwhere pre-trained models already perform well, we develop a set of mathematical\ntasks where fine-tuning yields substantial performance gains, which are closer\nto the practical setting. In our experiments, we identify circuits at various\ncheckpoints during fine-tuning and examine the interplay between circuit\nanalysis, fine-tuning methods, and task complexities. First, we find that while\ncircuits maintain high node similarity before and after fine-tuning, their\nedges undergo significant changes, in contrast to prior work that shows\ncircuits only add some additional components after fine-tuning. Based on these\nobservations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method,\nwhich assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning significantly improves the performance of Large Language Models\n(LLMs), yet its underlying mechanisms remain poorly understood. This paper aims\nto provide an in-depth interpretation of the fine-tuning process through\ncircuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike\nprevious studies (Prakash et al. 2024; Chhabra et al. 2024) that focus on tasks\nwhere pre-trained models already perform well, we develop a set of mathematical\ntasks where fine-tuning yields substantial performance gains, which are closer\nto the practical setting. In our experiments, we identify circuits at various\ncheckpoints during fine-tuning and examine the interplay between circuit\nanalysis, fine-tuning methods, and task complexities. First, we find that while\ncircuits maintain high node similarity before and after fine-tuning, their\nedges undergo significant changes, in contrast to prior work that shows\ncircuits only add some additional components after fine-tuning. Based on these\nobservations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method,\nwhich assigns ranks to layers based on edge changes in the circuits.\nExperimental results demonstrate that our circuit-based LoRA algorithm achieves\nan average performance improvement of 2.46% over standard LoRA with similar\nparameter sizes. Furthermore, we explore how combining circuits from subtasks\ncan enhance fine-tuning in compositional tasks, providing new insights into the\ndesign of such tasks and deepening the understanding of circuit dynamics and\nfine-tuning mechanisms."
                },
                "authors": [
                    {
                        "name": "Xu Wang"
                    },
                    {
                        "name": "Yan Hu"
                    },
                    {
                        "name": "Wenyu Du"
                    },
                    {
                        "name": "Reynold Cheng"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Difan Zou"
                    }
                ],
                "author_detail": {
                    "name": "Difan Zou"
                },
                "author": "Difan Zou",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11603v1",
                "updated": "2025-06-13T09:17:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    17,
                    36,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T09:17:36Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    17,
                    36,
                    4,
                    164,
                    0
                ],
                "title": "TongSearch-QR: Reinforced Query Reasoning for Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TongSearch-QR: Reinforced Query Reasoning for Retrieval"
                },
                "summary": "Traditional information retrieval (IR) methods excel at textual and semantic\nmatching but struggle in reasoning-intensive retrieval tasks that require\nmulti-hop inference or complex semantic understanding between queries and\ndocuments. One promising solution is to explicitly rewrite or augment queries\nusing large language models (LLMs) to elicit reasoning-relevant content prior\nto retrieval. However, the widespread use of large-scale language models like\nGPT-4 or LLaMA3-70B remains impractical due to their high inference cost and\nlimited deployability in real-world systems. In this work, we introduce\nTongSearch QR (Previously Known as \"TongSearch Reasoner\"), a family of\nsmall-scale language models for query reasoning and rewriting in\nreasoning-intensive retrieval. With a novel semi-rule-based reward function, we\nemploy reinforcement learning approaches enabling smaller language models, e,g,\nQwen2.5-7B-Instruct and Qwen2.5-1.5B-Instruct, to achieve query reasoning\nperformance rivaling large-scale language models without their prohibitive\ninference costs. Experiment results on BRIGHT benchmark show that with BM25 as\nretrievers, both TongSearch QR-7B and TongSearch QR-1.5B models significantly\noutperform existing baselines, including prompt-based query reasoners and some\nlatest dense retrievers trained for reasoning-intensive retrieval tasks,\noffering superior adaptability for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional information retrieval (IR) methods excel at textual and semantic\nmatching but struggle in reasoning-intensive retrieval tasks that require\nmulti-hop inference or complex semantic understanding between queries and\ndocuments. One promising solution is to explicitly rewrite or augment queries\nusing large language models (LLMs) to elicit reasoning-relevant content prior\nto retrieval. However, the widespread use of large-scale language models like\nGPT-4 or LLaMA3-70B remains impractical due to their high inference cost and\nlimited deployability in real-world systems. In this work, we introduce\nTongSearch QR (Previously Known as \"TongSearch Reasoner\"), a family of\nsmall-scale language models for query reasoning and rewriting in\nreasoning-intensive retrieval. With a novel semi-rule-based reward function, we\nemploy reinforcement learning approaches enabling smaller language models, e,g,\nQwen2.5-7B-Instruct and Qwen2.5-1.5B-Instruct, to achieve query reasoning\nperformance rivaling large-scale language models without their prohibitive\ninference costs. Experiment results on BRIGHT benchmark show that with BM25 as\nretrievers, both TongSearch QR-7B and TongSearch QR-1.5B models significantly\noutperform existing baselines, including prompt-based query reasoners and some\nlatest dense retrievers trained for reasoning-intensive retrieval tasks,\noffering superior adaptability for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Xubo Qin"
                    },
                    {
                        "name": "Jun Bai"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Zilong Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zilong Zheng"
                },
                "author": "Zilong Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11602v1",
                "updated": "2025-06-13T09:17:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    17,
                    8,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T09:17:08Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    17,
                    8,
                    4,
                    164,
                    0
                ],
                "title": "Are LLMs Good Text Diacritizers? An Arabic and Yorùbá Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Good Text Diacritizers? An Arabic and Yorùbá Case Study"
                },
                "summary": "We investigate the effectiveness of large language models (LLMs) for text\ndiacritization in two typologically distinct languages: Arabic and Yoruba. To\nenable a rigorous evaluation, we introduce a novel multilingual dataset\nMultiDiac, with diverse samples that capture a range of diacritic ambiguities.\nWe evaluate 14 LLMs varying in size, accessibility, and language coverage, and\nbenchmark them against 6 specialized diacritization models. Additionally, we\nfine-tune four small open-source models using LoRA for Yoruba. Our results show\nthat many off-the-shelf LLMs outperform specialized diacritization models for\nboth Arabic and Yoruba, but smaller models suffer from hallucinations.\nFine-tuning on a small dataset can help improve diacritization performance and\nreduce hallucination rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the effectiveness of large language models (LLMs) for text\ndiacritization in two typologically distinct languages: Arabic and Yoruba. To\nenable a rigorous evaluation, we introduce a novel multilingual dataset\nMultiDiac, with diverse samples that capture a range of diacritic ambiguities.\nWe evaluate 14 LLMs varying in size, accessibility, and language coverage, and\nbenchmark them against 6 specialized diacritization models. Additionally, we\nfine-tune four small open-source models using LoRA for Yoruba. Our results show\nthat many off-the-shelf LLMs outperform specialized diacritization models for\nboth Arabic and Yoruba, but smaller models suffer from hallucinations.\nFine-tuning on a small dataset can help improve diacritization performance and\nreduce hallucination rates."
                },
                "authors": [
                    {
                        "name": "Hawau Olamide Toyin"
                    },
                    {
                        "name": "Samar M. Magdy"
                    },
                    {
                        "name": "Hanan Aldarmaki"
                    }
                ],
                "author_detail": {
                    "name": "Hanan Aldarmaki"
                },
                "author": "Hanan Aldarmaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11521v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11521v3",
                "updated": "2025-06-13T09:11:46Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    11,
                    46,
                    4,
                    164,
                    0
                ],
                "published": "2024-11-18T12:31:22Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    12,
                    31,
                    22,
                    0,
                    323,
                    0
                ],
                "title": "Preempting Text Sanitization Utility in Resource-Constrained\n  Privacy-Preserving LLM Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preempting Text Sanitization Utility in Resource-Constrained\n  Privacy-Preserving LLM Interactions"
                },
                "summary": "Interactions with online Large Language Models raise privacy issues where\nproviders can gather sensitive information about users and their companies from\nthe prompts. While textual prompts can be sanitized using Differential Privacy,\nwe show that it is difficult to anticipate the performance of an LLM on such\nsanitized prompt. Poor performance has clear monetary consequences for LLM\nservices charging on a pay-per-use model as well as great amount of computing\nresources wasted. To this end, we propose a middleware architecture leveraging\na Small Language Model to predict the utility of a given sanitized prompt\nbefore it is sent to the LLM. We experimented on a summarization task and a\ntranslation task to show that our architecture helps prevent such resource\nwaste for up to 20% of the prompts. During our study, we also reproduced\nexperiments from one of the most cited paper on text sanitization using DP and\nshow that a potential performance-driven implementation choice dramatically\nchanges the output while not being explicitly acknowledged in the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactions with online Large Language Models raise privacy issues where\nproviders can gather sensitive information about users and their companies from\nthe prompts. While textual prompts can be sanitized using Differential Privacy,\nwe show that it is difficult to anticipate the performance of an LLM on such\nsanitized prompt. Poor performance has clear monetary consequences for LLM\nservices charging on a pay-per-use model as well as great amount of computing\nresources wasted. To this end, we propose a middleware architecture leveraging\na Small Language Model to predict the utility of a given sanitized prompt\nbefore it is sent to the LLM. We experimented on a summarization task and a\ntranslation task to show that our architecture helps prevent such resource\nwaste for up to 20% of the prompts. During our study, we also reproduced\nexperiments from one of the most cited paper on text sanitization using DP and\nshow that a potential performance-driven implementation choice dramatically\nchanges the output while not being explicitly acknowledged in the paper."
                },
                "authors": [
                    {
                        "name": "Robin Carpentier"
                    },
                    {
                        "name": "Benjamin Zi Hao Zhao"
                    },
                    {
                        "name": "Hassan Jameel Asghar"
                    },
                    {
                        "name": "Dali Kaafar"
                    }
                ],
                "author_detail": {
                    "name": "Dali Kaafar"
                },
                "author": "Dali Kaafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11521v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11521v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11600v1",
                "updated": "2025-06-13T09:09:08Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    9,
                    8,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T09:09:08Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    9,
                    8,
                    4,
                    164,
                    0
                ],
                "title": "GraphRAG-Causal: A novel graph-augmented framework for causal reasoning\n  and annotation in news",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphRAG-Causal: A novel graph-augmented framework for causal reasoning\n  and annotation in news"
                },
                "summary": "GraphRAG-Causal introduces an innovative framework that combines graph-based\nretrieval with large language models to enhance causal reasoning in news\nanalysis. Traditional NLP approaches often struggle with identifying complex,\nimplicit causal links, especially in low-data scenarios. Our approach addresses\nthese challenges by transforming annotated news headlines into structured\ncausal knowledge graphs. It then employs a hybrid retrieval system that merges\nsemantic embeddings with graph-based structural cues leveraging Neo4j to\naccurately match and retrieve relevant events. The framework is built on a\nthree-stage pipeline: First, during Data Preparation, news sentences are\nmeticulously annotated and converted into causal graphs capturing cause,\neffect, and trigger relationships. Next, the Graph Retrieval stage stores these\ngraphs along with their embeddings in a Neo4j database and utilizes hybrid\nCypher queries to efficiently identify events that share both semantic and\nstructural similarities with a given query. Finally, the LLM Inference stage\nutilizes these retrieved causal graphs in a few-shot learning setup with\nXML-based prompting, enabling robust classification and tagging of causal\nrelationships. Experimental evaluations demonstrate that GraphRAG-Causal\nachieves an impressive F1-score of 82.1% on causal classification using just 20\nfew-shot examples. This approach significantly boosts accuracy and consistency,\nmaking it highly suitable for real-time applications in news reliability\nassessment, misinformation detection, and policy analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphRAG-Causal introduces an innovative framework that combines graph-based\nretrieval with large language models to enhance causal reasoning in news\nanalysis. Traditional NLP approaches often struggle with identifying complex,\nimplicit causal links, especially in low-data scenarios. Our approach addresses\nthese challenges by transforming annotated news headlines into structured\ncausal knowledge graphs. It then employs a hybrid retrieval system that merges\nsemantic embeddings with graph-based structural cues leveraging Neo4j to\naccurately match and retrieve relevant events. The framework is built on a\nthree-stage pipeline: First, during Data Preparation, news sentences are\nmeticulously annotated and converted into causal graphs capturing cause,\neffect, and trigger relationships. Next, the Graph Retrieval stage stores these\ngraphs along with their embeddings in a Neo4j database and utilizes hybrid\nCypher queries to efficiently identify events that share both semantic and\nstructural similarities with a given query. Finally, the LLM Inference stage\nutilizes these retrieved causal graphs in a few-shot learning setup with\nXML-based prompting, enabling robust classification and tagging of causal\nrelationships. Experimental evaluations demonstrate that GraphRAG-Causal\nachieves an impressive F1-score of 82.1% on causal classification using just 20\nfew-shot examples. This approach significantly boosts accuracy and consistency,\nmaking it highly suitable for real-time applications in news reliability\nassessment, misinformation detection, and policy analysis."
                },
                "authors": [
                    {
                        "name": "Abdul Haque"
                    },
                    {
                        "name": "Umm e Hani"
                    },
                    {
                        "name": "Ahmad Din"
                    },
                    {
                        "name": "Muhammad Babar"
                    },
                    {
                        "name": "Ali Abbas"
                    },
                    {
                        "name": "Insaf Ullah"
                    }
                ],
                "author_detail": {
                    "name": "Insaf Ullah"
                },
                "author": "Insaf Ullah",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10805v2",
                "updated": "2025-06-13T09:04:48Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    9,
                    4,
                    48,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-12T15:20:33Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    15,
                    20,
                    33,
                    3,
                    163,
                    0
                ],
                "title": "Detecting High-Stakes Interactions with Activation Probes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting High-Stakes Interactions with Activation Probes"
                },
                "summary": "Monitoring is an important aspect of safely deploying Large Language Models\n(LLMs). This paper examines activation probes for detecting \"high-stakes\"\ninteractions -- where the text indicates that the interaction might lead to\nsignificant harm -- as a critical, yet underexplored, target for such\nmonitoring. We evaluate several probe architectures trained on synthetic data,\nand find them to exhibit robust generalization to diverse, out-of-distribution,\nreal-world data. Probes' performance is comparable to that of prompted or\nfinetuned medium-sized LLM monitors, while offering computational savings of\nsix orders-of-magnitude. Our experiments also highlight the potential of\nbuilding resource-aware hierarchical monitoring systems, where probes serve as\nan efficient initial filter and flag cases for more expensive downstream\nanalysis. We release our novel synthetic dataset and codebase to encourage\nfurther study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring is an important aspect of safely deploying Large Language Models\n(LLMs). This paper examines activation probes for detecting \"high-stakes\"\ninteractions -- where the text indicates that the interaction might lead to\nsignificant harm -- as a critical, yet underexplored, target for such\nmonitoring. We evaluate several probe architectures trained on synthetic data,\nand find them to exhibit robust generalization to diverse, out-of-distribution,\nreal-world data. Probes' performance is comparable to that of prompted or\nfinetuned medium-sized LLM monitors, while offering computational savings of\nsix orders-of-magnitude. Our experiments also highlight the potential of\nbuilding resource-aware hierarchical monitoring systems, where probes serve as\nan efficient initial filter and flag cases for more expensive downstream\nanalysis. We release our novel synthetic dataset and codebase to encourage\nfurther study."
                },
                "authors": [
                    {
                        "name": "Alex McKenzie"
                    },
                    {
                        "name": "Urja Pawar"
                    },
                    {
                        "name": "Phil Blandfort"
                    },
                    {
                        "name": "William Bankes"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    },
                    {
                        "name": "Dmitrii Krasheninnikov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitrii Krasheninnikov"
                },
                "author": "Dmitrii Krasheninnikov",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14218v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14218v3",
                "updated": "2025-06-13T08:57:36Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    57,
                    36,
                    4,
                    164,
                    0
                ],
                "published": "2025-04-19T07:53:37Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    7,
                    53,
                    37,
                    5,
                    109,
                    0
                ],
                "title": "Understanding the Repeat Curse in Large Language Models from a Feature\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Repeat Curse in Large Language Models from a Feature\n  Perspective"
                },
                "summary": "Large language models (LLMs) have made remarkable progress in various\ndomains, yet they often suffer from repetitive text generation, a phenomenon we\nrefer to as the \"Repeat Curse\". While previous studies have proposed decoding\nstrategies to mitigate repetition, the underlying mechanism behind this issue\nremains insufficiently explored. In this work, we investigate the root causes\nof repetition in LLMs through the lens of mechanistic interpretability.\nInspired by recent advances in Sparse Autoencoders (SAEs), which enable\nmonosemantic feature extraction, we propose a novel approach, \"Duplicatus\nCharm\", to induce and analyze the Repeat Curse. Our method systematically\nidentifies \"Repetition Features\" -the key model activations responsible for\ngenerating repetitive outputs. First, we locate the layers most involved in\nrepetition through logit analysis. Next, we extract and stimulate relevant\nfeatures using SAE-based activation manipulation. To validate our approach, we\nconstruct a repetition dataset covering token and paragraph level repetitions\nand introduce an evaluation pipeline to quantify the influence of identified\nrepetition features. Furthermore, by deactivating these features, we have\neffectively mitigated the Repeat Curse. The source code of our work is publicly\navailable at: https://github.com/kaustpradalab/repeat-curse-llm",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have made remarkable progress in various\ndomains, yet they often suffer from repetitive text generation, a phenomenon we\nrefer to as the \"Repeat Curse\". While previous studies have proposed decoding\nstrategies to mitigate repetition, the underlying mechanism behind this issue\nremains insufficiently explored. In this work, we investigate the root causes\nof repetition in LLMs through the lens of mechanistic interpretability.\nInspired by recent advances in Sparse Autoencoders (SAEs), which enable\nmonosemantic feature extraction, we propose a novel approach, \"Duplicatus\nCharm\", to induce and analyze the Repeat Curse. Our method systematically\nidentifies \"Repetition Features\" -the key model activations responsible for\ngenerating repetitive outputs. First, we locate the layers most involved in\nrepetition through logit analysis. Next, we extract and stimulate relevant\nfeatures using SAE-based activation manipulation. To validate our approach, we\nconstruct a repetition dataset covering token and paragraph level repetitions\nand introduce an evaluation pipeline to quantify the influence of identified\nrepetition features. Furthermore, by deactivating these features, we have\neffectively mitigated the Repeat Curse. The source code of our work is publicly\navailable at: https://github.com/kaustpradalab/repeat-curse-llm"
                },
                "authors": [
                    {
                        "name": "Junchi Yao"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Jianhua Xu"
                    },
                    {
                        "name": "Lijie Hu"
                    },
                    {
                        "name": "Mengdi Li"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "arxiv_comment": "Accepted by ACL 2025, Findings, Long Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14218v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14218v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11586v1",
                "updated": "2025-06-13T08:49:39Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    49,
                    39,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T08:49:39Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    49,
                    39,
                    4,
                    164,
                    0
                ],
                "title": "SecONNds: Secure Outsourced Neural Network Inference on ImageNet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecONNds: Secure Outsourced Neural Network Inference on ImageNet"
                },
                "summary": "The widespread adoption of outsourced neural network inference presents\nsignificant privacy challenges, as sensitive user data is processed on\nuntrusted remote servers. Secure inference offers a privacy-preserving\nsolution, but existing frameworks suffer from high computational overhead and\ncommunication costs, rendering them impractical for real-world deployment. We\nintroduce SecONNds, a non-intrusive secure inference framework optimized for\nlarge ImageNet-scale Convolutional Neural Networks. SecONNds integrates a novel\nfully Boolean Goldreich-Micali-Wigderson (GMW) protocol for secure comparison\n-- addressing Yao's millionaires' problem -- using preprocessed Beaver's bit\ntriples generated from Silent Random Oblivious Transfer. Our novel protocol\nachieves an online speedup of 17$\\times$ in nonlinear operations compared to\nstate-of-the-art solutions while reducing communication overhead. To further\nenhance performance, SecONNds employs Number Theoretic Transform (NTT)\npreprocessing and leverages GPU acceleration for homomorphic encryption\noperations, resulting in speedups of 1.6$\\times$ on CPU and 2.2$\\times$ on GPU\nfor linear operations. We also present SecONNds-P, a bit-exact variant that\nensures verifiable full-precision results in secure computation, matching the\nresults of plaintext computations. Evaluated on a 37-bit quantized SqueezeNet\nmodel, SecONNds achieves an end-to-end inference time of 2.8 s on GPU and 3.6 s\non CPU, with a total communication of just 420 MiB. SecONNds' efficiency and\nreduced computational load make it well-suited for deploying privacy-sensitive\napplications in resource-constrained environments. SecONNds is open source and\ncan be accessed from: https://github.com/shashankballa/SecONNds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of outsourced neural network inference presents\nsignificant privacy challenges, as sensitive user data is processed on\nuntrusted remote servers. Secure inference offers a privacy-preserving\nsolution, but existing frameworks suffer from high computational overhead and\ncommunication costs, rendering them impractical for real-world deployment. We\nintroduce SecONNds, a non-intrusive secure inference framework optimized for\nlarge ImageNet-scale Convolutional Neural Networks. SecONNds integrates a novel\nfully Boolean Goldreich-Micali-Wigderson (GMW) protocol for secure comparison\n-- addressing Yao's millionaires' problem -- using preprocessed Beaver's bit\ntriples generated from Silent Random Oblivious Transfer. Our novel protocol\nachieves an online speedup of 17$\\times$ in nonlinear operations compared to\nstate-of-the-art solutions while reducing communication overhead. To further\nenhance performance, SecONNds employs Number Theoretic Transform (NTT)\npreprocessing and leverages GPU acceleration for homomorphic encryption\noperations, resulting in speedups of 1.6$\\times$ on CPU and 2.2$\\times$ on GPU\nfor linear operations. We also present SecONNds-P, a bit-exact variant that\nensures verifiable full-precision results in secure computation, matching the\nresults of plaintext computations. Evaluated on a 37-bit quantized SqueezeNet\nmodel, SecONNds achieves an end-to-end inference time of 2.8 s on GPU and 3.6 s\non CPU, with a total communication of just 420 MiB. SecONNds' efficiency and\nreduced computational load make it well-suited for deploying privacy-sensitive\napplications in resource-constrained environments. SecONNds is open source and\ncan be accessed from: https://github.com/shashankballa/SecONNds."
                },
                "authors": [
                    {
                        "name": "Shashank Balla"
                    }
                ],
                "author_detail": {
                    "name": "Shashank Balla"
                },
                "author": "Shashank Balla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11578v1",
                "updated": "2025-06-13T08:35:50Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    35,
                    50,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T08:35:50Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    35,
                    50,
                    4,
                    164,
                    0
                ],
                "title": "Collaborative LLM Inference via Planning for Efficient Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative LLM Inference via Planning for Efficient Reasoning"
                },
                "summary": "Large language models (LLMs) excel at complex reasoning tasks, but those with\nstrong capabilities (e.g., whose numbers of parameters are larger than 100B)\nare often accessible only through paid APIs, making them too costly for\napplications of frequent use. In contrast, smaller open-sourced LLMs (e.g.,\nwhose numbers of parameters are less than 3B) are freely available and easy to\ndeploy locally (e.g., under a single GPU having 8G VRAM), but lack suff icient\nreasoning ability. This trade-off raises a natural question: can small (free)\nand large (costly) models collaborate at test time to combine their strengths?\nWe propose a test-time collaboration framework in which a planner model first\ngenerates a plan, defined as a distilled and high-level abstraction of the\nproblem.\n  This plan serves as a lightweight intermediate that guides a reasoner model,\nwhich generates a complete solution. Small and large models take turns acting\nas planner and reasoner, exchanging plans in a multi-round cascade to\ncollaboratively solve complex tasks. Our method achieves accuracy comparable to\nstrong proprietary models alone, while significantly reducing reliance on paid\ninference. These results highlight planning as an effective prior for\norchestrating cost-aware, cross-model inference under real-world deployment\nconstraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at complex reasoning tasks, but those with\nstrong capabilities (e.g., whose numbers of parameters are larger than 100B)\nare often accessible only through paid APIs, making them too costly for\napplications of frequent use. In contrast, smaller open-sourced LLMs (e.g.,\nwhose numbers of parameters are less than 3B) are freely available and easy to\ndeploy locally (e.g., under a single GPU having 8G VRAM), but lack suff icient\nreasoning ability. This trade-off raises a natural question: can small (free)\nand large (costly) models collaborate at test time to combine their strengths?\nWe propose a test-time collaboration framework in which a planner model first\ngenerates a plan, defined as a distilled and high-level abstraction of the\nproblem.\n  This plan serves as a lightweight intermediate that guides a reasoner model,\nwhich generates a complete solution. Small and large models take turns acting\nas planner and reasoner, exchanging plans in a multi-round cascade to\ncollaboratively solve complex tasks. Our method achieves accuracy comparable to\nstrong proprietary models alone, while significantly reducing reliance on paid\ninference. These results highlight planning as an effective prior for\norchestrating cost-aware, cross-model inference under real-world deployment\nconstraints."
                },
                "authors": [
                    {
                        "name": "Byeongchan Lee"
                    },
                    {
                        "name": "Jonghoon Lee"
                    },
                    {
                        "name": "Dongyoung Kim"
                    },
                    {
                        "name": "Jaehyung Kim"
                    },
                    {
                        "name": "Jinwoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Jinwoo Shin"
                },
                "author": "Jinwoo Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04065v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04065v4",
                "updated": "2025-06-13T08:32:26Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    32,
                    26,
                    4,
                    164,
                    0
                ],
                "published": "2024-05-07T07:14:38Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    7,
                    14,
                    38,
                    1,
                    128,
                    0
                ],
                "title": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashBack:Efficient Retrieval-Augmented Language Modeling for Long\n  Context Inference"
                },
                "summary": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Language Modeling (RALM) by integrating large language\nmodels (LLM) with relevant documents from an external corpus is a proven method\nfor enabling the LLM to generate information beyond the scope of its\npre-training corpus. Previous work utilizing retrieved content by simply\nprepending it to the input poses a high runtime issue, which degrades the\ninference efficiency of the LLMs because they fail to use the Key-Value (KV)\ncache efficiently. In this paper, we propose FlashBack, a modular RALM designed\nto improve the inference efficiency of RALM with appending context pattern\nwhile maintaining decent performance after fine-tuning by Low-Rank Adaption.\nFlashBack appends retrieved documents at the end of the context for efficiently\nutilizing the KV cache instead of prepending them. And we introduce Marking\nToken as two special prompt tokens for marking the boundary of the appending\ncontext during fine-tuning. Our experiments on testing generation quality show\nthat FlashBack can remain decent generation quality in perplexity. And the\ninference speed of FlashBack is up to $4\\times$ faster than the prepending\ncounterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing\nunnecessary re-computation, it demonstrates an advancement by achieving\nsignificantly faster inference speed, and this heightened efficiency will\nsubstantially reduce inferential cost."
                },
                "authors": [
                    {
                        "name": "Runheng Liu"
                    },
                    {
                        "name": "Xingchen Xiao"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Zewen Chi"
                    },
                    {
                        "name": "Zhijing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Wu"
                },
                "author": "Zhijing Wu",
                "arxiv_comment": "ACL 2025 Findings, 14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04065v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04065v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00812v2",
                "updated": "2025-06-13T08:31:34Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    31,
                    34,
                    4,
                    164,
                    0
                ],
                "published": "2025-04-01T14:03:46Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    3,
                    46,
                    1,
                    91,
                    0
                ],
                "title": "Scaling Prompt Instructed Zero Shot Composed Image Retrieval with\n  Image-Only Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Prompt Instructed Zero Shot Composed Image Retrieval with\n  Image-Only Data"
                },
                "summary": "Composed Image Retrieval (CIR) is the task of retrieving images matching a\nreference image augmented with a text, where the text describes changes to the\nreference image in natural language. Traditionally, models designed for CIR\nhave relied on triplet data containing a reference image, reformulation text,\nand a target image. However, curating such triplet data often necessitates\nhuman intervention, leading to prohibitive costs. This challenge has hindered\nthe scalability of CIR model training even with the availability of abundant\nunlabeled data. With the recent advances in foundational models, we advocate a\nshift in the CIR training paradigm where human annotations can be efficiently\nreplaced by large language models (LLMs). Specifically, we demonstrate the\ncapability of large captioning and language models in efficiently generating\ndata for CIR only relying on unannotated image collections. Additionally, we\nintroduce an embedding reformulation architecture that effectively combines\nimage and text modalities. Our model, named InstructCIR, outperforms\nstate-of-the-art methods in zero-shot composed image retrieval on CIRR and\nFashionIQ datasets. Furthermore, we demonstrate that by increasing the amount\nof generated data, our zero-shot model gets closer to the performance of\nsupervised baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composed Image Retrieval (CIR) is the task of retrieving images matching a\nreference image augmented with a text, where the text describes changes to the\nreference image in natural language. Traditionally, models designed for CIR\nhave relied on triplet data containing a reference image, reformulation text,\nand a target image. However, curating such triplet data often necessitates\nhuman intervention, leading to prohibitive costs. This challenge has hindered\nthe scalability of CIR model training even with the availability of abundant\nunlabeled data. With the recent advances in foundational models, we advocate a\nshift in the CIR training paradigm where human annotations can be efficiently\nreplaced by large language models (LLMs). Specifically, we demonstrate the\ncapability of large captioning and language models in efficiently generating\ndata for CIR only relying on unannotated image collections. Additionally, we\nintroduce an embedding reformulation architecture that effectively combines\nimage and text modalities. Our model, named InstructCIR, outperforms\nstate-of-the-art methods in zero-shot composed image retrieval on CIRR and\nFashionIQ datasets. Furthermore, we demonstrate that by increasing the amount\nof generated data, our zero-shot model gets closer to the performance of\nsupervised baselines."
                },
                "authors": [
                    {
                        "name": "Yiqun Duan"
                    },
                    {
                        "name": "Sameera Ramasinghe"
                    },
                    {
                        "name": "Stephen Gould"
                    },
                    {
                        "name": "Ajanthan Thalaiyasingam"
                    }
                ],
                "author_detail": {
                    "name": "Ajanthan Thalaiyasingam"
                },
                "author": "Ajanthan Thalaiyasingam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09565v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09565v2",
                "updated": "2025-06-13T08:30:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    30,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-11T09:56:39Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    56,
                    39,
                    2,
                    162,
                    0
                ],
                "title": "SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware\n  Gaussian Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware\n  Gaussian Fields"
                },
                "summary": "Holistic 3D scene understanding, which jointly models geometry, appearance,\nand semantics, is crucial for applications like augmented reality and robotic\ninteraction. Existing feed-forward 3D scene understanding methods (e.g., LSM)\nare limited to extracting language-based semantics from scenes, failing to\nachieve holistic scene comprehension. Additionally, they suffer from\nlow-quality geometry reconstruction and noisy artifacts. In contrast, per-scene\noptimization methods rely on dense input views, which reduces practicality and\nincreases complexity during deployment. In this paper, we propose\nSemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which\nunifies 3D Gaussians with latent semantic attributes for joint\ngeometry-appearance-semantics modeling. To predict the semantic anisotropic\nGaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a\ncost volume representation that stores cross-view feature similarities,\nenhancing coherent and accurate scene comprehension. Leveraging a two-stage\ndistillation framework, SemanticSplat reconstructs a holistic multi-modal\nsemantic feature field from sparse-view images. Experiments demonstrate the\neffectiveness of our method for 3D scene understanding tasks like promptable\nand open-vocabulary segmentation. Video results are available at\nhttps://semanticsplat.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holistic 3D scene understanding, which jointly models geometry, appearance,\nand semantics, is crucial for applications like augmented reality and robotic\ninteraction. Existing feed-forward 3D scene understanding methods (e.g., LSM)\nare limited to extracting language-based semantics from scenes, failing to\nachieve holistic scene comprehension. Additionally, they suffer from\nlow-quality geometry reconstruction and noisy artifacts. In contrast, per-scene\noptimization methods rely on dense input views, which reduces practicality and\nincreases complexity during deployment. In this paper, we propose\nSemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which\nunifies 3D Gaussians with latent semantic attributes for joint\ngeometry-appearance-semantics modeling. To predict the semantic anisotropic\nGaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a\ncost volume representation that stores cross-view feature similarities,\nenhancing coherent and accurate scene comprehension. Leveraging a two-stage\ndistillation framework, SemanticSplat reconstructs a holistic multi-modal\nsemantic feature field from sparse-view images. Experiments demonstrate the\neffectiveness of our method for 3D scene understanding tasks like promptable\nand open-vocabulary segmentation. Video results are available at\nhttps://semanticsplat.github.io."
                },
                "authors": [
                    {
                        "name": "Qijing Li"
                    },
                    {
                        "name": "Jingxiang Sun"
                    },
                    {
                        "name": "Liang An"
                    },
                    {
                        "name": "Zhaoqi Su"
                    },
                    {
                        "name": "Hongwen Zhang"
                    },
                    {
                        "name": "Yebin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yebin Liu"
                },
                "author": "Yebin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09565v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09565v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02455v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02455v2",
                "updated": "2025-06-13T08:22:40Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    22,
                    40,
                    4,
                    164,
                    0
                ],
                "published": "2025-04-03T10:20:16Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    20,
                    16,
                    3,
                    93,
                    0
                ],
                "title": "QPanda3: A High-Performance Software-Hardware Collaborative Framework\n  for Large-Scale Quantum-Classical Computing Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPanda3: A High-Performance Software-Hardware Collaborative Framework\n  for Large-Scale Quantum-Classical Computing Integration"
                },
                "summary": "In emerging quantum-classical integration applications, the classical time\ncost-especially from compilation and protocol-level communication often exceeds\nthe execution time of quantum circuits themselves, posing a severe bottleneck\nto practical deployment. To overcome these limitations, QPanda3 has been\nextensively optimized as a high-performance quantum programming framework\ntailored for the demands of the NISQ era and quantum-classical hybrid\nworkflows. It features optimized circuit compilation, a custom binary\ninstruction stream (OriginBIS), and hardware-aware execution strategies to\nsignificantly reduce latency and communication overhead. OriginBIS achieves up\nto 86.9$\\times$ faster encoding and 35.6$\\times$ faster decoding than OpenQASM\n2.0, addressing critical bottlenecks in hybrid quantum systems. Benchmarks show\n10.7$\\times$ compilation speedup and up to 597$\\times$ acceleration in\ncompiling large-scale circuits (e.g., a 118-qubit W-state) compared to Qiskit.\nn high-performance simulation, QPanda3 excels in variational quantum\nalgorithms, achieving up to 26$\\times$ faster gradient computation than Qiskit,\nwith minimal time-complexity growth across circuit depths. These capabilities\nmake QPanda3 well-suited for scalable quantum algorithm development in finance,\nmaterials science, and combinatorial optimization, while supporting industrial\ndeployment and cloud-based execution in quantum-classical hybrid computing\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In emerging quantum-classical integration applications, the classical time\ncost-especially from compilation and protocol-level communication often exceeds\nthe execution time of quantum circuits themselves, posing a severe bottleneck\nto practical deployment. To overcome these limitations, QPanda3 has been\nextensively optimized as a high-performance quantum programming framework\ntailored for the demands of the NISQ era and quantum-classical hybrid\nworkflows. It features optimized circuit compilation, a custom binary\ninstruction stream (OriginBIS), and hardware-aware execution strategies to\nsignificantly reduce latency and communication overhead. OriginBIS achieves up\nto 86.9$\\times$ faster encoding and 35.6$\\times$ faster decoding than OpenQASM\n2.0, addressing critical bottlenecks in hybrid quantum systems. Benchmarks show\n10.7$\\times$ compilation speedup and up to 597$\\times$ acceleration in\ncompiling large-scale circuits (e.g., a 118-qubit W-state) compared to Qiskit.\nn high-performance simulation, QPanda3 excels in variational quantum\nalgorithms, achieving up to 26$\\times$ faster gradient computation than Qiskit,\nwith minimal time-complexity growth across circuit depths. These capabilities\nmake QPanda3 well-suited for scalable quantum algorithm development in finance,\nmaterials science, and combinatorial optimization, while supporting industrial\ndeployment and cloud-based execution in quantum-classical hybrid computing\nscenarios."
                },
                "authors": [
                    {
                        "name": "Tianrui Zou"
                    },
                    {
                        "name": "Yuan Fang"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Menghan Dou"
                    },
                    {
                        "name": "Jun Fu"
                    },
                    {
                        "name": "ZiQiang Zhao"
                    },
                    {
                        "name": "ShuBin Zhao"
                    },
                    {
                        "name": "Lei Yu"
                    },
                    {
                        "name": "Dongyi Zhao"
                    },
                    {
                        "name": "Zhaoyun Chen"
                    },
                    {
                        "name": "Guoping Guo"
                    }
                ],
                "author_detail": {
                    "name": "Guoping Guo"
                },
                "author": "Guoping Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02455v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02455v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00744v2",
                "updated": "2025-06-13T08:18:12Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    18,
                    12,
                    4,
                    164,
                    0
                ],
                "published": "2025-02-02T10:32:55Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    10,
                    32,
                    55,
                    6,
                    33,
                    0
                ],
                "title": "CoNNect: Connectivity-Based Regularization for Structural Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoNNect: Connectivity-Based Regularization for Structural Pruning"
                },
                "summary": "Pruning encompasses a range of techniques aimed at increasing the sparsity of\nneural networks (NNs). These techniques can generally be framed as minimizing a\nloss function subject to an $L_0$ norm constraint. This paper introduces\nCoNNect, a novel differentiable regularizer for sparse NN training that ensures\nconnectivity between input and output layers. We prove that CoNNect\napproximates $L_0$ regularization, guaranteeing maximally connected network\nstructures while avoiding issues like layer collapse. Moreover, CoNNect is\neasily integrated with established structural pruning strategies. Numerical\nexperiments demonstrate that CoNNect can improve classical pruning strategies\nand enhance state-of-the-art one-shot pruners, such as DepGraph and LLM-pruner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning encompasses a range of techniques aimed at increasing the sparsity of\nneural networks (NNs). These techniques can generally be framed as minimizing a\nloss function subject to an $L_0$ norm constraint. This paper introduces\nCoNNect, a novel differentiable regularizer for sparse NN training that ensures\nconnectivity between input and output layers. We prove that CoNNect\napproximates $L_0$ regularization, guaranteeing maximally connected network\nstructures while avoiding issues like layer collapse. Moreover, CoNNect is\neasily integrated with established structural pruning strategies. Numerical\nexperiments demonstrate that CoNNect can improve classical pruning strategies\nand enhance state-of-the-art one-shot pruners, such as DepGraph and LLM-pruner."
                },
                "authors": [
                    {
                        "name": "Christian Franssen"
                    },
                    {
                        "name": "Jinyang Jiang"
                    },
                    {
                        "name": "Yijie Peng"
                    },
                    {
                        "name": "Bernd Heidergott"
                    }
                ],
                "author_detail": {
                    "name": "Bernd Heidergott"
                },
                "author": "Bernd Heidergott",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11561v1",
                "updated": "2025-06-13T08:15:45Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    15,
                    45,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T08:15:45Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    15,
                    45,
                    4,
                    164,
                    0
                ],
                "title": "Identifying Helpful Context for LLM-based Vulnerability Repair: A\n  Preliminary Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Helpful Context for LLM-based Vulnerability Repair: A\n  Preliminary Study"
                },
                "summary": "Recent advancements in large language models (LLMs) have shown promise for\nautomated vulnerability detection and repair in software systems. This paper\ninvestigates the performance of GPT-4o in repairing Java vulnerabilities from a\nwidely used dataset (Vul4J), exploring how different contextual information\naffects automated vulnerability repair (AVR) capabilities. We compare the\nlatest GPT-4o's performance against previous results with GPT-4 using identical\nprompts. We evaluated nine additional prompts crafted by us that contain\nvarious contextual information such as CWE or CVE information, and manually\nextracted code contexts. Each prompt was executed three times on 42\nvulnerabilities, and the resulting fix candidates were validated using Vul4J's\nautomated testing framework.\n  Our results show that GPT-4o performed 11.9\\% worse on average than GPT-4\nwith the same prompt, but was able to fix 10.5\\% more distinct vulnerabilities\nin the three runs together. CVE information significantly improved repair\nrates, while the length of the task description had minimal impact. Combining\nCVE guidance with manually extracted code context resulted in the best\nperformance. Using our \\textsc{Top}-3 prompts together, GPT-4o repaired 26\n(62\\%) vulnerabilities at least once, outperforming both the original baseline\n(40\\%) and its reproduction (45\\%), suggesting that ensemble prompt strategies\ncould improve vulnerability repair in zero-shot settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have shown promise for\nautomated vulnerability detection and repair in software systems. This paper\ninvestigates the performance of GPT-4o in repairing Java vulnerabilities from a\nwidely used dataset (Vul4J), exploring how different contextual information\naffects automated vulnerability repair (AVR) capabilities. We compare the\nlatest GPT-4o's performance against previous results with GPT-4 using identical\nprompts. We evaluated nine additional prompts crafted by us that contain\nvarious contextual information such as CWE or CVE information, and manually\nextracted code contexts. Each prompt was executed three times on 42\nvulnerabilities, and the resulting fix candidates were validated using Vul4J's\nautomated testing framework.\n  Our results show that GPT-4o performed 11.9\\% worse on average than GPT-4\nwith the same prompt, but was able to fix 10.5\\% more distinct vulnerabilities\nin the three runs together. CVE information significantly improved repair\nrates, while the length of the task description had minimal impact. Combining\nCVE guidance with manually extracted code context resulted in the best\nperformance. Using our \\textsc{Top}-3 prompts together, GPT-4o repaired 26\n(62\\%) vulnerabilities at least once, outperforming both the original baseline\n(40\\%) and its reproduction (45\\%), suggesting that ensemble prompt strategies\ncould improve vulnerability repair in zero-shot settings."
                },
                "authors": [
                    {
                        "name": "Gábor Antal"
                    },
                    {
                        "name": "Bence Bogenfürst"
                    },
                    {
                        "name": "Rudolf Ferenc"
                    },
                    {
                        "name": "Péter Hegedűs"
                    }
                ],
                "author_detail": {
                    "name": "Péter Hegedűs"
                },
                "author": "Péter Hegedűs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11558v1",
                "updated": "2025-06-13T08:13:05Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    13,
                    5,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T08:13:05Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    13,
                    5,
                    4,
                    164,
                    0
                ],
                "title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning\n  with Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning\n  with Video LLMs"
                },
                "summary": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with GPT-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with GPT-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling."
                },
                "authors": [
                    {
                        "name": "Bo-Cheng Chiu"
                    },
                    {
                        "name": "Jen-Jee Chen"
                    },
                    {
                        "name": "Yu-Chee Tseng"
                    },
                    {
                        "name": "Feng-Chi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Feng-Chi Chen"
                },
                "author": "Feng-Chi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11555v1",
                "updated": "2025-06-13T08:06:49Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    6,
                    49,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T08:06:49Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    6,
                    49,
                    4,
                    164,
                    0
                ],
                "title": "RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware\n  Reasoning"
                },
                "summary": "The integration of external knowledge through Retrieval-Augmented Generation\n(RAG) has become foundational in enhancing large language models (LLMs) for\nknowledge-intensive tasks. However, existing RAG paradigms often overlook the\ncognitive step of applying knowledge, leaving a gap between retrieved facts and\ntask-specific reasoning. In this work, we introduce RAG+, a principled and\nmodular extension that explicitly incorporates application-aware reasoning into\nthe RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and\naligned application examples, created either manually or automatically, and\nretrieves both jointly during inference. This design enables LLMs not only to\naccess relevant information but also to apply it within structured,\ngoal-oriented reasoning processes. Experiments across mathematical, legal, and\nmedical domains, conducted on multiple models, demonstrate that RAG+\nconsistently outperforms standard RAG variants, achieving average improvements\nof 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval\nwith actionable application, RAG+ advances a more cognitively grounded\nframework for knowledge integration, representing a step toward more\ninterpretable and capable LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of external knowledge through Retrieval-Augmented Generation\n(RAG) has become foundational in enhancing large language models (LLMs) for\nknowledge-intensive tasks. However, existing RAG paradigms often overlook the\ncognitive step of applying knowledge, leaving a gap between retrieved facts and\ntask-specific reasoning. In this work, we introduce RAG+, a principled and\nmodular extension that explicitly incorporates application-aware reasoning into\nthe RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and\naligned application examples, created either manually or automatically, and\nretrieves both jointly during inference. This design enables LLMs not only to\naccess relevant information but also to apply it within structured,\ngoal-oriented reasoning processes. Experiments across mathematical, legal, and\nmedical domains, conducted on multiple models, demonstrate that RAG+\nconsistently outperforms standard RAG variants, achieving average improvements\nof 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval\nwith actionable application, RAG+ advances a more cognitively grounded\nframework for knowledge integration, representing a step toward more\ninterpretable and capable LLMs."
                },
                "authors": [
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Shiwan Zhao"
                    },
                    {
                        "name": "Ming Fan"
                    },
                    {
                        "name": "Zhihu Wang"
                    },
                    {
                        "name": "Yubo Zhang"
                    },
                    {
                        "name": "Xicheng Zhang"
                    },
                    {
                        "name": "Zhengfan Wang"
                    },
                    {
                        "name": "Heyuan Huang"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14023v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14023v4",
                "updated": "2025-06-13T08:04:19Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    4,
                    19,
                    4,
                    164,
                    0
                ],
                "published": "2024-06-20T06:42:08Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    6,
                    42,
                    8,
                    3,
                    172,
                    0
                ],
                "title": "Evaluating Implicit Bias in Large Language Models by Attacking From a\n  Psychometric Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Implicit Bias in Large Language Models by Attacking From a\n  Psychometric Perspective"
                },
                "summary": "As large language models (LLMs) become an important way of information\naccess, there have been increasing concerns that LLMs may intensify the spread\nof unethical content, including implicit bias that hurts certain populations\nwithout explicit harmful words. In this paper, we conduct a rigorous evaluation\nof LLMs' implicit bias towards certain demographics by attacking them from a\npsychometric perspective to elicit agreements to biased viewpoints. Inspired by\npsychometric principles in cognitive and social psychology, we propose three\nattack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the\ncorresponding attack instructions, we built two benchmarks: (1) a bilingual\ndataset with biased statements covering four bias types (2.7K instances) for\nextensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning\nnine common bias types (12.7K instances) for comprehensive evaluation.\nExtensive evaluation of popular commercial and open-source LLMs shows that our\nmethods can elicit LLMs' inner bias more effectively than competitive\nbaselines. Our attack methodology and benchmarks offer an effective means of\nassessing the ethical risks of LLMs, driving progress toward greater\naccountability in their development. Our code, data, and benchmarks are\navailable at https://yuchenwen1.github.io/ImplicitBiasEvaluation/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become an important way of information\naccess, there have been increasing concerns that LLMs may intensify the spread\nof unethical content, including implicit bias that hurts certain populations\nwithout explicit harmful words. In this paper, we conduct a rigorous evaluation\nof LLMs' implicit bias towards certain demographics by attacking them from a\npsychometric perspective to elicit agreements to biased viewpoints. Inspired by\npsychometric principles in cognitive and social psychology, we propose three\nattack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the\ncorresponding attack instructions, we built two benchmarks: (1) a bilingual\ndataset with biased statements covering four bias types (2.7K instances) for\nextensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning\nnine common bias types (12.7K instances) for comprehensive evaluation.\nExtensive evaluation of popular commercial and open-source LLMs shows that our\nmethods can elicit LLMs' inner bias more effectively than competitive\nbaselines. Our attack methodology and benchmarks offer an effective means of\nassessing the ethical risks of LLMs, driving progress toward greater\naccountability in their development. Our code, data, and benchmarks are\navailable at https://yuchenwen1.github.io/ImplicitBiasEvaluation/."
                },
                "authors": [
                    {
                        "name": "Yuchen Wen"
                    },
                    {
                        "name": "Keping Bi"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "Accepted to ACL 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14023v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14023v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20445v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20445v3",
                "updated": "2025-06-13T08:02:21Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    2,
                    21,
                    4,
                    164,
                    0
                ],
                "published": "2024-10-27T13:51:09Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    13,
                    51,
                    9,
                    6,
                    301,
                    0
                ],
                "title": "TrajAgent: An LLM-based Agent Framework for Automated Trajectory\n  Modeling via Collaboration of Large and Small Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrajAgent: An LLM-based Agent Framework for Automated Trajectory\n  Modeling via Collaboration of Large and Small Models"
                },
                "summary": "Trajectory modeling, which includes research on trajectory data pattern\nmining and future prediction, has widespread applications in areas such as life\nservices, urban transportation, and public administration. Numerous methods\nhave been proposed to address specific problems within trajectory modeling.\nHowever, the heterogeneity of data and the diversity of trajectory tasks make\neffective and reliable trajectory modeling an important yet highly challenging\nendeavor, even for domain experts. In this paper, we propose\n\\textit{TrajAgent}, a agent framework powered by large language models (LLMs),\ndesigned to facilitate robust and efficient trajectory modeling through\nautomation modeling. This framework leverages and optimizes diverse specialized\nmodels to address various trajectory modeling tasks across different datasets\neffectively. In \\textit{TrajAgent}, we first develop \\textit{UniEnv}, an\nexecution environment with a unified data and model interface, to support the\nexecution and training of various models. Building on \\textit{UniEnv}, we\nintroduce an agentic workflow designed for automatic trajectory modeling across\nvarious trajectory tasks and data. Furthermore, we introduce collaborative\nlearning schema between LLM-based agents and small speciallized models, to\nenhance the performance of the whole framework effectively. Extensive\nexperiments on four tasks using four real-world datasets demonstrate the\neffectiveness of \\textit{TrajAgent} in automated trajectory modeling, achieving\na performance improvement of 2.38\\%-34.96\\% over baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory modeling, which includes research on trajectory data pattern\nmining and future prediction, has widespread applications in areas such as life\nservices, urban transportation, and public administration. Numerous methods\nhave been proposed to address specific problems within trajectory modeling.\nHowever, the heterogeneity of data and the diversity of trajectory tasks make\neffective and reliable trajectory modeling an important yet highly challenging\nendeavor, even for domain experts. In this paper, we propose\n\\textit{TrajAgent}, a agent framework powered by large language models (LLMs),\ndesigned to facilitate robust and efficient trajectory modeling through\nautomation modeling. This framework leverages and optimizes diverse specialized\nmodels to address various trajectory modeling tasks across different datasets\neffectively. In \\textit{TrajAgent}, we first develop \\textit{UniEnv}, an\nexecution environment with a unified data and model interface, to support the\nexecution and training of various models. Building on \\textit{UniEnv}, we\nintroduce an agentic workflow designed for automatic trajectory modeling across\nvarious trajectory tasks and data. Furthermore, we introduce collaborative\nlearning schema between LLM-based agents and small speciallized models, to\nenhance the performance of the whole framework effectively. Extensive\nexperiments on four tasks using four real-world datasets demonstrate the\neffectiveness of \\textit{TrajAgent} in automated trajectory modeling, achieving\na performance improvement of 2.38\\%-34.96\\% over baseline methods."
                },
                "authors": [
                    {
                        "name": "Yuwei Du"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Jie Zhao"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "the code will be openly accessible at:\n  https://github.com/tsinghua-fib-lab/TrajAgent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20445v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20445v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11548v1",
                "updated": "2025-06-13T08:00:38Z",
                "updated_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    0,
                    38,
                    4,
                    164,
                    0
                ],
                "published": "2025-06-13T08:00:38Z",
                "published_parsed": [
                    2025,
                    6,
                    13,
                    8,
                    0,
                    38,
                    4,
                    164,
                    0
                ],
                "title": "Augmenting the Generality and Performance of Large Language Models for\n  Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmenting the Generality and Performance of Large Language Models for\n  Software Engineering"
                },
                "summary": "Large Language Models (LLMs) are revolutionizing software engineering (SE),\nwith special emphasis on code generation and analysis. However, their\napplications to broader SE practices including conceptualization, design, and\nother non-code tasks, remain partially underexplored. This research aims to\naugment the generality and performance of LLMs for SE by (1) advancing the\nunderstanding of how LLMs with different characteristics perform on various\nnon-code tasks, (2) evaluating them as sources of foundational knowledge in SE,\nand (3) effectively detecting hallucinations on SE statements. The expected\ncontributions include a variety of LLMs trained and evaluated on\ndomain-specific datasets, new benchmarks on foundational knowledge in SE, and\nmethods for detecting hallucinations. Initial results in terms of performance\nimprovements on various non-code tasks are promising.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are revolutionizing software engineering (SE),\nwith special emphasis on code generation and analysis. However, their\napplications to broader SE practices including conceptualization, design, and\nother non-code tasks, remain partially underexplored. This research aims to\naugment the generality and performance of LLMs for SE by (1) advancing the\nunderstanding of how LLMs with different characteristics perform on various\nnon-code tasks, (2) evaluating them as sources of foundational knowledge in SE,\nand (3) effectively detecting hallucinations on SE statements. The expected\ncontributions include a variety of LLMs trained and evaluated on\ndomain-specific datasets, new benchmarks on foundational knowledge in SE, and\nmethods for detecting hallucinations. Initial results in terms of performance\nimprovements on various non-code tasks are promising."
                },
                "authors": [
                    {
                        "name": "Fabian C. Peña"
                    }
                ],
                "author_detail": {
                    "name": "Fabian C. Peña"
                },
                "author": "Fabian C. Peña",
                "arxiv_journal_ref": "2025 IEEE/ACM 46th International Conference on Software\n  Engineering (ICSE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]