[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2407.14346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v2",
                "updated": "2024-10-18T13:59:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    59,
                    54,
                    4,
                    292,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14142v1",
                "updated": "2024-10-18T03:30:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T03:30:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense MEC Networks With Multi-Slope Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense MEC Networks With Multi-Slope Channels"
                },
                "summary": "Cache-assisted ultra-dense mobile edge computing (MEC) networks have been\nextensively seen as a promising solution to meeting the rapidly growing\nrequirements of massive mobile devices (MDs). To properly tackle the\ncomplicated, severe, and average interferences caused by small base stations\n(SBSs) ultra-densely deployed in such networks, the orthogonal frequency\ndivision multiple access (OFDMA), non-orthogonal multiple access (NOMA) and\nbase station (BS) clustering are jointly considered in this paper. To protect\nthe tasks of MDs offloaded to BSs for computing, which are exposed to multiple\nMDs, and vulnerable to eavesdropping and malicious attacks, some security\nmeasures are further introduced. After that, we develop a computation\noffloading scheme to minimize the energy consumed by MDs under the constraints\nof delay, power, computing resources, and security costs, which jointly\noptimizes the task execution decision, device association, channel selection,\nsecurity service assignment, power control, and computing resource allocation.\nTo solve the finally formulated problem, we develop a high-performance\nalgorithm by improving the existing hierarchical adaptive search algorithm.\nThen, the convergence, computation complexity, and parallel implementation\nanalyses are made for the proposed algorithms. Simulation results show that\nsuch algorithms may generally achieve lower total energy consumption and delay\nthan other algorithms under strict latency and cost constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-assisted ultra-dense mobile edge computing (MEC) networks have been\nextensively seen as a promising solution to meeting the rapidly growing\nrequirements of massive mobile devices (MDs). To properly tackle the\ncomplicated, severe, and average interferences caused by small base stations\n(SBSs) ultra-densely deployed in such networks, the orthogonal frequency\ndivision multiple access (OFDMA), non-orthogonal multiple access (NOMA) and\nbase station (BS) clustering are jointly considered in this paper. To protect\nthe tasks of MDs offloaded to BSs for computing, which are exposed to multiple\nMDs, and vulnerable to eavesdropping and malicious attacks, some security\nmeasures are further introduced. After that, we develop a computation\noffloading scheme to minimize the energy consumed by MDs under the constraints\nof delay, power, computing resources, and security costs, which jointly\noptimizes the task execution decision, device association, channel selection,\nsecurity service assignment, power control, and computing resource allocation.\nTo solve the finally formulated problem, we develop a high-performance\nalgorithm by improving the existing hierarchical adaptive search algorithm.\nThen, the convergence, computation complexity, and parallel implementation\nanalyses are made for the proposed algorithms. Simulation results show that\nsuch algorithms may generally achieve lower total energy consumption and delay\nthan other algorithms under strict latency and cost constraints."
                },
                "authors": [
                    {
                        "name": "Tianqing Zhou"
                    },
                    {
                        "name": "Bobo Wang"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Xuefang Nie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v1",
                "updated": "2024-10-17T20:11:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_comment": "RTSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v1",
                "updated": "2024-10-17T17:58:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction"
                },
                "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v4",
                "updated": "2024-10-17T15:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    15,
                    27,
                    30,
                    3,
                    291,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07979v2",
                "updated": "2024-10-17T08:54:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    54,
                    37,
                    3,
                    291,
                    0
                ],
                "published": "2024-04-11T17:57:22Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    57,
                    22,
                    3,
                    102,
                    0
                ],
                "title": "LLoCO: Learning Long Contexts Offline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLoCO: Learning Long Contexts Offline"
                },
                "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco."
                },
                "authors": [
                    {
                        "name": "Sijun Tan"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Shishir Patil"
                    },
                    {
                        "name": "Ziyang Wu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Raluca Ada Popa"
                    }
                ],
                "author_detail": {
                    "name": "Raluca Ada Popa"
                },
                "author": "Raluca Ada Popa",
                "arxiv_comment": "EMNLP 2024. The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13212v1",
                "updated": "2024-10-17T04:35:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:35:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations"
                },
                "summary": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters."
                },
                "authors": [
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v3",
                "updated": "2024-10-16T17:54:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    54,
                    15,
                    2,
                    290,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Accepted to PVLDB Volume 18",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12749v1",
                "updated": "2024-10-16T17:10:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:10:48Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "title": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM"
                },
                "summary": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads."
                },
                "authors": [
                    {
                        "name": "Juechu Dong"
                    },
                    {
                        "name": "Jonah Rosenblum"
                    },
                    {
                        "name": "Satish Narayanasamy"
                    }
                ],
                "author_detail": {
                    "name": "Satish Narayanasamy"
                },
                "author": "Satish Narayanasamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12605v1",
                "updated": "2024-10-16T14:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "title": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques"
                },
                "summary": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Rishal Ravikesh Chand"
                    },
                    {
                        "name": "Neeraj Anand Sharma"
                    },
                    {
                        "name": "Muhammad Ashad Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ashad Kabir"
                },
                "author": "Muhammad Ashad Kabir",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v1",
                "updated": "2024-10-16T12:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "17 pages, 6 figures, Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12423v1",
                "updated": "2024-10-16T10:06:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:06:22Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "title": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors"
                },
                "summary": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency."
                },
                "authors": [
                    {
                        "name": "Qinghang Zhao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Yixi Ji"
                    },
                    {
                        "name": "Jinjian Wu"
                    },
                    {
                        "name": "Guangming Shi"
                    }
                ],
                "author_detail": {
                    "name": "Guangming Shi"
                },
                "author": "Guangming Shi",
                "arxiv_doi": "10.1145/3676536.3676710",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676710",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.12423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12168v1",
                "updated": "2024-10-16T02:16:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T02:16:53Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "title": "COMET: Towards Partical W4A4KV4 LLMs Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMET: Towards Partical W4A4KV4 LLMs Serving"
                },
                "summary": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective."
                },
                "authors": [
                    {
                        "name": "Lian Liu"
                    },
                    {
                        "name": "Haimeng Ren"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Zhaohui Xu"
                    },
                    {
                        "name": "Yudong Pan"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Xiaowei Li"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang",
                "arxiv_comment": "14 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v2",
                "updated": "2024-10-15T15:58:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    58,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11417v1",
                "updated": "2024-10-15T09:07:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T09:07:25Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "title": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models"
                },
                "summary": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09297v3",
                "updated": "2024-10-15T08:45:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    45,
                    18,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-13T16:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    33,
                    44,
                    3,
                    165,
                    0
                ],
                "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding"
                },
                "summary": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv"
                },
                "authors": [
                    {
                        "name": "Zayd Muhammad Kawakibi Zuhri"
                    },
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v2",
                "updated": "2024-10-15T06:09:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    6,
                    9,
                    35,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v1",
                "updated": "2024-10-15T05:57:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v3",
                "updated": "2024-10-15T05:34:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    34,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v2",
                "updated": "2024-10-19T08:45:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    8,
                    45,
                    11,
                    5,
                    293,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11260v1",
                "updated": "2024-10-15T04:35:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T04:35:49Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "title": "A Zoned Storage Optimized Flash Cache on ZNS SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zoned Storage Optimized Flash Cache on ZNS SSDs"
                },
                "summary": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme."
                },
                "authors": [
                    {
                        "name": "Chongzhuo Yang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v4",
                "updated": "2024-10-14T19:12:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    12,
                    48,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stphane Pouget"
                    },
                    {
                        "name": "Louis-Nol Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10819v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads"
                },
                "summary": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v1",
                "updated": "2024-10-14T17:50:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10511v1",
                "updated": "2024-10-14T13:49:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:49:06Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling"
                },
                "summary": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities."
                },
                "authors": [
                    {
                        "name": "Wenze Liu"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Sheng Xia"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "19 pages, 17 figures, 8 tables, github repo:\n  https://github.com/poppuppy/SAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v2",
                "updated": "2024-10-14T09:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    35,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13378v2",
                "updated": "2024-10-14T07:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    58,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-22T06:19:43Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    6,
                    19,
                    43,
                    2,
                    143,
                    0
                ],
                "title": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation"
                },
                "summary": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency."
                },
                "authors": [
                    {
                        "name": "Quyang Pan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhiyuan Wu"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Jingyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Wang"
                },
                "author": "Jingyuan Wang",
                "arxiv_comment": "17 pages, 7 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10157v1",
                "updated": "2024-10-14T04:49:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "title": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI"
                },
                "summary": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI."
                },
                "authors": [
                    {
                        "name": "Meng Gao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Huafu Li"
                    },
                    {
                        "name": "Junqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Junqi Guo"
                },
                "author": "Junqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10149v1",
                "updated": "2024-10-14T04:30:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:30:38Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "title": "Fast and Accurate Neural Rendering Using Semi-Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Neural Rendering Using Semi-Gradients"
                },
                "summary": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss."
                },
                "authors": [
                    {
                        "name": "In-Young Cho"
                    },
                    {
                        "name": "Jaewoong Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoong Cho"
                },
                "author": "Jaewoong Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10071v1",
                "updated": "2024-10-14T01:25:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T01:25:56Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "title": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning"
                },
                "summary": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jinjin Shen"
                    },
                    {
                        "name": "Yan Lin"
                    },
                    {
                        "name": "Yijin Zhang"
                    },
                    {
                        "name": "Weibin Zhang"
                    },
                    {
                        "name": "Feng Shu"
                    },
                    {
                        "name": "Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Jun Li"
                },
                "author": "Jun Li",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09533v1",
                "updated": "2024-10-12T13:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T13:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "title": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence"
                },
                "summary": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24"
                },
                "authors": [
                    {
                        "name": "Felipe Cadar"
                    },
                    {
                        "name": "Guilherme Potje"
                    },
                    {
                        "name": "Renato Martins"
                    },
                    {
                        "name": "Cdric Demonceaux"
                    },
                    {
                        "name": "Erickson R. Nascimento"
                    }
                ],
                "author_detail": {
                    "name": "Erickson R. Nascimento"
                },
                "author": "Erickson R. Nascimento",
                "arxiv_comment": "Accepted in ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v1",
                "updated": "2024-10-12T10:38:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09397v1",
                "updated": "2024-10-12T07:01:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T07:01:30Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v3",
                "updated": "2024-10-12T02:11:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    2,
                    11,
                    14,
                    5,
                    286,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09237v1",
                "updated": "2024-10-11T20:23:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T20:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "title": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor"
                },
                "summary": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}."
                },
                "authors": [
                    {
                        "name": "Sahar Ahmadi"
                    },
                    {
                        "name": "Ali Cheraghian"
                    },
                    {
                        "name": "Morteza Saberi"
                    },
                    {
                        "name": "Md. Towsif Abir"
                    },
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Farookh Hussain"
                    },
                    {
                        "name": "Shafin Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Shafin Rahman"
                },
                "author": "Shafin Rahman",
                "arxiv_comment": "ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08895v1",
                "updated": "2024-10-11T15:12:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:12:30Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "title": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation"
                },
                "summary": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Kun Ding"
                    },
                    {
                        "name": "Qiang Yu"
                    },
                    {
                        "name": "Haojian Zhang"
                    },
                    {
                        "name": "Gaofeng Meng"
                    },
                    {
                        "name": "Shiming Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Shiming Xiang"
                },
                "author": "Shiming Xiang",
                "arxiv_comment": "submitted to IJCV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v1",
                "updated": "2024-10-11T12:19:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtrik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtrik"
                },
                "author": "Peter Richtrik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08618v1",
                "updated": "2024-10-11T08:33:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T08:33:58Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "title": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination"
                },
                "summary": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Qiulin Tian"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Tong Xin"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v1",
                "updated": "2024-10-11T07:24:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03462v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03462v3",
                "updated": "2024-10-11T02:18:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    2,
                    18,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-01-07T11:57:40Z",
                "published_parsed": [
                    2024,
                    1,
                    7,
                    11,
                    57,
                    40,
                    6,
                    7,
                    0
                ],
                "title": "Long Context Compression with Activation Beacon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Compression with Activation Beacon"
                },
                "summary": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}."
                },
                "authors": [
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Ninglu Shao"
                    },
                    {
                        "name": "Qiwei Ye"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Newer version of Activation Beacon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03462v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03462v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08391v1",
                "updated": "2024-10-10T21:55:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T21:55:11Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    21,
                    55,
                    11,
                    3,
                    284,
                    0
                ],
                "title": "KV Prediction for Improved Time to First Token",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Prediction for Improved Time to First Token"
                },
                "summary": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with transformer-based language models begins with a prompt\nprocessing step. In this step, the model generates the first output token and\nstores the KV cache needed for future generation steps. This prompt processing\nstep can be computationally expensive, taking 10s of seconds or more for\nbillion-parameter models on edge devices when prompt lengths or batch sizes\nrise. This degrades user experience by introducing significant latency into the\nmodel's outputs. To reduce the time spent producing the first output (known as\nthe ``time to first token'', or TTFT) of a pretrained model, we introduce a\nnovel method called KV Prediction. In our method, a small auxiliary model is\nused to process the prompt and produce an approximation of the KV cache used by\na base model. This approximated KV cache is then used with the base model for\nautoregressive generation without the need to query the auxiliary model again.\nWe demonstrate that our method produces a pareto-optimal efficiency-accuracy\ntrade-off when compared to baselines. On TriviaQA, we demonstrate relative\naccuracy improvements in the range of $15\\%-50\\%$ across a range of TTFT FLOPs\nbudgets. We also demonstrate accuracy improvements of up to $30\\%$ on HumanEval\npython code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark\nmodels on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs\ntranslates to a TTFT speedup on hardware. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/kv-prediction ."
                },
                "authors": [
                    {
                        "name": "Maxwell Horton"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Chenfan Sun"
                    },
                    {
                        "name": "Yanzi Jin"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Moin Nabi"
                    }
                ],
                "author_detail": {
                    "name": "Moin Nabi"
                },
                "author": "Moin Nabi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11284v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11284v3",
                "updated": "2024-10-10T16:57:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    16,
                    57,
                    34,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-17T11:48:14Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    11,
                    48,
                    14,
                    2,
                    108,
                    0
                ],
                "title": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amplifying Main Memory-Based Timing Covert and Side Channels using\n  Processing-in-Memory Operations"
                },
                "summary": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of processing-in-memory (PiM) architectures has been gaining\nmomentum because they provide high performance and low energy consumption by\nalleviating the data movement bottleneck. Yet, the security of such\narchitectures has not been thoroughly explored. The adoption of PiM solutions\nprovides a new way to directly access main memory, which malicious user\napplications can exploit. We show that this new way to access main memory opens\nopportunities for high-throughput timing attacks that are hard-to-mitigate\nwithout significant performance overhead.\n  We introduce IMPACT, a set of high-throughput main memory-based timing\nattacks that leverage characteristics of PiM architectures to establish covert\nand side channels. IMPACT enables high-throughput communication and private\ninformation leakage by exploiting the shared DRAM row buffer. To achieve high\nthroughput, IMPACT (i) eliminates cache bypassing steps required by\nprocessor-centric main memory and cache-based timing attacks and (ii) leverages\nthe intrinsic parallelism of PiM operations. We showcase two applications of\nIMPACT. First, we build two covert-channel attacks that run on the host CPU and\nleverage different PiM approaches to gain direct and fast access to main memory\nand establish high-throughput communication covert channels. Second, we\nshowcase a side-channel attack that leaks private information of concurrently\nrunning victim applications that are accelerated with PiM. Our results\ndemonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s\ncommunication throughput, respectively, which is up to 4.91x and 5.41x faster\nthan the state-of-the-art main memory-based covert channels, and (ii) our\nside-channel attack allows the attacker to leak secrets with a low error rate.\nTo avoid such covert and side channels in emerging PiM systems, we propose and\nevaluate three defenses."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellopoulos"
                    },
                    {
                        "name": "F. Nisa Bostanci"
                    },
                    {
                        "name": "Ataberk Olgun"
                    },
                    {
                        "name": "A. Giray Yaglikci"
                    },
                    {
                        "name": "Ismail Emir Yuksel"
                    },
                    {
                        "name": "Nika Mansouri Ghiasi"
                    },
                    {
                        "name": "Zulal Bingol"
                    },
                    {
                        "name": "Mohammad Sadrosadati"
                    },
                    {
                        "name": "Onur Mutlu"
                    }
                ],
                "author_detail": {
                    "name": "Onur Mutlu"
                },
                "author": "Onur Mutlu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11284v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11284v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12850v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12850v1",
                "updated": "2024-10-10T15:24:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    24,
                    12,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T15:24:12Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    15,
                    24,
                    12,
                    3,
                    284,
                    0
                ],
                "title": "RecurFormer: Not All Transformer Heads Need Self-Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RecurFormer: Not All Transformer Heads Need Self-Attention"
                },
                "summary": "Transformer-based large language models (LLMs) excel in modeling complex\nlanguage patterns but face significant computational costs during inference,\nespecially with long inputs due to the attention mechanism's memory overhead.\nWe observe that certain attention heads exhibit a distribution where the\nattention weights concentrate on tokens near the query token, termed as recency\naware, which focuses on local and short-range dependencies. Leveraging this\ninsight, we propose RecurFormer, a novel architecture that replaces these\nattention heads with linear recurrent neural networks (RNNs), specifically the\nMamba architecture. This replacement reduces the cache size without evicting\ntokens, thus maintaining generation quality. RecurFormer retains the ability to\nmodel long-range dependencies through the remaining attention heads and allows\nfor reusing pre-trained Transformer-based LLMs weights with continual training.\nExperiments demonstrate that RecurFormer matches the original model's\nperformance while significantly enhancing inference efficiency. Our approach\nprovides a practical solution to the computational challenges of\nTransformer-based LLMs inference, making it highly attractive for tasks\ninvolving long inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) excel in modeling complex\nlanguage patterns but face significant computational costs during inference,\nespecially with long inputs due to the attention mechanism's memory overhead.\nWe observe that certain attention heads exhibit a distribution where the\nattention weights concentrate on tokens near the query token, termed as recency\naware, which focuses on local and short-range dependencies. Leveraging this\ninsight, we propose RecurFormer, a novel architecture that replaces these\nattention heads with linear recurrent neural networks (RNNs), specifically the\nMamba architecture. This replacement reduces the cache size without evicting\ntokens, thus maintaining generation quality. RecurFormer retains the ability to\nmodel long-range dependencies through the remaining attention heads and allows\nfor reusing pre-trained Transformer-based LLMs weights with continual training.\nExperiments demonstrate that RecurFormer matches the original model's\nperformance while significantly enhancing inference efficiency. Our approach\nprovides a practical solution to the computational challenges of\nTransformer-based LLMs inference, making it highly attractive for tasks\ninvolving long inputs."
                },
                "authors": [
                    {
                        "name": "Ruiqing Yan"
                    },
                    {
                        "name": "Linghan Zheng"
                    },
                    {
                        "name": "Xingbo Du"
                    },
                    {
                        "name": "Han Zou"
                    },
                    {
                        "name": "Yufeng Guo"
                    },
                    {
                        "name": "Jianfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Yang"
                },
                "author": "Jianfei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12850v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12850v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01195v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01195v2",
                "updated": "2024-10-10T11:01:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    11,
                    1,
                    44,
                    3,
                    284,
                    0
                ],
                "published": "2024-06-03T10:58:32Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    10,
                    58,
                    32,
                    0,
                    155,
                    0
                ],
                "title": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel\n  Mapping"
                },
                "summary": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a compact, cumulative and coalescible probabilistic voxel\nmapping method to enhance performance, accuracy and memory efficiency in LiDAR\nodometry. Probabilistic voxel mapping requires storing past point clouds and\nre-iterating on them to update the uncertainty every iteration, which consumes\nlarge memory space and CPU cycles. To solve this problem, we propose a\ntwo-folded strategy. First, we introduce a compact point-free representation\nfor probabilistic voxels and derive a cumulative update of the planar\nuncertainty without caching original point clouds. Our voxel structure only\nkeeps track of a predetermined set of statistics for points that lie inside it.\nThis method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space\ncomplexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$\nis the number of points. Second, to further minimize memory usage and enhance\nmapping accuracy, we provide a strategy to dynamically merge voxels associated\nwith the same physical planes by taking advantage of the geometric features in\nthe real world. Rather than scanning for these coalescible voxels constantly at\nevery iteration, our merging strategy accumulates voxels in a\nlocality-sensitive hash and triggers merging lazily. On-demand merging not only\nreduces memory footprint with minimal computational overhead but also improves\nlocalization accuracy thanks to cross-voxel denoising. Experiments exhibit 20%\nhigher accuracy, 20% faster performance and 70% lower memory consumption than\nthe state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Xu Yang"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Qijie Ge"
                    },
                    {
                        "name": "Lulu Suo"
                    },
                    {
                        "name": "Weijie Tang"
                    },
                    {
                        "name": "Zhengyu Wei"
                    },
                    {
                        "name": "Longxiang Huang"
                    },
                    {
                        "name": "Bo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wang"
                },
                "author": "Bo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01195v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01195v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04793v2",
                "updated": "2024-10-10T05:11:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    5,
                    11,
                    52,
                    3,
                    284,
                    0
                ],
                "published": "2024-04-07T03:08:14Z",
                "published_parsed": [
                    2024,
                    4,
                    7,
                    3,
                    8,
                    14,
                    6,
                    98,
                    0
                ],
                "title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via\n  Layer-wise Optimal Budget"
                },
                "summary": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has\nbeen considered critical to saving the cost of inference. Most of the existing\nKV-cache compression algorithms attempted to sparsify the sequence of tokens by\ntaking advantage of the different importance of tokens. However, most of these\nmethods treat all layers equally, allocating the same KV budget to each layer.\nThis approach is suboptimal, as some layers may be less sensitive to input\ntokens yet still receive the same budget as others. In this work, we found that\nby identifying the importance of attention layers, we could optimize the\nKV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based\non our observations regarding layer-wise importance in inference, we propose\nSqueezeAttention to precisely optimize the allocation of KV-cache budget among\nlayers on-the-fly and then incorporate three representative sequence-wise\nalgorithms to compress the KV-cache for each layer with its very own budget.\nSpecifically, we first measure each layer's importance by calculating the\ncosine similarity of the input prompt differences before and after the\nself-attention layers. Based on this similarity, we then categorize the layers\ninto two groups and adjust their KV budgets accordingly. By optimizing the\nKV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves\naround 30% to 70% of the memory reductions and up to 2.2 times of throughput\nimprovements in a wide range of LLMs and benchmarks. The code is available at\nhttps://github.com/hetailang/SqueezeAttention."
                },
                "authors": [
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Shaoduo Gan"
                    }
                ],
                "author_detail": {
                    "name": "Shaoduo Gan"
                },
                "author": "Shaoduo Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07590v1",
                "updated": "2024-10-10T03:52:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:52:54Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    52,
                    54,
                    3,
                    284,
                    0
                ],
                "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\n  KV Caches for Chunked Text"
                },
                "summary": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process\nnumerous retrieved document chunks for prefill which requires a large volume of\ncomputation, therefore leading to significant latency in time-to-first-token\n(TTFT). To reduce the computation overhead as well as TTFT, we introduce\nTurboRAG, a novel RAG system that redesigns the inference paradigm of the\ncurrent RAG system by first pre-computing and storing the key-value (KV) caches\nof documents offline, and then directly retrieving the saved KV cache for\nprefill. Hence, online computation of KV caches is eliminated during inference.\nIn addition, we provide a number of insights into the mask matrix and\npositional embedding mechanisms, plus fine-tune a pretrained language model to\nmaintain model accuracy of TurboRAG. Our approach is applicable to most\nexisting large language models and their applications without any requirement\nin modification of models and inference systems. Experimental results across a\nsuite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x\ncompared to the conventional RAG systems (on an average of 8.6x), but reserving\ncomparable performance to the standard RAG systems."
                },
                "authors": [
                    {
                        "name": "Songshuo Lu"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Yutian Rong"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07579v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07579v1",
                "updated": "2024-10-10T03:28:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "published": "2024-10-10T03:28:46Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    3,
                    28,
                    46,
                    3,
                    284,
                    0
                ],
                "title": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teddy: Efficient Large-Scale Dataset Distillation via\n  Taylor-Approximated Matching"
                },
                "summary": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset distillation or condensation refers to compressing a large-scale\ndataset into a much smaller one, enabling models trained on this synthetic\ndataset to generalize effectively on real data. Tackling this challenge, as\ndefined, relies on a bi-level optimization algorithm: a novel model is trained\nin each iteration within a nested loop, with gradients propagated through an\nunrolled computation graph. However, this approach incurs high memory and time\ncomplexity, posing difficulties in scaling up to large datasets such as\nImageNet. Addressing these concerns, this paper introduces Teddy, a\nTaylor-approximated dataset distillation framework designed to handle\nlarge-scale dataset and enhance efficiency. On the one hand, backed up by\ntheoretical analysis, we propose a memory-efficient approximation derived from\nTaylor expansion, which transforms the original form dependent on multi-step\ngradients to a first-order one. On the other hand, rather than repeatedly\ntraining a novel model in each iteration, we unveil that employing a pre-cached\npool of weak models, which can be generated from a single base model, enhances\nboth time efficiency and performance concurrently, particularly when dealing\nwith large-scale datasets. Extensive experiments demonstrate that the proposed\nTeddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet\nand original-sized ImageNet-1K dataset, notably surpassing prior methods by up\nto 12.8%, while reducing 46.6% runtime. Our code will be available at\nhttps://github.com/Lexie-YU/Teddy."
                },
                "authors": [
                    {
                        "name": "Ruonan Yu"
                    },
                    {
                        "name": "Songhua Liu"
                    },
                    {
                        "name": "Jingwen Ye"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang",
                "arxiv_comment": "Accepted by ECCV2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07579v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19519v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19519v3",
                "updated": "2024-10-09T15:57:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    57,
                    3,
                    2,
                    283,
                    0
                ],
                "published": "2024-03-28T15:52:15Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    15,
                    52,
                    15,
                    3,
                    88,
                    0
                ],
                "title": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage"
                },
                "summary": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the physics of electromagnetic pulse emission and nozzle damage\nis critical for the long-term operation of laser experiments with gas targets,\nparticularly at facilities looking to produce stable sources of radiation at\nhigh repetition rate. We present a theoretical model of plasma formation and\nelectrostatic charging when high-power lasers are focused inside gases. The\nmodel can be used to estimate the amplitude of gigahertz electromagnetic pulses\n(EMPs) produced by the laser and the extent of damage to the gas jet nozzle.\nLooking at a range of laser and target properties relevant to existing\nhigh-power laser systems, we find that EMP fields of tens to hundreds of kV/m\ncan be generated several metres from the gas jet. Model predictions are\ncompared with measurements of EMP, plasma formation and nozzle damage from two\nexperiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt\nlaser."
                },
                "authors": [
                    {
                        "name": "Philip Wykeham Bradford"
                    },
                    {
                        "name": "Valeria Ospina-Bohorquez"
                    },
                    {
                        "name": "Michael Ehret"
                    },
                    {
                        "name": "Jose-Luis Henares"
                    },
                    {
                        "name": "Pilar Puyuelo-Valdes"
                    },
                    {
                        "name": "Tomasz Chodukowski"
                    },
                    {
                        "name": "Tadeusz Pisarczyk"
                    },
                    {
                        "name": "Zofia Rusiniak"
                    },
                    {
                        "name": "Carlos Salgado-Lopez"
                    },
                    {
                        "name": "Christos Vlachos"
                    },
                    {
                        "name": "Massimiliano Sciscio"
                    },
                    {
                        "name": "Martina Salvadori"
                    },
                    {
                        "name": "Claudio Verona"
                    },
                    {
                        "name": "George Hicks"
                    },
                    {
                        "name": "Oliver Ettlinger"
                    },
                    {
                        "name": "Zulfikar Najmudin"
                    },
                    {
                        "name": "Jean-Raphael Marques"
                    },
                    {
                        "name": "Laurent Gremillet"
                    },
                    {
                        "name": "Joao Jorge Santos"
                    },
                    {
                        "name": "Fabrizio Consoli"
                    },
                    {
                        "name": "Vladimir Tikhonchuk"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Tikhonchuk"
                },
                "author": "Vladimir Tikhonchuk",
                "arxiv_comment": "18 pages (total), 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.19519v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19519v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06934v1",
                "updated": "2024-10-09T14:28:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T14:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    14,
                    28,
                    59,
                    2,
                    283,
                    0
                ],
                "title": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VEC-Sim: A Simulation Platform for Evaluating Service Caching and\n  Computation Offloading Policies in Vehicular Edge Networks"
                },
                "summary": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer simulation platforms offer an alternative solution by emulating\ncomplex systems in a controlled manner. However, existing Edge Computing (EC)\nsimulators, as well as general-purpose vehicular network simulators, are not\ntailored for VEC and lack dedicated support for modeling the distinct access\npattern, entity mobility trajectory and other unique characteristics of VEC\nnetworks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation\nplatform for in-depth evaluation and analysis of various service caching and\ncomputation offloading policies in VEC networks. VEC-Sim incorporates realistic\nmechanisms to replicate real-world access patterns, including service feature\nvector, vehicle mobility modeling, evolving service popularity, new service\nupload and user preference shifts, etc. Moreover, its modular architecture and\nextensive Application Programming Interfaces (APIs) allow seamless integration\nof customized scheduling policies and user-defined metrics. A comprehensive\nevaluation of VEC-Sim's capabilities is undertaken in comparison to real-world\nground truths. Results prove it to be accurate in reproducing classical\nscheduling algorithms and extremely effective in conducting case studies."
                },
                "authors": [
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Xiaolong Xu"
                    },
                    {
                        "name": "Muhammad Bilal"
                    },
                    {
                        "name": "Xiangwei Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Siyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Siyu Wu"
                },
                "author": "Siyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v3",
                "updated": "2024-10-09T11:40:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    11,
                    40,
                    31,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 69x and reduces SLO violation rates by\n28.7%, significantly enhancing the user experience."
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06627v1",
                "updated": "2024-10-09T07:22:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T07:22:40Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    7,
                    22,
                    40,
                    2,
                    283,
                    0
                ],
                "title": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variations in Multi-Agent Actor-Critic Frameworks for Joint\n  Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and\n  Directions"
                },
                "summary": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can\neffectively execute surveillance, connectivity, and computing services to\nground users (GUs). These missions require trajectory planning, UAV-GUs\nassociation, task offloading, next-hop selection, and resources such as\ntransmit power, bandwidth, caching, and computing allocation to improve network\nperformances. Owing to the highly dynamic topology, limited resources, and\nnon-availability of global knowledge, optimizing network performance in UAVSNs\nis very intricate. Hence, it requires an adaptive joint optimization framework\nthat can tackle both discrete and continuous decision variables to ensure\noptimal network performance under dynamic constraints. Multi-agent deep\nreinforcement learning-based adaptive actor-critic framework can efficiently\naddress these problems. This paper investigates the recent evolutions of\nactor-critic frameworks to deal with joint optimization problems in UAVSNs. In\naddition, challenges and potential solutions are addressed as research\ndirections."
                },
                "authors": [
                    {
                        "name": "Muhammad Morshed Alam"
                    },
                    {
                        "name": "Muhammad Yeasir Aarafat"
                    },
                    {
                        "name": "Tamim Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Tamim Hossain"
                },
                "author": "Tamim Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13941v2",
                "updated": "2024-10-09T04:11:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    4,
                    11,
                    28,
                    2,
                    283,
                    0
                ],
                "published": "2024-06-20T02:20:21Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    2,
                    20,
                    21,
                    3,
                    172,
                    0
                ],
                "title": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDLRM: Accelerating Personalized Recommendation using Real-World PIM\n  Architecture"
                },
                "summary": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Recommendation Models (DLRMs) have gained popularity in\nrecommendation systems due to their effectiveness in handling large-scale\nrecommendation tasks. The embedding layers of DLRMs have become the performance\nbottleneck due to their intensive needs on memory capacity and memory\nbandwidth. In this paper, we propose UpDLRM, which utilizes real-world\nprocessingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth\nand reduce recommendation latency. The parallel nature of the DPU memory can\nprovide high aggregated bandwidth for the large number of irregular memory\naccesses in embedding lookups, thus offering great potential to reduce the\ninference latency. To fully utilize the DPU memory bandwidth, we further\nstudied the embedding table partitioning problem to achieve good\nworkload-balance and efficient data caching. Evaluations using real-world\ndatasets show that, UpDLRM achieves much lower inference time for DLRM compared\nto both CPU-only and CPU-GPU hybrid counterparts."
                },
                "authors": [
                    {
                        "name": "Sitian Chen"
                    },
                    {
                        "name": "Haobin Tan"
                    },
                    {
                        "name": "Amelie Chi Zhou"
                    },
                    {
                        "name": "Yusen Li"
                    },
                    {
                        "name": "Pavan Balaji"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Balaji"
                },
                "author": "Pavan Balaji",
                "arxiv_doi": "10.1145/3649329.3658266",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3658266",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.13941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by DAC 2024",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06497v1",
                "updated": "2024-10-09T02:51:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "published": "2024-10-09T02:51:27Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    2,
                    51,
                    27,
                    2,
                    283,
                    0
                ],
                "title": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERCache: An Efficient and Reliable Caching Framework for Large-Scale\n  User Representations in Meta's Ads System"
                },
                "summary": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of deep learning models used for calculating user\nrepresentations presents significant challenges, particularly with limited\ncomputational resources and strict service-level agreements (SLAs). Previous\nresearch efforts have focused on optimizing model inference but have overlooked\na critical question: is it necessary to perform user model inference for every\nad request in large-scale social networks? To address this question and these\nchallenges, we first analyze user access patterns at Meta and find that most\nuser model inferences occur within a short timeframe. T his observation reveals\na triangular relationship among model complexity, embedding freshness, and\nservice SLAs. Building on this insight, we designed, implemented, and evaluated\nERCache, an efficient and robust caching framework for large-scale user\nrepresentations in ads recommendation systems on social networks. ERCache\ncategorizes cache into direct and failover types and applies customized\nsettings and eviction policies for each model, effectively balancing model\ncomplexity, embedding freshness, and service SLAs, even considering the\nstaleness introduced by caching. ERCache has been deployed at Meta for over six\nmonths, supporting more than 30 ranking models while efficiently conserving\ncomputational resources and complying with service SLA requirements."
                },
                "authors": [
                    {
                        "name": "Fang Zhou"
                    },
                    {
                        "name": "Yaning Huang"
                    },
                    {
                        "name": "Dong Liang"
                    },
                    {
                        "name": "Dai Li"
                    },
                    {
                        "name": "Zhongke Zhang"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Xiao Xin"
                    },
                    {
                        "name": "Abdallah Aboelela"
                    },
                    {
                        "name": "Zheliang Jiang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Jeff Song"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Chen Liang"
                    },
                    {
                        "name": "Huayu Li"
                    },
                    {
                        "name": "ChongLin Sun"
                    },
                    {
                        "name": "Hang Yang"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Zhan Shu"
                    },
                    {
                        "name": "Mindi Yuan"
                    },
                    {
                        "name": "Emanuele Maccherani"
                    },
                    {
                        "name": "Taha Hayat"
                    },
                    {
                        "name": "John Guo"
                    },
                    {
                        "name": "Varna Puvvada"
                    },
                    {
                        "name": "Uladzimir Pashkevich"
                    }
                ],
                "author_detail": {
                    "name": "Uladzimir Pashkevich"
                },
                "author": "Uladzimir Pashkevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10443v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10443v4",
                "updated": "2024-10-09T01:12:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    9,
                    1,
                    12,
                    19,
                    2,
                    283,
                    0
                ],
                "published": "2024-05-16T21:07:42Z",
                "published_parsed": [
                    2024,
                    5,
                    16,
                    21,
                    7,
                    42,
                    3,
                    137,
                    0
                ],
                "title": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in\n  Fine-tuning LLMs for Simultaneous Translation"
                },
                "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved state-of-the-art performance in\nvarious language processing tasks, motivating their adoption in simultaneous\ntranslation. Current fine-tuning methods to adapt LLMs for simultaneous\ntranslation focus on prompting optimization strategies using either data\naugmentation or prompt structure modifications. However, these methods suffer\nfrom several issues, such as unnecessarily expanded training sets,\ncomputational inefficiency from dumping the key and value cache, increased\nprompt sizes, or restriction to a single decision policy. To eliminate these\nissues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs\nfor simultaneous translation. It utilizes a novel attention mask approach that\nmodels simultaneous translation during fine-tuning by masking attention for a\ndesired decision policy. Applying the proposed SimulMask on a Falcon LLM for\nthe IWSLT 2017 dataset, we have observed a significant translation quality\nimprovement compared to state-of-the-art prompting optimization strategies on\nfive language pairs while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Matthew Raffel"
                    },
                    {
                        "name": "Victor Agostinelli"
                    },
                    {
                        "name": "Lizhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lizhong Chen"
                },
                "author": "Lizhong Chen",
                "arxiv_comment": "Accepted at EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10443v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10443v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01527v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01527v2",
                "updated": "2024-10-08T19:34:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    19,
                    34,
                    3,
                    1,
                    282,
                    0
                ],
                "published": "2024-07-01T17:59:47Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    59,
                    47,
                    0,
                    183,
                    0
                ],
                "title": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Compression, But What Must We Give in Return? A Comprehensive\n  Benchmark of Long Context Capable Approaches"
                },
                "summary": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context capability is a crucial competency for large language models\n(LLMs) as it mitigates the human struggle to digest long-form texts. This\ncapability enables complex task-solving scenarios such as book summarization,\ncode assistance, and many more tasks that are traditionally manpower-intensive.\nHowever, transformer-based LLMs face significant challenges with long context\ninput due to the growing size of the KV cache and the intrinsic complexity of\nattending to extended inputs; where multiple schools of efficiency-driven\napproaches - such as KV cache quantization, token dropping, prompt compression,\nlinear-time sequence models, and hybrid architectures - have been proposed to\nproduce efficient yet long context-capable models. Despite these advancements,\nno existing work has comprehensively benchmarked these methods in a reasonably\naligned environment. In this work, we fill this gap by providing a taxonomy of\ncurrent methods and evaluating 10+ state-of-the-art approaches across seven\ncategories of long context tasks. Our work reveals numerous previously unknown\nphenomena and offers insights - as well as a friendly workbench - for the\nfuture development of long context-capable LLMs. The source code is available\nat https://github.com/henryzhongsc/longctx_bench."
                },
                "authors": [
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongyi Liu"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Yu-Neng Chuang"
                    },
                    {
                        "name": "Songchen Li"
                    },
                    {
                        "name": "Guanchu Wang"
                    },
                    {
                        "name": "Duy Le"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01527v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01527v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05927v1",
                "updated": "2024-10-08T11:28:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T11:28:30Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    11,
                    28,
                    30,
                    1,
                    282,
                    0
                ],
                "title": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerical analysis of partial discharge ignition in H2 bubbles floating\n  in dielectric oils, for High-Voltage Solid State Transformer applications"
                },
                "summary": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on a self-consistent numerical analysis campaign of partial\ndischarge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We\ninvestigate various configurations (bubble sizes, bubble position, existence of\nprotrusion) on a cylinder-to-cylinder setup that emulates a specific SST module\n(from SSTAR Horizon Europe project) under transient overvoltage as well as in\nits design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our\nresults on electrical characteristics and plasma dynamics leading to the PD\nignition, indicate that under transient overvoltage and for mm size bubbles\n(diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage,\nwhile the peak inception voltage is higher than 70 kV. The existence of\nmetallic protrusion can affect the inception voltage of a remote floating\nbubble only slightly and when this is close to the sharp tip. The extreme\nscenario of a protrusion in contact (inside) a gas bubble severely affects the\ninsulation properties and drops the PD inception voltage remarkably. The larger\nthe bubble and the sharper the tip of the protrusion the lower the inception\npeak voltage, that can reach values well below 40 kV. On the contrary and under\ndesign operation, larger bubbles increase the severity and probability of PD\nevents, leading to lower instantaneous inception voltages. Current pulses\nproduced in bubbles can quickly transit to intense streamer discharges (which\ncan also transit to catastrophic arcing) if the operational frequency is\nreduced and/or under transient, HF overvoltage."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kourtzanidis"
                    },
                    {
                        "name": "Panagiotis Dimitrakellis"
                    },
                    {
                        "name": "Dimitrios Rakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Rakopoulos"
                },
                "author": "Dimitrios Rakopoulos",
                "arxiv_comment": "Submitted to IEEE Transactions on Dielectrics and Electrical\n  Insulation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05863v1",
                "updated": "2024-10-08T09:53:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    53,
                    10,
                    1,
                    282,
                    0
                ],
                "title": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Playback Performance in Video Recommender Systems with an\n  On-Device Gating and Ranking Framework"
                },
                "summary": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video recommender systems (RSs) have gained increasing attention in recent\nyears. Existing mainstream RSs focus on optimizing the matching function\nbetween users and items. However, we noticed that users frequently encounter\nplayback issues such as slow loading or stuttering while browsing the videos,\nespecially in weak network conditions, which will lead to a subpar browsing\nexperience, and may cause users to leave, even when the video content and\nrecommendations are superior. It is quite a serious issue, yet easily\noverlooked. To tackle this issue, we propose an on-device Gating and Ranking\nFramework (GRF) that cooperates with server-side RS. Specifically, we utilize a\ngate model to identify videos that may have playback issues in real-time, and\nthen we employ a ranking model to select the optimal result from a\nlocally-cached pool to replace the stuttering videos. Our solution has been\nfully deployed on Kwai, a large-scale short video platform with hundreds of\nmillions of users globally. Moreover, it significantly enhances video playback\nperformance and improves overall user experience and retention rates."
                },
                "authors": [
                    {
                        "name": "Yunfei Yang"
                    },
                    {
                        "name": "Zhenghao Qi"
                    },
                    {
                        "name": "Honghuan Wu"
                    },
                    {
                        "name": "Qi Song"
                    },
                    {
                        "name": "Tieyao Zhang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Yimin Tu"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ben Wang"
                },
                "author": "Ben Wang",
                "arxiv_comment": "CIKM 2024 applied research track, 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05854v1",
                "updated": "2024-10-08T09:46:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "published": "2024-10-08T09:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    9,
                    46,
                    38,
                    1,
                    282,
                    0
                ],
                "title": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Scalable State Sharing Protocol for Low-Resource Validator Nodes in\n  Blockchain Networks"
                },
                "summary": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The perpetual growth of data stored on popular blockchains such as Ethereum\nleads to significant scalability challenges and substantial storage costs for\noperators of full nodes. Increasing costs may lead to fewer independently\noperated nodes in the network, which poses risks to decentralization (and hence\nnetwork security), but also pushes decentralized app developers towards\ncentrally hosted API services.\n  This paper introduces a new protocol that allows validator nodes to\nparticipate in a blockchain network without the need to store the full state of\nthe network on each node. The key idea is to use the blockchain network as both\na replicated state machine and as a distributed storage system. By distributing\nstates across nodes and enabling efficient data retrieval through a\nKademlia-inspired routing protocol, we reduce storage costs for validators.\nCryptographic proofs (such as Merkle proofs) are used to allow nodes to verify\ndata stored by other nodes without having to trust those nodes directly. While\nthe protocol trades off data storage for increased network bandwidth, we show\nhow gossiping and caching can minimize the increased bandwidth needs.\n  To validate our state sharing protocol, we conduct an extensive quantitative\nanalysis of Ethereum's data storage and data access patterns. Our findings\nindicate that while our protocol significantly lowers storage needs, it comes\nwith an increased bandwidth usage ranging from 1.5 MB to 5 MB per block,\ntranslating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite\nthis, the size remains small enough such that it can be passed to all nodes and\nvalidated within Ethereum's 12-second block validation window. Further analysis\nshows that Merkle proofs are the most significant contributor to the additional\nbandwidth. To address this concern, we also analyze the impact of switching to\nthe more space-efficient Verkle Proofs."
                },
                "authors": [
                    {
                        "name": "Ruben Hias"
                    },
                    {
                        "name": "Weihong Wang"
                    },
                    {
                        "name": "Jan Vanhoof"
                    },
                    {
                        "name": "Tom Van Cutsem"
                    }
                ],
                "author_detail": {
                    "name": "Tom Van Cutsem"
                },
                "author": "Tom Van Cutsem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12018v2",
                "updated": "2024-10-08T04:25:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    8,
                    4,
                    25,
                    41,
                    1,
                    282,
                    0
                ],
                "published": "2024-06-17T18:34:58Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    34,
                    58,
                    0,
                    169,
                    0
                ],
                "title": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CItruS: Chunked Instruction-aware State Eviction for Long Sequence\n  Modeling"
                },
                "summary": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long sequence modeling has gained broad interest as large language models\n(LLMs) continue to advance. Recent research has identified that a large portion\nof hidden states within the key-value caches of Transformer models can be\ndiscarded (also termed evicted) without affecting the perplexity performance in\ngenerating long sequences. However, we show that these methods, despite\npreserving perplexity performance, often drop information that is important for\nsolving downstream tasks, a problem which we call information neglect. To\naddress this issue, we introduce Chunked Instruction-aware State Eviction\n(CItruS), a novel modeling technique that integrates the attention preferences\nuseful for a downstream task into the eviction process of hidden states. In\naddition, we design a method for chunked sequence processing to further improve\nefficiency. Our training-free method exhibits superior performance on long\nsequence comprehension and retrieval tasks over several strong baselines under\nthe same memory budget, while preserving language modeling perplexity. The code\nand data have been released at https://github.com/ybai-nlp/CItruS."
                },
                "authors": [
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Xiyuan Zou"
                    },
                    {
                        "name": "Heyan Huang"
                    },
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Marc-Antoine Rondeau"
                    },
                    {
                        "name": "Yang Gao"
                    },
                    {
                        "name": "Jackie Chi Kit Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Jackie Chi Kit Cheung"
                },
                "author": "Jackie Chi Kit Cheung",
                "arxiv_comment": "EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v1",
                "updated": "2024-10-07T17:59:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers\n  in LLMs"
                },
                "summary": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is essential for deploying Large Language Models (LLMs) by\nenhancing memory efficiency and inference speed. Existing methods for\nactivation quantization mainly address channel-wise outliers, often neglecting\ntoken-wise outliers, leading to reliance on costly per-token dynamic\nquantization. To address this, we introduce PrefixQuant, a novel technique that\nisolates outlier tokens offline without re-training. Specifically, PrefixQuant\nidentifies high-frequency outlier tokens and prefixes them in the KV cache,\npreventing the generation of outlier tokens during inference and simplifying\nquantization. To our knowledge, PrefixQuant is the first to enable efficient\nper-tensor static quantization to outperform expensive per-token dynamic\nquantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and\n4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization\nachieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5\ncommon-sense reasoning tasks, outperforming previous per-token dynamic\nquantization methods like QuaRot with 0.98 perplexity improvement and +5.98\npoints accuracy. Additionally, the inference speed of W4A4 quantized models\nusing PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot\nmodels by 1.2x to 1.3x. Our code is available at\n\\url{https://github.com/ChenMnZ/PrefixQuant}."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "A PTQ method to significantly boost the performance of static\n  activation quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05516v3",
                "updated": "2024-10-07T17:21:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    21,
                    57,
                    0,
                    281,
                    0
                ],
                "published": "2023-12-09T09:55:07Z",
                "published_parsed": [
                    2023,
                    12,
                    9,
                    9,
                    55,
                    7,
                    5,
                    343,
                    0
                ],
                "title": "Stateful Large Language Model Serving with Pensieve",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful Large Language Model Serving with Pensieve"
                },
                "summary": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are wildly popular today and it is important to\nserve them efficiently. Existing LLM serving systems are stateless across\nrequests. Consequently, when LLMs are used in the common setting of multi-turn\nconversations, a growing log of the conversation history must be processed\nalongside any request by the serving system at each turn, resulting in repeated\nprocessing.\n  In this paper, we design $Pensieve$, a system optimized for multi-turn\nconversation LLM serving. $Pensieve$ maintains the conversation state across\nrequests by caching previously processed history to avoid duplicate processing.\n$Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to\nefficiently store and retrieve cached data. $Pensieve$ also generalizes the\nrecent PagedAttention kernel to support attention between multiple input tokens\nwith a GPU cache spread over non-contiguous memory. Our evaluation shows that\n$Pensieve$ can achieve $1.14$-$3.0\\times$ the throughput of vLLM and\nTensorRT-LLM and significantly reduce latency."
                },
                "authors": [
                    {
                        "name": "Lingfan Yu"
                    },
                    {
                        "name": "Jinkun Lin"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "arxiv_doi": "10.1145/3689031.3696086",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3689031.3696086",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.05516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00161v2",
                "updated": "2024-10-07T15:07:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    7,
                    9,
                    0,
                    281,
                    0
                ],
                "published": "2024-09-30T19:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    9,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head"
                },
                "summary": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches."
                },
                "authors": [
                    {
                        "name": "Isaac Rehg"
                    }
                ],
                "author_detail": {
                    "name": "Isaac Rehg"
                },
                "author": "Isaac Rehg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05076v1",
                "updated": "2024-10-07T14:30:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T14:30:27Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    14,
                    30,
                    27,
                    0,
                    281,
                    0
                ],
                "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent\n  Sparse Attention"
                },
                "summary": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have driven significant advancements across\ndiverse NLP tasks, with long-context models gaining prominence for handling\nextended inputs. However, the expanding key-value (KV) cache size required by\nTransformer architectures intensifies the memory constraints, particularly\nduring the decoding phase, creating a significant bottleneck. Existing sparse\nattention mechanisms designed to address this bottleneck have two limitations:\n(1) they often fail to reliably identify the most relevant tokens for\nattention, and (2) they overlook the spatial coherence of token selection\nacross consecutive Transformer layers, which can lead to performance\ndegradation and substantial overhead in token selection. This paper introduces\nTidalDecode, a simple yet effective algorithm and system for fast and accurate\nLLM decoding through position persistent sparse attention. TidalDecode\nleverages the spatial coherence of tokens selected by existing sparse attention\nmethods and introduces a few token selection layers that perform full attention\nto identify the tokens with the highest attention scores, while all other\nlayers perform sparse attention with the pre-selected tokens. This design\nenables TidalDecode to substantially reduce the overhead of token selection for\nsparse attention without sacrificing the quality of the generated results.\nEvaluation on a diverse set of LLMs and tasks shows that TidalDecode closely\nmatches the generative performance of full attention methods while reducing the\nLLM decoding latency by up to 2.1x."
                },
                "authors": [
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Zikun Li"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05033v1",
                "updated": "2024-10-07T13:33:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:33:23Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    33,
                    23,
                    0,
                    281,
                    0
                ],
                "title": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extended Functional Representation Lemma: A Tool For Privacy, Semantic\n  Representation, Caching, and Compression Design"
                },
                "summary": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an overview of a problem in information-theoretic privacy\nmechanism design, addressing two scenarios in which private data is either\nobservable or hidden. In each scenario, different privacy measures are used,\nincluding bounded mutual information and two types of per-letter privacy\nconstraints. Considering the first scenario, an agent observes useful data that\nis correlated with private data, and wants to disclose the useful information\nto a user. Due to the privacy concerns, direct disclosure is prohibited. Hence,\na privacy mechanism is designed to generate disclosed data which maximizes the\nrevealed information about the useful data while satisfying a privacy\nconstraint. In the second scenario, the agent has additionally access to the\nprivate data. We discuss how the Functional Representation Lemma, the Strong\nFunctional Representation Lemma, and their extended versions are useful for\ndesigning low-complexity privacy mechanisms that achieve optimal\nprivacy-utility trade-offs under certain constraints. Furthermore, another\nprivacy design problem is presented where part of the private attribute is more\nprivate than the remaining part. Finally, we provide applications including\nsemantic communications, caching and delivery, and compression designs, where\nthe approach can be applied."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2212.12475",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05004v1",
                "updated": "2024-10-07T13:03:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "published": "2024-10-07T13:03:45Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    3,
                    45,
                    0,
                    281,
                    0
                ],
                "title": "Fast State Restoration in LLM Serving with HCache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast State Restoration in LLM Serving with HCache"
                },
                "summary": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of LLM usage today, e.g., multi-round conversation and\nretrieval-augmented generation (RAG), makes contextual states (i.e., KV cache)\nreusable across user requests. Given the capacity constraints of GPU memory,\nonly a limited number of contexts can be cached on GPU for reusing. Existing\ninference systems typically evict part of the KV cache and restore it by\nrecomputing it from the original tokens or offloading it to host storage for\nlater retrieval, both of which introduce substantial computational or I/O\noverheads. We propose HCache, a novel LLM state restoration method. Its key\nidea is to restore LLM states from intermediate activations and thus utilize\ncomputational and I/O resources with low overhead. We enhance HCache with two\ntechniques, including i) a bubble-free restoration scheduler that integrates\nresource-complementary methods to optimize the balance between computation and\nIO tasks; and ii) a chunk-based storage manager to address the layout mismatch\nissue (i.e., layer-before-token saving versus token-before-layer restoration).\nOur evaluations, conducted using real-world tasks, show that HCache reduces the\nTTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less\nstorage space; compared to token recomputation, HCache achieves up to 5.73X\nreduction in TTFT."
                },
                "authors": [
                    {
                        "name": "Shiwei Gao"
                    },
                    {
                        "name": "Youmin Chen"
                    },
                    {
                        "name": "Jiwu Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwu Shu"
                },
                "author": "Jiwu Shu",
                "arxiv_comment": "EuroSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16406v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16406v3",
                "updated": "2024-10-07T01:27:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    7,
                    1,
                    27,
                    59,
                    0,
                    281,
                    0
                ],
                "published": "2024-05-26T02:15:49Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    2,
                    15,
                    49,
                    6,
                    147,
                    0
                ],
                "title": "SpinQuant: LLM quantization with learned rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpinQuant: LLM quantization with learned rotations"
                },
                "summary": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) techniques applied to weights, activations,\nand the KV cache greatly reduce memory usage, latency, and power consumption of\nLarge Language Models (LLMs), but may lead to large quantization errors when\noutliers are present. Rotating activation or weight matrices helps remove\noutliers and benefits quantization. In this work, we identify a collection of\napplicable rotation parameterizations that lead to identical outputs in\nfull-precision Transformer architectures while enhancing quantization accuracy.\nIn addition, we find that some random rotations lead to much better\nquantization than others, with an up to 13 points difference in downstream\nzero-shot reasoning performance. As a result, we propose SpinQuant, a novel\napproach that incorporates learned rotation matrices for optimal quantized\nnetwork accuracy. With 4-bit quantization of weight, activation, and KV-cache,\nSpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full\nprecision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by\n19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also\noutperforms concurrent work QuaRot, which applies random rotations to remove\noutliers. In particular, for LLaMA-3 8B models that are hard to quantize,\nSpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot."
                },
                "authors": [
                    {
                        "name": "Zechun Liu"
                    },
                    {
                        "name": "Changsheng Zhao"
                    },
                    {
                        "name": "Igor Fedorov"
                    },
                    {
                        "name": "Bilge Soran"
                    },
                    {
                        "name": "Dhruv Choudhary"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Tijmen Blankevoort"
                    }
                ],
                "author_detail": {
                    "name": "Tijmen Blankevoort"
                },
                "author": "Tijmen Blankevoort",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16406v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16406v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v4",
                "updated": "2024-10-06T22:13:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    22,
                    13,
                    16,
                    6,
                    280,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v1",
                "updated": "2024-10-06T19:36:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual\ncalorimeter capable of estimating the energy of incident particles through both\nthe ionization charge and the scintillation light. Our studies show that due to\nthe mechanisms of charge recombination and light generation involved in the\nenergy dissipation in liquid argon, light calorimetry in LArTPCs is inherently\nself-compensating: the missing energy in the hadronic component is compensated\nfor by the extra recombination luminescence compared to the electromagnetic\ncomponent. Good compensation of the electron-to-hadron response ratio (e/h)\naround unity can be achieved across a broad range of drift electric fields from\n0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light\ncalorimetry in LArTPCs, complementing the well-established charge calorimetry.\nUsing GeV neutrinos as a case study, we show that light calorimetry can achieve\nan energy resolution comparable to the more sophisticated charge imaging\ncalorimetry. The synergy between light and charge calorimetry offers a novel\napproach to evaluating and mitigating systematic uncertainties in energy\nmeasurements with LArTPCs."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04252v1",
                "updated": "2024-10-05T18:20:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "published": "2024-10-05T18:20:37Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    18,
                    20,
                    37,
                    5,
                    279,
                    0
                ],
                "title": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lazy Qubit Reordering for Accelerating Parallel State-Vector-based\n  Quantum Circuit Simulation"
                },
                "summary": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes two quantum operation scheduling methods for accelerating\nparallel state-vector-based quantum circuit simulation using multiple graphics\nprocessing units (GPUs). The proposed methods reduce all-to-all communication\ncaused by qubit reordering (QR), which can dominate the overhead of parallel\nsimulation. Our approach eliminates redundant QRs by introducing intentional\ndelays in QR communications such that multiple QRs can be aggregated into a\nsingle QR. The delays are carefully introduced based on the principles of\ntime-space tiling, or a cache optimization technique for classical computers,\nwhich we use to arrange the execution order of quantum operations. Moreover, we\npresent an extended scheduling method for the hierarchical interconnection of\nGPU cluster systems to avoid slow inter-node communication. We develop these\nmethods tailored for two primary procedures in variational quantum eigensolver\n(VQE) simulation: quantum state update (QSU) and expectation value computation\n(EVC). Experimental validation on 32-GPU executions demonstrates acceleration\nin QSU and EVC -- up to 54$\\times$ and 606$\\times$, respectively -- compared to\nexisting methods. Moreover, our extended scheduling method further reduced\ncommunication time by up to 15\\% in a two-layered interconnected cluster\nsystem. Our approach is useful for any quantum circuit simulations, including\nQSU and/or EVC."
                },
                "authors": [
                    {
                        "name": "Yusuke Teranishi"
                    },
                    {
                        "name": "Shoma Hiraoka"
                    },
                    {
                        "name": "Wataru Mizukami"
                    },
                    {
                        "name": "Masao Okita"
                    },
                    {
                        "name": "Fumihiko Ino"
                    }
                ],
                "author_detail": {
                    "name": "Fumihiko Ino"
                },
                "author": "Fumihiko Ino",
                "arxiv_comment": "24 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v1",
                "updated": "2024-10-04T22:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11857v1",
                "updated": "2024-10-04T15:23:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T15:23:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "LLMProxy: Reducing Cost to Access Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMProxy: Reducing Cost to Access Large Language Models"
                },
                "summary": "In this paper, we make a case for a proxy for large language models which has\nexplicit support for cost-saving optimizations. We design LLMProxy, which\nsupports three key optimizations: model selection, context management, and\ncaching. These optimizations present tradeoffs in terms of cost, inference\ntime, and response quality, which applications can navigate through our high\nlevel, bidirectional interface. As a case study, we implement a WhatsApp-based\nQ&A service that uses LLMProxy to provide a rich set of features to the users.\nThis service is deployed on a small scale (100+ users) leveraging the cloud; it\nhas been operational for 15+ weeks and users have asked 1400+ questions so far.\nWe report on the experiences of running this service as well as microbenchmark\nthe specific benefits of the various cost-optimizations we present in this\npaper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we make a case for a proxy for large language models which has\nexplicit support for cost-saving optimizations. We design LLMProxy, which\nsupports three key optimizations: model selection, context management, and\ncaching. These optimizations present tradeoffs in terms of cost, inference\ntime, and response quality, which applications can navigate through our high\nlevel, bidirectional interface. As a case study, we implement a WhatsApp-based\nQ&A service that uses LLMProxy to provide a rich set of features to the users.\nThis service is deployed on a small scale (100+ users) leveraging the cloud; it\nhas been operational for 15+ weeks and users have asked 1400+ questions so far.\nWe report on the experiences of running this service as well as microbenchmark\nthe specific benefits of the various cost-optimizations we present in this\npaper."
                },
                "authors": [
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Swaminathan Lamelas"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v2",
                "updated": "2024-10-04T10:14:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    10,
                    14,
                    17,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Code will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v2",
                "updated": "2024-10-04T07:54:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    7,
                    54,
                    58,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12016v2",
                "updated": "2024-10-04T06:26:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    6,
                    26,
                    20,
                    4,
                    278,
                    0
                ],
                "published": "2024-06-17T18:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    18,
                    33,
                    44,
                    0,
                    169,
                    0
                ],
                "title": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefixing Attention Sinks can Mitigate Activation Outliers for Large\n  Language Model Quantization"
                },
                "summary": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advances in LLM quantization, activation quantization remains\nto be challenging due to the activation outliers. Conventional remedies, e.g.,\nmixing precisions for different channels, introduce extra overhead and reduce\nthe speedup. In this work, we develop a simple yet effective strategy to\nfacilitate per-tensor activation quantization by preventing the generation of\nproblematic tokens. Precisely, we propose a method to find a set of key-value\ncache, coined CushionCache, which mitigates outliers in subsequent tokens when\ninserted as a prefix. CushionCache works in two steps: First, we greedily\nsearch for a prompt token sequence that minimizes the maximum activation values\nin subsequent tokens. Then, we further tune the token cache to regularize the\nactivations of subsequent tokens to be more quantization-friendly. The proposed\nmethod successfully addresses activation outliers of LLMs, providing a\nsubstantial performance boost for per-tensor activation quantization methods.\nWe thoroughly evaluate our method over a wide range of models and benchmarks\nand find that it significantly surpasses the established baseline of per-tensor\nW8A8 quantization and can be seamlessly integrated with the recent activation\nquantization method."
                },
                "authors": [
                    {
                        "name": "Seungwoo Son"
                    },
                    {
                        "name": "Wonpyo Park"
                    },
                    {
                        "name": "Woohyun Han"
                    },
                    {
                        "name": "Kyuyeun Kim"
                    },
                    {
                        "name": "Jaeho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jaeho Lee"
                },
                "author": "Jaeho Lee",
                "arxiv_comment": "EMNLP 2024 Main (Long)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03111v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03111v1",
                "updated": "2024-10-04T03:10:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T03:10:53Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    3,
                    10,
                    53,
                    4,
                    278,
                    0
                ],
                "title": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy"
                },
                "summary": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance."
                },
                "authors": [
                    {
                        "name": "Rongzhi Zhang"
                    },
                    {
                        "name": "Kuang Wang"
                    },
                    {
                        "name": "Liyuan Liu"
                    },
                    {
                        "name": "Shuohang Wang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yelong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yelong Shen"
                },
                "author": "Yelong Shen",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03111v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v1",
                "updated": "2024-10-04T02:32:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large\n  Language Model Inference"
                },
                "summary": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) is challenging due to their high\nmemory and computational demands, especially during long-context inference.\nWhile key-value (KV) caching accelerates inference by reusing previously\ncomputed keys and values, it also introduces significant memory overhead.\nExisting KV cache compression methods such as eviction and merging typically\ncompress the KV cache after it is generated and overlook the eviction of hidden\nstates, failing to improve the speed of the prefilling stage. Additionally,\napplying a uniform compression rate across different attention heads can harm\ncrucial retrieval heads in needle-in-a-haystack tasks due to excessive\ncompression. In this paper, we propose UNComp, an uncertainty-aware compression\nscheme that leverages matrix entropy to estimate model uncertainty across\nlayers and heads at the token sequence level. By grouping layers and heads\nbased on their uncertainty, UNComp adaptively compresses both the hidden states\nand the KV cache. Our method achieves a 1.6x speedup in the prefilling stage\nand reduces the KV cache to 4.74% of its original size, resulting in a 6.4x\nincrease in throughput and a 1.4x speedup in inference with only a 1.41%\nperformance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms\nthe full-size KV cache even when compressed to 9.38% of its original size. Our\napproach offers an efficient, training-free Grouped-Query Attention paradigm\nthat can be seamlessly integrated into existing KV cache schemes."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03065v1",
                "updated": "2024-10-04T01:11:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "published": "2024-10-04T01:11:09Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    1,
                    11,
                    9,
                    4,
                    278,
                    0
                ],
                "title": "Compute Or Load KV Cache? Why Not Both?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Or Load KV Cache? Why Not Both?"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nincreased context window sizes, enabling sophisticated applications but also\nintroducing substantial computational overheads, particularly computing\nkey-value (KV) cache in the prefill stage. Prefix caching has emerged to save\nGPU power in this scenario, which saves KV cache at disks and reuse them across\nmultiple queries. However, traditional prefix caching mechanisms often suffer\nfrom substantial latency because the speed of loading KV cache from disks to\nGPU memory is bottlenecked by the throughput of I/O devices. To optimize the\nlatency of long-context prefill, we propose Cake, a novel KV cache loader,\nwhich employs a bidirectional parallelized KV cache generation strategy. Upon\nreceiving a prefill task, Cake simultaneously and dynamically loads saved KV\ncache from prefix cache locations and computes KV cache on local GPUs,\nmaximizing the utilization of available computation and I/O bandwidth\nresources. Additionally, Cake automatically adapts to diverse system statuses\nwithout manual parameter. tuning. In experiments on various prompt datasets,\nGPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT)\nreduction compare with compute-only method and 94.6% TTFT reduction compare\nwith I/O-only method."
                },
                "authors": [
                    {
                        "name": "Shuowei Jin"
                    },
                    {
                        "name": "Xueshen Liu"
                    },
                    {
                        "name": "Qingzhao Zhang"
                    },
                    {
                        "name": "Z. Morley Mao"
                    }
                ],
                "author_detail": {
                    "name": "Z. Morley Mao"
                },
                "author": "Z. Morley Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00242v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00242v3",
                "updated": "2024-10-03T22:17:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    17,
                    1,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-30T04:34:54Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    4,
                    34,
                    54,
                    5,
                    90,
                    0
                ],
                "title": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured\n  LLM Inference"
                },
                "summary": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly employed for complex tasks that\nprocess multiple generation calls in a tree structure with shared prefixes of\ntokens, including few-shot prompting, multi-step reasoning, speculative\ndecoding, etc. However, existing inference systems for tree-based applications\nare inefficient due to improper partitioning of queries and KV cache during\nattention calculation. This leads to two main issues: (1) a lack of memory\naccess (IO) reuse for KV cache of shared prefixes, and (2) poor load\nbalancing.As a result, there is redundant KV cache IO between GPU global memory\nand shared memory, along with low GPU utilization. To address these challenges,\nwe propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient\nattention algorithm with prefix-aware and load-balanced KV cache partitions.\nDeFT reduces the number of read/write operations of KV cache during attention\ncalculation through KV-Guided Grouping, a method that avoids repeatedly loading\nKV cache of shared prefixes in attention computation. Additionally, we propose\nFlattened Tree KV Splitting, a mechanism that ensures even distribution of the\nKV cache across partitions with little computation redundancy, enhancing GPU\nutilization during attention computations. By reducing 73-99 KV cache IO and\nnearly 100 IO for partial results during attention calculation, DeFT achieves\nup to 2.52/3.82x speedup in the end-to-end/attention latency across three\npractical tree-based workloads compared to state-of-the-art attention\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Jinwei Yao"
                    },
                    {
                        "name": "Kaiqi Chen"
                    },
                    {
                        "name": "Kexun Zhang"
                    },
                    {
                        "name": "Jiaxuan You"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Zeke Wang"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "Update DeFT-v3 with more ablation studies. DeFT-v1 was accepted by\n  ICLR'24 AGI Workshop ( https://openreview.net/forum?id=HqfLHoX8bR ). Code\n  will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00242v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00242v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v2",
                "updated": "2024-10-03T22:11:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    22,
                    11,
                    19,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02751v1",
                "updated": "2024-10-03T17:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "title": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI"
                },
                "summary": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic"
                },
                "authors": [
                    {
                        "name": "Ahmad Elawady"
                    },
                    {
                        "name": "Gunjan Chhablani"
                    },
                    {
                        "name": "Ram Ramrakhya"
                    },
                    {
                        "name": "Karmesh Yadav"
                    },
                    {
                        "name": "Dhruv Batra"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Andrew Szot"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Szot"
                },
                "author": "Andrew Szot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00023v2",
                "updated": "2024-10-03T17:50:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    50,
                    33,
                    3,
                    277,
                    0
                ],
                "published": "2024-05-08T06:30:58Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    6,
                    30,
                    58,
                    2,
                    129,
                    0
                ],
                "title": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving"
                },
                "summary": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency."
                },
                "authors": [
                    {
                        "name": "Vikranth Srivatsa"
                    },
                    {
                        "name": "Zijian He"
                    },
                    {
                        "name": "Reyna Abhyankar"
                    },
                    {
                        "name": "Dongming Li"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02599v1",
                "updated": "2024-10-03T15:41:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:41:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing"
                },
                "summary": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02527v1",
                "updated": "2024-10-03T14:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:35:35Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "title": "Learning from Offline Foundation Features with Tensor Augmentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Offline Foundation Features with Tensor Augmentations"
                },
                "summary": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model."
                },
                "authors": [
                    {
                        "name": "Emir Konuk"
                    },
                    {
                        "name": "Christos Matsoukas"
                    },
                    {
                        "name": "Moein Sorkhei"
                    },
                    {
                        "name": "Phitchapha Lertsiravaramet"
                    },
                    {
                        "name": "Kevin Smith"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Smith"
                },
                "author": "Kevin Smith",
                "arxiv_comment": "Accepted to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v2",
                "updated": "2024-10-03T11:47:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    11,
                    47,
                    21,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v3",
                "updated": "2024-10-03T08:46:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    46,
                    42,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v2",
                "updated": "2024-10-03T03:03:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    3,
                    3,
                    29,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v1",
                "updated": "2024-10-02T17:59:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads"
                },
                "summary": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v1",
                "updated": "2024-10-02T17:14:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v1",
                "updated": "2024-10-02T15:22:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill: a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from linear to square root\nrelative to the context length. Additionally, FutureFill requires a prefill\ncache sized only by the number of tokens generated, which is smaller than the\ncache requirements for standard convolutional and attention-based models. We\nvalidate our theoretical findings with experimental evidence demonstrating\ncorrectness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01518v1",
                "updated": "2024-10-02T13:09:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T13:09:41Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "title": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs"
                },
                "summary": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v1",
                "updated": "2024-10-02T12:35:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12335v2",
                "updated": "2024-10-02T00:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    0,
                    19,
                    13,
                    2,
                    276,
                    0
                ],
                "published": "2024-06-18T07:01:11Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    7,
                    1,
                    11,
                    1,
                    170,
                    0
                ],
                "title": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters"
                },
                "summary": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhiyu Guo"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "Accepted at EMNLP 2024 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00644v1",
                "updated": "2024-10-01T12:55:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T12:55:47Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "title": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines"
                },
                "summary": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs."
                },
                "authors": [
                    {
                        "name": "Francesco Quaglia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Quaglia"
                },
                "author": "Francesco Quaglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00455v1",
                "updated": "2024-10-01T07:19:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T07:19:21Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "title": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache"
                },
                "summary": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes."
                },
                "authors": [
                    {
                        "name": "Jin Zhang"
                    },
                    {
                        "name": "Jincheng Zhou"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Di Ma"
                    },
                    {
                        "name": "Chunye Gong"
                    }
                ],
                "author_detail": {
                    "name": "Chunye Gong"
                },
                "author": "Chunye Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00359v1",
                "updated": "2024-10-01T03:14:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T03:14:12Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "title": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness"
                },
                "summary": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models."
                },
                "authors": [
                    {
                        "name": "Xiao Peng"
                    },
                    {
                        "name": "Xufan Geng"
                    }
                ],
                "author_detail": {
                    "name": "Xufan Geng"
                },
                "author": "Xufan Geng",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v4",
                "updated": "2024-09-30T22:44:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    44,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.09166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.09166v2",
                "updated": "2024-09-30T18:23:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    23,
                    7,
                    0,
                    274,
                    0
                ],
                "published": "2022-09-19T16:35:28Z",
                "published_parsed": [
                    2022,
                    9,
                    19,
                    16,
                    35,
                    28,
                    0,
                    262,
                    0
                ],
                "title": "Cache-Oblivious Representation of B-Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Oblivious Representation of B-Tree Structures"
                },
                "summary": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor."
                },
                "authors": [
                    {
                        "name": "Luk Ondrek"
                    },
                    {
                        "name": "Ondej Mika"
                    }
                ],
                "author_detail": {
                    "name": "Ondej Mika"
                },
                "author": "Ondej Mika",
                "arxiv_comment": "30 pages + 7 pages of algorithms, 9 figures; changes: paper structure\n  improved, general (sub)tree (re)build added, DFS alg. simplified, build\n  complexity lowered,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.09166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.09166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v1",
                "updated": "2024-09-30T15:53:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages"
                },
                "summary": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability"
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.14677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14677v1",
                "updated": "2024-10-18T17:59:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    57,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:59:57Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    57,
                    4,
                    292,
                    0
                ],
                "title": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts"
                },
                "summary": "The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld."
                },
                "authors": [
                    {
                        "name": "German Gritsai"
                    },
                    {
                        "name": "Anastasia Voznyuk"
                    },
                    {
                        "name": "Andrey Grabovoy"
                    },
                    {
                        "name": "Yury Chekhovich"
                    }
                ],
                "author_detail": {
                    "name": "Yury Chekhovich"
                },
                "author": "Yury Chekhovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14676v1",
                "updated": "2024-10-18T17:59:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    51,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:59:51Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    51,
                    4,
                    292,
                    0
                ],
                "title": "SudoLM: Learning Access Control of Parametric Knowledge with\n  Authorization Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SudoLM: Learning Access Control of Parametric Knowledge with\n  Authorization Alignment"
                },
                "summary": "Existing preference alignment is a one-size-fits-all alignment mechanism,\nwhere the part of the large language model (LLM) parametric knowledge with\nnon-preferred features is uniformly blocked to all the users. However, this\npart of knowledge can be useful to advanced users whose expertise qualifies\nthem to handle these information. The one-size-fits-all alignment mechanism\nundermines LLM's utility for these qualified users. To address this problem, we\npropose SudoLM, a framework that lets LLMs learn access control over specific\nparametric knowledge for users with different credentials via authorization\nalignment. SudoLM allows authorized users to unlock their access to all the\nparametric knowledge with an assigned SUDO key while blocking access to\nnon-qualified users. Experiments on two application scenarios demonstrate that\nSudoLM effectively controls the user's access to the parametric knowledge and\nmaintains its general utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing preference alignment is a one-size-fits-all alignment mechanism,\nwhere the part of the large language model (LLM) parametric knowledge with\nnon-preferred features is uniformly blocked to all the users. However, this\npart of knowledge can be useful to advanced users whose expertise qualifies\nthem to handle these information. The one-size-fits-all alignment mechanism\nundermines LLM's utility for these qualified users. To address this problem, we\npropose SudoLM, a framework that lets LLMs learn access control over specific\nparametric knowledge for users with different credentials via authorization\nalignment. SudoLM allows authorized users to unlock their access to all the\nparametric knowledge with an assigned SUDO key while blocking access to\nnon-qualified users. Experiments on two application scenarios demonstrate that\nSudoLM effectively controls the user's access to the parametric knowledge and\nmaintains its general utility."
                },
                "authors": [
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Muhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Muhao Chen"
                },
                "author": "Muhao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14675v1",
                "updated": "2024-10-18T17:59:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    47,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:59:47Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    47,
                    4,
                    292,
                    0
                ],
                "title": "Enhancing Large Language Models' Situated Faithfulness to External\n  Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Models' Situated Faithfulness to External\n  Contexts"
                },
                "summary": "Large Language Models (LLMs) are often augmented with external information as\ncontexts, but this external information can sometimes be inaccurate or even\nintentionally misleading. We argue that robust LLMs should demonstrate situated\nfaithfulness, dynamically calibrating their trust in external information based\non their confidence in the internal knowledge and the external context. To\nbenchmark this capability, we evaluate LLMs across several QA datasets,\nincluding a newly created dataset called RedditQA featuring in-the-wild\nincorrect contexts sourced from Reddit posts. We show that when provided with\nboth correct and incorrect contexts, both open-source and proprietary models\ntend to overly rely on external information, regardless of its factual\naccuracy. To enhance situated faithfulness, we propose two approaches:\nSelf-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning\n(RCR). SCR enables models to self-access the confidence of external information\nrelative to their own internal knowledge to produce the most accurate answer.\nRCR, in contrast, extracts explicit confidence signals from the LLM and\ndetermines the final answer using predefined rules. Our results show that for\nLLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR\noutperforms RCR, achieving improvements of up to 24.2% over a direct input\naugmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR\noutperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct\nPreference Optimization (CR-DPO) method improves performance on both seen and\nunseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In\naddition to quantitative results, we offer insights into the relative strengths\nof SCR and RCR. Our findings highlight promising avenues for improving situated\nfaithfulness in LLMs. The data and code are released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are often augmented with external information as\ncontexts, but this external information can sometimes be inaccurate or even\nintentionally misleading. We argue that robust LLMs should demonstrate situated\nfaithfulness, dynamically calibrating their trust in external information based\non their confidence in the internal knowledge and the external context. To\nbenchmark this capability, we evaluate LLMs across several QA datasets,\nincluding a newly created dataset called RedditQA featuring in-the-wild\nincorrect contexts sourced from Reddit posts. We show that when provided with\nboth correct and incorrect contexts, both open-source and proprietary models\ntend to overly rely on external information, regardless of its factual\naccuracy. To enhance situated faithfulness, we propose two approaches:\nSelf-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning\n(RCR). SCR enables models to self-access the confidence of external information\nrelative to their own internal knowledge to produce the most accurate answer.\nRCR, in contrast, extracts explicit confidence signals from the LLM and\ndetermines the final answer using predefined rules. Our results show that for\nLLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR\noutperforms RCR, achieving improvements of up to 24.2% over a direct input\naugmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR\noutperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct\nPreference Optimization (CR-DPO) method improves performance on both seen and\nunseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In\naddition to quantitative results, we offer insights into the relative strengths\nof SCR and RCR. Our findings highlight promising avenues for improving situated\nfaithfulness in LLMs. The data and code are released."
                },
                "authors": [
                    {
                        "name": "Yukun Huang"
                    },
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Hongyi Cai"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    }
                ],
                "author_detail": {
                    "name": "Bhuwan Dhingra"
                },
                "author": "Bhuwan Dhingra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14674v1",
                "updated": "2024-10-18T17:59:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    44,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:59:44Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    44,
                    4,
                    292,
                    0
                ],
                "title": "Effects of waveform systematics on inferences of neutron star population\n  properties and the nuclear equation of state",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effects of waveform systematics on inferences of neutron star population\n  properties and the nuclear equation of state"
                },
                "summary": "Gravitational waves from inspiralling neutron stars carry information about\nmatter at extreme gravity and density. The binary neutron star (BNS) event\nGW170817 provided, for the first time, insight into dense matter through this\nwindow. Since then, another BNS (GW190425) and several neutron star-black hole\nevents have been detected, although the tidal measurements were not expected to\nbe well-constrained from them. Collective information regarding the behavior of\nnuclear matter at extreme densities can be done by performing a joint\npopulation inference for the masses, spins, and equation-of-state [1] to enable\nbetter understanding. This population inference, in turn, relies on accurate\nestimates of intrinsic parameters of individual events. In this study, we\ninvestigate how the differences in parameter inference of BNS events using\ndifferent waveform models can affect the eventual inference of the nuclear\nequation-of-state. We use the state-of-the-art model TEOBResumS with IMRPhenomD\nNRTidalv2 as a comparison model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves from inspiralling neutron stars carry information about\nmatter at extreme gravity and density. The binary neutron star (BNS) event\nGW170817 provided, for the first time, insight into dense matter through this\nwindow. Since then, another BNS (GW190425) and several neutron star-black hole\nevents have been detected, although the tidal measurements were not expected to\nbe well-constrained from them. Collective information regarding the behavior of\nnuclear matter at extreme densities can be done by performing a joint\npopulation inference for the masses, spins, and equation-of-state [1] to enable\nbetter understanding. This population inference, in turn, relies on accurate\nestimates of intrinsic parameters of individual events. In this study, we\ninvestigate how the differences in parameter inference of BNS events using\ndifferent waveform models can affect the eventual inference of the nuclear\nequation-of-state. We use the state-of-the-art model TEOBResumS with IMRPhenomD\nNRTidalv2 as a comparison model."
                },
                "authors": [
                    {
                        "name": "Anjali B. Yelikar"
                    },
                    {
                        "name": "Richard O'Shaughnessy"
                    },
                    {
                        "name": "Daniel Wysocki"
                    },
                    {
                        "name": "Leslie Wade"
                    }
                ],
                "author_detail": {
                    "name": "Leslie Wade"
                },
                "author": "Leslie Wade",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14670v1",
                "updated": "2024-10-18T17:58:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    58,
                    53,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:58:53Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    58,
                    53,
                    4,
                    292,
                    0
                ],
                "title": "Decomposing The Dark Matter of Sparse Autoencoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposing The Dark Matter of Sparse Autoencoders"
                },
                "summary": "Sparse autoencoders (SAEs) are a promising technique for decomposing language\nmodel activations into interpretable linear features. However, current SAEs\nfall short of completely explaining model performance, resulting in \"dark\nmatter\": unexplained variance in activations. This work investigates dark\nmatter as an object of study in its own right. Surprisingly, we find that much\nof SAE dark matter--about half of the error vector itself and >90% of its\nnorm--can be linearly predicted from the initial activation vector.\nAdditionally, we find that the scaling behavior of SAE error norms at a per\ntoken level is remarkably predictable: larger SAEs mostly struggle to\nreconstruct the same contexts as smaller SAEs. We build on the linear\nrepresentation hypothesis to propose models of activations that might lead to\nthese observations, including postulating a new type of \"introduced error\";\nthese insights imply that the part of the SAE error vector that cannot be\nlinearly predicted (\"nonlinear\" error) might be fundamentally different from\nthe linearly predictable component. To validate this hypothesis, we empirically\nanalyze nonlinear SAE error and show that 1) it contains fewer not yet learned\nfeatures, 2) SAEs trained on it are quantitatively worse, 3) it helps predict\nSAE per-token scaling behavior, and 4) it is responsible for a proportional\namount of the downstream increase in cross entropy loss when SAE activations\nare inserted into the model. Finally, we examine two methods to reduce\nnonlinear SAE error at a fixed sparsity: inference time gradient pursuit, which\nleads to a very slight decrease in nonlinear error, and linear transformations\nfrom earlier layer SAE outputs, which leads to a larger reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse autoencoders (SAEs) are a promising technique for decomposing language\nmodel activations into interpretable linear features. However, current SAEs\nfall short of completely explaining model performance, resulting in \"dark\nmatter\": unexplained variance in activations. This work investigates dark\nmatter as an object of study in its own right. Surprisingly, we find that much\nof SAE dark matter--about half of the error vector itself and >90% of its\nnorm--can be linearly predicted from the initial activation vector.\nAdditionally, we find that the scaling behavior of SAE error norms at a per\ntoken level is remarkably predictable: larger SAEs mostly struggle to\nreconstruct the same contexts as smaller SAEs. We build on the linear\nrepresentation hypothesis to propose models of activations that might lead to\nthese observations, including postulating a new type of \"introduced error\";\nthese insights imply that the part of the SAE error vector that cannot be\nlinearly predicted (\"nonlinear\" error) might be fundamentally different from\nthe linearly predictable component. To validate this hypothesis, we empirically\nanalyze nonlinear SAE error and show that 1) it contains fewer not yet learned\nfeatures, 2) SAEs trained on it are quantitatively worse, 3) it helps predict\nSAE per-token scaling behavior, and 4) it is responsible for a proportional\namount of the downstream increase in cross entropy loss when SAE activations\nare inserted into the model. Finally, we examine two methods to reduce\nnonlinear SAE error at a fixed sparsity: inference time gradient pursuit, which\nleads to a very slight decrease in nonlinear error, and linear transformations\nfrom earlier layer SAE outputs, which leads to a larger reduction."
                },
                "authors": [
                    {
                        "name": "Joshua Engels"
                    },
                    {
                        "name": "Logan Riggs"
                    },
                    {
                        "name": "Max Tegmark"
                    }
                ],
                "author_detail": {
                    "name": "Max Tegmark"
                },
                "author": "Max Tegmark",
                "arxiv_comment": "Code at https://github.com/JoshEngels/SAE-Dark-Matter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06331v2",
                "updated": "2024-10-18T17:53:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    53,
                    46,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-08T20:12:11Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    20,
                    12,
                    11,
                    1,
                    282,
                    0
                ],
                "title": "Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing"
                },
                "summary": "The locate-then-edit paradigm has shown significant promise for knowledge\nediting (KE) in Large Language Models (LLMs). While previous methods perform\nwell on single-hop fact recall tasks, they consistently struggle with multi-hop\nfactual recall tasks involving newly edited knowledge. In this paper,\nleveraging tools in mechanistic interpretability, we first identify that in\nmulti-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper\nMLP layers, unlike single-hop tasks, which rely on earlier layers. This\ndistinction explains the poor performance of current methods in multi-hop\nqueries, as they primarily focus on editing shallow layers, leaving deeper\nlayers unchanged. To address this, we propose IFMET, a novel locate-then-edit\nKE approach designed to edit both shallow and deep MLP layers. IFMET employs\nmulti-hop editing prompts and supplementary sets to locate and modify knowledge\nacross different reasoning stages. Experimental results demonstrate that IFMET\nsignificantly improves performance on multi-hop factual recall tasks,\neffectively overcoming the limitations of previous locate-then-edit methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The locate-then-edit paradigm has shown significant promise for knowledge\nediting (KE) in Large Language Models (LLMs). While previous methods perform\nwell on single-hop fact recall tasks, they consistently struggle with multi-hop\nfactual recall tasks involving newly edited knowledge. In this paper,\nleveraging tools in mechanistic interpretability, we first identify that in\nmulti-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper\nMLP layers, unlike single-hop tasks, which rely on earlier layers. This\ndistinction explains the poor performance of current methods in multi-hop\nqueries, as they primarily focus on editing shallow layers, leaving deeper\nlayers unchanged. To address this, we propose IFMET, a novel locate-then-edit\nKE approach designed to edit both shallow and deep MLP layers. IFMET employs\nmulti-hop editing prompts and supplementary sets to locate and modify knowledge\nacross different reasoning stages. Experimental results demonstrate that IFMET\nsignificantly improves performance on multi-hop factual recall tasks,\neffectively overcoming the limitations of previous locate-then-edit methods."
                },
                "authors": [
                    {
                        "name": "Zhuoran Zhang"
                    },
                    {
                        "name": "Yongxiang Li"
                    },
                    {
                        "name": "Zijian Kan"
                    },
                    {
                        "name": "Keyuan Cheng"
                    },
                    {
                        "name": "Lijie Hu"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14660v1",
                "updated": "2024-10-18T17:51:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    51,
                    51,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:51:51Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    51,
                    51,
                    4,
                    292,
                    0
                ],
                "title": "A Large Language Model-Driven Reward Design Framework via Dynamic\n  Feedback for Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model-Driven Reward Design Framework via Dynamic\n  Feedback for Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have shown significant potential in designing\nreward functions for Reinforcement Learning (RL) tasks. However, obtaining\nhigh-quality reward code often involves human intervention, numerous LLM\nqueries, or repetitive RL training. To address these issues, we propose CARD, a\nLLM-driven Reward Design framework that iteratively generates and improves\nreward function code. Specifically, CARD includes a Coder that generates and\nverifies the code, while a Evaluator provides dynamic feedback to guide the\nCoder in improving the code, eliminating the need for human feedback. In\naddition to process feedback and trajectory feedback, we introduce Trajectory\nPreference Evaluation (TPE), which evaluates the current reward function based\non trajectory preferences. If the code fails the TPE, the Evaluator provides\npreference feedback, avoiding RL training at every iteration and making the\nreward function better aligned with the task objective. Empirical results on\nMeta-World and ManiSkill2 demonstrate that our method achieves an effective\nbalance between task performance and token efficiency, outperforming or\nmatching the baselines across all tasks. On 10 out of 12 tasks, CARD shows\nbetter or comparable performance to policies trained with expert-designed\nrewards, and our method even surpasses the oracle on 3 tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant potential in designing\nreward functions for Reinforcement Learning (RL) tasks. However, obtaining\nhigh-quality reward code often involves human intervention, numerous LLM\nqueries, or repetitive RL training. To address these issues, we propose CARD, a\nLLM-driven Reward Design framework that iteratively generates and improves\nreward function code. Specifically, CARD includes a Coder that generates and\nverifies the code, while a Evaluator provides dynamic feedback to guide the\nCoder in improving the code, eliminating the need for human feedback. In\naddition to process feedback and trajectory feedback, we introduce Trajectory\nPreference Evaluation (TPE), which evaluates the current reward function based\non trajectory preferences. If the code fails the TPE, the Evaluator provides\npreference feedback, avoiding RL training at every iteration and making the\nreward function better aligned with the task objective. Empirical results on\nMeta-World and ManiSkill2 demonstrate that our method achieves an effective\nbalance between task performance and token efficiency, outperforming or\nmatching the baselines across all tasks. On 10 out of 12 tasks, CARD shows\nbetter or comparable performance to policies trained with expert-designed\nrewards, and our method even surpasses the oracle on 3 tasks."
                },
                "authors": [
                    {
                        "name": "Shengjie Sun"
                    },
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Jiafei Lyu"
                    },
                    {
                        "name": "Jing-Wen Yang"
                    },
                    {
                        "name": "Liangpeng Zhang"
                    },
                    {
                        "name": "Xiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiu Li"
                },
                "author": "Xiu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14655v1",
                "updated": "2024-10-18T17:48:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    48,
                    27,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:48:27Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    48,
                    27,
                    4,
                    292,
                    0
                ],
                "title": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated\n  Tokens"
                },
                "summary": "Language models are often trained to maximize the likelihood of the next\ntoken given past tokens in the training dataset. However, during inference\ntime, they are utilized differently, generating text sequentially and\nauto-regressively by using previously generated tokens as input to predict the\nnext one. Marginal differences in predictions at each step can cascade over\nsuccessive steps, resulting in different distributions from what the models\nwere trained for and potentially leading to unpredictable behavior. This paper\nproposes two simple approaches based on model own generation to address this\ndiscrepancy between the training and inference time. Our first approach is\nBatch-Scheduled Sampling, where, during training, we stochastically choose\nbetween the ground-truth token from the dataset and the model's own generated\ntoken as input to predict the next token. This is done in an offline manner,\nmodifying the context window by interleaving ground-truth tokens with those\ngenerated by the model. Our second approach is Reference-Answer-based\nCorrection, where we explicitly incorporate a self-correction capability into\nthe model during training. This enables the model to effectively self-correct\nthe gaps between the generated sequences and the ground truth data without\nrelying on an external oracle model. By incorporating our proposed strategies\nduring training, we have observed an overall improvement in performance\ncompared to baseline methods, as demonstrated by our extensive experiments\nusing summarization, general question-answering, and math question-answering\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are often trained to maximize the likelihood of the next\ntoken given past tokens in the training dataset. However, during inference\ntime, they are utilized differently, generating text sequentially and\nauto-regressively by using previously generated tokens as input to predict the\nnext one. Marginal differences in predictions at each step can cascade over\nsuccessive steps, resulting in different distributions from what the models\nwere trained for and potentially leading to unpredictable behavior. This paper\nproposes two simple approaches based on model own generation to address this\ndiscrepancy between the training and inference time. Our first approach is\nBatch-Scheduled Sampling, where, during training, we stochastically choose\nbetween the ground-truth token from the dataset and the model's own generated\ntoken as input to predict the next token. This is done in an offline manner,\nmodifying the context window by interleaving ground-truth tokens with those\ngenerated by the model. Our second approach is Reference-Answer-based\nCorrection, where we explicitly incorporate a self-correction capability into\nthe model during training. This enables the model to effectively self-correct\nthe gaps between the generated sequences and the ground truth data without\nrelying on an external oracle model. By incorporating our proposed strategies\nduring training, we have observed an overall improvement in performance\ncompared to baseline methods, as demonstrated by our extensive experiments\nusing summarization, general question-answering, and math question-answering\ntasks."
                },
                "authors": [
                    {
                        "name": "Zhepeng Cen"
                    },
                    {
                        "name": "Yao Liu"
                    },
                    {
                        "name": "Siliang Zeng"
                    },
                    {
                        "name": "Pratik Chaudhar"
                    },
                    {
                        "name": "Huzefa Rangwala"
                    },
                    {
                        "name": "George Karypis"
                    },
                    {
                        "name": "Rasool Fakoor"
                    }
                ],
                "author_detail": {
                    "name": "Rasool Fakoor"
                },
                "author": "Rasool Fakoor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14651v1",
                "updated": "2024-10-18T17:47:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    47,
                    11,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:47:11Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    47,
                    11,
                    4,
                    292,
                    0
                ],
                "title": "Real-time Fake News from Adversarial Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Fake News from Adversarial Feedback"
                },
                "summary": "We show that existing evaluations for fake news detection based on\nconventional sources, such as claims on fact-checking websites, result in an\nincreasing accuracy over time for LLM-based detectors -- even after their\nknowledge cutoffs. This suggests that recent popular political claims, which\nform the majority of fake news on such sources, are easily classified using\nsurface-level shallow patterns. Instead, we argue that a proper fake news\ndetection dataset should test a model's ability to reason factually about the\ncurrent world by retrieving and reading related evidence. To this end, we\ndevelop a novel pipeline that leverages natural language feedback from a\nRAG-based detector to iteratively modify real-time news into deceptive fake\nnews that challenges LLMs. Our iterative rewrite decreases the binary\nclassification AUC by an absolute 17.5 percent for a strong RAG GPT-4o\ndetector. Our experiments reveal the important role of RAG in both detecting\nand generating fake news, as retrieval-free LLM detectors are vulnerable to\nunseen events and adversarial attacks, while feedback from RAG detection helps\ndiscover more deceitful patterns in fake news.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that existing evaluations for fake news detection based on\nconventional sources, such as claims on fact-checking websites, result in an\nincreasing accuracy over time for LLM-based detectors -- even after their\nknowledge cutoffs. This suggests that recent popular political claims, which\nform the majority of fake news on such sources, are easily classified using\nsurface-level shallow patterns. Instead, we argue that a proper fake news\ndetection dataset should test a model's ability to reason factually about the\ncurrent world by retrieving and reading related evidence. To this end, we\ndevelop a novel pipeline that leverages natural language feedback from a\nRAG-based detector to iteratively modify real-time news into deceptive fake\nnews that challenges LLMs. Our iterative rewrite decreases the binary\nclassification AUC by an absolute 17.5 percent for a strong RAG GPT-4o\ndetector. Our experiments reveal the important role of RAG in both detecting\nand generating fake news, as retrieval-free LLM detectors are vulnerable to\nunseen events and adversarial attacks, while feedback from RAG detection helps\ndiscover more deceitful patterns in fake news."
                },
                "authors": [
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Yukun Huang"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    }
                ],
                "author_detail": {
                    "name": "Bhuwan Dhingra"
                },
                "author": "Bhuwan Dhingra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14649v1",
                "updated": "2024-10-18T17:46:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    46,
                    37,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:46:37Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    46,
                    37,
                    4,
                    292,
                    0
                ],
                "title": "EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary\n  Search"
                },
                "summary": "The high computational costs of large language models (LLMs) have led to a\nflurry of research on LLM compression, via methods such as quantization,\nsparsification, or structured pruning. A new frontier in this area is given by\n\\emph{dynamic, non-uniform} compression methods, which adjust the compression\nlevels (e.g., sparsity) per-block or even per-layer in order to minimize\naccuracy loss, while guaranteeing a global compression threshold. Yet, current\nmethods rely on heuristics for identifying the \"importance\" of a given layer\ntowards the loss, based on assumptions such as \\emph{error monotonicity}, i.e.\nthat the end-to-end model compression error is proportional to the sum of\nlayer-wise errors. In this paper, we revisit this area, and propose a new and\ngeneral approach for dynamic compression that is provably optimal in a given\ninput range. We begin from the motivating observation that, in general,\n\\emph{error monotonicity does not hold for LLMs}: compressed models with lower\nsum of per-layer errors can perform \\emph{worse} than models with higher error\nsums. To address this, we propose a new general evolutionary framework for\ndynamic LLM compression called EvoPress, which has provable convergence, and\nlow sample and evaluation complexity. We show that these theoretical guarantees\nlead to highly competitive practical performance for dynamic compression of\nLlama, Mistral and Phi models. Via EvoPress, we set new state-of-the-art\nresults across all compression approaches: structural pruning (block/layer\ndropping), unstructured sparsity, as well as quantization with dynamic\nbitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high computational costs of large language models (LLMs) have led to a\nflurry of research on LLM compression, via methods such as quantization,\nsparsification, or structured pruning. A new frontier in this area is given by\n\\emph{dynamic, non-uniform} compression methods, which adjust the compression\nlevels (e.g., sparsity) per-block or even per-layer in order to minimize\naccuracy loss, while guaranteeing a global compression threshold. Yet, current\nmethods rely on heuristics for identifying the \"importance\" of a given layer\ntowards the loss, based on assumptions such as \\emph{error monotonicity}, i.e.\nthat the end-to-end model compression error is proportional to the sum of\nlayer-wise errors. In this paper, we revisit this area, and propose a new and\ngeneral approach for dynamic compression that is provably optimal in a given\ninput range. We begin from the motivating observation that, in general,\n\\emph{error monotonicity does not hold for LLMs}: compressed models with lower\nsum of per-layer errors can perform \\emph{worse} than models with higher error\nsums. To address this, we propose a new general evolutionary framework for\ndynamic LLM compression called EvoPress, which has provable convergence, and\nlow sample and evaluation complexity. We show that these theoretical guarantees\nlead to highly competitive practical performance for dynamic compression of\nLlama, Mistral and Phi models. Via EvoPress, we set new state-of-the-art\nresults across all compression approaches: structural pruning (block/layer\ndropping), unstructured sparsity, as well as quantization with dynamic\nbitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress."
                },
                "authors": [
                    {
                        "name": "Oliver Sieberling"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14641v1",
                "updated": "2024-10-18T17:41:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    41,
                    19,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:41:19Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    41,
                    19,
                    4,
                    292,
                    0
                ],
                "title": "Distance between Relevant Information Pieces Causes Bias in Long-Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distance between Relevant Information Pieces Causes Bias in Long-Context\n  LLMs"
                },
                "summary": "Positional bias in large language models (LLMs) hinders their ability to\neffectively process long inputs. A prominent example is the \"lost in the\nmiddle\" phenomenon, where LLMs struggle to utilize relevant information\nsituated in the middle of the input. While prior research primarily focuses on\nsingle pieces of relevant information, real-world applications often involve\nmultiple relevant information pieces. To bridge this gap, we present\nLongPiBench, a benchmark designed to assess positional bias involving multiple\npieces of relevant information. Thorough experiments are conducted with five\ncommercial and six open-source models. These experiments reveal that while most\ncurrent models are robust against the \"lost in the middle\" issue, there exist\nsignificant biases related to the spacing of relevant information pieces. These\nfindings highlight the importance of evaluating and reducing positional biases\nto advance LLM's capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positional bias in large language models (LLMs) hinders their ability to\neffectively process long inputs. A prominent example is the \"lost in the\nmiddle\" phenomenon, where LLMs struggle to utilize relevant information\nsituated in the middle of the input. While prior research primarily focuses on\nsingle pieces of relevant information, real-world applications often involve\nmultiple relevant information pieces. To bridge this gap, we present\nLongPiBench, a benchmark designed to assess positional bias involving multiple\npieces of relevant information. Thorough experiments are conducted with five\ncommercial and six open-source models. These experiments reveal that while most\ncurrent models are robust against the \"lost in the middle\" issue, there exist\nsignificant biases related to the spacing of relevant information pieces. These\nfindings highlight the importance of evaluating and reducing positional biases\nto advance LLM's capabilities."
                },
                "authors": [
                    {
                        "name": "Runchu Tian"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Yuepeng Fu"
                    },
                    {
                        "name": "Siyang Deng"
                    },
                    {
                        "name": "Qinyu Luo"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Zhong Zhang"
                    },
                    {
                        "name": "Yesai Wu"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Huadong Wang"
                    },
                    {
                        "name": "Xiaojiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojiang Liu"
                },
                "author": "Xiaojiang Liu",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14635v1",
                "updated": "2024-10-18T17:36:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    36,
                    53,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:36:53Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    36,
                    53,
                    4,
                    292,
                    0
                ],
                "title": "GenEOL: Harnessing the Generative Power of LLMs for Training-Free\n  Sentence Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenEOL: Harnessing the Generative Power of LLMs for Training-Free\n  Sentence Embeddings"
                },
                "summary": "Training-free embedding methods directly leverage pretrained large language\nmodels (LLMs) to embed text, bypassing the costly and complex procedure of\ncontrastive learning. Previous training-free embedding methods have mainly\nfocused on optimizing embedding prompts and have overlooked the benefits of\nutilizing the generative abilities of LLMs. We propose a novel method, GenEOL,\nwhich uses LLMs to generate diverse transformations of a sentence that preserve\nits meaning, and aggregates the resulting embeddings of these transformations\nto enhance the overall sentence embedding. GenEOL significantly outperforms the\nexisting training-free embedding methods by an average of 2.85 points across\nseveral LLMs on the sentence semantic text similarity (STS) benchmark. Our\nanalysis shows that GenEOL stabilizes representation quality across LLM layers\nand is robust to perturbations of embedding prompts. GenEOL also achieves\nnotable gains on multiple clustering, reranking and pair-classification tasks\nfrom the MTEB benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free embedding methods directly leverage pretrained large language\nmodels (LLMs) to embed text, bypassing the costly and complex procedure of\ncontrastive learning. Previous training-free embedding methods have mainly\nfocused on optimizing embedding prompts and have overlooked the benefits of\nutilizing the generative abilities of LLMs. We propose a novel method, GenEOL,\nwhich uses LLMs to generate diverse transformations of a sentence that preserve\nits meaning, and aggregates the resulting embeddings of these transformations\nto enhance the overall sentence embedding. GenEOL significantly outperforms the\nexisting training-free embedding methods by an average of 2.85 points across\nseveral LLMs on the sentence semantic text similarity (STS) benchmark. Our\nanalysis shows that GenEOL stabilizes representation quality across LLM layers\nand is robust to perturbations of embedding prompts. GenEOL also achieves\nnotable gains on multiple clustering, reranking and pair-classification tasks\nfrom the MTEB benchmark."
                },
                "authors": [
                    {
                        "name": "Raghuveer Thirukovalluru"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    }
                ],
                "author_detail": {
                    "name": "Bhuwan Dhingra"
                },
                "author": "Bhuwan Dhingra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2109.09889v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2109.09889v3",
                "updated": "2024-10-18T17:32:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    32,
                    27,
                    4,
                    292,
                    0
                ],
                "published": "2021-09-21T00:09:03Z",
                "published_parsed": [
                    2021,
                    9,
                    21,
                    0,
                    9,
                    3,
                    1,
                    264,
                    0
                ],
                "title": "A Distance-based Anomaly Detection Framework for Deep Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distance-based Anomaly Detection Framework for Deep Reinforcement\n  Learning"
                },
                "summary": "In deep reinforcement learning (RL) systems, abnormal states pose significant\nrisks by potentially triggering unpredictable behaviors and unsafe actions,\nthus impeding the deployment of RL systems in real-world scenarios. It is\ncrucial for reliable decision-making systems to have the capability to cast an\nalert whenever they encounter unfamiliar observations that they are not\nequipped to handle. In this paper, we propose a novel Mahalanobis\ndistance-based (MD) anomaly detection framework, called \\textit{MDX}, for deep\nRL algorithms. MDX simultaneously addresses random, adversarial, and\nout-of-distribution (OOD) state outliers in both offline and online settings.\nIt utilizes Mahalanobis distance within class-conditional distributions for\neach action and operates within a statistical hypothesis testing framework\nunder the Gaussian assumption. We further extend it to robust and\ndistribution-free versions by incorporating Robust MD and conformal inference\ntechniques. Through extensive experiments on classical control environments,\nAtari games, and autonomous driving scenarios, we demonstrate the effectiveness\nof our MD-based detection framework. MDX offers a simple, unified, and\npractical anomaly detection tool for enhancing the safety and reliability of RL\nsystems in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In deep reinforcement learning (RL) systems, abnormal states pose significant\nrisks by potentially triggering unpredictable behaviors and unsafe actions,\nthus impeding the deployment of RL systems in real-world scenarios. It is\ncrucial for reliable decision-making systems to have the capability to cast an\nalert whenever they encounter unfamiliar observations that they are not\nequipped to handle. In this paper, we propose a novel Mahalanobis\ndistance-based (MD) anomaly detection framework, called \\textit{MDX}, for deep\nRL algorithms. MDX simultaneously addresses random, adversarial, and\nout-of-distribution (OOD) state outliers in both offline and online settings.\nIt utilizes Mahalanobis distance within class-conditional distributions for\neach action and operates within a statistical hypothesis testing framework\nunder the Gaussian assumption. We further extend it to robust and\ndistribution-free versions by incorporating Robust MD and conformal inference\ntechniques. Through extensive experiments on classical control environments,\nAtari games, and autonomous driving scenarios, we demonstrate the effectiveness\nof our MD-based detection framework. MDX offers a simple, unified, and\npractical anomaly detection tool for enhancing the safety and reliability of RL\nsystems in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Ke Sun"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Linglong Kong"
                    },
                    {
                        "name": "Martin Mller"
                    }
                ],
                "author_detail": {
                    "name": "Martin Mller"
                },
                "author": "Martin Mller",
                "arxiv_comment": "19 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2109.09889v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2109.09889v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14632v1",
                "updated": "2024-10-18T17:32:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    32,
                    22,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:32:22Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    32,
                    22,
                    4,
                    292,
                    0
                ],
                "title": "Diverging Preferences: When do Annotators Disagree and do Models Know?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverging Preferences: When do Annotators Disagree and do Models Know?"
                },
                "summary": "We examine diverging preferences in human-labeled preference datasets. We\ndevelop a taxonomy of disagreement sources spanning 10 categories across four\nhigh-level classes -- task underspecification, response style, refusals, and\nannotation errors. We find that the majority of disagreements are in opposition\nwith standard reward modeling approaches, which are designed with the\nassumption that annotator disagreement is noise. We then explore how these\nfindings impact two areas of LLM development: reward modeling and evaluation.\nIn our experiments, we demonstrate how standard reward modeling methods, like\nthe Bradley-Terry model, fail to differentiate whether a given preference\njudgment is the result of unanimous agreement among annotators or the majority\nopinion among diverging user preferences. We also find that these tendencies\nare also echoed by popular LLM-as-Judge evaluation methods, which consistently\nidentify a winning response in cases of diverging preferences. These findings\nhighlight remaining challenges in LLM evaluations, which are greatly influenced\nby divisive features like response style, and in developing pluralistically\naligned LLMs. To address these issues, we develop methods for identifying\ndiverging preferences to mitigate their influence on evaluation and training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine diverging preferences in human-labeled preference datasets. We\ndevelop a taxonomy of disagreement sources spanning 10 categories across four\nhigh-level classes -- task underspecification, response style, refusals, and\nannotation errors. We find that the majority of disagreements are in opposition\nwith standard reward modeling approaches, which are designed with the\nassumption that annotator disagreement is noise. We then explore how these\nfindings impact two areas of LLM development: reward modeling and evaluation.\nIn our experiments, we demonstrate how standard reward modeling methods, like\nthe Bradley-Terry model, fail to differentiate whether a given preference\njudgment is the result of unanimous agreement among annotators or the majority\nopinion among diverging user preferences. We also find that these tendencies\nare also echoed by popular LLM-as-Judge evaluation methods, which consistently\nidentify a winning response in cases of diverging preferences. These findings\nhighlight remaining challenges in LLM evaluations, which are greatly influenced\nby divisive features like response style, and in developing pluralistically\naligned LLMs. To address these issues, we develop methods for identifying\ndiverging preferences to mitigate their influence on evaluation and training."
                },
                "authors": [
                    {
                        "name": "Michael JQ Zhang"
                    },
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Jena D. Hwang"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Eunsol Choi"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Valentina Pyatkin"
                    }
                ],
                "author_detail": {
                    "name": "Valentina Pyatkin"
                },
                "author": "Valentina Pyatkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10989v2",
                "updated": "2024-10-18T17:21:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    21,
                    17,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-14T18:17:01Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    18,
                    17,
                    1,
                    0,
                    288,
                    0
                ],
                "title": "Liger Kernel: Efficient Triton Kernels for LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liger Kernel: Efficient Triton Kernels for LLM Training"
                },
                "summary": "Training Large Language Models (LLMs) efficiently at scale presents a\nformidable challenge, driven by their ever-increasing computational demands and\nthe need for enhanced performance. In this work, we introduce Liger-Kernel, an\nopen-sourced set of Triton kernels developed specifically for LLM training.\nWith kernel optimization techniques like kernel operation fusing and input\nchunking, our kernels achieve on average a 20% increase in training throughput\nand a 60% reduction in GPU memory usage for popular LLMs compared to\nHuggingFace implementations. In addition, Liger-Kernel is designed with\nmodularity, accessibility, and adaptability in mind, catering to both casual\nand expert users. Comprehensive benchmarks and integration tests are built in\nto ensure compatibility, performance, correctness, and convergence across\ndiverse computing environments and model architectures.\n  The source code is available under a permissive license at:\ngithub.com/linkedin/Liger-Kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models (LLMs) efficiently at scale presents a\nformidable challenge, driven by their ever-increasing computational demands and\nthe need for enhanced performance. In this work, we introduce Liger-Kernel, an\nopen-sourced set of Triton kernels developed specifically for LLM training.\nWith kernel optimization techniques like kernel operation fusing and input\nchunking, our kernels achieve on average a 20% increase in training throughput\nand a 60% reduction in GPU memory usage for popular LLMs compared to\nHuggingFace implementations. In addition, Liger-Kernel is designed with\nmodularity, accessibility, and adaptability in mind, catering to both casual\nand expert users. Comprehensive benchmarks and integration tests are built in\nto ensure compatibility, performance, correctness, and convergence across\ndiverse computing environments and model architectures.\n  The source code is available under a permissive license at:\ngithub.com/linkedin/Liger-Kernel."
                },
                "authors": [
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Yun Dai"
                    },
                    {
                        "name": "Vignesh Kothapalli"
                    },
                    {
                        "name": "Qingquan Song"
                    },
                    {
                        "name": "Shao Tang"
                    },
                    {
                        "name": "Siyu Zhu"
                    },
                    {
                        "name": "Steven Shimizu"
                    },
                    {
                        "name": "Shivam Sahni"
                    },
                    {
                        "name": "Haowen Ning"
                    },
                    {
                        "name": "Yanning Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yanning Chen"
                },
                "author": "Yanning Chen",
                "arxiv_comment": "17 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14617v1",
                "updated": "2024-10-18T17:15:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    15,
                    13,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:15:13Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    15,
                    13,
                    4,
                    292,
                    0
                ],
                "title": "On the Use of Proxies in Political Ad Targeting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Use of Proxies in Political Ad Targeting"
                },
                "summary": "Detailed targeting of advertisements has long been one of the core offerings\nof online platforms. Unfortunately, malicious advertisers have frequently\nabused such targeting features, with results that range from violating civil\nrights laws to driving division, polarization, and even social unrest.\nPlatforms have often attempted to mitigate this behavior by removing targeting\nattributes deemed problematic, such as inferred political leaning, religion, or\nethnicity. In this work, we examine the effectiveness of these mitigations by\ncollecting data from political ads placed on Facebook in the lead up to the\n2022 U.S. midterm elections. We show that major political advertisers\ncircumvented these mitigations by targeting proxy attributes: seemingly\ninnocuous targeting criteria that closely correspond to political and racial\ndivides in American society. We introduce novel methods for directly measuring\nthe skew of various targeting criteria to quantify their effectiveness as\nproxies, and then examine the scale at which those attributes are used. Our\nfindings have crucial implications for the ongoing discussion on the regulation\nof political advertising and emphasize the urgency for increased transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detailed targeting of advertisements has long been one of the core offerings\nof online platforms. Unfortunately, malicious advertisers have frequently\nabused such targeting features, with results that range from violating civil\nrights laws to driving division, polarization, and even social unrest.\nPlatforms have often attempted to mitigate this behavior by removing targeting\nattributes deemed problematic, such as inferred political leaning, religion, or\nethnicity. In this work, we examine the effectiveness of these mitigations by\ncollecting data from political ads placed on Facebook in the lead up to the\n2022 U.S. midterm elections. We show that major political advertisers\ncircumvented these mitigations by targeting proxy attributes: seemingly\ninnocuous targeting criteria that closely correspond to political and racial\ndivides in American society. We introduce novel methods for directly measuring\nthe skew of various targeting criteria to quantify their effectiveness as\nproxies, and then examine the scale at which those attributes are used. Our\nfindings have crucial implications for the ongoing discussion on the regulation\nof political advertising and emphasize the urgency for increased transparency."
                },
                "authors": [
                    {
                        "name": "Piotr Sapiezynski"
                    },
                    {
                        "name": "Levi Kaplan"
                    },
                    {
                        "name": "Alan Mislove"
                    },
                    {
                        "name": "Aleksandra Korolova"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandra Korolova"
                },
                "author": "Aleksandra Korolova",
                "arxiv_doi": "10.1145/3686917",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3686917",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.14617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the ACM on Human-Computer Interaction 8. CSCW\n  (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06402v2",
                "updated": "2024-10-18T17:10:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    10,
                    5,
                    4,
                    292,
                    0
                ],
                "published": "2024-03-11T03:28:13Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    3,
                    28,
                    13,
                    0,
                    71,
                    0
                ],
                "title": "One size doesn't fit all: Predicting the Number of Examples for\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One size doesn't fit all: Predicting the Number of Examples for\n  In-Context Learning"
                },
                "summary": "In-context learning (ICL) refers to the process of adding a small number of\nlocalized examples (ones that are semantically similar to the input) from a\ntraining set of labelled data to an LLM's prompt with an objective to\neffectively control the generative process seeking to improve the downstream\ntask performance. Existing ICL approaches use an identical number of examples\n(a pre-configured hyper-parameter) for each data instance. Our work alleviates\nthe limitations of this 'one fits all' approach by dynamically predicting the\nnumber of examples for each data instance to be used in few-shot inference with\nLLMs. In particular, we employ a multi-label classifier, the parameters of\nwhich are fitted using a training set, where the label for each instance in the\ntraining set indicates if using a specific value of k (number of most similar\nexamples from 0 up to a maximum value) leads to correct k-shot downstream\npredictions. Our experiments on a number of text classification benchmarks show\nthat AICL substantially outperforms standard ICL by up to 17%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) refers to the process of adding a small number of\nlocalized examples (ones that are semantically similar to the input) from a\ntraining set of labelled data to an LLM's prompt with an objective to\neffectively control the generative process seeking to improve the downstream\ntask performance. Existing ICL approaches use an identical number of examples\n(a pre-configured hyper-parameter) for each data instance. Our work alleviates\nthe limitations of this 'one fits all' approach by dynamically predicting the\nnumber of examples for each data instance to be used in few-shot inference with\nLLMs. In particular, we employ a multi-label classifier, the parameters of\nwhich are fitted using a training set, where the label for each instance in the\ntraining set indicates if using a specific value of k (number of most similar\nexamples from 0 up to a maximum value) leads to correct k-shot downstream\npredictions. Our experiments on a number of text classification benchmarks show\nthat AICL substantially outperforms standard ICL by up to 17%."
                },
                "authors": [
                    {
                        "name": "Manish Chandra"
                    },
                    {
                        "name": "Debasis Ganguly"
                    },
                    {
                        "name": "Iadh Ounis"
                    }
                ],
                "author_detail": {
                    "name": "Iadh Ounis"
                },
                "author": "Iadh Ounis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14610v1",
                "updated": "2024-10-18T17:03:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    3,
                    40,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:03:40Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    3,
                    40,
                    4,
                    292,
                    0
                ],
                "title": "Super-Jeans fragmentation in massive star-forming regions revealed by\n  triangulation analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super-Jeans fragmentation in massive star-forming regions revealed by\n  triangulation analysis"
                },
                "summary": "Understanding the fragmentation of the gas cloud and the formation of massive\nstars remains one of the most challenging questions of modern astrophysical\nresearch. Either the gas fragmentation in a Jeans-like fashion, after which the\nfragments grow through accretion, or the fragmentation length is larger than\nthe Jeans length from the start. Despite significant observational efforts, a\nconsensus has not been reached. The key is to infer the initial density\ndistribution upon which gravitational fragmentation occurs. Since cores are the\nproducts of the fragmentation process, the distances between adjacent cores\nserve as a scale indicator. Based on this observation, we propose a Delaunay\ntriangulation-based approach to infer the density structure before the\nfragmentation and establish the link between density distribution and gas\nfragmentation length. We find that at low density, the fragmenting is\nJeans-like, and at high densities, the core separations are larger than the\nprediction of the Jeans fragmentation. This super-Jeans fragmentation, which\noften occurs in groups, is responsible for the clustered formation of massive\nstars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the fragmentation of the gas cloud and the formation of massive\nstars remains one of the most challenging questions of modern astrophysical\nresearch. Either the gas fragmentation in a Jeans-like fashion, after which the\nfragments grow through accretion, or the fragmentation length is larger than\nthe Jeans length from the start. Despite significant observational efforts, a\nconsensus has not been reached. The key is to infer the initial density\ndistribution upon which gravitational fragmentation occurs. Since cores are the\nproducts of the fragmentation process, the distances between adjacent cores\nserve as a scale indicator. Based on this observation, we propose a Delaunay\ntriangulation-based approach to infer the density structure before the\nfragmentation and establish the link between density distribution and gas\nfragmentation length. We find that at low density, the fragmenting is\nJeans-like, and at high densities, the core separations are larger than the\nprediction of the Jeans fragmentation. This super-Jeans fragmentation, which\noften occurs in groups, is responsible for the clustered formation of massive\nstars."
                },
                "authors": [
                    {
                        "name": "Guang-Xing Li"
                    },
                    {
                        "name": "Mengke Zhao"
                    },
                    {
                        "name": "Xing Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xing Lu"
                },
                "author": "Xing Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14609v1",
                "updated": "2024-10-18T17:03:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    3,
                    17,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:03:17Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    3,
                    17,
                    4,
                    292,
                    0
                ],
                "title": "DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual\n  Distillation in Conversational Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual\n  Distillation in Conversational Search"
                },
                "summary": "Conversational Search (CS) is the task of retrieving relevant documents from\na corpus within a conversational context, combining retrieval with\nconversational context modeling. With the explosion of Large Language Models\n(LLMs), the CS field has seen major improvements with LLMs rewriting user\nqueries, accounting for conversational context. However, engaging LLMs at\ninference time harms efficiency. Current methods address this by distilling\nembeddings from human-rewritten queries to learn the context modeling task.\nYet, these approaches predominantly focus on context modeling, and only treat\nthe contrastive component of the retrieval task within a\ndistillation-independent loss term. To address these limitations, we propose a\nnew distillation method, as a relaxation of the previous objective, unifying\nretrieval and context modeling. We relax the existing training objectives by\ndistilling similarity scores between conversations and documents, rather than\nrelying solely on representation learning. Our proposed distillation objective\nallows for more freedom in the representation space and leverages the\ncontrastive nature of document relevance. Through experiments on Learned Sparse\nRetrieval (LSR) across 5 CS datasets, our approach demonstrates substantial\nimprovements in both in-domain and out-of-domain retrieval performance,\noutperforming state-of-the-art with gains of up to 6 points in recall for\nout-of-domain datasets. Additionally, through the relaxation of the objective,\nwe propose a multi-teacher distillation, using multiple LLMs as teachers,\nyielding additional gains, and outperforming the teachers themselves in\nin-domain experiments. Finally, analysis of the sparsity of the models reveals\nthat our distillation allows for better control over the sparsity of the\ntrained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Search (CS) is the task of retrieving relevant documents from\na corpus within a conversational context, combining retrieval with\nconversational context modeling. With the explosion of Large Language Models\n(LLMs), the CS field has seen major improvements with LLMs rewriting user\nqueries, accounting for conversational context. However, engaging LLMs at\ninference time harms efficiency. Current methods address this by distilling\nembeddings from human-rewritten queries to learn the context modeling task.\nYet, these approaches predominantly focus on context modeling, and only treat\nthe contrastive component of the retrieval task within a\ndistillation-independent loss term. To address these limitations, we propose a\nnew distillation method, as a relaxation of the previous objective, unifying\nretrieval and context modeling. We relax the existing training objectives by\ndistilling similarity scores between conversations and documents, rather than\nrelying solely on representation learning. Our proposed distillation objective\nallows for more freedom in the representation space and leverages the\ncontrastive nature of document relevance. Through experiments on Learned Sparse\nRetrieval (LSR) across 5 CS datasets, our approach demonstrates substantial\nimprovements in both in-domain and out-of-domain retrieval performance,\noutperforming state-of-the-art with gains of up to 6 points in recall for\nout-of-domain datasets. Additionally, through the relaxation of the objective,\nwe propose a multi-teacher distillation, using multiple LLMs as teachers,\nyielding additional gains, and outperforming the teachers themselves in\nin-domain experiments. Finally, analysis of the sparsity of the models reveals\nthat our distillation allows for better control over the sparsity of the\ntrained models."
                },
                "authors": [
                    {
                        "name": "Simon Lupart"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Kanoulas"
                },
                "author": "Evangelos Kanoulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14599v1",
                "updated": "2024-10-18T16:53:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    53,
                    49,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:53:49Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    53,
                    49,
                    4,
                    292,
                    0
                ],
                "title": "Bayesian Multi-wavelength Imaging of the LMC SN1987A with SRG/eROSITA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Multi-wavelength Imaging of the LMC SN1987A with SRG/eROSITA"
                },
                "summary": "The EDR and eRASS1 data have already revealed a remarkable number of\nundiscovered X-ray sources. Using Bayesian inference and generative modeling\ntechniques for X-ray imaging, we aim to increase the sensitivity and scientific\nvalue of these observations by denoising, deconvolving, and decomposing the\nX-ray sky. Leveraging information field theory, we can exploit the spatial and\nspectral correlation structures of the different physical components of the sky\nwith non-parametric priors to enhance the image reconstruction. By\nincorporating instrumental effects into the forward model, we develop a\ncomprehensive Bayesian imaging algorithm for eROSITA pointing observations.\nFinally, we apply the developed algorithm to EDR data of the LMC SN1987A,\nfusing data sets from observations made by five different telescope modules.\nThe final result is a denoised, deconvolved, and decomposed view of the LMC,\nwhich enables the analysis of its fine-scale structures, the creation of point\nsource catalogues of this region, and enhanced calibration for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The EDR and eRASS1 data have already revealed a remarkable number of\nundiscovered X-ray sources. Using Bayesian inference and generative modeling\ntechniques for X-ray imaging, we aim to increase the sensitivity and scientific\nvalue of these observations by denoising, deconvolving, and decomposing the\nX-ray sky. Leveraging information field theory, we can exploit the spatial and\nspectral correlation structures of the different physical components of the sky\nwith non-parametric priors to enhance the image reconstruction. By\nincorporating instrumental effects into the forward model, we develop a\ncomprehensive Bayesian imaging algorithm for eROSITA pointing observations.\nFinally, we apply the developed algorithm to EDR data of the LMC SN1987A,\nfusing data sets from observations made by five different telescope modules.\nThe final result is a denoised, deconvolved, and decomposed view of the LMC,\nwhich enables the analysis of its fine-scale structures, the creation of point\nsource catalogues of this region, and enhanced calibration for future work."
                },
                "authors": [
                    {
                        "name": "Vincent Eberle"
                    },
                    {
                        "name": "Matteo Guardiani"
                    },
                    {
                        "name": "Margret Westerkamp"
                    },
                    {
                        "name": "Philipp Frank"
                    },
                    {
                        "name": "Michael Freyberg"
                    },
                    {
                        "name": "Mara Salvato"
                    },
                    {
                        "name": "Torsten Enlin"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Enlin"
                },
                "author": "Torsten Enlin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14597v1",
                "updated": "2024-10-18T16:50:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    50,
                    53,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:50:53Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    50,
                    53,
                    4,
                    292,
                    0
                ],
                "title": "The interplay of astrophysics and nuclear physics in determining the\n  properties of neutron stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The interplay of astrophysics and nuclear physics in determining the\n  properties of neutron stars"
                },
                "summary": "Neutron star properties depend on both nuclear physics and astrophysical\nprocesses, and thus observations of neutron stars offer constraints on both\nlarge-scale astrophysics and the behavior of cold, dense matter. In this study,\nwe use astronomical data to jointly infer the universal equation of state of\ndense matter along with two distinct astrophysical populations: Galactic\nneutron stars observed electromagnetically and merging neutron stars in\nbinaries observed with gravitational waves. We place constraints on neutron\nstar properties and quantify the extent to which they are attributable to\nmacrophysics or microphysics. We confirm previous results indicating that the\nGalactic and merging neutron stars have distinct mass distributions. The\ninferred maximum mass of both Galactic neutron stars, $M_{\\rm pop,\nEM}=2.05^{+0.11}_{-0.06}\\,M_{\\odot}$ (median and 90\\% symmetric credible\ninterval), and merging neutron star binaries, $M_{\\rm pop,\nGW}=1.85^{+0.39}_{-0.16}\\,M_{\\odot}$, are consistent with the maximum mass of\nnonrotating neutron stars set by nuclear physics, $M_{\\rm TOV}\n=2.28^{+0.41}_{-0.21}\\,M_\\odot$. The radius of a $1.4\\,M_{\\odot}$ neutron star\nis $12.2^{+0.8}_{-0.9}\\,$km, consistent with, though $\\sim 20\\%$ tighter than,\nprevious results using an identical equation of state model. Even though\nobserved Galactic and merging neutron stars originate from populations with\ndistinct properties, there is currently no evidence that astrophysical\nprocesses cannot produce neutron stars up to the maximum value imposed by\nnuclear physics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron star properties depend on both nuclear physics and astrophysical\nprocesses, and thus observations of neutron stars offer constraints on both\nlarge-scale astrophysics and the behavior of cold, dense matter. In this study,\nwe use astronomical data to jointly infer the universal equation of state of\ndense matter along with two distinct astrophysical populations: Galactic\nneutron stars observed electromagnetically and merging neutron stars in\nbinaries observed with gravitational waves. We place constraints on neutron\nstar properties and quantify the extent to which they are attributable to\nmacrophysics or microphysics. We confirm previous results indicating that the\nGalactic and merging neutron stars have distinct mass distributions. The\ninferred maximum mass of both Galactic neutron stars, $M_{\\rm pop,\nEM}=2.05^{+0.11}_{-0.06}\\,M_{\\odot}$ (median and 90\\% symmetric credible\ninterval), and merging neutron star binaries, $M_{\\rm pop,\nGW}=1.85^{+0.39}_{-0.16}\\,M_{\\odot}$, are consistent with the maximum mass of\nnonrotating neutron stars set by nuclear physics, $M_{\\rm TOV}\n=2.28^{+0.41}_{-0.21}\\,M_\\odot$. The radius of a $1.4\\,M_{\\odot}$ neutron star\nis $12.2^{+0.8}_{-0.9}\\,$km, consistent with, though $\\sim 20\\%$ tighter than,\nprevious results using an identical equation of state model. Even though\nobserved Galactic and merging neutron stars originate from populations with\ndistinct properties, there is currently no evidence that astrophysical\nprocesses cannot produce neutron stars up to the maximum value imposed by\nnuclear physics."
                },
                "authors": [
                    {
                        "name": "Jacob Golomb"
                    },
                    {
                        "name": "Isaac Legred"
                    },
                    {
                        "name": "Katerina Chatziioannou"
                    },
                    {
                        "name": "Philippe Landry"
                    }
                ],
                "author_detail": {
                    "name": "Philippe Landry"
                },
                "author": "Philippe Landry",
                "arxiv_comment": "16 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14596v1",
                "updated": "2024-10-18T16:49:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    49,
                    36,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:49:36Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    49,
                    36,
                    4,
                    292,
                    0
                ],
                "title": "Teaching Models to Balance Resisting and Accepting Persuasion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Models to Balance Resisting and Accepting Persuasion"
                },
                "summary": "Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Balanced Training (or PBT),\nwhich leverages multi-agent recursive dialogue trees to create data and trains\nmodels via preference optimization to accept persuasion when appropriate. PBT\nconsistently improves resistance to misinformation and resilience to being\nchallenged while also resulting in the best overall performance on holistic\ndata containing both positive and negative persuasion. Crucially, we show that\nPBT models are better teammates in multi-agent debates. We find that without\nPBT, pairs of stronger and weaker models have unstable performance, with the\norder in which the models present their answers determining whether the team\nobtains the stronger or weaker model's performance. PBT leads to better and\nmore stable results and less order dependence, with the stronger model\nconsistently pulling the weaker one up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Balanced Training (or PBT),\nwhich leverages multi-agent recursive dialogue trees to create data and trains\nmodels via preference optimization to accept persuasion when appropriate. PBT\nconsistently improves resistance to misinformation and resilience to being\nchallenged while also resulting in the best overall performance on holistic\ndata containing both positive and negative persuasion. Crucially, we show that\nPBT models are better teammates in multi-agent debates. We find that without\nPBT, pairs of stronger and weaker models have unstable performance, with the\norder in which the models present their answers determining whether the team\nobtains the stronger or weaker model's performance. PBT leads to better and\nmore stable results and less order dependence, with the stronger model\nconsistently pulling the weaker one up."
                },
                "authors": [
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Peter Hase"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Code: https://github.com/esteng/persuasion_balanced_training",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14594v1",
                "updated": "2024-10-18T16:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    44,
                    22,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:44:22Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    44,
                    22,
                    4,
                    292,
                    0
                ],
                "title": "Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and\n  Tool Knowledge Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and\n  Tool Knowledge Bases"
                },
                "summary": "Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks\nlike secure database interactions and multi-agent code development. However,\nscaling tool capacity beyond agent reasoning or model limits remains a\nchallenge. In this paper, we address these challenges by introducing Toolshed\nKnowledge Bases, a tool knowledge base (vector database) designed to store\nenhanced tool representations and optimize tool selection for large-scale\ntool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a\nnovel ensemble of tool-applied advanced retrieval-augmented generation (RAG)\ntechniques across the pre-retrieval, intra-retrieval, and post-retrieval\nphases, without requiring model fine-tuning. During pre-retrieval, tool\ndocuments are enhanced with key information and stored in the Toolshed\nKnowledge Base. Intra-retrieval focuses on query planning and transformation to\nincrease retrieval accuracy. Post-retrieval refines the retrieved tool\ndocuments and enables self-reflection. Furthermore, by varying both the total\nnumber of tools (tool-M) an Agent has access to and the tool selection\nthreshold (top-k), we address trade-offs between retrieval accuracy, agent\nperformance, and token cost. Our approach achieves 46%, 56%, and 47% absolute\nimprovements on the ToolE single-tool, ToolE multi-tool and Seal-Tools\nbenchmark datasets, respectively (Recall@5).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks\nlike secure database interactions and multi-agent code development. However,\nscaling tool capacity beyond agent reasoning or model limits remains a\nchallenge. In this paper, we address these challenges by introducing Toolshed\nKnowledge Bases, a tool knowledge base (vector database) designed to store\nenhanced tool representations and optimize tool selection for large-scale\ntool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a\nnovel ensemble of tool-applied advanced retrieval-augmented generation (RAG)\ntechniques across the pre-retrieval, intra-retrieval, and post-retrieval\nphases, without requiring model fine-tuning. During pre-retrieval, tool\ndocuments are enhanced with key information and stored in the Toolshed\nKnowledge Base. Intra-retrieval focuses on query planning and transformation to\nincrease retrieval accuracy. Post-retrieval refines the retrieved tool\ndocuments and enables self-reflection. Furthermore, by varying both the total\nnumber of tools (tool-M) an Agent has access to and the tool selection\nthreshold (top-k), we address trade-offs between retrieval accuracy, agent\nperformance, and token cost. Our approach achieves 46%, 56%, and 47% absolute\nimprovements on the ToolE single-tool, ToolE multi-tool and Seal-Tools\nbenchmark datasets, respectively (Recall@5)."
                },
                "authors": [
                    {
                        "name": "Elias Lumer"
                    }
                ],
                "author_detail": {
                    "name": "Elias Lumer"
                },
                "author": "Elias Lumer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.13370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.13370v2",
                "updated": "2024-10-18T16:44:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    44,
                    5,
                    4,
                    292,
                    0
                ],
                "published": "2024-04-20T13:15:27Z",
                "published_parsed": [
                    2024,
                    4,
                    20,
                    13,
                    15,
                    27,
                    5,
                    111,
                    0
                ],
                "title": "Movie101v2: Improved Movie Narration Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Movie101v2: Improved Movie Narration Benchmark"
                },
                "summary": "Automatic movie narration aims to generate video-aligned plot descriptions to\nassist visually impaired audiences. Unlike standard video captioning, it\ninvolves not only describing key visual details but also inferring plots that\nunfold across multiple movie shots, presenting distinct and complex challenges.\nTo advance this field, we introduce Movie101v2, a large-scale, bilingual\ndataset with enhanced data quality specifically designed for movie narration.\nRevisiting the task, we propose breaking down the ultimate goal of automatic\nmovie narration into three progressive stages, offering a clear roadmap with\ncorresponding evaluation metrics. Based on our new benchmark, we baseline a\nrange of large vision-language models, including GPT-4V, and conduct an\nin-depth analysis of the challenges in narration generation. Our findings\nhighlight that achieving applicable movie narration generation is a fascinating\ngoal that requires significant research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic movie narration aims to generate video-aligned plot descriptions to\nassist visually impaired audiences. Unlike standard video captioning, it\ninvolves not only describing key visual details but also inferring plots that\nunfold across multiple movie shots, presenting distinct and complex challenges.\nTo advance this field, we introduce Movie101v2, a large-scale, bilingual\ndataset with enhanced data quality specifically designed for movie narration.\nRevisiting the task, we propose breaking down the ultimate goal of automatic\nmovie narration into three progressive stages, offering a clear roadmap with\ncorresponding evaluation metrics. Based on our new benchmark, we baseline a\nrange of large vision-language models, including GPT-4V, and conduct an\nin-depth analysis of the challenges in narration generation. Our findings\nhighlight that achieving applicable movie narration generation is a fascinating\ngoal that requires significant research."
                },
                "authors": [
                    {
                        "name": "Zihao Yue"
                    },
                    {
                        "name": "Yepeng Zhang"
                    },
                    {
                        "name": "Ziheng Wang"
                    },
                    {
                        "name": "Qin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Qin Jin"
                },
                "author": "Qin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.13370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.13370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13191v2",
                "updated": "2024-10-18T16:42:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    42,
                    1,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-17T03:38:29Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    3,
                    38,
                    29,
                    3,
                    291,
                    0
                ],
                "title": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with\n  Iterative Self-Critique, Correction, and Comparison Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with\n  Iterative Self-Critique, Correction, and Comparison Feedback"
                },
                "summary": "Automatic question generation (QG) is essential for AI and NLP, particularly\nin intelligent tutoring, dialogue systems, and fact verification. Generating\nmultiple-choice questions (MCQG) for professional exams, like the United States\nMedical Licensing Examination (USMLE), is particularly challenging, requiring\ndomain expertise and complex multi-hop reasoning for high-quality questions.\nHowever, current large language models (LLMs) like GPT-4 struggle with\nprofessional MCQG due to outdated knowledge, hallucination issues, and prompt\nsensitivity, resulting in unsatisfactory quality and difficulty. To address\nthese challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique\nand Correction) framework for converting medical cases into high-quality\nUSMLE-style questions. By integrating expert-driven prompt engineering with\niterative self-critique and self-correction feedback, MCQG-SRefine\nsignificantly enhances human expert satisfaction regarding both the quality and\ndifficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based\nautomatic metric to replace the complex and costly expert evaluation process,\nensuring reliable and expert-aligned assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic question generation (QG) is essential for AI and NLP, particularly\nin intelligent tutoring, dialogue systems, and fact verification. Generating\nmultiple-choice questions (MCQG) for professional exams, like the United States\nMedical Licensing Examination (USMLE), is particularly challenging, requiring\ndomain expertise and complex multi-hop reasoning for high-quality questions.\nHowever, current large language models (LLMs) like GPT-4 struggle with\nprofessional MCQG due to outdated knowledge, hallucination issues, and prompt\nsensitivity, resulting in unsatisfactory quality and difficulty. To address\nthese challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique\nand Correction) framework for converting medical cases into high-quality\nUSMLE-style questions. By integrating expert-driven prompt engineering with\niterative self-critique and self-correction feedback, MCQG-SRefine\nsignificantly enhances human expert satisfaction regarding both the quality and\ndifficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based\nautomatic metric to replace the complex and costly expert evaluation process,\nensuring reliable and expert-aligned assessments."
                },
                "authors": [
                    {
                        "name": "Zonghai Yao"
                    },
                    {
                        "name": "Aditya Parashar"
                    },
                    {
                        "name": "Huixue Zhou"
                    },
                    {
                        "name": "Won Seok Jang"
                    },
                    {
                        "name": "Feiyun Ouyang"
                    },
                    {
                        "name": "Zhichao Yang"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "Equal contribution for the first two authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14587v1",
                "updated": "2024-10-18T16:37:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    37,
                    52,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:37:52Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    37,
                    52,
                    4,
                    292,
                    0
                ],
                "title": "Neuro-Symbolic Traders: Assessing the Wisdom of AI Crowds in Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic Traders: Assessing the Wisdom of AI Crowds in Markets"
                },
                "summary": "Deep generative models are becoming increasingly used as tools for financial\nanalysis. However, it is unclear how these models will influence financial\nmarkets, especially when they infer financial value in a semi-autonomous way.\nIn this work, we explore the interplay between deep generative models and\nmarket dynamics. We develop a form of virtual traders that use deep generative\nmodels to make buy/sell decisions, which we term neuro-symbolic traders, and\nexpose them to a virtual market. Under our framework, neuro-symbolic traders\nare agents that use vision-language models to discover a model of the\nfundamental value of an asset. Agents develop this model as a stochastic\ndifferential equation, calibrated to market data using gradient descent. We\ntest our neuro-symbolic traders on both synthetic data and real financial time\nseries, including an equity stock, commodity, and a foreign exchange pair. We\nthen expose several groups of neuro-symbolic traders to a virtual market\nenvironment. This market environment allows for feedback between the traders\nbelief of the underlying value to the observed price dynamics. We find that\nthis leads to price suppression compared to the historical data, highlighting a\nfuture risk to market stability. Our work is a first step towards quantifying\nthe effect of deep generative agents on markets dynamics and sets out some of\nthe potential risks and benefits of this approach in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep generative models are becoming increasingly used as tools for financial\nanalysis. However, it is unclear how these models will influence financial\nmarkets, especially when they infer financial value in a semi-autonomous way.\nIn this work, we explore the interplay between deep generative models and\nmarket dynamics. We develop a form of virtual traders that use deep generative\nmodels to make buy/sell decisions, which we term neuro-symbolic traders, and\nexpose them to a virtual market. Under our framework, neuro-symbolic traders\nare agents that use vision-language models to discover a model of the\nfundamental value of an asset. Agents develop this model as a stochastic\ndifferential equation, calibrated to market data using gradient descent. We\ntest our neuro-symbolic traders on both synthetic data and real financial time\nseries, including an equity stock, commodity, and a foreign exchange pair. We\nthen expose several groups of neuro-symbolic traders to a virtual market\nenvironment. This market environment allows for feedback between the traders\nbelief of the underlying value to the observed price dynamics. We find that\nthis leads to price suppression compared to the historical data, highlighting a\nfuture risk to market stability. Our work is a first step towards quantifying\nthe effect of deep generative agents on markets dynamics and sets out some of\nthe potential risks and benefits of this approach in the future."
                },
                "authors": [
                    {
                        "name": "Namid R. Stillman"
                    },
                    {
                        "name": "Rory Baggott"
                    }
                ],
                "author_detail": {
                    "name": "Rory Baggott"
                },
                "author": "Rory Baggott",
                "arxiv_comment": "8 pages, 4 figures, ACM format",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14582v1",
                "updated": "2024-10-18T16:32:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    32,
                    10,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:32:10Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    32,
                    10,
                    4,
                    292,
                    0
                ],
                "title": "Do LLMs estimate uncertainty well in instruction-following?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs estimate uncertainty well in instruction-following?"
                },
                "summary": "Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents."
                },
                "authors": [
                    {
                        "name": "Juyeon Heo"
                    },
                    {
                        "name": "Miao Xiong"
                    },
                    {
                        "name": "Christina Heinze-Deml"
                    },
                    {
                        "name": "Jaya Narain"
                    }
                ],
                "author_detail": {
                    "name": "Jaya Narain"
                },
                "author": "Jaya Narain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14578v1",
                "updated": "2024-10-18T16:26:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    26,
                    45,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:26:45Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    26,
                    45,
                    4,
                    292,
                    0
                ],
                "title": "Large Language Models Are Overparameterized Text Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Overparameterized Text Encoders"
                },
                "summary": "Large language models (LLMs) demonstrate strong performance as text embedding\nmodels when finetuned with supervised contrastive training. However, their\nlarge size balloons inference time and memory requirements. In this paper, we\nshow that by pruning the last $p\\%$ layers of an LLM before supervised training\nfor only 1000 steps, we can achieve a proportional reduction in memory and\ninference time. We evaluate four different state-of-the-art LLMs on text\nembedding tasks and find that our method can prune up to 30\\% of layers with\nnegligible impact on performance and up to 80\\% with only a modest drop. With\nonly three lines of code, our method is easily implemented in any pipeline for\ntransforming LLMs to text encoders. We also propose $\\text{L}^3 \\text{Prune}$,\na novel layer-pruning strategy based on the model's initial loss that provides\ntwo optimal pruning configurations: a large variant with negligible performance\nloss and a small variant for resource-constrained settings. On average, the\nlarge variant prunes 21\\% of the parameters with a $-0.3$ performance drop, and\nthe small variant only suffers from a $-5.1$ decrease while pruning 74\\% of the\nmodel. We consider these results strong evidence that LLMs are\noverparameterized for text embedding tasks, and can be easily pruned.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong performance as text embedding\nmodels when finetuned with supervised contrastive training. However, their\nlarge size balloons inference time and memory requirements. In this paper, we\nshow that by pruning the last $p\\%$ layers of an LLM before supervised training\nfor only 1000 steps, we can achieve a proportional reduction in memory and\ninference time. We evaluate four different state-of-the-art LLMs on text\nembedding tasks and find that our method can prune up to 30\\% of layers with\nnegligible impact on performance and up to 80\\% with only a modest drop. With\nonly three lines of code, our method is easily implemented in any pipeline for\ntransforming LLMs to text encoders. We also propose $\\text{L}^3 \\text{Prune}$,\na novel layer-pruning strategy based on the model's initial loss that provides\ntwo optimal pruning configurations: a large variant with negligible performance\nloss and a small variant for resource-constrained settings. On average, the\nlarge variant prunes 21\\% of the parameters with a $-0.3$ performance drop, and\nthe small variant only suffers from a $-5.1$ decrease while pruning 74\\% of the\nmodel. We consider these results strong evidence that LLMs are\noverparameterized for text embedding tasks, and can be easily pruned."
                },
                "authors": [
                    {
                        "name": "Thennal D K"
                    },
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Chris Biemann"
                    }
                ],
                "author_detail": {
                    "name": "Chris Biemann"
                },
                "author": "Chris Biemann",
                "arxiv_comment": "8 pages of content + 1 for limitations and ethical considerations, 14\n  pages in total including references and appendix, 5+1 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14572v1",
                "updated": "2024-10-18T16:18:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    18,
                    50,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:18:50Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    18,
                    50,
                    4,
                    292,
                    0
                ],
                "title": "Constraining a relativistic mean field model using neutron star\n  mass-radius measurements II: Hyperonic models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining a relativistic mean field model using neutron star\n  mass-radius measurements II: Hyperonic models"
                },
                "summary": "We investigate whether measurements of the neutron star mass and radius or\nthe tidal deformability can provide information about the presence of hyperons\ninside a neutron star. This is achieved by considering two inference models,\nwith and without hyperons, based on a field-theoretical approach. While current\nobservations do not distinguish between the two scenarios, we have shown that\ndata simulating expected observations from future large area X-ray timing\ntelescopes could provide some information through Bayes factors. Inference\nusing simulated data generated from an EOS containing hyperons decisively\nfavours the hyperonic model over the nucleonic model. However, a 2\\%\nuncertainty in the mass and radius determination may not be sufficient to\nconstrain the parameters of the model when only six neutron star mass-radius\nmeasurements are considered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate whether measurements of the neutron star mass and radius or\nthe tidal deformability can provide information about the presence of hyperons\ninside a neutron star. This is achieved by considering two inference models,\nwith and without hyperons, based on a field-theoretical approach. While current\nobservations do not distinguish between the two scenarios, we have shown that\ndata simulating expected observations from future large area X-ray timing\ntelescopes could provide some information through Bayes factors. Inference\nusing simulated data generated from an EOS containing hyperons decisively\nfavours the hyperonic model over the nucleonic model. However, a 2\\%\nuncertainty in the mass and radius determination may not be sufficient to\nconstrain the parameters of the model when only six neutron star mass-radius\nmeasurements are considered."
                },
                "authors": [
                    {
                        "name": "Chun Huang"
                    },
                    {
                        "name": "Laura Tolos"
                    },
                    {
                        "name": "Constana Providncia"
                    },
                    {
                        "name": "Anna Watts"
                    }
                ],
                "author_detail": {
                    "name": "Anna Watts"
                },
                "author": "Anna Watts",
                "arxiv_comment": "Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14571v1",
                "updated": "2024-10-18T16:17:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    17,
                    10,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:17:10Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    17,
                    10,
                    4,
                    292,
                    0
                ],
                "title": "TransBox: EL++-closed Ontology Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransBox: EL++-closed Ontology Embedding"
                },
                "summary": "OWL (Web Ontology Language) ontologies, which are able to represent both\nrelational and type facts as standard knowledge graphs and complex domain\nknowledge in Description Logic (DL) axioms, are widely adopted in domains such\nas healthcare and bioinformatics. Inspired by the success of knowledge graph\nembeddings, embedding OWL ontologies has gained significant attention in recent\nyears. Current methods primarily focus on learning embeddings for atomic\nconcepts and roles, enabling the evaluation based on normalized axioms through\nspecially designed score functions. However, they often neglect the embedding\nof complex concepts, making it difficult to infer with more intricate axioms.\nThis limitation reduces their effectiveness in advanced reasoning tasks, such\nas Ontology Learning and ontology-mediated Query Answering. In this paper, we\npropose EL++-closed ontology embeddings which are able to represent any logical\nexpressions in DL via composition. Furthermore, we develop TransBox, an\neffective EL++-closed ontology embedding method that can handle many-to-one,\none-to-many and many-to-many relations. Our extensive experiments demonstrate\nthat TransBox often achieves state-of-the-art performance across various\nreal-world datasets for predicting complex axioms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OWL (Web Ontology Language) ontologies, which are able to represent both\nrelational and type facts as standard knowledge graphs and complex domain\nknowledge in Description Logic (DL) axioms, are widely adopted in domains such\nas healthcare and bioinformatics. Inspired by the success of knowledge graph\nembeddings, embedding OWL ontologies has gained significant attention in recent\nyears. Current methods primarily focus on learning embeddings for atomic\nconcepts and roles, enabling the evaluation based on normalized axioms through\nspecially designed score functions. However, they often neglect the embedding\nof complex concepts, making it difficult to infer with more intricate axioms.\nThis limitation reduces their effectiveness in advanced reasoning tasks, such\nas Ontology Learning and ontology-mediated Query Answering. In this paper, we\npropose EL++-closed ontology embeddings which are able to represent any logical\nexpressions in DL via composition. Furthermore, we develop TransBox, an\neffective EL++-closed ontology embedding method that can handle many-to-one,\none-to-many and many-to-many relations. Our extensive experiments demonstrate\nthat TransBox often achieves state-of-the-art performance across various\nreal-world datasets for predicting complex axioms."
                },
                "authors": [
                    {
                        "name": "Hui Yang"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Uli Sattler"
                    }
                ],
                "author_detail": {
                    "name": "Uli Sattler"
                },
                "author": "Uli Sattler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14569v1",
                "updated": "2024-10-18T16:16:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    16,
                    34,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:16:34Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    16,
                    34,
                    4,
                    292,
                    0
                ],
                "title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have established them as\nagentic systems capable of planning and interacting with various tools. These\nLLM agents are often paired with web-based tools, enabling access to diverse\nsources and real-time information. Although these advancements offer\nsignificant benefits across various applications, they also increase the risk\nof malicious use, particularly in cyberattacks involving personal information.\nIn this work, we investigate the risks associated with misuse of LLM agents in\ncyberattacks involving personal data. Specifically, we aim to understand: 1)\nhow potent LLM agents can be when directed to conduct cyberattacks, 2) how\ncyberattacks are enhanced by web-based tools, and 3) how affordable and easy it\nbecomes to launch cyberattacks using LLM agents. We examine three attack\nscenarios: the collection of Personally Identifiable Information (PII), the\ngeneration of impersonation posts, and the creation of spear-phishing emails.\nOur experiments reveal the effectiveness of LLM agents in these attacks: LLM\nagents achieved a precision of up to 95.9% in collecting PII, up to 93.9% of\nimpersonation posts created by LLM agents were evaluated as authentic, and the\nclick rate for links in spear phishing emails created by LLM agents reached up\nto 46.67%. Additionally, our findings underscore the limitations of existing\nsafeguards in contemporary commercial LLMs, emphasizing the urgent need for\nmore robust security measures to prevent the misuse of LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have established them as\nagentic systems capable of planning and interacting with various tools. These\nLLM agents are often paired with web-based tools, enabling access to diverse\nsources and real-time information. Although these advancements offer\nsignificant benefits across various applications, they also increase the risk\nof malicious use, particularly in cyberattacks involving personal information.\nIn this work, we investigate the risks associated with misuse of LLM agents in\ncyberattacks involving personal data. Specifically, we aim to understand: 1)\nhow potent LLM agents can be when directed to conduct cyberattacks, 2) how\ncyberattacks are enhanced by web-based tools, and 3) how affordable and easy it\nbecomes to launch cyberattacks using LLM agents. We examine three attack\nscenarios: the collection of Personally Identifiable Information (PII), the\ngeneration of impersonation posts, and the creation of spear-phishing emails.\nOur experiments reveal the effectiveness of LLM agents in these attacks: LLM\nagents achieved a precision of up to 95.9% in collecting PII, up to 93.9% of\nimpersonation posts created by LLM agents were evaluated as authentic, and the\nclick rate for links in spear phishing emails created by LLM agents reached up\nto 46.67%. Additionally, our findings underscore the limitations of existing\nsafeguards in contemporary commercial LLMs, emphasizing the urgent need for\nmore robust security measures to prevent the misuse of LLM agents."
                },
                "authors": [
                    {
                        "name": "Hanna Kim"
                    },
                    {
                        "name": "Minkyoo Song"
                    },
                    {
                        "name": "Seung Ho Na"
                    },
                    {
                        "name": "Seungwon Shin"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14567v1",
                "updated": "2024-10-18T16:11:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    11,
                    29,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:11:29Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    11,
                    29,
                    4,
                    292,
                    0
                ],
                "title": "RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions"
                },
                "summary": "Conversational AI agents use Retrieval Augmented Generation (RAG) to provide\nverifiable document-grounded responses to user inquiries. However, many natural\nquestions do not have good answers: about 25\\% contain false\nassumptions~\\cite{Yu2023:CREPE}, and over 50\\% are\nambiguous~\\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve\ntheir responses to confusing questions. This paper presents a novel synthetic\ndata generation method to efficiently create a diverse set of context-grounded\nconfusing questions from a given document corpus. We conduct an empirical\ncomparative evaluation of several large language models as RAG agents to\nmeasure the accuracy of confusion detection and appropriate response\ngeneration. We contribute a benchmark dataset to the public domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational AI agents use Retrieval Augmented Generation (RAG) to provide\nverifiable document-grounded responses to user inquiries. However, many natural\nquestions do not have good answers: about 25\\% contain false\nassumptions~\\cite{Yu2023:CREPE}, and over 50\\% are\nambiguous~\\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve\ntheir responses to confusing questions. This paper presents a novel synthetic\ndata generation method to efficiently create a diverse set of context-grounded\nconfusing questions from a given document corpus. We conduct an empirical\ncomparative evaluation of several large language models as RAG agents to\nmeasure the accuracy of confusion detection and appropriate response\ngeneration. We contribute a benchmark dataset to the public domain."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Jinming Nian"
                    },
                    {
                        "name": "Alexandre Evfimievski"
                    },
                    {
                        "name": "Yi Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Fang"
                },
                "author": "Yi Fang",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14917v2",
                "updated": "2024-10-18T15:42:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    42,
                    6,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-23T11:13:25Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    11,
                    13,
                    25,
                    0,
                    267,
                    0
                ],
                "title": "With Ears to See and Eyes to Hear: Sound Symbolism Experiments with\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Ears to See and Eyes to Hear: Sound Symbolism Experiments with\n  Multimodal Large Language Models"
                },
                "summary": "Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have\ndemonstrated aptitude as potential substitutes for human participants in\nexperiments testing psycholinguistic phenomena. However, an understudied\nquestion is to what extent models that only have access to vision and text\nmodalities are able to implicitly understand sound-based phenomena via abstract\nreasoning from orthography and imagery alone. To investigate this, we analyse\nthe ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise\na non-arbitrary link between sounds and concepts) as well as their ability to\n\"hear\" via the interplay of the language and vision modules of open and\nclosed-source multimodal models. We perform multiple experiments, including\nreplicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism\ntasks, and comparing human judgements of linguistic iconicity with that of\nLLMs. Our results show that VLMs demonstrate varying levels of agreement with\nhuman labels, and more task information may be required for VLMs versus their\nhuman counterparts for in silico experimentation. We additionally see through\nhigher maximum agreement levels that Magnitude Symbolism is an easier pattern\nfor VLMs to identify than Shape Symbolism, and that an understanding of\nlinguistic iconicity is highly dependent on model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have\ndemonstrated aptitude as potential substitutes for human participants in\nexperiments testing psycholinguistic phenomena. However, an understudied\nquestion is to what extent models that only have access to vision and text\nmodalities are able to implicitly understand sound-based phenomena via abstract\nreasoning from orthography and imagery alone. To investigate this, we analyse\nthe ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise\na non-arbitrary link between sounds and concepts) as well as their ability to\n\"hear\" via the interplay of the language and vision modules of open and\nclosed-source multimodal models. We perform multiple experiments, including\nreplicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism\ntasks, and comparing human judgements of linguistic iconicity with that of\nLLMs. Our results show that VLMs demonstrate varying levels of agreement with\nhuman labels, and more task information may be required for VLMs versus their\nhuman counterparts for in silico experimentation. We additionally see through\nhigher maximum agreement levels that Magnitude Symbolism is an easier pattern\nfor VLMs to identify than Shape Symbolism, and that an understanding of\nlinguistic iconicity is highly dependent on model size."
                },
                "authors": [
                    {
                        "name": "Tyler Loakman"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Accepted to EMNLP 2024 (Camera Ready)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14546v1",
                "updated": "2024-10-18T15:41:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    41,
                    42,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T15:41:42Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    41,
                    42,
                    4,
                    292,
                    0
                ],
                "title": "High-Precision Galaxy Clustering Predictions from Small-Volume\n  Hydrodynamical Simulations via Control Variates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Precision Galaxy Clustering Predictions from Small-Volume\n  Hydrodynamical Simulations via Control Variates"
                },
                "summary": "Cosmological simulations of galaxy formation are an invaluable tool for\nunderstanding galaxy formation and its impact on cosmological parameter\ninference from large-scale structure. However, their high computational cost is\na significant obstacle for running simulations that probe cosmological volumes\ncomparable to those analyzed by contemporary large-scale structure experiments.\nIn this work, we explore the possibility of obtaining high-precision galaxy\nclustering predictions from small-volume hydrodynamical simulations such as\nMilleniumTNG and FLAMINGO via control variates. In this approach, the\nhydrodynamical full-physics simulation is paired with a matched low-resolution\ngravity-only simulation. By learning the galaxy-halo connection from the\nhydrodynamical simulation and applying it to the gravity-only counterpart, one\nobtains a galaxy population that closely mimics the one in the more expensive\nsimulation. One can then construct an estimator of galaxy clustering that\ncombines the clustering amplitudes in the small-volume hydrodynamical and\ngravity-only simulations with clustering amplitudes in a large-volume\ngravity-only simulation. Depending on the galaxy sample, clustering statistic,\nand scale, this galaxy clustering estimator can have an effective volume of up\nto around $100$ times the volume of the original hydrodynamical simulation in\nthe non-linear regime. With this approach, we can construct galaxy clustering\npredictions from existing simulations that are precise enough for mock analyses\nof next-generation large-scale structure surveys such as the Dark Energy\nSpectroscopic Instrument and the Legacy Survey of Space and Time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cosmological simulations of galaxy formation are an invaluable tool for\nunderstanding galaxy formation and its impact on cosmological parameter\ninference from large-scale structure. However, their high computational cost is\na significant obstacle for running simulations that probe cosmological volumes\ncomparable to those analyzed by contemporary large-scale structure experiments.\nIn this work, we explore the possibility of obtaining high-precision galaxy\nclustering predictions from small-volume hydrodynamical simulations such as\nMilleniumTNG and FLAMINGO via control variates. In this approach, the\nhydrodynamical full-physics simulation is paired with a matched low-resolution\ngravity-only simulation. By learning the galaxy-halo connection from the\nhydrodynamical simulation and applying it to the gravity-only counterpart, one\nobtains a galaxy population that closely mimics the one in the more expensive\nsimulation. One can then construct an estimator of galaxy clustering that\ncombines the clustering amplitudes in the small-volume hydrodynamical and\ngravity-only simulations with clustering amplitudes in a large-volume\ngravity-only simulation. Depending on the galaxy sample, clustering statistic,\nand scale, this galaxy clustering estimator can have an effective volume of up\nto around $100$ times the volume of the original hydrodynamical simulation in\nthe non-linear regime. With this approach, we can construct galaxy clustering\npredictions from existing simulations that are precise enough for mock analyses\nof next-generation large-scale structure surveys such as the Dark Energy\nSpectroscopic Instrument and the Legacy Survey of Space and Time."
                },
                "authors": [
                    {
                        "name": "Alexandra Doytcheva"
                    },
                    {
                        "name": "Filomela V. Gerou"
                    },
                    {
                        "name": "Johannes U. Lange"
                    }
                ],
                "author_detail": {
                    "name": "Johannes U. Lange"
                },
                "author": "Johannes U. Lange",
                "arxiv_comment": "12 pages, 5 figures, submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14545v1",
                "updated": "2024-10-18T15:40:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    40,
                    48,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T15:40:48Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    40,
                    48,
                    4,
                    292,
                    0
                ],
                "title": "Tell me what I need to know: Exploring LLM-based (Personalized)\n  Abstractive Multi-Source Meeting Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tell me what I need to know: Exploring LLM-based (Personalized)\n  Abstractive Multi-Source Meeting Summarization"
                },
                "summary": "Meeting summarization is crucial in digital communication, but existing\nsolutions struggle with salience identification to generate personalized,\nworkable summaries, and context understanding to fully comprehend the meetings'\ncontent. Previous attempts to address these issues by considering related\nsupplementary resources (e.g., presentation slides) alongside transcripts are\nhindered by models' limited context sizes and handling the additional\ncomplexities of the multi-source tasks, such as identifying relevant\ninformation in additional files and seamlessly aligning it with the meeting\ncontent. This work explores multi-source meeting summarization considering\nsupplementary materials through a three-stage large language model approach:\nidentifying transcript passages needing additional context, inferring relevant\ndetails from supplementary materials and inserting them into the transcript,\nand generating a summary from this enriched transcript. Our multi-source\napproach enhances model understanding, increasing summary relevance by ~9% and\nproducing more content-rich outputs. We introduce a personalization protocol\nthat extracts participant characteristics and tailors summaries accordingly,\nimproving informativeness by ~10%. This work further provides insights on\nperformance-cost trade-offs across four leading model families, including\nedge-device capable options. Our approach can be extended to similar complex\ngenerative tasks benefitting from additional resources and personalization,\nsuch as dialogue systems and action planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meeting summarization is crucial in digital communication, but existing\nsolutions struggle with salience identification to generate personalized,\nworkable summaries, and context understanding to fully comprehend the meetings'\ncontent. Previous attempts to address these issues by considering related\nsupplementary resources (e.g., presentation slides) alongside transcripts are\nhindered by models' limited context sizes and handling the additional\ncomplexities of the multi-source tasks, such as identifying relevant\ninformation in additional files and seamlessly aligning it with the meeting\ncontent. This work explores multi-source meeting summarization considering\nsupplementary materials through a three-stage large language model approach:\nidentifying transcript passages needing additional context, inferring relevant\ndetails from supplementary materials and inserting them into the transcript,\nand generating a summary from this enriched transcript. Our multi-source\napproach enhances model understanding, increasing summary relevance by ~9% and\nproducing more content-rich outputs. We introduce a personalization protocol\nthat extracts participant characteristics and tailors summaries accordingly,\nimproving informativeness by ~10%. This work further provides insights on\nperformance-cost trade-offs across four leading model families, including\nedge-device capable options. Our approach can be extended to similar complex\ngenerative tasks benefitting from additional resources and personalization,\nsuch as dialogue systems and action planning."
                },
                "authors": [
                    {
                        "name": "Frederic Kirstein"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Robert Kratel"
                    },
                    {
                        "name": "Bela Gipp"
                    }
                ],
                "author_detail": {
                    "name": "Bela Gipp"
                },
                "author": "Bela Gipp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13901v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13901v3",
                "updated": "2024-10-18T15:25:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    25,
                    44,
                    4,
                    292,
                    0
                ],
                "published": "2024-03-20T18:13:17Z",
                "published_parsed": [
                    2024,
                    3,
                    20,
                    18,
                    13,
                    17,
                    2,
                    80,
                    0
                ],
                "title": "Train & Constrain: Phonologically Informed Tongue-Twister Generation\n  from Topics and Paraphrases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Train & Constrain: Phonologically Informed Tongue-Twister Generation\n  from Topics and Paraphrases"
                },
                "summary": "Previous work in phonologically and phonetically grounded language generation\nhas mainly focused on domains such as puns and poetry. In this article, we\npresent new work on the generation of English tongue twisters - a form of\nlanguage that is required to be conditioned on a phoneme level to maximize\nsound overlap, while maintaining semantic consistency with an input topic or\nphrase and still being grammatically correct. We present TwisterLister, a\npipeline for generating phonologically informed tongue twisters from large\nlanguage models (LLMs) that we use to generate TwistList 2.0, the largest\nannotated dataset of tongue twisters to date, consisting of 17K+ examples from\na combination of human and LLM authors. Our generation pipeline involves the\nuse of a phonologically constrained vocabulary alongside LLM prompting to\ngenerate novel, non-derivative tongue twister examples. We additionally present\nthe results of automatic and human evaluation of smaller models trained on our\ngenerated dataset to demonstrate the extent to which phonologically motivated\nlanguage types can be generated without explicit injection of phonological\nknowledge. Additionally, we introduce a phoneme-aware constrained decoding\nmodule (PACD) that can be integrated into an autoregressive language model and\ndemonstrate that this method generates good quality tongue twisters both with\nand without fine-tuning the underlying language model. We also design and\nimplement a range of automatic metrics for the task of tongue twister\ngeneration that is phonologically motivated and captures the unique essence of\ntongue twisters, primarily based on phonemic edit distance (PED)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous work in phonologically and phonetically grounded language generation\nhas mainly focused on domains such as puns and poetry. In this article, we\npresent new work on the generation of English tongue twisters - a form of\nlanguage that is required to be conditioned on a phoneme level to maximize\nsound overlap, while maintaining semantic consistency with an input topic or\nphrase and still being grammatically correct. We present TwisterLister, a\npipeline for generating phonologically informed tongue twisters from large\nlanguage models (LLMs) that we use to generate TwistList 2.0, the largest\nannotated dataset of tongue twisters to date, consisting of 17K+ examples from\na combination of human and LLM authors. Our generation pipeline involves the\nuse of a phonologically constrained vocabulary alongside LLM prompting to\ngenerate novel, non-derivative tongue twister examples. We additionally present\nthe results of automatic and human evaluation of smaller models trained on our\ngenerated dataset to demonstrate the extent to which phonologically motivated\nlanguage types can be generated without explicit injection of phonological\nknowledge. Additionally, we introduce a phoneme-aware constrained decoding\nmodule (PACD) that can be integrated into an autoregressive language model and\ndemonstrate that this method generates good quality tongue twisters both with\nand without fine-tuning the underlying language model. We also design and\nimplement a range of automatic metrics for the task of tongue twister\ngeneration that is phonologically motivated and captures the unique essence of\ntongue twisters, primarily based on phonemic edit distance (PED)"
                },
                "authors": [
                    {
                        "name": "Tyler Loakman"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Accepted Final Version to Computational Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13901v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13901v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07864v2",
                "updated": "2024-10-18T15:23:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    23,
                    26,
                    4,
                    292,
                    0
                ],
                "published": "2024-04-11T15:57:12Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    15,
                    57,
                    12,
                    3,
                    102,
                    0
                ],
                "title": "Inferring Change Points in High-Dimensional Regression via Approximate\n  Message Passing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Change Points in High-Dimensional Regression via Approximate\n  Message Passing"
                },
                "summary": "We consider the problem of localizing change points in a generalized linear\nmodel (GLM), a model that covers many widely studied problems in statistical\nlearning including linear, logistic, and rectified linear regression. We\npropose a novel and computationally efficient Approximate Message Passing (AMP)\nalgorithm for estimating both the signals and the change point locations, and\nrigorously characterize its performance in the high-dimensional limit where the\nnumber of parameters $p$ is proportional to the number of samples $n$. This\ncharacterization is in terms of a state evolution recursion, which allows us to\nprecisely compute performance measures such as the asymptotic Hausdorff error\nof our change point estimates, and allows us to tailor the algorithm to take\nadvantage of any prior structural information on the signals and change points.\nMoreover, we show how our AMP iterates can be used to efficiently compute a\nBayesian posterior distribution over the change point locations in the\nhigh-dimensional limit. We validate our theory via numerical experiments, and\ndemonstrate the favorable performance of our estimators on both synthetic and\nreal data in the settings of linear, logistic, and rectified linear regression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of localizing change points in a generalized linear\nmodel (GLM), a model that covers many widely studied problems in statistical\nlearning including linear, logistic, and rectified linear regression. We\npropose a novel and computationally efficient Approximate Message Passing (AMP)\nalgorithm for estimating both the signals and the change point locations, and\nrigorously characterize its performance in the high-dimensional limit where the\nnumber of parameters $p$ is proportional to the number of samples $n$. This\ncharacterization is in terms of a state evolution recursion, which allows us to\nprecisely compute performance measures such as the asymptotic Hausdorff error\nof our change point estimates, and allows us to tailor the algorithm to take\nadvantage of any prior structural information on the signals and change points.\nMoreover, we show how our AMP iterates can be used to efficiently compute a\nBayesian posterior distribution over the change point locations in the\nhigh-dimensional limit. We validate our theory via numerical experiments, and\ndemonstrate the favorable performance of our estimators on both synthetic and\nreal data in the settings of linear, logistic, and rectified linear regression."
                },
                "authors": [
                    {
                        "name": "Gabriel Arpino"
                    },
                    {
                        "name": "Xiaoqi Liu"
                    },
                    {
                        "name": "Julia Gontarek"
                    },
                    {
                        "name": "Ramji Venkataramanan"
                    }
                ],
                "author_detail": {
                    "name": "Ramji Venkataramanan"
                },
                "author": "Ramji Venkataramanan",
                "arxiv_comment": "43 pages, 9 figures. A preliminary version of this paper appeared in\n  ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16571v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16571v5",
                "updated": "2024-10-18T15:11:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    11,
                    15,
                    4,
                    292,
                    0
                ],
                "published": "2024-01-29T21:13:01Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    21,
                    13,
                    1,
                    0,
                    29,
                    0
                ],
                "title": "Individualized Multi-Treatment Response Curves Estimation using RBF-net\n  with Shared Neurons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Individualized Multi-Treatment Response Curves Estimation using RBF-net\n  with Shared Neurons"
                },
                "summary": "Heterogeneous treatment effect estimation is an important problem in\nprecision medicine. Specific interests lie in identifying the differential\neffect of different treatments based on some external covariates. We propose a\nnovel non-parametric treatment effect estimation method in a multi-treatment\nsetting. Our non-parametric modeling of the response curves relies on radial\nbasis function (RBF)-nets with shared hidden neurons. Our model thus\nfacilitates modeling commonality among the treatment outcomes. The estimation\nand inference schemes are developed under a Bayesian framework using\nthresholded best linear projections and implemented via an efficient Markov\nchain Monte Carlo algorithm, appropriately accommodating uncertainty in all\naspects of the analysis. The numerical performance of the method is\ndemonstrated through simulation experiments. Applying our proposed method to\nMIMIC data, we obtain several interesting findings related to the impact of\ndifferent treatment strategies on the length of ICU stay and 12-hour SOFA score\nfor sepsis patients who are home-discharged.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous treatment effect estimation is an important problem in\nprecision medicine. Specific interests lie in identifying the differential\neffect of different treatments based on some external covariates. We propose a\nnovel non-parametric treatment effect estimation method in a multi-treatment\nsetting. Our non-parametric modeling of the response curves relies on radial\nbasis function (RBF)-nets with shared hidden neurons. Our model thus\nfacilitates modeling commonality among the treatment outcomes. The estimation\nand inference schemes are developed under a Bayesian framework using\nthresholded best linear projections and implemented via an efficient Markov\nchain Monte Carlo algorithm, appropriately accommodating uncertainty in all\naspects of the analysis. The numerical performance of the method is\ndemonstrated through simulation experiments. Applying our proposed method to\nMIMIC data, we obtain several interesting findings related to the impact of\ndifferent treatment strategies on the length of ICU stay and 12-hour SOFA score\nfor sepsis patients who are home-discharged."
                },
                "authors": [
                    {
                        "name": "Peter Chang"
                    },
                    {
                        "name": "Arkaprava Roy"
                    }
                ],
                "author_detail": {
                    "name": "Arkaprava Roy"
                },
                "author": "Arkaprava Roy",
                "arxiv_comment": "17 pages (not including references), with 7 tables and 1 figure in\n  the main paper, 2 figures in the supplementary materials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16571v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16571v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v3",
                "updated": "2024-10-18T15:03:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    3,
                    8,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Although current Multi-modal Large Language Models (MLLMs) demonstrate\npromising results in video understanding, processing extremely long videos\nremains an ongoing challenge. Typically, MLLMs struggle with handling thousands\nof visual tokens that exceed the maximum context length, and they suffer from\nthe information decay due to token aggregation. Another challenge is the high\ncomputational cost stemming from the large number of video tokens. To tackle\nthese issues, we propose Video-XL, an extra-long vision language model designed\nfor efficient hour-scale video understanding. Specifically, we argue that LLMs\ncan be adapted as effective visual condensers and propose Visual Context Latent\nSummarization which condenses visual contexts into highly compact forms.\nExtensive experiments demonstrate that our model achieves promising results on\npopular long video understanding benchmarks. For example, Video-XL outperforms\nthe current state-of-the-art method on VNBench by nearly 10\\% in accuracy.\nMoreover, Video-XL presents an impressive balance between efficiency and\neffectiveness, processing 2048 frames on a single 80GB GPU while achieving\nnearly 95% accuracy in the Needle-in-a-Haystack evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although current Multi-modal Large Language Models (MLLMs) demonstrate\npromising results in video understanding, processing extremely long videos\nremains an ongoing challenge. Typically, MLLMs struggle with handling thousands\nof visual tokens that exceed the maximum context length, and they suffer from\nthe information decay due to token aggregation. Another challenge is the high\ncomputational cost stemming from the large number of video tokens. To tackle\nthese issues, we propose Video-XL, an extra-long vision language model designed\nfor efficient hour-scale video understanding. Specifically, we argue that LLMs\ncan be adapted as effective visual condensers and propose Visual Context Latent\nSummarization which condenses visual contexts into highly compact forms.\nExtensive experiments demonstrate that our model achieves promising results on\npopular long video understanding benchmarks. For example, Video-XL outperforms\nthe current state-of-the-art method on VNBench by nearly 10\\% in accuracy.\nMoreover, Video-XL presents an impressive balance between efficiency and\neffectiveness, processing 2048 frames on a single 80GB GPU while achieving\nnearly 95% accuracy in the Needle-in-a-Haystack evaluation."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14516v1",
                "updated": "2024-10-18T14:55:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    55,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T14:55:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    55,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "Do LLMs \"know\" internally when they follow instructions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs \"know\" internally when they follow instructions?"
                },
                "summary": "Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents."
                },
                "authors": [
                    {
                        "name": "Juyeon Heo"
                    },
                    {
                        "name": "Christina Heinze-Deml"
                    },
                    {
                        "name": "Oussama Elachqar"
                    },
                    {
                        "name": "Shirley Ren"
                    },
                    {
                        "name": "Udhay Nallasamy"
                    },
                    {
                        "name": "Andy Miller"
                    },
                    {
                        "name": "Kwan Ho Ryan Chan"
                    },
                    {
                        "name": "Jaya Narain"
                    }
                ],
                "author_detail": {
                    "name": "Jaya Narain"
                },
                "author": "Jaya Narain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14515v1",
                "updated": "2024-10-18T14:54:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    54,
                    40,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T14:54:40Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    54,
                    40,
                    4,
                    292,
                    0
                ],
                "title": "Efficient Annotator Reliability Assessment and Sample Weighting for\n  Knowledge-Based Misinformation Detection on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Annotator Reliability Assessment and Sample Weighting for\n  Knowledge-Based Misinformation Detection on Social Media"
                },
                "summary": "Misinformation spreads rapidly on social media, confusing the truth and\ntargetting potentially vulnerable people. To effectively mitigate the negative\nimpact of misinformation, it must first be accurately detected before applying\na mitigation strategy, such as X's community notes, which is currently a manual\nprocess. This study takes a knowledge-based approach to misinformation\ndetection, modelling the problem similarly to one of natural language\ninference. The EffiARA annotation framework is introduced, aiming to utilise\ninter- and intra-annotator agreement to understand the reliability of each\nannotator and influence the training of large language models for\nclassification based on annotator reliability. In assessing the EffiARA\nannotation framework, the Russo-Ukrainian Conflict Knowledge-Based\nMisinformation Classification Dataset (RUC-MCD) was developed and made publicly\navailable. This study finds that sample weighting using annotator reliability\nperforms the best, utilising both inter- and intra-annotator agreement and\nsoft-label training. The highest classification performance achieved using\nLlama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misinformation spreads rapidly on social media, confusing the truth and\ntargetting potentially vulnerable people. To effectively mitigate the negative\nimpact of misinformation, it must first be accurately detected before applying\na mitigation strategy, such as X's community notes, which is currently a manual\nprocess. This study takes a knowledge-based approach to misinformation\ndetection, modelling the problem similarly to one of natural language\ninference. The EffiARA annotation framework is introduced, aiming to utilise\ninter- and intra-annotator agreement to understand the reliability of each\nannotator and influence the training of large language models for\nclassification based on annotator reliability. In assessing the EffiARA\nannotation framework, the Russo-Ukrainian Conflict Knowledge-Based\nMisinformation Classification Dataset (RUC-MCD) was developed and made publicly\navailable. This study finds that sample weighting using annotator reliability\nperforms the best, utilising both inter- and intra-annotator agreement and\nsoft-label training. The highest classification performance achieved using\nLlama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large."
                },
                "authors": [
                    {
                        "name": "Owen Cook"
                    },
                    {
                        "name": "Charlie Grimshaw"
                    },
                    {
                        "name": "Ben Wu"
                    },
                    {
                        "name": "Sophie Dillon"
                    },
                    {
                        "name": "Jack Hicks"
                    },
                    {
                        "name": "Luke Jones"
                    },
                    {
                        "name": "Thomas Smith"
                    },
                    {
                        "name": "Matyas Szert"
                    },
                    {
                        "name": "Xingyi Song"
                    }
                ],
                "author_detail": {
                    "name": "Xingyi Song"
                },
                "author": "Xingyi Song",
                "arxiv_comment": "8 pages, 3 figures, 3 tables. Code available here:\n  https://github.com/MiniEggz/ruc-misinfo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07361v2",
                "updated": "2024-10-18T14:38:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    38,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-11T15:28:48Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    15,
                    28,
                    48,
                    1,
                    163,
                    0
                ],
                "title": "Deep Implicit Optimization for Robust and Flexible Image Registration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Implicit Optimization for Robust and Flexible Image Registration"
                },
                "summary": "Deep Learning in Image Registration (DLIR) methods have been tremendously\nsuccessful in image registration due to their speed and ability to incorporate\nweak label supervision at training time. However, DLIR methods forego many of\nthe benefits of classical optimization-based methods. The functional nature of\ndeep networks do not guarantee that the predicted transformation is a local\nminima of the registration objective, the representation of the transformation\n(displacement/velocity field/affine) is fixed, and the networks are not robust\nto domain shift. Our method aims to bridge this gap between classical and\nlearning methods by incorporating optimization as a layer in a deep network. A\ndeep network is trained to predict multi-scale dense feature images that are\nregistered using a black box iterative optimization solver. This optimal warp\nis then used to minimize image and label alignment errors. By implicitly\ndifferentiating end-to-end through an iterative optimization solver, our\nlearned features are registration and label-aware, and the warp functions are\nguaranteed to be local minima of the registration objective in the feature\nspace. Our framework shows excellent performance on in-domain datasets, and is\nagnostic to domain shift such as anisotropy and varying intensity profiles. For\nthe first time, our method allows switching between arbitrary transformation\nrepresentations (free-form to diffeomorphic) at test time with zero retraining.\nEnd-to-end feature learning also facilitates interpretability of features, and\nout-of-the-box promptability using additional label-fidelity terms at\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning in Image Registration (DLIR) methods have been tremendously\nsuccessful in image registration due to their speed and ability to incorporate\nweak label supervision at training time. However, DLIR methods forego many of\nthe benefits of classical optimization-based methods. The functional nature of\ndeep networks do not guarantee that the predicted transformation is a local\nminima of the registration objective, the representation of the transformation\n(displacement/velocity field/affine) is fixed, and the networks are not robust\nto domain shift. Our method aims to bridge this gap between classical and\nlearning methods by incorporating optimization as a layer in a deep network. A\ndeep network is trained to predict multi-scale dense feature images that are\nregistered using a black box iterative optimization solver. This optimal warp\nis then used to minimize image and label alignment errors. By implicitly\ndifferentiating end-to-end through an iterative optimization solver, our\nlearned features are registration and label-aware, and the warp functions are\nguaranteed to be local minima of the registration objective in the feature\nspace. Our framework shows excellent performance on in-domain datasets, and is\nagnostic to domain shift such as anisotropy and varying intensity profiles. For\nthe first time, our method allows switching between arbitrary transformation\nrepresentations (free-form to diffeomorphic) at test time with zero retraining.\nEnd-to-end feature learning also facilitates interpretability of features, and\nout-of-the-box promptability using additional label-fidelity terms at\ninference."
                },
                "authors": [
                    {
                        "name": "Rohit Jena"
                    },
                    {
                        "name": "Pratik Chaudhari"
                    },
                    {
                        "name": "James C. Gee"
                    }
                ],
                "author_detail": {
                    "name": "James C. Gee"
                },
                "author": "James C. Gee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14485v1",
                "updated": "2024-10-18T14:10:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    10,
                    16,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T14:10:16Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    10,
                    16,
                    4,
                    292,
                    0
                ],
                "title": "CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers and\n  Fully-Connected Neural Networks for Causally Constrained Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers and\n  Fully-Connected Neural Networks for Causally Constrained Predictions"
                },
                "summary": "Artificial Neural Networks (ANNs), including fully-connected networks and\ntransformers, are highly flexible and powerful function approximators, widely\napplied in fields like computer vision and natural language processing.\nHowever, their inability to inherently respect causal structures can limit\ntheir robustness, making them vulnerable to covariate shift and difficult to\ninterpret/explain. This poses significant challenges for their reliability in\nreal-world applications. In this paper, we introduce Causal Fully-Connected\nNeural Networks (CFCNs) and Causal Transformers (CaTs), two general model\nfamilies designed to operate under predefined causal constraints, as specified\nby a Directed Acyclic Graph (DAG). These models retain the powerful function\napproximation abilities of traditional neural networks while adhering to the\nunderlying structural constraints, improving robustness, reliability, and\ninterpretability at inference time. This approach opens new avenues for\ndeploying neural networks in more demanding, real-world scenarios where\nrobustness and explainability is critical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Neural Networks (ANNs), including fully-connected networks and\ntransformers, are highly flexible and powerful function approximators, widely\napplied in fields like computer vision and natural language processing.\nHowever, their inability to inherently respect causal structures can limit\ntheir robustness, making them vulnerable to covariate shift and difficult to\ninterpret/explain. This poses significant challenges for their reliability in\nreal-world applications. In this paper, we introduce Causal Fully-Connected\nNeural Networks (CFCNs) and Causal Transformers (CaTs), two general model\nfamilies designed to operate under predefined causal constraints, as specified\nby a Directed Acyclic Graph (DAG). These models retain the powerful function\napproximation abilities of traditional neural networks while adhering to the\nunderlying structural constraints, improving robustness, reliability, and\ninterpretability at inference time. This approach opens new avenues for\ndeploying neural networks in more demanding, real-world scenarios where\nrobustness and explainability is critical."
                },
                "authors": [
                    {
                        "name": "Matthew J. Vowels"
                    },
                    {
                        "name": "Mathieu Rochat"
                    },
                    {
                        "name": "Sina Akbari"
                    }
                ],
                "author_detail": {
                    "name": "Sina Akbari"
                },
                "author": "Sina Akbari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14483v1",
                "updated": "2024-10-18T14:06:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    6,
                    49,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T14:06:49Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    6,
                    49,
                    4,
                    292,
                    0
                ],
                "title": "Spectral Representations for Accurate Causal Uncertainty Quantification\n  with Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral Representations for Accurate Causal Uncertainty Quantification\n  with Gaussian Processes"
                },
                "summary": "Accurate uncertainty quantification for causal effects is essential for\nrobust decision making in complex systems, but remains challenging in\nnon-parametric settings. One promising framework represents conditional\ndistributions in a reproducing kernel Hilbert space and places Gaussian process\npriors on them to infer posteriors on causal effects, but requires restrictive\nnuclear dominant kernels and approximations that lead to unreliable uncertainty\nestimates. In this work, we introduce a method, IMPspec, that addresses these\nlimitations via a spectral representation of the Hilbert space. We show that\nposteriors in this model can be obtained explicitly, by extending a result in\nHilbert space regression theory. We also learn the spectral representation to\noptimise posterior calibration. Our method achieves state-of-the-art\nperformance in uncertainty quantification and causal Bayesian optimisation\nacross simulations and a healthcare application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate uncertainty quantification for causal effects is essential for\nrobust decision making in complex systems, but remains challenging in\nnon-parametric settings. One promising framework represents conditional\ndistributions in a reproducing kernel Hilbert space and places Gaussian process\npriors on them to infer posteriors on causal effects, but requires restrictive\nnuclear dominant kernels and approximations that lead to unreliable uncertainty\nestimates. In this work, we introduce a method, IMPspec, that addresses these\nlimitations via a spectral representation of the Hilbert space. We show that\nposteriors in this model can be obtained explicitly, by extending a result in\nHilbert space regression theory. We also learn the spectral representation to\noptimise posterior calibration. Our method achieves state-of-the-art\nperformance in uncertainty quantification and causal Bayesian optimisation\nacross simulations and a healthcare application."
                },
                "authors": [
                    {
                        "name": "Hugh Dance"
                    },
                    {
                        "name": "Peter Orbanz"
                    },
                    {
                        "name": "Arthur Gretton"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Gretton"
                },
                "author": "Arthur Gretton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14480v1",
                "updated": "2024-10-18T14:03:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    3,
                    52,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T14:03:52Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    3,
                    52,
                    4,
                    292,
                    0
                ],
                "title": "Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of\n  Language Models"
                },
                "summary": "As large language models (LLMs) continue to advance, the need for precise and\nefficient evaluation metrics becomes more pressing. Traditional approaches,\nwhile informative, often face limitations in computational demands and\ninterpretability. In this paper, we introduce a novel hybrid evaluation method\nthat integrates two established techniques: entropy derived from covariance\nmatrices and the Matrix Nuclear Norm (MNN). Our method begins by normalizing\nhidden states from LLMs, then computes the covariance matrix and MNN from these\nrepresentations. We further calculate the entropy of the covariance matrix to\ncapture uncertainty and redundancy in the model's outputs. By combining these\nmetrics into a composite score, we offer a comprehensive evaluation framework\nthat balances accuracy with computational efficiency. Additionally, our\napproach allows for flexibility in adjusting the weightings between entropy and\nMNN, tailoring the evaluation for different objectives. Through a series of\nexperiments on various LLMs, we demonstrate the robustness and efficacy of our\nmethod, offering deeper insights into model performance. This work contributes\nto the ongoing development of LLM evaluation and opens avenues for future\ninnovations in model assessment techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the need for precise and\nefficient evaluation metrics becomes more pressing. Traditional approaches,\nwhile informative, often face limitations in computational demands and\ninterpretability. In this paper, we introduce a novel hybrid evaluation method\nthat integrates two established techniques: entropy derived from covariance\nmatrices and the Matrix Nuclear Norm (MNN). Our method begins by normalizing\nhidden states from LLMs, then computes the covariance matrix and MNN from these\nrepresentations. We further calculate the entropy of the covariance matrix to\ncapture uncertainty and redundancy in the model's outputs. By combining these\nmetrics into a composite score, we offer a comprehensive evaluation framework\nthat balances accuracy with computational efficiency. Additionally, our\napproach allows for flexibility in adjusting the weightings between entropy and\nMNN, tailoring the evaluation for different objectives. Through a series of\nexperiments on various LLMs, we demonstrate the robustness and efficacy of our\nmethod, offering deeper insights into model performance. This work contributes\nto the ongoing development of LLM evaluation and opens avenues for future\ninnovations in model assessment techniques."
                },
                "authors": [
                    {
                        "name": "James Vo"
                    }
                ],
                "author_detail": {
                    "name": "James Vo"
                },
                "author": "James Vo",
                "arxiv_comment": "The method is currently under experimentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09804v2",
                "updated": "2024-10-18T14:03:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    3,
                    5,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-13T11:15:38Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    11,
                    15,
                    38,
                    6,
                    287,
                    0
                ],
                "title": "BlackDAN: A Black-Box Multi-Objective Approach for Effective and\n  Contextual Jailbreaking of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlackDAN: A Black-Box Multi-Objective Approach for Effective and\n  Contextual Jailbreaking of Large Language Models"
                },
                "summary": "While large language models (LLMs) exhibit remarkable capabilities across\nvarious tasks, they encounter potential security risks such as jailbreak\nattacks, which exploit vulnerabilities to bypass security measures and generate\nharmful outputs. Existing jailbreak strategies mainly focus on maximizing\nattack success rate (ASR), frequently neglecting other critical factors,\nincluding the relevance of the jailbreak response to the query and the level of\nstealthiness. This narrow focus on single objectives can result in ineffective\nattacks that either lack contextual relevance or are easily recognizable. In\nthis work, we introduce BlackDAN, an innovative black-box attack framework with\nmulti-objective optimization, aiming to generate high-quality prompts that\neffectively facilitate jailbreaking while maintaining contextual relevance and\nminimizing detectability. BlackDAN leverages Multiobjective Evolutionary\nAlgorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks\nacross multiple objectives including ASR, stealthiness, and semantic relevance.\nBy integrating mechanisms like mutation, crossover, and Pareto-dominance,\nBlackDAN provides a transparent and interpretable process for generating\njailbreaks. Furthermore, the framework allows customization based on user\npreferences, enabling the selection of prompts that balance harmfulness,\nrelevance, and other factors. Experimental results demonstrate that BlackDAN\noutperforms traditional single-objective methods, yielding higher success rates\nand improved robustness across various LLMs and multimodal LLMs, while ensuring\njailbreak responses are both relevant and less detectable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) exhibit remarkable capabilities across\nvarious tasks, they encounter potential security risks such as jailbreak\nattacks, which exploit vulnerabilities to bypass security measures and generate\nharmful outputs. Existing jailbreak strategies mainly focus on maximizing\nattack success rate (ASR), frequently neglecting other critical factors,\nincluding the relevance of the jailbreak response to the query and the level of\nstealthiness. This narrow focus on single objectives can result in ineffective\nattacks that either lack contextual relevance or are easily recognizable. In\nthis work, we introduce BlackDAN, an innovative black-box attack framework with\nmulti-objective optimization, aiming to generate high-quality prompts that\neffectively facilitate jailbreaking while maintaining contextual relevance and\nminimizing detectability. BlackDAN leverages Multiobjective Evolutionary\nAlgorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks\nacross multiple objectives including ASR, stealthiness, and semantic relevance.\nBy integrating mechanisms like mutation, crossover, and Pareto-dominance,\nBlackDAN provides a transparent and interpretable process for generating\njailbreaks. Furthermore, the framework allows customization based on user\npreferences, enabling the selection of prompts that balance harmfulness,\nrelevance, and other factors. Experimental results demonstrate that BlackDAN\noutperforms traditional single-objective methods, yielding higher success rates\nand improved robustness across various LLMs and multimodal LLMs, while ensuring\njailbreak responses are both relevant and less detectable."
                },
                "authors": [
                    {
                        "name": "Xinyuan Wang"
                    },
                    {
                        "name": "Victor Shea-Jay Huang"
                    },
                    {
                        "name": "Renmiao Chen"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Chengwei Pan"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14479v1",
                "updated": "2024-10-18T14:02:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    2,
                    34,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T14:02:34Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    2,
                    34,
                    4,
                    292,
                    0
                ],
                "title": "Backdoored Retrievers for Prompt Injection Attacks on Retrieval\n  Augmented Generation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoored Retrievers for Prompt Injection Attacks on Retrieval\n  Augmented Generation of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating coherent text but remain limited by the static nature of their\ntraining data. Retrieval Augmented Generation (RAG) addresses this issue by\ncombining LLMs with up-to-date information retrieval, but also expand the\nattack surface of the system. This paper investigates prompt injection attacks\non RAG, focusing on malicious objectives beyond misinformation, such as\ninserting harmful links, promoting unauthorized services, and initiating\ndenial-of-service behaviors. We build upon existing corpus poisoning techniques\nand propose a novel backdoor attack aimed at the fine-tuning process of the\ndense retriever component. Our experiments reveal that corpus poisoning can\nachieve significant attack success rates through the injection of a small\nnumber of compromised documents into the retriever corpus. In contrast,\nbackdoor attacks demonstrate even higher success rates but necessitate a more\ncomplex setup, as the victim must fine-tune the retriever using the attacker\npoisoned dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating coherent text but remain limited by the static nature of their\ntraining data. Retrieval Augmented Generation (RAG) addresses this issue by\ncombining LLMs with up-to-date information retrieval, but also expand the\nattack surface of the system. This paper investigates prompt injection attacks\non RAG, focusing on malicious objectives beyond misinformation, such as\ninserting harmful links, promoting unauthorized services, and initiating\ndenial-of-service behaviors. We build upon existing corpus poisoning techniques\nand propose a novel backdoor attack aimed at the fine-tuning process of the\ndense retriever component. Our experiments reveal that corpus poisoning can\nachieve significant attack success rates through the injection of a small\nnumber of compromised documents into the retriever corpus. In contrast,\nbackdoor attacks demonstrate even higher success rates but necessitate a more\ncomplex setup, as the victim must fine-tune the retriever using the attacker\npoisoned dataset."
                },
                "authors": [
                    {
                        "name": "Cody Clop"
                    },
                    {
                        "name": "Yannick Teglia"
                    }
                ],
                "author_detail": {
                    "name": "Yannick Teglia"
                },
                "author": "Yannick Teglia",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v2",
                "updated": "2024-10-18T13:59:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    59,
                    54,
                    4,
                    292,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00552v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00552v3",
                "updated": "2024-10-18T13:49:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    49,
                    19,
                    4,
                    292,
                    0
                ],
                "published": "2024-05-01T14:50:58Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    14,
                    50,
                    58,
                    2,
                    122,
                    0
                ],
                "title": "Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs"
                },
                "summary": "We present a novel approach for long-term human trajectory prediction in\nindoor human-centric environments, which is essential for long-horizon robot\nplanning in these environments. State-of-the-art human trajectory prediction\nmethods are limited by their focus on collision avoidance and short-term\nplanning, and their inability to model complex interactions of humans with the\nenvironment. In contrast, our approach overcomes these limitations by\npredicting sequences of human interactions with the environment and using this\ninformation to guide trajectory predictions over a horizon of up to 60s. We\nleverage Large Language Models (LLMs) to predict interactions with the\nenvironment by conditioning the LLM prediction on rich contextual information\nabout the scene. This information is given as a 3D Dynamic Scene Graph that\nencodes the geometry, semantics, and traversability of the environment into a\nhierarchical representation. We then ground these interaction sequences into\nmulti-modal spatio-temporal distributions over human positions using a\nprobabilistic approach based on continuous-time Markov Chains. To evaluate our\napproach, we introduce a new semi-synthetic dataset of long-term human\ntrajectories in complex indoor environments, which also includes annotations of\nhuman-object interactions. We show in thorough experimental evaluations that\nour approach achieves a 54% lower average negative log-likelihood and a 26.5%\nlower Best-of-20 displacement error compared to the best non-privileged (i.e.,\nevaluated in a zero-shot fashion on the dataset) baselines for a time horizon\nof 60s.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach for long-term human trajectory prediction in\nindoor human-centric environments, which is essential for long-horizon robot\nplanning in these environments. State-of-the-art human trajectory prediction\nmethods are limited by their focus on collision avoidance and short-term\nplanning, and their inability to model complex interactions of humans with the\nenvironment. In contrast, our approach overcomes these limitations by\npredicting sequences of human interactions with the environment and using this\ninformation to guide trajectory predictions over a horizon of up to 60s. We\nleverage Large Language Models (LLMs) to predict interactions with the\nenvironment by conditioning the LLM prediction on rich contextual information\nabout the scene. This information is given as a 3D Dynamic Scene Graph that\nencodes the geometry, semantics, and traversability of the environment into a\nhierarchical representation. We then ground these interaction sequences into\nmulti-modal spatio-temporal distributions over human positions using a\nprobabilistic approach based on continuous-time Markov Chains. To evaluate our\napproach, we introduce a new semi-synthetic dataset of long-term human\ntrajectories in complex indoor environments, which also includes annotations of\nhuman-object interactions. We show in thorough experimental evaluations that\nour approach achieves a 54% lower average negative log-likelihood and a 26.5%\nlower Best-of-20 displacement error compared to the best non-privileged (i.e.,\nevaluated in a zero-shot fashion on the dataset) baselines for a time horizon\nof 60s."
                },
                "authors": [
                    {
                        "name": "Nicolas Gorlo"
                    },
                    {
                        "name": "Lukas Schmid"
                    },
                    {
                        "name": "Luca Carlone"
                    }
                ],
                "author_detail": {
                    "name": "Luca Carlone"
                },
                "author": "Luca Carlone",
                "arxiv_doi": "10.1109/LRA.2024.3482169",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2024.3482169",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.00552v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00552v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 6 figures. Accepted at IEEE Robotics and Automation Letters\n  (RA-L). Code released at: https://github.com/MIT-SPARK/LP2",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14464v1",
                "updated": "2024-10-18T13:48:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    48,
                    1,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:48:01Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    48,
                    1,
                    4,
                    292,
                    0
                ],
                "title": "Electrocardiogram-Language Model for Few-Shot Question Answering with\n  Meta Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrocardiogram-Language Model for Few-Shot Question Answering with\n  Meta Learning"
                },
                "summary": "Electrocardiogram (ECG) interpretation requires specialized expertise, often\ninvolving synthesizing insights from ECG signals with complex clinical queries\nposed in natural language. The scarcity of labeled ECG data coupled with the\ndiverse nature of clinical inquiries presents a significant challenge for\ndeveloping robust and adaptable ECG diagnostic systems. This work introduces a\nnovel multimodal meta-learning method for few-shot ECG question answering,\naddressing the challenge of limited labeled data while leveraging the rich\nknowledge encoded within large language models (LLMs). Our LLM-agnostic\napproach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA\nand Gemma) via a trainable fusion module, enabling the language model to reason\nabout ECG data and generate clinically meaningful answers. Extensive\nexperiments demonstrate superior generalization to unseen diagnostic tasks\ncompared to supervised baselines, achieving notable performance even with\nlimited ECG leads. For instance, in a 5-way 5-shot setting, our method using\nLLaMA-3.1-8B achieves accuracy of 84.6%, 77.3%, and 69.6% on single verify,\nchoose and query question types, respectively. These results highlight the\npotential of our method to enhance clinical ECG interpretation by combining\nsignal processing with the nuanced language understanding capabilities of LLMs,\nparticularly in data-constrained scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrocardiogram (ECG) interpretation requires specialized expertise, often\ninvolving synthesizing insights from ECG signals with complex clinical queries\nposed in natural language. The scarcity of labeled ECG data coupled with the\ndiverse nature of clinical inquiries presents a significant challenge for\ndeveloping robust and adaptable ECG diagnostic systems. This work introduces a\nnovel multimodal meta-learning method for few-shot ECG question answering,\naddressing the challenge of limited labeled data while leveraging the rich\nknowledge encoded within large language models (LLMs). Our LLM-agnostic\napproach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA\nand Gemma) via a trainable fusion module, enabling the language model to reason\nabout ECG data and generate clinically meaningful answers. Extensive\nexperiments demonstrate superior generalization to unseen diagnostic tasks\ncompared to supervised baselines, achieving notable performance even with\nlimited ECG leads. For instance, in a 5-way 5-shot setting, our method using\nLLaMA-3.1-8B achieves accuracy of 84.6%, 77.3%, and 69.6% on single verify,\nchoose and query question types, respectively. These results highlight the\npotential of our method to enhance clinical ECG interpretation by combining\nsignal processing with the nuanced language understanding capabilities of LLMs,\nparticularly in data-constrained scenarios."
                },
                "authors": [
                    {
                        "name": "Jialu Tang"
                    },
                    {
                        "name": "Tong Xia"
                    },
                    {
                        "name": "Yuan Lu"
                    },
                    {
                        "name": "Cecilia Mascolo"
                    },
                    {
                        "name": "Aaqib Saeed"
                    }
                ],
                "author_detail": {
                    "name": "Aaqib Saeed"
                },
                "author": "Aaqib Saeed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13663v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13663v4",
                "updated": "2024-10-18T13:16:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    16,
                    57,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-19T16:10:26Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    16,
                    10,
                    26,
                    2,
                    171,
                    0
                ],
                "title": "Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation"
                },
                "summary": "Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution."
                },
                "authors": [
                    {
                        "name": "Jirui Qi"
                    },
                    {
                        "name": "Gabriele Sarti"
                    },
                    {
                        "name": "Raquel Fernndez"
                    },
                    {
                        "name": "Arianna Bisazza"
                    }
                ],
                "author_detail": {
                    "name": "Arianna Bisazza"
                },
                "author": "Arianna Bisazza",
                "arxiv_comment": "Accepted by EMNLP 2024 Main Conference. Code and data released at\n  https://github.com/Betswish/MIRAGE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13663v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13663v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11877v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11877v5",
                "updated": "2024-10-18T13:03:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    3,
                    5,
                    4,
                    292,
                    0
                ],
                "published": "2024-05-20T08:41:15Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    8,
                    41,
                    15,
                    0,
                    141,
                    0
                ],
                "title": "A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI:\n  The First Romanian Natural Language Inference Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI:\n  The First Romanian Natural Language Inference Corpus"
                },
                "summary": "Natural language inference (NLI), the task of recognizing the entailment\nrelationship in sentence pairs, is an actively studied topic serving as a proxy\nfor natural language understanding. Despite the relevance of the task in\nbuilding conversational agents and improving text classification, machine\ntranslation and other NLP tasks, to the best of our knowledge, there is no\npublicly available NLI corpus for the Romanian language. To this end, we\nintroduce the first Romanian NLI corpus (RoNLI) comprising 58K training\nsentence pairs, which are obtained via distant supervision, and 6K validation\nand test sentence pairs, which are manually annotated with the correct labels.\nWe conduct experiments with multiple machine learning methods based on distant\nlearning, ranging from shallow models based on word embeddings to\ntransformer-based neural networks, to establish a set of competitive baselines.\nFurthermore, we improve on the best model by employing a new curriculum\nlearning strategy based on data cartography. Our dataset and code to reproduce\nthe baselines are available at https://github.com/Eduard6421/RONLI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language inference (NLI), the task of recognizing the entailment\nrelationship in sentence pairs, is an actively studied topic serving as a proxy\nfor natural language understanding. Despite the relevance of the task in\nbuilding conversational agents and improving text classification, machine\ntranslation and other NLP tasks, to the best of our knowledge, there is no\npublicly available NLI corpus for the Romanian language. To this end, we\nintroduce the first Romanian NLI corpus (RoNLI) comprising 58K training\nsentence pairs, which are obtained via distant supervision, and 6K validation\nand test sentence pairs, which are manually annotated with the correct labels.\nWe conduct experiments with multiple machine learning methods based on distant\nlearning, ranging from shallow models based on word embeddings to\ntransformer-based neural networks, to establish a set of competitive baselines.\nFurthermore, we improve on the best model by employing a new curriculum\nlearning strategy based on data cartography. Our dataset and code to reproduce\nthe baselines are available at https://github.com/Eduard6421/RONLI."
                },
                "authors": [
                    {
                        "name": "Eduard Poesina"
                    },
                    {
                        "name": "Cornelia Caragea"
                    },
                    {
                        "name": "Radu Tudor Ionescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Tudor Ionescu"
                },
                "author": "Radu Tudor Ionescu",
                "arxiv_comment": "Accepted at ACL 2024 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11877v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11877v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10996v2",
                "updated": "2024-10-18T12:54:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    54,
                    21,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-16T16:17:46Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    16,
                    17,
                    46,
                    6,
                    168,
                    0
                ],
                "title": "Towards Lifelong Dialogue Agents via Relation-aware Memory Construction\n  and Timeline-augmented Response Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Lifelong Dialogue Agents via Relation-aware Memory Construction\n  and Timeline-augmented Response Generation"
                },
                "summary": "To achieve lifelong human-agent interaction, dialogue agents need to\nconstantly memorize perceived information and properly retrieve it for response\ngeneration (RG). While prior work focuses on getting rid of outdated memories\nto improve retrieval quality, we argue that such memories provide rich,\nimportant contextual cues for RG (e.g., changes in user behaviors) in long-term\nconversations. We present Theanine, a framework for LLM-based lifelong dialogue\nagents. Theanine discards memory removal and manages large-scale memories by\nlinking them based on their temporal and cause-effect relation. Enabled by this\nlinking structure, Theanine augments RG with memory timelines - series of\nmemories representing the evolution or causality of relevant past events. Along\nwith Theanine, we introduce TeaFarm, a counterfactual-driven evaluation scheme,\naddressing the limitation of G-Eval and human efforts in measuring\nmemory-augmented dialogue agents. A supplementary video for Theanine and data\nfor TeaFarm are at https://huggingface.co/spaces/ResearcherScholar/Theanine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve lifelong human-agent interaction, dialogue agents need to\nconstantly memorize perceived information and properly retrieve it for response\ngeneration (RG). While prior work focuses on getting rid of outdated memories\nto improve retrieval quality, we argue that such memories provide rich,\nimportant contextual cues for RG (e.g., changes in user behaviors) in long-term\nconversations. We present Theanine, a framework for LLM-based lifelong dialogue\nagents. Theanine discards memory removal and manages large-scale memories by\nlinking them based on their temporal and cause-effect relation. Enabled by this\nlinking structure, Theanine augments RG with memory timelines - series of\nmemories representing the evolution or causality of relevant past events. Along\nwith Theanine, we introduce TeaFarm, a counterfactual-driven evaluation scheme,\naddressing the limitation of G-Eval and human efforts in measuring\nmemory-augmented dialogue agents. A supplementary video for Theanine and data\nfor TeaFarm are at https://huggingface.co/spaces/ResearcherScholar/Theanine."
                },
                "authors": [
                    {
                        "name": "Kai Tzu-iunn Ong"
                    },
                    {
                        "name": "Namyoung Kim"
                    },
                    {
                        "name": "Minju Gwak"
                    },
                    {
                        "name": "Hyungjoo Chae"
                    },
                    {
                        "name": "Taeyoon Kwon"
                    },
                    {
                        "name": "Yohan Jo"
                    },
                    {
                        "name": "Seung-won Hwang"
                    },
                    {
                        "name": "Dongha Lee"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    }
                ],
                "author_detail": {
                    "name": "Jinyoung Yeo"
                },
                "author": "Jinyoung Yeo",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14436v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14436v1",
                "updated": "2024-10-18T12:53:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    53,
                    23,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T12:53:23Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    53,
                    23,
                    4,
                    292,
                    0
                ],
                "title": "Learning to refine domain knowledge for biological network inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to refine domain knowledge for biological network inference"
                },
                "summary": "Perturbation experiments allow biologists to discover causal relationships\nbetween variables of interest, but the sparsity and high dimensionality of\nthese data pose significant challenges for causal structure learning\nalgorithms. Biological knowledge graphs can bootstrap the inference of causal\nstructures in these situations, but since they compile vastly diverse\ninformation, they can bias predictions towards well-studied systems.\nAlternatively, amortized causal structure learning algorithms encode inductive\nbiases through data simulation and train supervised models to recapitulate\nthese synthetic graphs. However, realistically simulating biology is arguably\neven harder than understanding a specific system. In this work, we take\ninspiration from both strategies and propose an amortized algorithm for\nrefining domain knowledge, based on data observations. On real and synthetic\ndatasets, we show that our approach outperforms baselines in recovering ground\ntruth causal graphs and identifying errors in the prior knowledge with limited\ninterventional data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perturbation experiments allow biologists to discover causal relationships\nbetween variables of interest, but the sparsity and high dimensionality of\nthese data pose significant challenges for causal structure learning\nalgorithms. Biological knowledge graphs can bootstrap the inference of causal\nstructures in these situations, but since they compile vastly diverse\ninformation, they can bias predictions towards well-studied systems.\nAlternatively, amortized causal structure learning algorithms encode inductive\nbiases through data simulation and train supervised models to recapitulate\nthese synthetic graphs. However, realistically simulating biology is arguably\neven harder than understanding a specific system. In this work, we take\ninspiration from both strategies and propose an amortized algorithm for\nrefining domain knowledge, based on data observations. On real and synthetic\ndatasets, we show that our approach outperforms baselines in recovering ground\ntruth causal graphs and identifying errors in the prior knowledge with limited\ninterventional data."
                },
                "authors": [
                    {
                        "name": "Peiwen Li"
                    },
                    {
                        "name": "Menghua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Menghua Wu"
                },
                "author": "Menghua Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14436v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14436v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07349v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07349v2",
                "updated": "2024-10-18T12:50:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    50,
                    52,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-09T18:01:43Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    18,
                    1,
                    43,
                    2,
                    283,
                    0
                ],
                "title": "Psi-GAN: A power-spectrum-informed generative adversarial network for\n  the emulation of large-scale structure maps across cosmologies and redshifts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Psi-GAN: A power-spectrum-informed generative adversarial network for\n  the emulation of large-scale structure maps across cosmologies and redshifts"
                },
                "summary": "Simulations of the dark matter distribution throughout the Universe are\nessential in order to analyse data from cosmological surveys. $N$-body\nsimulations are computationally expensive, and many cheaper alternatives (such\nas lognormal random fields) fail to reproduce accurate statistics of the\nsmaller, non-linear scales. In this work, we present Psi-GAN\n(Power-spectrum-informed generative adversarial network), a machine learning\nmodel which takes a two-dimensional lognormal dark matter density field and\ntransforms it into a more realistic field. We construct Psi-GAN so that it is\ncontinuously conditional, and can therefore generate realistic realisations of\nthe dark matter density field across a range of cosmologies and redshifts in $z\n\\in [0, 3]$. We train Psi-GAN as a generative adversarial network on $2\\,000$\nsimulation boxes from the Quijote simulation suite. We use a novel critic\narchitecture that utilises the power spectrum as the basis for discrimination\nbetween real and generated samples. Psi-GAN shows agreement with $N$-body\nsimulations over a range of redshifts and cosmologies, consistently\noutperforming the lognormal approximation on all tests of non-linear structure,\nsuch as being able to reproduce both the power spectrum up to wavenumbers of\n$1~h~\\mathrm{Mpc}^{-1}$, and the bispectra of target $N$-body simulations to\nwithin ${\\sim}5$ per cent. Our improved ability to model non-linear structure\nshould allow more robust constraints on cosmological parameters when used in\ntechniques such as simulation-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulations of the dark matter distribution throughout the Universe are\nessential in order to analyse data from cosmological surveys. $N$-body\nsimulations are computationally expensive, and many cheaper alternatives (such\nas lognormal random fields) fail to reproduce accurate statistics of the\nsmaller, non-linear scales. In this work, we present Psi-GAN\n(Power-spectrum-informed generative adversarial network), a machine learning\nmodel which takes a two-dimensional lognormal dark matter density field and\ntransforms it into a more realistic field. We construct Psi-GAN so that it is\ncontinuously conditional, and can therefore generate realistic realisations of\nthe dark matter density field across a range of cosmologies and redshifts in $z\n\\in [0, 3]$. We train Psi-GAN as a generative adversarial network on $2\\,000$\nsimulation boxes from the Quijote simulation suite. We use a novel critic\narchitecture that utilises the power spectrum as the basis for discrimination\nbetween real and generated samples. Psi-GAN shows agreement with $N$-body\nsimulations over a range of redshifts and cosmologies, consistently\noutperforming the lognormal approximation on all tests of non-linear structure,\nsuch as being able to reproduce both the power spectrum up to wavenumbers of\n$1~h~\\mathrm{Mpc}^{-1}$, and the bispectra of target $N$-body simulations to\nwithin ${\\sim}5$ per cent. Our improved ability to model non-linear structure\nshould allow more robust constraints on cosmological parameters when used in\ntechniques such as simulation-based inference."
                },
                "authors": [
                    {
                        "name": "Prabh Bhambra"
                    },
                    {
                        "name": "Benjamin Joachimi"
                    },
                    {
                        "name": "Ofer Lahav"
                    },
                    {
                        "name": "Davide Piras"
                    }
                ],
                "author_detail": {
                    "name": "Davide Piras"
                },
                "author": "Davide Piras",
                "arxiv_comment": "20 pages, 11 figures, 3 tables, 1 appendix. Submitted for publication\n  by Monthly Notices of the Royal Astronomical Society",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07349v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07349v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.17819v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.17819v3",
                "updated": "2024-10-18T12:49:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    49,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2023-05-28T22:46:21Z",
                "published_parsed": [
                    2023,
                    5,
                    28,
                    22,
                    46,
                    21,
                    6,
                    148,
                    0
                ],
                "title": "Large Language Models, scientific knowledge and factuality: A framework\n  to streamline human expert evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models, scientific knowledge and factuality: A framework\n  to streamline human expert evaluation"
                },
                "summary": "The paper introduces a framework for the evaluation of the encoding of\nfactual scientific knowledge, designed to streamline the manual evaluation\nprocess typically conducted by domain experts. Inferring over and extracting\ninformation from Large Language Models (LLMs) trained on a large corpus of\nscientific literature can potentially define a step change in biomedical\ndiscovery, reducing the barriers for accessing and integrating existing medical\nevidence. This work explores the potential of LLMs for dialoguing with\nbiomedical background knowledge, using the context of antibiotic discovery. The\nframework involves of three evaluation steps, each assessing different aspects\nsequentially: fluency, prompt alignment, semantic coherence, factual knowledge,\nand specificity of the generated responses. By splitting these tasks between\nnon-experts and experts, the framework reduces the effort required from the\nlatter. The work provides a systematic assessment on the ability of eleven\nstate-of-the-art models LLMs, including ChatGPT, GPT-4 and Llama 2, in two\nprompting-based tasks: chemical compound definition generation and chemical\ncompound-fungus relation determination. Although recent models have improved in\nfluency, factual accuracy is still low and models are biased towards\nover-represented entities. The ability of LLMs to serve as biomedical knowledge\nbases is questioned, and the need for additional systematic evaluation\nframeworks is highlighted. While LLMs are currently not fit for purpose to be\nused as biomedical factual knowledge bases in a zero-shot setting, there is a\npromising emerging property in the direction of factuality as the models become\ndomain specialised, scale-up in size and level of human feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper introduces a framework for the evaluation of the encoding of\nfactual scientific knowledge, designed to streamline the manual evaluation\nprocess typically conducted by domain experts. Inferring over and extracting\ninformation from Large Language Models (LLMs) trained on a large corpus of\nscientific literature can potentially define a step change in biomedical\ndiscovery, reducing the barriers for accessing and integrating existing medical\nevidence. This work explores the potential of LLMs for dialoguing with\nbiomedical background knowledge, using the context of antibiotic discovery. The\nframework involves of three evaluation steps, each assessing different aspects\nsequentially: fluency, prompt alignment, semantic coherence, factual knowledge,\nand specificity of the generated responses. By splitting these tasks between\nnon-experts and experts, the framework reduces the effort required from the\nlatter. The work provides a systematic assessment on the ability of eleven\nstate-of-the-art models LLMs, including ChatGPT, GPT-4 and Llama 2, in two\nprompting-based tasks: chemical compound definition generation and chemical\ncompound-fungus relation determination. Although recent models have improved in\nfluency, factual accuracy is still low and models are biased towards\nover-represented entities. The ability of LLMs to serve as biomedical knowledge\nbases is questioned, and the need for additional systematic evaluation\nframeworks is highlighted. While LLMs are currently not fit for purpose to be\nused as biomedical factual knowledge bases in a zero-shot setting, there is a\npromising emerging property in the direction of factuality as the models become\ndomain specialised, scale-up in size and level of human feedback."
                },
                "authors": [
                    {
                        "name": "Magdalena Wysocka"
                    },
                    {
                        "name": "Oskar Wysocki"
                    },
                    {
                        "name": "Maxime Delmas"
                    },
                    {
                        "name": "Vincent Mutel"
                    },
                    {
                        "name": "Andre Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andre Freitas"
                },
                "author": "Andre Freitas",
                "arxiv_doi": "10.1016/j.jbi.2024.104724",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jbi.2024.104724",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.17819v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.17819v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the Journal of Biomedical Informatics, Volume 158,\n  October 2024, 104724",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11885v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11885v2",
                "updated": "2024-10-18T12:46:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    46,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-13T08:23:01Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    8,
                    23,
                    1,
                    3,
                    165,
                    0
                ],
                "title": "A Trifecta of Modelling Tools: A Bayesian Binary Black Hole Model\n  Selection combining Population Synthesis and Galaxy Formation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Trifecta of Modelling Tools: A Bayesian Binary Black Hole Model\n  Selection combining Population Synthesis and Galaxy Formation Models"
                },
                "summary": "Gravitational waves (GWs) have revealed surprising properties of binary black\nhole (BBH) populations, but there is still mystery surrounding how these\ncompact objects evolve. We apply Bayesian inference and an efficient method to\ncalculate the BBH merger rates in the Shark host galaxies, to determine the\ncombination of COMPAS parameters that outputs a population most like the GW\nsources from the LVK transient catalogue. For our COMPAS models, we calculate\nthe likelihood with and without the dependence on the predicted number of BBH\nmerger events. We find strong correlations between hyper-parameters governing\nthe specific angular momentum (AM) of mass lost during mass transfer, the\nmass-loss rates of Wolf-Rayet stars via winds and the chemically homogeneous\nevolution (CHE) formation channel. We conclude that analysing the marginalised\nand unmarginalised likelihood is a good indicator of whether the population\nparameters distribution and number of observed events reflect the LVK data. In\ndoing so, we see that the majority of the models preferred in terms of the\npopulation-level parameters of the BBHs greatly overpredict the number of\nevents we should have observed to date. Looking at the smaller number of models\nwhich perform well with both likelihoods, we find that those with no CHE, AM\nloss occurring closer to the donor during the first mass-transfer event, and/or\nhigher rates of mass-loss from Wolf-Rayet winds are generally preferred by\ncurrent data. We find these conclusions to be robust to our choice of selection\ncriteria.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves (GWs) have revealed surprising properties of binary black\nhole (BBH) populations, but there is still mystery surrounding how these\ncompact objects evolve. We apply Bayesian inference and an efficient method to\ncalculate the BBH merger rates in the Shark host galaxies, to determine the\ncombination of COMPAS parameters that outputs a population most like the GW\nsources from the LVK transient catalogue. For our COMPAS models, we calculate\nthe likelihood with and without the dependence on the predicted number of BBH\nmerger events. We find strong correlations between hyper-parameters governing\nthe specific angular momentum (AM) of mass lost during mass transfer, the\nmass-loss rates of Wolf-Rayet stars via winds and the chemically homogeneous\nevolution (CHE) formation channel. We conclude that analysing the marginalised\nand unmarginalised likelihood is a good indicator of whether the population\nparameters distribution and number of observed events reflect the LVK data. In\ndoing so, we see that the majority of the models preferred in terms of the\npopulation-level parameters of the BBHs greatly overpredict the number of\nevents we should have observed to date. Looking at the smaller number of models\nwhich perform well with both likelihoods, we find that those with no CHE, AM\nloss occurring closer to the donor during the first mass-transfer event, and/or\nhigher rates of mass-loss from Wolf-Rayet winds are generally preferred by\ncurrent data. We find these conclusions to be robust to our choice of selection\ncriteria."
                },
                "authors": [
                    {
                        "name": "Liana Rauf"
                    },
                    {
                        "name": "Cullan Howlett"
                    },
                    {
                        "name": "Simon Stevenson"
                    },
                    {
                        "name": "Jeff Riley"
                    },
                    {
                        "name": "Reinhold Willcox"
                    }
                ],
                "author_detail": {
                    "name": "Reinhold Willcox"
                },
                "author": "Reinhold Willcox",
                "arxiv_comment": "27 pages, 19 figures, 6 tables, Accepted by MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11885v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11885v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14425v1",
                "updated": "2024-10-18T12:39:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    39,
                    32,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T12:39:32Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    39,
                    32,
                    4,
                    292,
                    0
                ],
                "title": "Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge\n  Distillation"
                },
                "summary": "Parameter-efficient fine-tuning (PEFT) can bridge the gap between large\nlanguage models (LLMs) and downstream tasks. However, PEFT has been proven\nvulnerable to malicious attacks. Research indicates that poisoned LLMs, even\nafter PEFT, retain the capability to activate internalized backdoors when input\nsamples contain predefined triggers. In this paper, we introduce a novel\nweak-to-strong unlearning algorithm to defend against backdoor attacks based on\nfeature alignment knowledge distillation, named W2SDefense. Specifically, we\nfirst train a small-scale language model through full-parameter fine-tuning to\nserve as the clean teacher model. Then, this teacher model guides the\nlarge-scale poisoned student model in unlearning the backdoor, leveraging PEFT.\nTheoretical analysis suggests that W2SDefense has the potential to enhance the\nstudent model's ability to unlearn backdoor features, preventing the activation\nof the backdoor. We conduct experiments on text classification tasks involving\nthree state-of-the-art language models and three different backdoor attack\nalgorithms. Our empirical results demonstrate the outstanding performance of\nW2SDefense in defending against backdoor attacks without compromising model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) can bridge the gap between large\nlanguage models (LLMs) and downstream tasks. However, PEFT has been proven\nvulnerable to malicious attacks. Research indicates that poisoned LLMs, even\nafter PEFT, retain the capability to activate internalized backdoors when input\nsamples contain predefined triggers. In this paper, we introduce a novel\nweak-to-strong unlearning algorithm to defend against backdoor attacks based on\nfeature alignment knowledge distillation, named W2SDefense. Specifically, we\nfirst train a small-scale language model through full-parameter fine-tuning to\nserve as the clean teacher model. Then, this teacher model guides the\nlarge-scale poisoned student model in unlearning the backdoor, leveraging PEFT.\nTheoretical analysis suggests that W2SDefense has the potential to enhance the\nstudent model's ability to unlearn backdoor features, preventing the activation\nof the backdoor. We conduct experiments on text classification tasks involving\nthree state-of-the-art language models and three different backdoor attack\nalgorithms. Our empirical results demonstrate the outstanding performance of\nW2SDefense in defending against backdoor attacks without compromising model\nperformance."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Cong-Duy Nguyen"
                    },
                    {
                        "name": "Meihuizi Jia"
                    },
                    {
                        "name": "Yichao Feng"
                    },
                    {
                        "name": "Luu Anh Tuan"
                    }
                ],
                "author_detail": {
                    "name": "Luu Anh Tuan"
                },
                "author": "Luu Anh Tuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10918v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10918v5",
                "updated": "2024-10-18T12:27:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    27,
                    7,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-16T12:46:40Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    12,
                    46,
                    40,
                    6,
                    168,
                    0
                ],
                "title": "Multi-LLM QA with Embodied Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-LLM QA with Embodied Exploration"
                },
                "summary": "Large language models (LLMs) have grown in popularity due to their natural\nlanguage interface and pre trained knowledge, leading to rapidly increasing\nsuccess in question-answering (QA) tasks. More recently, multi-agent systems\nwith LLM-based agents (Multi-LLM) have been utilized increasingly more for QA.\nIn these scenarios, the models may each answer the question and reach a\nconsensus or each model is specialized to answer different domain questions.\nHowever, most prior work dealing with Multi-LLM QA has focused on scenarios\nwhere the models are asked in a zero-shot manner or are given information\nsources to extract the answer. For question answering of an unknown\nenvironment, embodied exploration of the environment is first needed to answer\nthe question. This skill is necessary for personalizing embodied AI to\nenvironments such as households. There is a lack of insight into whether a\nMulti-LLM system can handle question-answering based on observations from\nembodied exploration. In this work, we address this gap by investigating the\nuse of Multi-Embodied LLM Explorers (MELE) for QA in an unknown environment.\nMultiple LLM-based agents independently explore and then answer queries about a\nhousehold environment. We analyze different aggregation methods to generate a\nsingle, final answer for each query: debating, majority voting, and training a\ncentral answer module (CAM). Using CAM, we observe a $46\\%$ higher accuracy\ncompared against the other non-learning-based aggregation methods. We provide\ncode and the query dataset for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have grown in popularity due to their natural\nlanguage interface and pre trained knowledge, leading to rapidly increasing\nsuccess in question-answering (QA) tasks. More recently, multi-agent systems\nwith LLM-based agents (Multi-LLM) have been utilized increasingly more for QA.\nIn these scenarios, the models may each answer the question and reach a\nconsensus or each model is specialized to answer different domain questions.\nHowever, most prior work dealing with Multi-LLM QA has focused on scenarios\nwhere the models are asked in a zero-shot manner or are given information\nsources to extract the answer. For question answering of an unknown\nenvironment, embodied exploration of the environment is first needed to answer\nthe question. This skill is necessary for personalizing embodied AI to\nenvironments such as households. There is a lack of insight into whether a\nMulti-LLM system can handle question-answering based on observations from\nembodied exploration. In this work, we address this gap by investigating the\nuse of Multi-Embodied LLM Explorers (MELE) for QA in an unknown environment.\nMultiple LLM-based agents independently explore and then answer queries about a\nhousehold environment. We analyze different aggregation methods to generate a\nsingle, final answer for each query: debating, majority voting, and training a\ncentral answer module (CAM). Using CAM, we observe a $46\\%$ higher accuracy\ncompared against the other non-learning-based aggregation methods. We provide\ncode and the query dataset for further research."
                },
                "authors": [
                    {
                        "name": "Bhrij Patel"
                    },
                    {
                        "name": "Vishnu Sashank Dorbala"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "Dinesh Manocha"
                    }
                ],
                "author_detail": {
                    "name": "Dinesh Manocha"
                },
                "author": "Dinesh Manocha",
                "arxiv_comment": "16 pages, 9 Figures, 5 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10918v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10918v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12950v2",
                "updated": "2024-10-18T12:19:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    19,
                    41,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-18T12:54:47Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    12,
                    54,
                    47,
                    1,
                    170,
                    0
                ],
                "title": "MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular\n  Property Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular\n  Property Prediction"
                },
                "summary": "Molecular property prediction (MPP) is a fundamental and crucial task in drug\ndiscovery. However, prior methods are limited by the requirement for a large\nnumber of labeled molecules and their restricted ability to generalize for\nunseen and new tasks, both of which are essential for real-world applications.\nTo address these challenges, we present MolecularGPT for few-shot MPP. From a\nperspective on instruction tuning, we fine-tune large language models (LLMs)\nbased on curated molecular instructions spanning over 1000 property prediction\ntasks. This enables building a versatile and specialized LLM that can be\nadapted to novel MPP tasks without any fine-tuning through zero- and few-shot\nin-context learning (ICL). MolecularGPT exhibits competitive in-context\nreasoning capabilities across 10 downstream evaluation datasets, setting new\nbenchmarks for few-shot molecular prediction tasks. More importantly, with just\ntwo-shot examples, MolecularGPT can outperform standard supervised graph neural\nnetwork methods on 4 out of 7 datasets. It also excels state-of-the-art LLM\nbaselines by up to 15.7% increase on classification accuracy and decrease of\n17.9 on regression metrics (e.g., RMSE) under zero-shot. This study\ndemonstrates the potential of LLMs as effective few-shot molecular property\npredictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular property prediction (MPP) is a fundamental and crucial task in drug\ndiscovery. However, prior methods are limited by the requirement for a large\nnumber of labeled molecules and their restricted ability to generalize for\nunseen and new tasks, both of which are essential for real-world applications.\nTo address these challenges, we present MolecularGPT for few-shot MPP. From a\nperspective on instruction tuning, we fine-tune large language models (LLMs)\nbased on curated molecular instructions spanning over 1000 property prediction\ntasks. This enables building a versatile and specialized LLM that can be\nadapted to novel MPP tasks without any fine-tuning through zero- and few-shot\nin-context learning (ICL). MolecularGPT exhibits competitive in-context\nreasoning capabilities across 10 downstream evaluation datasets, setting new\nbenchmarks for few-shot molecular prediction tasks. More importantly, with just\ntwo-shot examples, MolecularGPT can outperform standard supervised graph neural\nnetwork methods on 4 out of 7 datasets. It also excels state-of-the-art LLM\nbaselines by up to 15.7% increase on classification accuracy and decrease of\n17.9 on regression metrics (e.g., RMSE) under zero-shot. This study\ndemonstrates the potential of LLMs as effective few-shot molecular property\npredictors. The code is available at https://github.com/NYUSHCS/MolecularGPT."
                },
                "authors": [
                    {
                        "name": "Yuyan Liu"
                    },
                    {
                        "name": "Sirui Ding"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Qiaoyu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Qiaoyu Tan"
                },
                "author": "Qiaoyu Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14403v1",
                "updated": "2024-10-18T12:06:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    6,
                    39,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T12:06:39Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    6,
                    39,
                    4,
                    292,
                    0
                ],
                "title": "A novel understanding of the role of plasma-molecular kinetics on\n  divertor power exhaust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A novel understanding of the role of plasma-molecular kinetics on\n  divertor power exhaust"
                },
                "summary": "During detachment, a buffer of neutral atoms and molecules builds up between\nthe target and the ionising plasma. Collisions between the plasma and the\nmolecules play an important role in the detachment process. Studies of\nplasma-molecular kinetics indicate that the gas temperature is increased during\ndetachment for a wide range of conditions on the MAST-U and TCV tokamaks. This\nis related to an increased $\\mathrm{D}_2$ lifetime during detachment, leading\nto more plasma-molecule collisions that raise the molecular temperature. Such\ncollisions subsequently result in significant power and momentum losses to the\ndivertor plasma during detachment. Using a simplified inference, these losses\nare estimated using the rotational temperature, neutral pressure and ionisation\nfront position. Significant power losses (about $10\\%$ of $P_{SOL}$) and\ndominant momentum losses (majority of the upstream pressure) from\nplasma-molecule collisions are inferred experimentally in long-legged, strongly\nbaffled, detached divertors (MAST-U Super-X divertor), consistent with\nSOLPS-ITER simulations. The vibrational distribution obtained is compared with\nan Eirene-like collisional-radiative model setup, indicating some qualitative\nagreements and disagreements, potentially highlighting model gaps.\n  These interpretations highlight the importance of plasma-molecular\ncollisions, leading to power and momentum losses during detachment. Our\nanalysis and reduced modelling of these processes provide further insights into\ndetachment control observations, the workings of long-legged divertors and\ndivertor power balance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "During detachment, a buffer of neutral atoms and molecules builds up between\nthe target and the ionising plasma. Collisions between the plasma and the\nmolecules play an important role in the detachment process. Studies of\nplasma-molecular kinetics indicate that the gas temperature is increased during\ndetachment for a wide range of conditions on the MAST-U and TCV tokamaks. This\nis related to an increased $\\mathrm{D}_2$ lifetime during detachment, leading\nto more plasma-molecule collisions that raise the molecular temperature. Such\ncollisions subsequently result in significant power and momentum losses to the\ndivertor plasma during detachment. Using a simplified inference, these losses\nare estimated using the rotational temperature, neutral pressure and ionisation\nfront position. Significant power losses (about $10\\%$ of $P_{SOL}$) and\ndominant momentum losses (majority of the upstream pressure) from\nplasma-molecule collisions are inferred experimentally in long-legged, strongly\nbaffled, detached divertors (MAST-U Super-X divertor), consistent with\nSOLPS-ITER simulations. The vibrational distribution obtained is compared with\nan Eirene-like collisional-radiative model setup, indicating some qualitative\nagreements and disagreements, potentially highlighting model gaps.\n  These interpretations highlight the importance of plasma-molecular\ncollisions, leading to power and momentum losses during detachment. Our\nanalysis and reduced modelling of these processes provide further insights into\ndetachment control observations, the workings of long-legged divertors and\ndivertor power balance."
                },
                "authors": [
                    {
                        "name": "N. Osborne"
                    },
                    {
                        "name": "K. Verhaegh"
                    },
                    {
                        "name": "D. Moulton"
                    },
                    {
                        "name": "H. Reimerdes"
                    },
                    {
                        "name": "P. Ryan"
                    },
                    {
                        "name": "N. Lonigro"
                    },
                    {
                        "name": "S. Mijin"
                    },
                    {
                        "name": "R. Osawa"
                    },
                    {
                        "name": "K. Murray"
                    },
                    {
                        "name": "S. Kobussen"
                    },
                    {
                        "name": "Y. Dimizia"
                    },
                    {
                        "name": "A. Perek"
                    },
                    {
                        "name": "C. Theiler"
                    },
                    {
                        "name": "R. Ducker"
                    },
                    {
                        "name": "D. Mykytchuk"
                    }
                ],
                "author_detail": {
                    "name": "D. Mykytchuk"
                },
                "author": "D. Mykytchuk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.08552v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.08552v2",
                "updated": "2024-10-18T12:05:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    5,
                    21,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-12T18:00:08Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    18,
                    0,
                    8,
                    2,
                    164,
                    0
                ],
                "title": "DiTFastAttn: Attention Compression for Diffusion Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTFastAttn: Attention Compression for Diffusion Transformer Models"
                },
                "summary": "Diffusion Transformers (DiT) excel at image and video generation but face\ncomputational challenges due to the quadratic complexity of self-attention\noperators. We propose DiTFastAttn, a post-training compression method to\nalleviate the computational bottleneck of DiT. We identify three key\nredundancies in the attention computation during DiT inference: (1) spatial\nredundancy, where many attention heads focus on local information; (2) temporal\nredundancy, with high similarity between the attention outputs of neighboring\nsteps; (3) conditional redundancy, where conditional and unconditional\ninferences exhibit significant similarity. We propose three techniques to\nreduce these redundancies: (1) Window Attention with Residual Sharing to reduce\nspatial redundancy; (2) Attention Sharing across Timesteps to exploit the\nsimilarity between steps; (3) Attention Sharing across CFG to skip redundant\ncomputations during conditional generation. We apply DiTFastAttn to DiT,\nPixArt-Sigma for image generation tasks, and OpenSora for video generation\ntasks. Our results show that for image generation, our method reduces up to 76%\nof the attention FLOPs and achieves up to 1.8x end-to-end speedup at\nhigh-resolution (2k x 2k) generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) excel at image and video generation but face\ncomputational challenges due to the quadratic complexity of self-attention\noperators. We propose DiTFastAttn, a post-training compression method to\nalleviate the computational bottleneck of DiT. We identify three key\nredundancies in the attention computation during DiT inference: (1) spatial\nredundancy, where many attention heads focus on local information; (2) temporal\nredundancy, with high similarity between the attention outputs of neighboring\nsteps; (3) conditional redundancy, where conditional and unconditional\ninferences exhibit significant similarity. We propose three techniques to\nreduce these redundancies: (1) Window Attention with Residual Sharing to reduce\nspatial redundancy; (2) Attention Sharing across Timesteps to exploit the\nsimilarity between steps; (3) Attention Sharing across CFG to skip redundant\ncomputations during conditional generation. We apply DiTFastAttn to DiT,\nPixArt-Sigma for image generation tasks, and OpenSora for video generation\ntasks. Our results show that for image generation, our method reduces up to 76%\nof the attention FLOPs and achieves up to 1.8x end-to-end speedup at\nhigh-resolution (2k x 2k) generation."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Pu Lu"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Tianchen Zhao"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.08552v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.08552v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14399v1",
                "updated": "2024-10-18T12:02:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    2,
                    41,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T12:02:41Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    2,
                    41,
                    4,
                    292,
                    0
                ],
                "title": "SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic\n  Reasoning"
                },
                "summary": "Syllogistic reasoning is crucial for Natural Language Inference (NLI). This\ncapability is particularly significant in specialized domains such as\nbiomedicine, where it can support automatic evidence interpretation and\nscientific discovery. This paper presents SylloBio-NLI, a novel framework that\nleverages external ontologies to systematically instantiate diverse syllogistic\narguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language\nModels (LLMs) on identifying valid conclusions and extracting supporting\nevidence across 28 syllogistic schemes instantiated with human genome pathways.\nExtensive experiments reveal that biomedical syllogistic reasoning is\nparticularly challenging for zero-shot LLMs, which achieve an average accuracy\nbetween 70% on generalized modus ponens and 23% on disjunctive syllogism. At\nthe same time, we found that few-shot prompting can boost the performance of\ndifferent LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper\nanalysis shows that both techniques exhibit high sensitivity to superficial\nlexical variations, highlighting a dependency between reliability, models'\narchitecture, and pre-training regime. Overall, our results indicate that,\nwhile in-context examples have the potential to elicit syllogistic reasoning in\nLLMs, existing models are still far from achieving the robustness and\nconsistency required for safe biomedical NLI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Syllogistic reasoning is crucial for Natural Language Inference (NLI). This\ncapability is particularly significant in specialized domains such as\nbiomedicine, where it can support automatic evidence interpretation and\nscientific discovery. This paper presents SylloBio-NLI, a novel framework that\nleverages external ontologies to systematically instantiate diverse syllogistic\narguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language\nModels (LLMs) on identifying valid conclusions and extracting supporting\nevidence across 28 syllogistic schemes instantiated with human genome pathways.\nExtensive experiments reveal that biomedical syllogistic reasoning is\nparticularly challenging for zero-shot LLMs, which achieve an average accuracy\nbetween 70% on generalized modus ponens and 23% on disjunctive syllogism. At\nthe same time, we found that few-shot prompting can boost the performance of\ndifferent LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper\nanalysis shows that both techniques exhibit high sensitivity to superficial\nlexical variations, highlighting a dependency between reliability, models'\narchitecture, and pre-training regime. Overall, our results indicate that,\nwhile in-context examples have the potential to elicit syllogistic reasoning in\nLLMs, existing models are still far from achieving the robustness and\nconsistency required for safe biomedical NLI applications."
                },
                "authors": [
                    {
                        "name": "Magdalena Wysocka"
                    },
                    {
                        "name": "Danilo S. Carvalho"
                    },
                    {
                        "name": "Oskar Wysocki"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "Andre Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andre Freitas"
                },
                "author": "Andre Freitas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10717v2",
                "updated": "2024-10-18T12:01:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    1,
                    2,
                    4,
                    292,
                    0
                ],
                "published": "2024-05-17T11:59:25Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    11,
                    59,
                    25,
                    4,
                    138,
                    0
                ],
                "title": "Constraining the Properties of the Thermonuclear Burst Oscillation\n  Source XTE J1814-338 Through Pulse Profile Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining the Properties of the Thermonuclear Burst Oscillation\n  Source XTE J1814-338 Through Pulse Profile Modelling"
                },
                "summary": "Pulse profile modelling (PPM) is a comprehensive relativistic ray-tracing\ntechnique employed to determine the properties of neutron stars. In this study,\nwe apply this technique to the Type I X-ray burster and accretion-powered\nmillisecond pulsar XTE J1814-338, extracting its fundamental properties using\nPPM of its thermonuclear burst oscillations. Using data from its 2003 outburst,\nand a single uniform temperature hot spot model, we infer XTE J1814-338 to be\nlocated at a distance of $7.2^{+0.3}_{-0.4}$ kpc, with a mass of\n$1.21^{+0.05}_{-0.05}$ M$_\\odot$ and an equatorial radius of\n$7.0^{+0.4}_{-0.4}$ km. Our results also offer insight into the time evolution\nof the hot spot but point to some potential shortcomings of the single uniform\ntemperature hot spot model. We explore the implications of this result,\nincluding what we can learn about thermonuclear burst oscillation mechanisms\nand the importance of modelling the accretion contribution to the emission\nduring the burst.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pulse profile modelling (PPM) is a comprehensive relativistic ray-tracing\ntechnique employed to determine the properties of neutron stars. In this study,\nwe apply this technique to the Type I X-ray burster and accretion-powered\nmillisecond pulsar XTE J1814-338, extracting its fundamental properties using\nPPM of its thermonuclear burst oscillations. Using data from its 2003 outburst,\nand a single uniform temperature hot spot model, we infer XTE J1814-338 to be\nlocated at a distance of $7.2^{+0.3}_{-0.4}$ kpc, with a mass of\n$1.21^{+0.05}_{-0.05}$ M$_\\odot$ and an equatorial radius of\n$7.0^{+0.4}_{-0.4}$ km. Our results also offer insight into the time evolution\nof the hot spot but point to some potential shortcomings of the single uniform\ntemperature hot spot model. We explore the implications of this result,\nincluding what we can learn about thermonuclear burst oscillation mechanisms\nand the importance of modelling the accretion contribution to the emission\nduring the burst."
                },
                "authors": [
                    {
                        "name": "Yves Kini"
                    },
                    {
                        "name": "Tuomo Salmi"
                    },
                    {
                        "name": "Serena Vinciguerra"
                    },
                    {
                        "name": "Anna L. Watts"
                    },
                    {
                        "name": "Anna Bilous"
                    },
                    {
                        "name": "Duncan K. Galloway"
                    },
                    {
                        "name": "Emma van der Wateren"
                    },
                    {
                        "name": "Guru Partap Khalsa"
                    },
                    {
                        "name": "Slavko Bogdanov"
                    },
                    {
                        "name": "Johannes Buchner"
                    },
                    {
                        "name": "Valery Suleimanov"
                    }
                ],
                "author_detail": {
                    "name": "Valery Suleimanov"
                },
                "author": "Valery Suleimanov",
                "arxiv_comment": "Accepted for publication in MNRAS. The Zenodo link is public",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.19723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.19723v2",
                "updated": "2024-10-18T11:54:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    54,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-04-30T17:17:07Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    17,
                    17,
                    7,
                    1,
                    121,
                    0
                ],
                "title": "Attention-Constrained Inference for Robust Decoder-Only Text-to-Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention-Constrained Inference for Robust Decoder-Only Text-to-Speech"
                },
                "summary": "Recent popular decoder-only text-to-speech models are known for their ability\nof generating natural-sounding speech. However, such models sometimes suffer\nfrom word skipping and repeating due to the lack of explicit monotonic\nalignment constraints. In this paper, we notice from the attention maps that\nsome particular attention heads of the decoder-only model indicate the\nalignments between speech and text. We call the attention maps of those heads\nAlignment-Emerged Attention Maps (AEAMs). Based on this discovery, we propose a\nnovel inference method without altering the training process, named\nAttention-Constrained Inference (ACI), to facilitate monotonic synthesis. It\nfirst identifies AEAMs using the Attention Sweeping algorithm and then applies\nconstraining masks on AEAMs. Our experimental results on decoder-only TTS model\nVALL-E show that the WER of synthesized speech is reduced by up to 20.5%\nrelatively with ACI while the naturalness and speaker similarity are\ncomparable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent popular decoder-only text-to-speech models are known for their ability\nof generating natural-sounding speech. However, such models sometimes suffer\nfrom word skipping and repeating due to the lack of explicit monotonic\nalignment constraints. In this paper, we notice from the attention maps that\nsome particular attention heads of the decoder-only model indicate the\nalignments between speech and text. We call the attention maps of those heads\nAlignment-Emerged Attention Maps (AEAMs). Based on this discovery, we propose a\nnovel inference method without altering the training process, named\nAttention-Constrained Inference (ACI), to facilitate monotonic synthesis. It\nfirst identifies AEAMs using the Attention Sweeping algorithm and then applies\nconstraining masks on AEAMs. Our experimental results on decoder-only TTS model\nVALL-E show that the WER of synthesized speech is reduced by up to 20.5%\nrelatively with ACI while the naturalness and speaker similarity are\ncomparable."
                },
                "authors": [
                    {
                        "name": "Hankun Wang"
                    },
                    {
                        "name": "Chenpeng Du"
                    },
                    {
                        "name": "Yiwei Guo"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Xie Chen"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "Accepted by IEEE Spoken Language Technology (SLT) Workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.19723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.19723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14391v1",
                "updated": "2024-10-18T11:52:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    52,
                    10,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T11:52:10Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    52,
                    10,
                    4,
                    292,
                    0
                ],
                "title": "Analyzing Context Utilization of LLMs in Document-Level Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Context Utilization of LLMs in Document-Level Translation"
                },
                "summary": "Large language models (LLM) are increasingly strong contenders in machine\ntranslation. We study document-level translation, where some words cannot be\ntranslated without context from outside the sentence. We investigate the\nability of prominent LLMs to utilize context by analyzing models' robustness to\nperturbed and randomized document context. We find that LLMs' improved\ndocument-translation performance is not always reflected in pronoun translation\nperformance. We highlight the need for context-aware finetuning of LLMs with a\nfocus on relevant parts of the context to improve their reliability for\ndocument-level translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) are increasingly strong contenders in machine\ntranslation. We study document-level translation, where some words cannot be\ntranslated without context from outside the sentence. We investigate the\nability of prominent LLMs to utilize context by analyzing models' robustness to\nperturbed and randomized document context. We find that LLMs' improved\ndocument-translation performance is not always reflected in pronoun translation\nperformance. We highlight the need for context-aware finetuning of LLMs with a\nfocus on relevant parts of the context to improve their reliability for\ndocument-level translation."
                },
                "authors": [
                    {
                        "name": "Wafaa Mohammed"
                    },
                    {
                        "name": "Vlad Niculae"
                    }
                ],
                "author_detail": {
                    "name": "Vlad Niculae"
                },
                "author": "Vlad Niculae",
                "arxiv_comment": "4 pages, 2 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14388v1",
                "updated": "2024-10-18T11:44:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    44,
                    29,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T11:44:29Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    44,
                    29,
                    4,
                    292,
                    0
                ],
                "title": "Unscrambling disease progression at scale: fast inference of event\n  permutations with optimal transport",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unscrambling disease progression at scale: fast inference of event\n  permutations with optimal transport"
                },
                "summary": "Disease progression models infer group-level temporal trajectories of change\nin patients' features as a chronic degenerative condition plays out. They\nprovide unique insight into disease biology and staging systems with\nindividual-level clinical utility. Discrete models consider disease progression\nas a latent permutation of events, where each event corresponds to a feature\nbecoming measurably abnormal. However, permutation inference using traditional\nmaximum likelihood approaches becomes prohibitive due to combinatoric\nexplosion, severely limiting model dimensionality and utility. Here we leverage\nideas from optimal transport to model disease progression as a latent\npermutation matrix of events belonging to the Birkhoff polytope, facilitating\nfast inference via optimisation of the variational lower bound. This enables a\nfactor of 1000 times faster inference than the current state of the art and,\ncorrespondingly, supports models with several orders of magnitude more features\nthan the current state of the art can consider. Experiments demonstrate the\nincrease in speed, accuracy and robustness to noise in simulation. Further\nexperiments with real-world imaging data from two separate datasets, one from\nAlzheimer's disease patients, the other age-related macular degeneration,\nshowcase, for the first time, pixel-level disease progression events in the\nbrain and eye, respectively. Our method is low compute, interpretable and\napplicable to any progressive condition and data modality, giving it broad\npotential clinical utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disease progression models infer group-level temporal trajectories of change\nin patients' features as a chronic degenerative condition plays out. They\nprovide unique insight into disease biology and staging systems with\nindividual-level clinical utility. Discrete models consider disease progression\nas a latent permutation of events, where each event corresponds to a feature\nbecoming measurably abnormal. However, permutation inference using traditional\nmaximum likelihood approaches becomes prohibitive due to combinatoric\nexplosion, severely limiting model dimensionality and utility. Here we leverage\nideas from optimal transport to model disease progression as a latent\npermutation matrix of events belonging to the Birkhoff polytope, facilitating\nfast inference via optimisation of the variational lower bound. This enables a\nfactor of 1000 times faster inference than the current state of the art and,\ncorrespondingly, supports models with several orders of magnitude more features\nthan the current state of the art can consider. Experiments demonstrate the\nincrease in speed, accuracy and robustness to noise in simulation. Further\nexperiments with real-world imaging data from two separate datasets, one from\nAlzheimer's disease patients, the other age-related macular degeneration,\nshowcase, for the first time, pixel-level disease progression events in the\nbrain and eye, respectively. Our method is low compute, interpretable and\napplicable to any progressive condition and data modality, giving it broad\npotential clinical utility."
                },
                "authors": [
                    {
                        "name": "Peter A. Wijeratne"
                    },
                    {
                        "name": "Daniel C. Alexander"
                    }
                ],
                "author_detail": {
                    "name": "Daniel C. Alexander"
                },
                "author": "Daniel C. Alexander",
                "arxiv_comment": "Pre-print of version accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14387v1",
                "updated": "2024-10-18T11:39:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    39,
                    34,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T11:39:34Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    39,
                    34,
                    4,
                    292,
                    0
                ],
                "title": "How Do Multilingual Models Remember? Investigating Multilingual Factual\n  Recall Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do Multilingual Models Remember? Investigating Multilingual Factual\n  Recall Mechanisms"
                },
                "summary": "Large Language Models (LLMs) store and retrieve vast amounts of factual\nknowledge acquired during pre-training. Prior research has localized and\nidentified mechanisms behind knowledge recall; however, it has primarily\nfocused on English monolingual models. The question of how these processes\ngeneralize to other languages and multilingual LLMs remains unexplored. In this\npaper, we address this gap by conducting a comprehensive analysis of two highly\nmultilingual LLMs. We assess the extent to which previously identified\ncomponents and mechanisms of factual recall in English apply to a multilingual\ncontext. Then, we examine when language plays a role in the recall process,\nuncovering evidence of language-independent and language-dependent mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) store and retrieve vast amounts of factual\nknowledge acquired during pre-training. Prior research has localized and\nidentified mechanisms behind knowledge recall; however, it has primarily\nfocused on English monolingual models. The question of how these processes\ngeneralize to other languages and multilingual LLMs remains unexplored. In this\npaper, we address this gap by conducting a comprehensive analysis of two highly\nmultilingual LLMs. We assess the extent to which previously identified\ncomponents and mechanisms of factual recall in English apply to a multilingual\ncontext. Then, we examine when language plays a role in the recall process,\nuncovering evidence of language-independent and language-dependent mechanisms."
                },
                "authors": [
                    {
                        "name": "Constanza Fierro"
                    },
                    {
                        "name": "Negar Foroutan"
                    },
                    {
                        "name": "Desmond Elliott"
                    },
                    {
                        "name": "Anders Sgaard"
                    }
                ],
                "author_detail": {
                    "name": "Anders Sgaard"
                },
                "author": "Anders Sgaard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.03460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.03460v2",
                "updated": "2024-10-18T11:19:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    19,
                    33,
                    4,
                    292,
                    0
                ],
                "published": "2023-07-07T08:44:33Z",
                "published_parsed": [
                    2023,
                    7,
                    7,
                    8,
                    44,
                    33,
                    4,
                    188,
                    0
                ],
                "title": "On the convergence of dynamic implementations of Hamiltonian Monte Carlo\n  and No U-Turn Samplers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the convergence of dynamic implementations of Hamiltonian Monte Carlo\n  and No U-Turn Samplers"
                },
                "summary": "There is substantial empirical evidence about the success of dynamic\nimplementations of Hamiltonian Monte Carlo (HMC), such as the No U-Turn Sampler\n(NUTS), in many challenging inference problems but theoretical results about\ntheir behavior are scarce. The aim of this paper is to fill this gap. More\nprecisely, we consider a general class of MCMC algorithms we call dynamic HMC.\nWe show that this general framework encompasses NUTS as a particular case,\nimplying the invariance of the target distribution as a by-product. Second, we\nestablish conditions under which NUTS is irreducible and aperiodic and as a\ncorrolary ergodic. Under conditions similar to the ones existing for HMC, we\nalso show that NUTS is geometrically ergodic. Finally, we improve existing\nconvergence results for HMC showing that this method is ergodic without any\nboundedness condition on the stepsize and the number of leapfrog steps, in the\ncase where the target is a perturbation of a Gaussian distribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is substantial empirical evidence about the success of dynamic\nimplementations of Hamiltonian Monte Carlo (HMC), such as the No U-Turn Sampler\n(NUTS), in many challenging inference problems but theoretical results about\ntheir behavior are scarce. The aim of this paper is to fill this gap. More\nprecisely, we consider a general class of MCMC algorithms we call dynamic HMC.\nWe show that this general framework encompasses NUTS as a particular case,\nimplying the invariance of the target distribution as a by-product. Second, we\nestablish conditions under which NUTS is irreducible and aperiodic and as a\ncorrolary ergodic. Under conditions similar to the ones existing for HMC, we\nalso show that NUTS is geometrically ergodic. Finally, we improve existing\nconvergence results for HMC showing that this method is ergodic without any\nboundedness condition on the stepsize and the number of leapfrog steps, in the\ncase where the target is a perturbation of a Gaussian distribution."
                },
                "authors": [
                    {
                        "name": "Alain Durmus"
                    },
                    {
                        "name": "Samuel Gruffaz"
                    },
                    {
                        "name": "Miika Kailas"
                    },
                    {
                        "name": "Eero Saksman"
                    },
                    {
                        "name": "Matti Vihola"
                    }
                ],
                "author_detail": {
                    "name": "Matti Vihola"
                },
                "author": "Matti Vihola",
                "arxiv_comment": "24 pages without appendix and references, 2 figures, a future journal\n  paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.03460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.03460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17575v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17575v3",
                "updated": "2024-10-18T11:10:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    10,
                    42,
                    4,
                    292,
                    0
                ],
                "published": "2023-11-29T12:11:44Z",
                "published_parsed": [
                    2023,
                    11,
                    29,
                    12,
                    11,
                    44,
                    2,
                    333,
                    0
                ],
                "title": "Identifying Causal Effects of Discrete, Ordered and ContinuousTreatments\n  using Multiple Instrumental Variables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Causal Effects of Discrete, Ordered and ContinuousTreatments\n  using Multiple Instrumental Variables"
                },
                "summary": "Inferring causal relationships from observational data is often challenging\ndue to endogeneity. This paper provides new identification results for causal\neffects of discrete, ordered and continuous treatments using multiple binary\ninstruments. The key contribution is the identification of a new causal\nparameter that has a straightforward interpretation with a positive weighting\nscheme and is applicable in many settings due to a mild monotonicity\nassumption. This paper further leverages recent advances in causal machine\nlearning for both estimation and the detection of local violations of the\nunderlying monotonicity assumption. The methodology is applied to estimate the\nreturns to education and assess the impact of having an additional child on\nfemale labor market outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring causal relationships from observational data is often challenging\ndue to endogeneity. This paper provides new identification results for causal\neffects of discrete, ordered and continuous treatments using multiple binary\ninstruments. The key contribution is the identification of a new causal\nparameter that has a straightforward interpretation with a positive weighting\nscheme and is applicable in many settings due to a mild monotonicity\nassumption. This paper further leverages recent advances in causal machine\nlearning for both estimation and the detection of local violations of the\nunderlying monotonicity assumption. The methodology is applied to estimate the\nreturns to education and assess the impact of having an additional child on\nfemale labor market outcomes."
                },
                "authors": [
                    {
                        "name": "Nadja van 't Hoff"
                    }
                ],
                "author_detail": {
                    "name": "Nadja van 't Hoff"
                },
                "author": "Nadja van 't Hoff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17575v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17575v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14380v1",
                "updated": "2024-10-18T11:07:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    7,
                    26,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T11:07:26Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    7,
                    26,
                    4,
                    292,
                    0
                ],
                "title": "Dual-Label LearningWith Irregularly Present Labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Label LearningWith Irregularly Present Labels"
                },
                "summary": "In multi-task learning, we often encounter the case when the presence of\nlabels across samples exhibits irregular patterns: samples can be fully\nlabeled, partially labeled or unlabeled. Taking drug analysis as an example,\nmultiple toxicity properties of a drug molecule may not be concurrently\navailable due to experimental limitations. It triggers a demand for a new\ntraining and inference mechanism that could accommodate irregularly present\nlabels and maximize the utility of any available label information. In this\nwork, we focus on the two-label learning task, and propose a novel training and\ninference framework, Dual-Label Learning (DLL). The DLL framework formulates\nthe problem into a dual-function system, in which the two functions should\nsimultaneously satisfy standard supervision, structural duality and\nprobabilistic duality. DLL features a dual-tower model architecture that\nexplicitly captures the information exchange between labels, aimed at\nmaximizing the utility of partially available labels in understanding label\ncorrelation. During training, label imputation for missing labels is conducted\nas part of the forward propagation process, while during inference, labels are\nregarded as unknowns of a bivariate system of equations and are solved jointly.\nTheoretical analysis guarantees the feasibility of DLL, and extensive\nexperiments are conducted to verify that by explicitly modeling label\ncorrelation and maximizing the utility of available labels, our method makes\nconsistently better predictions than baseline approaches by up to a 10% gain in\nF1-score or MAPE. Remarkably, our method provided with data at a label missing\nrate as high as 60% can achieve similar or even better results than baseline\napproaches at a label missing rate of only 10%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-task learning, we often encounter the case when the presence of\nlabels across samples exhibits irregular patterns: samples can be fully\nlabeled, partially labeled or unlabeled. Taking drug analysis as an example,\nmultiple toxicity properties of a drug molecule may not be concurrently\navailable due to experimental limitations. It triggers a demand for a new\ntraining and inference mechanism that could accommodate irregularly present\nlabels and maximize the utility of any available label information. In this\nwork, we focus on the two-label learning task, and propose a novel training and\ninference framework, Dual-Label Learning (DLL). The DLL framework formulates\nthe problem into a dual-function system, in which the two functions should\nsimultaneously satisfy standard supervision, structural duality and\nprobabilistic duality. DLL features a dual-tower model architecture that\nexplicitly captures the information exchange between labels, aimed at\nmaximizing the utility of partially available labels in understanding label\ncorrelation. During training, label imputation for missing labels is conducted\nas part of the forward propagation process, while during inference, labels are\nregarded as unknowns of a bivariate system of equations and are solved jointly.\nTheoretical analysis guarantees the feasibility of DLL, and extensive\nexperiments are conducted to verify that by explicitly modeling label\ncorrelation and maximizing the utility of available labels, our method makes\nconsistently better predictions than baseline approaches by up to a 10% gain in\nF1-score or MAPE. Remarkably, our method provided with data at a label missing\nrate as high as 60% can achieve similar or even better results than baseline\napproaches at a label missing rate of only 10%."
                },
                "authors": [
                    {
                        "name": "Mingqian Li"
                    },
                    {
                        "name": "Qiao Han"
                    },
                    {
                        "name": "Yiteng Zhai"
                    },
                    {
                        "name": "Ruifeng Li"
                    },
                    {
                        "name": "Yao Yang"
                    },
                    {
                        "name": "Hongyang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Chen"
                },
                "author": "Hongyang Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14379v1",
                "updated": "2024-10-18T11:07:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    7,
                    12,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T11:07:12Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    7,
                    12,
                    4,
                    292,
                    0
                ],
                "title": "AnomalyNCD: Towards Novel Anomaly Class Discovery in Industrial\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnomalyNCD: Towards Novel Anomaly Class Discovery in Industrial\n  Scenarios"
                },
                "summary": "In the industrial scenario, anomaly detection could locate but cannot\nclassify anomalies. To complete their capability, we study to automatically\ndiscover and recognize visual classes of industrial anomalies. In terms of\nmulti-class anomaly classification, previous methods cluster anomalies\nrepresented by frozen pre-trained models but often fail due to poor\ndiscrimination. Novel class discovery (NCD) has the potential to tackle this.\nHowever, it struggles with non-prominent and semantically weak anomalies that\nchallenge network learning focus. To address these, we introduce AnomalyNCD, a\nmulti-class anomaly classification framework compatible with existing anomaly\ndetection methods. This framework learns anomaly-specific features and\nclassifies anomalies in a self-supervised manner. Initially, a technique called\nMain Element Binarization (MEBin) is first designed, which segments primary\nanomaly regions into masks to alleviate the impact of incorrect detections on\nlearning. Subsequently, we employ mask-guided contrastive representation\nlearning to improve feature discrimination, which focuses network attention on\nisolated anomalous regions and reduces the confusion of erroneous inputs\nthrough re-corrected pseudo labels. Finally, to enable flexible classification\nat both region and image levels during inference, we develop a region merging\nstrategy that determines the overall image category based on the classified\nanomaly regions. Our method outperforms the state-of-the-art works on the MVTec\nAD and MTD datasets. Compared with the current methods, AnomalyNCD combined\nwith zero-shot anomaly detection method achieves a 10.8% $F_1$ gain, 8.8% NMI\ngain, and 9.5% ARI gain on MVTec AD, 12.8% $F_1$ gain, 5.7% NMI gain, and 10.8%\nARI gain on MTD. The source code is available at\nhttps://github.com/HUST-SLOW/AnomalyNCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the industrial scenario, anomaly detection could locate but cannot\nclassify anomalies. To complete their capability, we study to automatically\ndiscover and recognize visual classes of industrial anomalies. In terms of\nmulti-class anomaly classification, previous methods cluster anomalies\nrepresented by frozen pre-trained models but often fail due to poor\ndiscrimination. Novel class discovery (NCD) has the potential to tackle this.\nHowever, it struggles with non-prominent and semantically weak anomalies that\nchallenge network learning focus. To address these, we introduce AnomalyNCD, a\nmulti-class anomaly classification framework compatible with existing anomaly\ndetection methods. This framework learns anomaly-specific features and\nclassifies anomalies in a self-supervised manner. Initially, a technique called\nMain Element Binarization (MEBin) is first designed, which segments primary\nanomaly regions into masks to alleviate the impact of incorrect detections on\nlearning. Subsequently, we employ mask-guided contrastive representation\nlearning to improve feature discrimination, which focuses network attention on\nisolated anomalous regions and reduces the confusion of erroneous inputs\nthrough re-corrected pseudo labels. Finally, to enable flexible classification\nat both region and image levels during inference, we develop a region merging\nstrategy that determines the overall image category based on the classified\nanomaly regions. Our method outperforms the state-of-the-art works on the MVTec\nAD and MTD datasets. Compared with the current methods, AnomalyNCD combined\nwith zero-shot anomaly detection method achieves a 10.8% $F_1$ gain, 8.8% NMI\ngain, and 9.5% ARI gain on MVTec AD, 12.8% $F_1$ gain, 5.7% NMI gain, and 10.8%\nARI gain on MTD. The source code is available at\nhttps://github.com/HUST-SLOW/AnomalyNCD."
                },
                "authors": [
                    {
                        "name": "Ziming Huang"
                    },
                    {
                        "name": "Xurui Li"
                    },
                    {
                        "name": "Haotian Liu"
                    },
                    {
                        "name": "Feng Xue"
                    },
                    {
                        "name": "Yuzhe Wang"
                    },
                    {
                        "name": "Yu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yu Zhou"
                },
                "author": "Yu Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14373v1",
                "updated": "2024-10-18T11:00:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    0,
                    17,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T11:00:17Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    0,
                    17,
                    4,
                    292,
                    0
                ],
                "title": "The Radcliffe Wave as traced by young open clusters: Stellar parameters,\n  activity indicators, and abundances of solar-type members of eight young\n  clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Radcliffe Wave as traced by young open clusters: Stellar parameters,\n  activity indicators, and abundances of solar-type members of eight young\n  clusters"
                },
                "summary": "The Radcliffe Wave has only recently been recognised as a about 3 kpc long\ncoherent gas structure encompassing most of the star forming regions in the\nsolar vicinity. Since its discovery, it has been mainly studied from the\nperspective of dynamics, but a detailed chemical study is necessary to\nunderstand its nature and the composition of the natal clouds that gave rise to\nit. In this paper we used some of the connected young open clusters (age\n$\\lesssim$ 100 Myr) as tracers of the molecular clouds. We performed\nhigh-resolution spectroscopy with GIARPS at the TNG of 53 stars that are bona\nfide members of seven clusters located at different positions along the\nRadcliffe Wave. We provide radial velocities and atmospheric parameters for all\nof them. For a subsample consisting of 41 FGK stars we also studied the\nchromospheric activity and the content of Li, from which we inferred the age of\nthe parent clusters. These values agree with the evolutionary ages reported in\nthe literature. For these FGK stars we determined the chemical abundances for\n25 species. Pleiades, ASCC 16 and NGC 7058 exhibit a solar metallicity while\nMelotte 20, ASCC 19, NGC 2232, and Roslund 6 show a slightly subsolar value\n($\\approx-$0.1 dex). On average, the clusters show a chemical composition\ncompatible with that of the Sun, especially for $\\alpha$- and Fe-peak elements.\nNeutron-capture elements, on the other hand, present a slight overabundance of\nabout 0.2 dex, specially barium. Finally, considering also ASCC 123, studied by\nour group in a previous research, we infer a correlation between the chemical\ncomposition and the age or position of the clusters along the Wave,\ndemonstrating their physical connection within an inhomogeneous mixing\nscenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Radcliffe Wave has only recently been recognised as a about 3 kpc long\ncoherent gas structure encompassing most of the star forming regions in the\nsolar vicinity. Since its discovery, it has been mainly studied from the\nperspective of dynamics, but a detailed chemical study is necessary to\nunderstand its nature and the composition of the natal clouds that gave rise to\nit. In this paper we used some of the connected young open clusters (age\n$\\lesssim$ 100 Myr) as tracers of the molecular clouds. We performed\nhigh-resolution spectroscopy with GIARPS at the TNG of 53 stars that are bona\nfide members of seven clusters located at different positions along the\nRadcliffe Wave. We provide radial velocities and atmospheric parameters for all\nof them. For a subsample consisting of 41 FGK stars we also studied the\nchromospheric activity and the content of Li, from which we inferred the age of\nthe parent clusters. These values agree with the evolutionary ages reported in\nthe literature. For these FGK stars we determined the chemical abundances for\n25 species. Pleiades, ASCC 16 and NGC 7058 exhibit a solar metallicity while\nMelotte 20, ASCC 19, NGC 2232, and Roslund 6 show a slightly subsolar value\n($\\approx-$0.1 dex). On average, the clusters show a chemical composition\ncompatible with that of the Sun, especially for $\\alpha$- and Fe-peak elements.\nNeutron-capture elements, on the other hand, present a slight overabundance of\nabout 0.2 dex, specially barium. Finally, considering also ASCC 123, studied by\nour group in a previous research, we infer a correlation between the chemical\ncomposition and the age or position of the clusters along the Wave,\ndemonstrating their physical connection within an inhomogeneous mixing\nscenario."
                },
                "authors": [
                    {
                        "name": "J. Alonso-Santiago"
                    },
                    {
                        "name": "A. Frasca"
                    },
                    {
                        "name": "A. Bragaglia"
                    },
                    {
                        "name": "G. Catanzaro"
                    },
                    {
                        "name": "X. Fu"
                    },
                    {
                        "name": "G. Andreuzzi"
                    },
                    {
                        "name": "L. Magrini"
                    },
                    {
                        "name": "S. Lucatello"
                    },
                    {
                        "name": "A. Vallenari"
                    },
                    {
                        "name": "M. Jian"
                    }
                ],
                "author_detail": {
                    "name": "M. Jian"
                },
                "author": "M. Jian",
                "arxiv_comment": "36 pages, 19 figures and 15 tables. Accepted for publication in\n  Astronomy and Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14368v1",
                "updated": "2024-10-18T10:53:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    53,
                    44,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T10:53:44Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    53,
                    44,
                    4,
                    292,
                    0
                ],
                "title": "CoMAL: Collaborative Multi-Agent Large Language Models for\n  Mixed-Autonomy Traffic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMAL: Collaborative Multi-Agent Large Language Models for\n  Mixed-Autonomy Traffic"
                },
                "summary": "The integration of autonomous vehicles into urban traffic has great potential\nto improve efficiency by reducing congestion and optimizing traffic flow\nsystematically. In this paper, we introduce CoMAL (Collaborative Multi-Agent\nLLMs), a framework designed to address the mixed-autonomy traffic problem by\ncollaboration among autonomous vehicles to optimize traffic flow. CoMAL is\nbuilt upon large language models, operating in an interactive traffic\nsimulation environment. It utilizes a Perception Module to observe surrounding\nagents and a Memory Module to store strategies for each agent. The overall\nworkflow includes a Collaboration Module that encourages autonomous vehicles to\ndiscuss the effective strategy and allocate roles, a reasoning engine to\ndetermine optimal behaviors based on assigned roles, and an Execution Module\nthat controls vehicle actions using a hybrid approach combining rule-based\nmodels. Experimental results demonstrate that CoMAL achieves superior\nperformance on the Flow benchmark. Additionally, we evaluate the impact of\ndifferent language models and compare our framework with reinforcement learning\napproaches. It highlights the strong cooperative capability of LLM agents and\npresents a promising solution to the mixed-autonomy traffic challenge. The code\nis available at https://github.com/Hyan-Yao/CoMAL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of autonomous vehicles into urban traffic has great potential\nto improve efficiency by reducing congestion and optimizing traffic flow\nsystematically. In this paper, we introduce CoMAL (Collaborative Multi-Agent\nLLMs), a framework designed to address the mixed-autonomy traffic problem by\ncollaboration among autonomous vehicles to optimize traffic flow. CoMAL is\nbuilt upon large language models, operating in an interactive traffic\nsimulation environment. It utilizes a Perception Module to observe surrounding\nagents and a Memory Module to store strategies for each agent. The overall\nworkflow includes a Collaboration Module that encourages autonomous vehicles to\ndiscuss the effective strategy and allocate roles, a reasoning engine to\ndetermine optimal behaviors based on assigned roles, and an Execution Module\nthat controls vehicle actions using a hybrid approach combining rule-based\nmodels. Experimental results demonstrate that CoMAL achieves superior\nperformance on the Flow benchmark. Additionally, we evaluate the impact of\ndifferent language models and compare our framework with reinforcement learning\napproaches. It highlights the strong cooperative capability of LLM agents and\npresents a promising solution to the mixed-autonomy traffic challenge. The code\nis available at https://github.com/Hyan-Yao/CoMAL."
                },
                "authors": [
                    {
                        "name": "Huaiyuan Yao"
                    },
                    {
                        "name": "Longchao Da"
                    },
                    {
                        "name": "Vishnu Nandam"
                    },
                    {
                        "name": "Justin Turnau"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Linsey Pang"
                    },
                    {
                        "name": "Hua Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wei"
                },
                "author": "Hua Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01129v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01129v4",
                "updated": "2024-10-18T10:46:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    46,
                    43,
                    4,
                    292,
                    0
                ],
                "published": "2024-08-02T09:18:41Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    9,
                    18,
                    41,
                    4,
                    215,
                    0
                ],
                "title": "A Survey of Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Mamba"
                },
                "summary": "As one of the most representative DL techniques, Transformer architecture has\nempowered numerous advanced models, especially the large language models (LLMs)\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space models\n(SSMs), has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering three main aspects:\nthe advancements of Mamba-based models, the techniques of adapting Mamba to\ndiverse data, and the applications where Mamba can excel. Specifically, we\nfirst review the foundational knowledge of various representative deep learning\nmodels and the details of Mamba-1&2 as preliminaries. Then, to showcase the\nsignificance of Mamba for AI, we comprehensively review the related studies\nfocusing on Mamba models' architecture design, data adaptability, and\napplications. Finally, we present a discussion of current limitations and\nexplore various promising research directions to provide deeper insights for\nfuture investigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As one of the most representative DL techniques, Transformer architecture has\nempowered numerous advanced models, especially the large language models (LLMs)\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space models\n(SSMs), has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering three main aspects:\nthe advancements of Mamba-based models, the techniques of adapting Mamba to\ndiverse data, and the applications where Mamba can excel. Specifically, we\nfirst review the foundational knowledge of various representative deep learning\nmodels and the details of Mamba-1&2 as preliminaries. Then, to showcase the\nsignificance of Mamba for AI, we comprehensively review the related studies\nfocusing on Mamba models' architecture design, data adaptability, and\napplications. Finally, we present a discussion of current limitations and\nexplore various promising research directions to provide deeper insights for\nfuture investigations."
                },
                "authors": [
                    {
                        "name": "Haohao Qu"
                    },
                    {
                        "name": "Liangbo Ning"
                    },
                    {
                        "name": "Rui An"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Tyler Derr"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01129v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01129v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.11244v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.11244v4",
                "updated": "2024-10-18T10:21:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    21,
                    31,
                    4,
                    292,
                    0
                ],
                "published": "2023-10-17T13:12:32Z",
                "published_parsed": [
                    2023,
                    10,
                    17,
                    13,
                    12,
                    32,
                    1,
                    290,
                    0
                ],
                "title": "Entity Matching using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity Matching using Large Language Models"
                },
                "summary": "Entity matching is the task of deciding whether two entity descriptions refer\nto the same real-world entity. Entity matching is a central step in most data\nintegration pipelines. Many state-of-the-art entity matching methods rely on\npre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks\nof these models for entity matching are that (i) the models require significant\namounts of task-specific training data and (ii) the fine-tuned models are not\nrobust concerning out-of-distribution entities. This paper investigates using\ngenerative large language models (LLMs) as a less task-specific training\ndata-dependent and more robust alternative to PLM-based matchers. The study\ncovers hosted and open-source LLMs which can be run locally. We evaluate these\nmodels in a zero-shot scenario and a scenario where task-specific training data\nis available. We compare different prompt designs and the prompt sensitivity of\nthe models. We show that there is no single best prompt but that the prompt\nneeds to be tuned for each model/dataset combination. We further investigate\n(i) the selection of in-context demonstrations, (ii) the generation of matching\nrules, as well as (iii) fine-tuning LLMs using the same pool of training data.\nOur experiments show that the best LLMs require no or only a few training\nexamples to perform comparably to PLMs that were fine-tuned using thousands of\nexamples. LLM-based matchers further exhibit higher robustness to unseen\nentities. We show that GPT4 can generate structured explanations for matching\ndecisions and can automatically identify potential causes of matching errors by\nanalyzing explanations of wrong decisions. We demonstrate that the model can\ngenerate meaningful textual descriptions of the identified error classes, which\ncan help data engineers to improve entity matching pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity matching is the task of deciding whether two entity descriptions refer\nto the same real-world entity. Entity matching is a central step in most data\nintegration pipelines. Many state-of-the-art entity matching methods rely on\npre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks\nof these models for entity matching are that (i) the models require significant\namounts of task-specific training data and (ii) the fine-tuned models are not\nrobust concerning out-of-distribution entities. This paper investigates using\ngenerative large language models (LLMs) as a less task-specific training\ndata-dependent and more robust alternative to PLM-based matchers. The study\ncovers hosted and open-source LLMs which can be run locally. We evaluate these\nmodels in a zero-shot scenario and a scenario where task-specific training data\nis available. We compare different prompt designs and the prompt sensitivity of\nthe models. We show that there is no single best prompt but that the prompt\nneeds to be tuned for each model/dataset combination. We further investigate\n(i) the selection of in-context demonstrations, (ii) the generation of matching\nrules, as well as (iii) fine-tuning LLMs using the same pool of training data.\nOur experiments show that the best LLMs require no or only a few training\nexamples to perform comparably to PLMs that were fine-tuned using thousands of\nexamples. LLM-based matchers further exhibit higher robustness to unseen\nentities. We show that GPT4 can generate structured explanations for matching\ndecisions and can automatically identify potential causes of matching errors by\nanalyzing explanations of wrong decisions. We demonstrate that the model can\ngenerate meaningful textual descriptions of the identified error classes, which\ncan help data engineers to improve entity matching pipelines."
                },
                "authors": [
                    {
                        "name": "Ralph Peeters"
                    },
                    {
                        "name": "Aaron Steiner"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "arxiv_comment": "Published in Proceedings of the 28th International Conference on\n  Extending Database Technology (EDBT), 25th March-28th March, 2025, ISBN\n  978-3-89318-098-1 on OpenProceedings.org",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.11244v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.11244v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19700v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19700v3",
                "updated": "2024-10-18T10:15:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    15,
                    29,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-29T13:16:37Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    13,
                    16,
                    37,
                    6,
                    273,
                    0
                ],
                "title": "2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding\n  for Large Language Models"
                },
                "summary": "Tables are ubiquitous across various domains for concisely representing\nstructured information. Empowering large language models (LLMs) to reason over\ntabular data represents an actively explored direction. However, since typical\nLLMs only support one-dimensional~(1D) inputs, existing methods often flatten\nthe two-dimensional~(2D) table structure into a sequence of tokens, which can\nseverely disrupt the spatial relationships and result in an inevitable loss of\nvital contextual information. In this paper, we first empirically demonstrate\nthe detrimental impact of such flattening operations on the performance of LLMs\nin capturing the spatial information of tables through two elaborate proxy\ntasks. Subsequently, we introduce a simple yet effective positional encoding\nmethod, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to\naddress this challenge. 2D-TPE enables each attention head to dynamically\nselect a permutation order of tokens within the context for attending to them,\nwhere each permutation represents a distinct traversal mode for the table, such\nas column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of\nlosing essential spatial information while preserving computational efficiency,\nthus better preserving the table structure. Extensive experiments across five\nbenchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring\nthe importance of preserving the table structure for accurate table\ncomprehension. Comprehensive analysis further reveals the substantially better\nscalability of 2D-TPE to large tables than baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables are ubiquitous across various domains for concisely representing\nstructured information. Empowering large language models (LLMs) to reason over\ntabular data represents an actively explored direction. However, since typical\nLLMs only support one-dimensional~(1D) inputs, existing methods often flatten\nthe two-dimensional~(2D) table structure into a sequence of tokens, which can\nseverely disrupt the spatial relationships and result in an inevitable loss of\nvital contextual information. In this paper, we first empirically demonstrate\nthe detrimental impact of such flattening operations on the performance of LLMs\nin capturing the spatial information of tables through two elaborate proxy\ntasks. Subsequently, we introduce a simple yet effective positional encoding\nmethod, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to\naddress this challenge. 2D-TPE enables each attention head to dynamically\nselect a permutation order of tokens within the context for attending to them,\nwhere each permutation represents a distinct traversal mode for the table, such\nas column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of\nlosing essential spatial information while preserving computational efficiency,\nthus better preserving the table structure. Extensive experiments across five\nbenchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring\nthe importance of preserving the table structure for accurate table\ncomprehension. Comprehensive analysis further reveals the substantially better\nscalability of 2D-TPE to large tables than baselines."
                },
                "authors": [
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19700v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19700v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.08102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.08102v2",
                "updated": "2024-10-18T09:58:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    58,
                    45,
                    4,
                    292,
                    0
                ],
                "published": "2023-02-16T06:01:31Z",
                "published_parsed": [
                    2023,
                    2,
                    16,
                    6,
                    1,
                    31,
                    3,
                    47,
                    0
                ],
                "title": "Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech\n  Recognition"
                },
                "summary": "Visual Speech Recognition (VSR) aims to infer speech into text depending on\nlip movements alone. As it focuses on visual information to model the speech,\nits performance is inherently sensitive to personal lip appearances and\nmovements, and this makes the VSR models show degraded performance when they\nare applied to unseen speakers. In this paper, to remedy the performance\ndegradation of the VSR model on unseen speakers, we propose prompt tuning\nmethods of Deep Neural Networks (DNNs) for speaker-adaptive VSR. Specifically,\nmotivated by recent advances in Natural Language Processing (NLP), we finetune\nprompts on adaptation data of target speakers instead of modifying the\npre-trained model parameters. Different from the previous prompt tuning methods\nmainly limited to Transformer variant architecture, we explore different types\nof prompts, the addition, the padding, and the concatenation form prompts that\ncan be applied to the VSR model which is composed of CNN and Transformer in\ngeneral. With the proposed prompt tuning, we show that the performance of the\npre-trained VSR model on unseen speakers can be largely improved by using a\nsmall amount of adaptation data (e.g., less than 5 minutes), even if the\npre-trained model is already developed with large speaker variations. Moreover,\nby analyzing the performance and parameters of different types of prompts, we\ninvestigate when the prompt tuning is preferred over the finetuning methods.\nThe effectiveness of the proposed method is evaluated on both word- and\nsentence-level VSR databases, LRW-ID and GRID.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Speech Recognition (VSR) aims to infer speech into text depending on\nlip movements alone. As it focuses on visual information to model the speech,\nits performance is inherently sensitive to personal lip appearances and\nmovements, and this makes the VSR models show degraded performance when they\nare applied to unseen speakers. In this paper, to remedy the performance\ndegradation of the VSR model on unseen speakers, we propose prompt tuning\nmethods of Deep Neural Networks (DNNs) for speaker-adaptive VSR. Specifically,\nmotivated by recent advances in Natural Language Processing (NLP), we finetune\nprompts on adaptation data of target speakers instead of modifying the\npre-trained model parameters. Different from the previous prompt tuning methods\nmainly limited to Transformer variant architecture, we explore different types\nof prompts, the addition, the padding, and the concatenation form prompts that\ncan be applied to the VSR model which is composed of CNN and Transformer in\ngeneral. With the proposed prompt tuning, we show that the performance of the\npre-trained VSR model on unseen speakers can be largely improved by using a\nsmall amount of adaptation data (e.g., less than 5 minutes), even if the\npre-trained model is already developed with large speaker variations. Moreover,\nby analyzing the performance and parameters of different types of prompts, we\ninvestigate when the prompt tuning is preferred over the finetuning methods.\nThe effectiveness of the proposed method is evaluated on both word- and\nsentence-level VSR databases, LRW-ID and GRID."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Hyung-Il Kim"
                    },
                    {
                        "name": "Yong Man Ro"
                    }
                ],
                "author_detail": {
                    "name": "Yong Man Ro"
                },
                "author": "Yong Man Ro",
                "arxiv_comment": "IEEE TPAMI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.08102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.08102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14340v1",
                "updated": "2024-10-18T09:51:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    51,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T09:51:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    51,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "Zero-shot Action Localization via the Confidence of Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Action Localization via the Confidence of Large\n  Vision-Language Models"
                },
                "summary": "Precise action localization in untrimmed video is vital for fields such as\nprofessional sports and minimally invasive surgery, where the delineation of\nparticular motions in recordings can dramatically enhance analysis. But in many\ncases, large scale datasets with video-label pairs for localization are\nunavailable, limiting the opportunity to fine-tune video-understanding models.\nRecent developments in large vision-language models (LVLM) address this need\nwith impressive zero-shot capabilities in a variety of video understanding\ntasks. However, the adaptation of image-based LVLMs, with their powerful visual\nquestion answering capabilities, to action localization in long-form video is\nstill relatively unexplored. To this end, we introduce a true ZEro-shot Action\nLocalization method (ZEAL). Specifically, we leverage the built-in action\nknowledge of a large language model (LLM) to inflate actions into\nhighly-detailed descriptions of the archetypal start and end of the action.\nThese descriptions serve as queries to LVLM for generating frame-level\nconfidence scores which can be aggregated to produce localization outputs. The\nsimplicity and flexibility of our method lends it amenable to more capable\nLVLMs as they are developed, and we demonstrate remarkable results in zero-shot\naction localization on a challenging benchmark, without any training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise action localization in untrimmed video is vital for fields such as\nprofessional sports and minimally invasive surgery, where the delineation of\nparticular motions in recordings can dramatically enhance analysis. But in many\ncases, large scale datasets with video-label pairs for localization are\nunavailable, limiting the opportunity to fine-tune video-understanding models.\nRecent developments in large vision-language models (LVLM) address this need\nwith impressive zero-shot capabilities in a variety of video understanding\ntasks. However, the adaptation of image-based LVLMs, with their powerful visual\nquestion answering capabilities, to action localization in long-form video is\nstill relatively unexplored. To this end, we introduce a true ZEro-shot Action\nLocalization method (ZEAL). Specifically, we leverage the built-in action\nknowledge of a large language model (LLM) to inflate actions into\nhighly-detailed descriptions of the archetypal start and end of the action.\nThese descriptions serve as queries to LVLM for generating frame-level\nconfidence scores which can be aggregated to produce localization outputs. The\nsimplicity and flexibility of our method lends it amenable to more capable\nLVLMs as they are developed, and we demonstrate remarkable results in zero-shot\naction localization on a challenging benchmark, without any training."
                },
                "authors": [
                    {
                        "name": "Josiah Aklilu"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13281v2",
                "updated": "2024-10-18T09:50:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    50,
                    41,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-17T07:15:15Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    7,
                    15,
                    15,
                    3,
                    291,
                    0
                ],
                "title": "BANTH: A Multi-label Hate Speech Detection Dataset for Transliterated\n  Bangla",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BANTH: A Multi-label Hate Speech Detection Dataset for Transliterated\n  Bangla"
                },
                "summary": "The proliferation of transliterated texts in digital spaces has emphasized\nthe need for detecting and classifying hate speech in languages beyond English,\nparticularly in low-resource languages. As online discourse can perpetuate\ndiscrimination based on target groups, e.g. gender, religion, and origin,\nmulti-label classification of hateful content can help in comprehending hate\nmotivation and enhance content moderation. While previous efforts have focused\non monolingual or binary hate classification tasks, no work has yet addressed\nthe challenge of multi-label hate speech classification in transliterated\nBangla. We introduce BanTH, the first multi-label transliterated Bangla hate\nspeech dataset comprising 37.3k samples. The samples are sourced from YouTube\ncomments, where each instance is labeled with one or more target groups,\nreflecting the regional demographic. We establish novel transformer\nencoder-based baselines by further pre-training on transliterated Bangla\ncorpus. We also propose a novel translation-based LLM prompting strategy for\ntransliterated text. Experiments reveal that our further pre-trained encoders\nare achieving state-of-the-art performance on the BanTH dataset, while our\ntranslation-based prompting outperforms other strategies in the zero-shot\nsetting. The introduction of BanTH not only fills a critical gap in hate speech\nresearch for Bangla but also sets the stage for future exploration into\ncode-mixed and multi-label classification challenges in underrepresented\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of transliterated texts in digital spaces has emphasized\nthe need for detecting and classifying hate speech in languages beyond English,\nparticularly in low-resource languages. As online discourse can perpetuate\ndiscrimination based on target groups, e.g. gender, religion, and origin,\nmulti-label classification of hateful content can help in comprehending hate\nmotivation and enhance content moderation. While previous efforts have focused\non monolingual or binary hate classification tasks, no work has yet addressed\nthe challenge of multi-label hate speech classification in transliterated\nBangla. We introduce BanTH, the first multi-label transliterated Bangla hate\nspeech dataset comprising 37.3k samples. The samples are sourced from YouTube\ncomments, where each instance is labeled with one or more target groups,\nreflecting the regional demographic. We establish novel transformer\nencoder-based baselines by further pre-training on transliterated Bangla\ncorpus. We also propose a novel translation-based LLM prompting strategy for\ntransliterated text. Experiments reveal that our further pre-trained encoders\nare achieving state-of-the-art performance on the BanTH dataset, while our\ntranslation-based prompting outperforms other strategies in the zero-shot\nsetting. The introduction of BanTH not only fills a critical gap in hate speech\nresearch for Bangla but also sets the stage for future exploration into\ncode-mixed and multi-label classification challenges in underrepresented\nlanguages."
                },
                "authors": [
                    {
                        "name": "Fabiha Haider"
                    },
                    {
                        "name": "Fariha Tanjim Shifat"
                    },
                    {
                        "name": "Md Farhan Ishmam"
                    },
                    {
                        "name": "Deeparghya Dutta Barua"
                    },
                    {
                        "name": "Md Sakib Ul Rahman Sourove"
                    },
                    {
                        "name": "Md Fahim"
                    },
                    {
                        "name": "Md Farhad Alam"
                    }
                ],
                "author_detail": {
                    "name": "Md Farhad Alam"
                },
                "author": "Md Farhad Alam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14335v1",
                "updated": "2024-10-18T09:46:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    46,
                    38,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T09:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    46,
                    38,
                    4,
                    292,
                    0
                ],
                "title": "Critical Questions Generation: Motivation and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical Questions Generation: Motivation and Challenges"
                },
                "summary": "The development of Large Language Models (LLMs) has brought impressive\nperformances on mitigation strategies against misinformation, such as\ncounterargument generation. However, LLMs are still seriously hindered by\noutdated knowledge and by their tendency to generate hallucinated content. In\norder to circumvent these issues, we propose a new task, namely, Critical\nQuestions Generation, consisting of processing an argumentative text to\ngenerate the critical questions (CQs) raised by it. In argumentation theory CQs\nare tools designed to lay bare the blind spots of an argument by pointing at\nthe information it could be missing. Thus, instead of trying to deploy LLMs to\nproduce knowledgeable and relevant counterarguments, we use them to question\narguments, without requiring any external knowledge. Research on CQs Generation\nusing LLMs requires a reference dataset for large scale experimentation. Thus,\nin this work we investigate two complementary methods to create such a\nresource: (i) instantiating CQs templates as defined by Walton's argumentation\ntheory and (ii), using LLMs as CQs generators. By doing so, we contribute with\na procedure to establish what is a valid CQ and conclude that, while LLMs are\nreasonable CQ generators, they still have a wide margin for improvement in this\ntask.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Large Language Models (LLMs) has brought impressive\nperformances on mitigation strategies against misinformation, such as\ncounterargument generation. However, LLMs are still seriously hindered by\noutdated knowledge and by their tendency to generate hallucinated content. In\norder to circumvent these issues, we propose a new task, namely, Critical\nQuestions Generation, consisting of processing an argumentative text to\ngenerate the critical questions (CQs) raised by it. In argumentation theory CQs\nare tools designed to lay bare the blind spots of an argument by pointing at\nthe information it could be missing. Thus, instead of trying to deploy LLMs to\nproduce knowledgeable and relevant counterarguments, we use them to question\narguments, without requiring any external knowledge. Research on CQs Generation\nusing LLMs requires a reference dataset for large scale experimentation. Thus,\nin this work we investigate two complementary methods to create such a\nresource: (i) instantiating CQs templates as defined by Walton's argumentation\ntheory and (ii), using LLMs as CQs generators. By doing so, we contribute with\na procedure to establish what is a valid CQ and conclude that, while LLMs are\nreasonable CQ generators, they still have a wide margin for improvement in this\ntask."
                },
                "authors": [
                    {
                        "name": "Blanca Calvo Figueras"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri",
                "arxiv_comment": "14 pages, 3 figures, 7 tables, to be published in the 28th Conference\n  on Computational Natural Language Learning (CoNLL 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14332v1",
                "updated": "2024-10-18T09:44:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    44,
                    25,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T09:44:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    44,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have catalyzed the\ndevelopment of Large Multimodal Models (LMMs). However, existing research\nprimarily focuses on tuning language and image instructions, ignoring the\ncritical pretraining phase where models learn to process textual and visual\nmodalities jointly. In this paper, we propose a new pretraining paradigm for\nLMMs to enhance the visual comprehension capabilities of LLMs by introducing a\nnovel cross-modal comprehension stage. Specifically, we design a dynamically\nlearnable prompt token pool and employ the Hungarian algorithm to replace part\nof the original visual tokens with the most relevant prompt tokens. Then, we\nconceptualize visual tokens as analogous to a \"foreign language\" for the LLMs\nand propose a mixed attention mechanism with bidirectional visual attention and\nunidirectional textual attention to comprehensively enhance the understanding\nof visual tokens. Meanwhile, we integrate a detailed caption generation task,\nleveraging rich descriptions to further facilitate LLMs in understanding visual\nsemantic information. After pretraining on 1.5 million publicly accessible\ndata, we present a new foundation model called Croc. Experimental results\ndemonstrate that Croc achieves new state-of-the-art performance on massive\nvision-language benchmarks. To support reproducibility and facilitate further\nresearch, we release the training code and pre-trained model weights at\nhttps://github.com/deepglint/Croc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have catalyzed the\ndevelopment of Large Multimodal Models (LMMs). However, existing research\nprimarily focuses on tuning language and image instructions, ignoring the\ncritical pretraining phase where models learn to process textual and visual\nmodalities jointly. In this paper, we propose a new pretraining paradigm for\nLMMs to enhance the visual comprehension capabilities of LLMs by introducing a\nnovel cross-modal comprehension stage. Specifically, we design a dynamically\nlearnable prompt token pool and employ the Hungarian algorithm to replace part\nof the original visual tokens with the most relevant prompt tokens. Then, we\nconceptualize visual tokens as analogous to a \"foreign language\" for the LLMs\nand propose a mixed attention mechanism with bidirectional visual attention and\nunidirectional textual attention to comprehensively enhance the understanding\nof visual tokens. Meanwhile, we integrate a detailed caption generation task,\nleveraging rich descriptions to further facilitate LLMs in understanding visual\nsemantic information. After pretraining on 1.5 million publicly accessible\ndata, we present a new foundation model called Croc. Experimental results\ndemonstrate that Croc achieves new state-of-the-art performance on massive\nvision-language benchmarks. To support reproducibility and facilitate further\nresearch, we release the training code and pre-trained model weights at\nhttps://github.com/deepglint/Croc."
                },
                "authors": [
                    {
                        "name": "Yin Xie"
                    },
                    {
                        "name": "Kaicheng Yang"
                    },
                    {
                        "name": "Ninghua Yang"
                    },
                    {
                        "name": "Weimo Deng"
                    },
                    {
                        "name": "Xiangzi Dai"
                    },
                    {
                        "name": "Tiancheng Gu"
                    },
                    {
                        "name": "Yumeng Wang"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Yongle Zhao"
                    },
                    {
                        "name": "Ziyong Feng"
                    },
                    {
                        "name": "Jiankang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Jiankang Deng"
                },
                "author": "Jiankang Deng",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14331v1",
                "updated": "2024-10-18T09:43:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    43,
                    30,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T09:43:30Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    43,
                    30,
                    4,
                    292,
                    0
                ],
                "title": "ChartifyText: Automated Chart Generation from Data-Involved Texts via\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChartifyText: Automated Chart Generation from Data-Involved Texts via\n  LLM"
                },
                "summary": "Text documents with numerical values involved are widely used in various\napplications such as scientific research, economy, public health and\njournalism. However, it is difficult for readers to quickly interpret such\ndata-involved texts and gain deep insights. To fill this research gap, this\nwork aims to automatically generate charts to accurately convey the underlying\ndata and ideas to readers, which is essentially a challenging task. The\nchallenges originate from text ambiguities, intrinsic sparsity and uncertainty\nof data in text documents, and subjective sentiment differences. Specifically,\nwe propose ChartifyText, a novel fully-automated approach that leverages Large\nLanguage Models (LLMs) to convert complex data-involved texts to expressive\ncharts. It consists of two major modules: tabular data inference and expressive\nchart generation. The tabular data inference module employs systematic prompt\nengineering to guide the LLM (e.g., GPT-4) to infer table data, where data\nranges, uncertainties, missing data values and corresponding subjective\nsentiments are explicitly considered. The expressive chart generation module\naugments standard charts with intuitive visual encodings and concise texts to\naccurately convey the underlying data and insights. We extensively evaluate the\neffectiveness of ChartifyText on real-world data-involved text documents\nthrough case studies, in-depth interviews with three visualization experts, and\na carefully-designed user study with 15 participants. The results demonstrate\nthe usefulness and effectiveness of ChartifyText in helping readers efficiently\nand effectively make sense of data-involved texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text documents with numerical values involved are widely used in various\napplications such as scientific research, economy, public health and\njournalism. However, it is difficult for readers to quickly interpret such\ndata-involved texts and gain deep insights. To fill this research gap, this\nwork aims to automatically generate charts to accurately convey the underlying\ndata and ideas to readers, which is essentially a challenging task. The\nchallenges originate from text ambiguities, intrinsic sparsity and uncertainty\nof data in text documents, and subjective sentiment differences. Specifically,\nwe propose ChartifyText, a novel fully-automated approach that leverages Large\nLanguage Models (LLMs) to convert complex data-involved texts to expressive\ncharts. It consists of two major modules: tabular data inference and expressive\nchart generation. The tabular data inference module employs systematic prompt\nengineering to guide the LLM (e.g., GPT-4) to infer table data, where data\nranges, uncertainties, missing data values and corresponding subjective\nsentiments are explicitly considered. The expressive chart generation module\naugments standard charts with intuitive visual encodings and concise texts to\naccurately convey the underlying data and insights. We extensively evaluate the\neffectiveness of ChartifyText on real-world data-involved text documents\nthrough case studies, in-depth interviews with three visualization experts, and\na carefully-designed user study with 15 participants. The results demonstrate\nthe usefulness and effectiveness of ChartifyText in helping readers efficiently\nand effectively make sense of data-involved texts."
                },
                "authors": [
                    {
                        "name": "Songheng Zhang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Yong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wang"
                },
                "author": "Yong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06750v2",
                "updated": "2024-10-18T09:43:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    43,
                    9,
                    4,
                    292,
                    0
                ],
                "published": "2024-04-10T05:34:07Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    5,
                    34,
                    7,
                    2,
                    101,
                    0
                ],
                "title": "Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of\n  Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of\n  Language Model Agents"
                },
                "summary": "Some have criticised Generative AI Systems for replicating the familiar\npathologies of already widely-deployed AI systems. Other critics highlight how\nthey foreshadow vastly more powerful future systems, which might threaten\nhumanity's survival. The first group says there is nothing new here; the other\nlooks through the present to a perhaps distant horizon. In this paper, I\ninstead pay attention to what makes these particular systems distinctive: both\ntheir remarkable scientific achievement, and the most likely and consequential\nways in which they will change society over the next five to ten years. In\nparticular, I explore the potential societal impacts and normative questions\nraised by the looming prospect of 'Language Model Agents', in which multimodal\nlarge language models (LLMs) form the executive centre of complex, tool-using\nAI systems that can take unsupervised sequences of actions towards some goal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some have criticised Generative AI Systems for replicating the familiar\npathologies of already widely-deployed AI systems. Other critics highlight how\nthey foreshadow vastly more powerful future systems, which might threaten\nhumanity's survival. The first group says there is nothing new here; the other\nlooks through the present to a perhaps distant horizon. In this paper, I\ninstead pay attention to what makes these particular systems distinctive: both\ntheir remarkable scientific achievement, and the most likely and consequential\nways in which they will change society over the next five to ten years. In\nparticular, I explore the potential societal impacts and normative questions\nraised by the looming prospect of 'Language Model Agents', in which multimodal\nlarge language models (LLMs) form the executive centre of complex, tool-using\nAI systems that can take unsupervised sequences of actions towards some goal."
                },
                "authors": [
                    {
                        "name": "Seth Lazar"
                    }
                ],
                "author_detail": {
                    "name": "Seth Lazar"
                },
                "author": "Seth Lazar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14321v1",
                "updated": "2024-10-18T09:32:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    32,
                    8,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T09:32:08Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    32,
                    8,
                    4,
                    292,
                    0
                ],
                "title": "From Solitary Directives to Interactive Encouragement! LLM Secure Code\n  Generation by Natural Language Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Solitary Directives to Interactive Encouragement! LLM Secure Code\n  Generation by Natural Language Prompting"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable potential in code\ngeneration, making them increasingly important in the field. However, the\nsecurity issues of generated code have not been fully addressed, and the\nusability of LLMs in code generation still requires further exploration.\n  This work introduces SecCode, a framework that leverages an innovative\ninteractive encouragement prompting (EP) technique for secure code generation\nwith \\textit{only NL} prompts. This approach ensures that the prompts can be\neasily shared and understood by general users. SecCode functions through three\nstages: 1) Code Generation using NL Prompts; 2) Code Vulnerability Detection\nand Fixing, utilising our proposed encouragement prompting; 3) Vulnerability\nCross-Checking and Code Security Refinement. These stages are executed in\nmultiple interactive iterations to progressively enhance security. By using\nboth proprietary LLMs (i.e., GPT-3.5 Turbo, GPT-4 and GPT-4o) and open-source\nLLMs (i.e., Llama 3.1 8B Instruct, DeepSeek Coder V2 Lite Instruct) evaluated\non three benchmark datasets, extensive experimental results show that our\nproposed SecCode greatly outperforms compared baselines, generating secure code\nwith a high vulnerability correction rate. For example, SecCode exhibits a high\nfix success rate of over 76\\% after running 5 automated EP interactive\niterations and over 89\\% after running 10 automated EP interactive iterations.\nTo the best of our knowledge, this work is the first to formulate secure code\ngeneration with NL prompts only. We have open-sourced our code and encourage\nthe community to focus on secure code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable potential in code\ngeneration, making them increasingly important in the field. However, the\nsecurity issues of generated code have not been fully addressed, and the\nusability of LLMs in code generation still requires further exploration.\n  This work introduces SecCode, a framework that leverages an innovative\ninteractive encouragement prompting (EP) technique for secure code generation\nwith \\textit{only NL} prompts. This approach ensures that the prompts can be\neasily shared and understood by general users. SecCode functions through three\nstages: 1) Code Generation using NL Prompts; 2) Code Vulnerability Detection\nand Fixing, utilising our proposed encouragement prompting; 3) Vulnerability\nCross-Checking and Code Security Refinement. These stages are executed in\nmultiple interactive iterations to progressively enhance security. By using\nboth proprietary LLMs (i.e., GPT-3.5 Turbo, GPT-4 and GPT-4o) and open-source\nLLMs (i.e., Llama 3.1 8B Instruct, DeepSeek Coder V2 Lite Instruct) evaluated\non three benchmark datasets, extensive experimental results show that our\nproposed SecCode greatly outperforms compared baselines, generating secure code\nwith a high vulnerability correction rate. For example, SecCode exhibits a high\nfix success rate of over 76\\% after running 5 automated EP interactive\niterations and over 89\\% after running 10 automated EP interactive iterations.\nTo the best of our knowledge, this work is the first to formulate secure code\ngeneration with NL prompts only. We have open-sourced our code and encourage\nthe community to focus on secure code generation."
                },
                "authors": [
                    {
                        "name": "Shigang Liu"
                    },
                    {
                        "name": "Bushra Sabir"
                    },
                    {
                        "name": "Seung Ick Jang"
                    },
                    {
                        "name": "Yuval Kansal"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Kristen Moore"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Surya Nepal"
                    }
                ],
                "author_detail": {
                    "name": "Surya Nepal"
                },
                "author": "Surya Nepal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14309v1",
                "updated": "2024-10-18T09:15:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    15,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T09:15:35Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    15,
                    35,
                    4,
                    292,
                    0
                ],
                "title": "LoGU: Long-form Generation with Uncertainty Expressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoGU: Long-form Generation with Uncertainty Expressions"
                },
                "summary": "While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses."
                },
                "authors": [
                    {
                        "name": "Ruihan Yang"
                    },
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Nigel Collier"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13632v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13632v3",
                "updated": "2024-10-18T09:07:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    7,
                    53,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-19T15:28:29Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    15,
                    28,
                    29,
                    2,
                    171,
                    0
                ],
                "title": "Can Few-shot Work in Long-Context? Recycling the Context to Generate\n  Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Few-shot Work in Long-Context? Recycling the Context to Generate\n  Demonstrations"
                },
                "summary": "Despite recent advancements in Large Language Models (LLMs), their\nperformance on tasks involving long contexts remains sub-optimal. In-Context\nLearning (ICL) with few-shot examples may be an appealing solution to enhance\nLLM performance in this scenario; However, na\\\"ively adding ICL examples with\nlong context introduces challenges, including substantial token overhead added\nfor each few-shot example and context mismatch between the demonstrations and\nthe target query. In this work, we propose to automatically generate few-shot\nexamples for long context QA tasks by recycling contexts. Specifically, given a\nlong input context (1-3k tokens) and a query, we generate additional\nquery-output pairs from the given context as few-shot examples, while\nintroducing the context only once. This ensures that the demonstrations are\nleveraging the same context as the target query while only adding a small\nnumber of tokens to the prompt. We further enhance each demonstration by\ninstructing the model to explicitly identify the relevant paragraphs before the\nanswer, which improves performance while providing fine-grained attribution to\nthe answer source. We apply our method on multiple LLMs and obtain substantial\nimprovements (+16 absolute points on average across models) on various QA\ndatasets with long context, especially when the answer lies within the middle\nof the context. Surprisingly, despite introducing only single-hop ICL examples,\nLLMs also successfully generalize to multi-hop long-context QA using our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advancements in Large Language Models (LLMs), their\nperformance on tasks involving long contexts remains sub-optimal. In-Context\nLearning (ICL) with few-shot examples may be an appealing solution to enhance\nLLM performance in this scenario; However, na\\\"ively adding ICL examples with\nlong context introduces challenges, including substantial token overhead added\nfor each few-shot example and context mismatch between the demonstrations and\nthe target query. In this work, we propose to automatically generate few-shot\nexamples for long context QA tasks by recycling contexts. Specifically, given a\nlong input context (1-3k tokens) and a query, we generate additional\nquery-output pairs from the given context as few-shot examples, while\nintroducing the context only once. This ensures that the demonstrations are\nleveraging the same context as the target query while only adding a small\nnumber of tokens to the prompt. We further enhance each demonstration by\ninstructing the model to explicitly identify the relevant paragraphs before the\nanswer, which improves performance while providing fine-grained attribution to\nthe answer source. We apply our method on multiple LLMs and obtain substantial\nimprovements (+16 absolute points on average across models) on various QA\ndatasets with long context, especially when the answer lies within the middle\nof the context. Surprisingly, despite introducing only single-hop ICL examples,\nLLMs also successfully generalize to multi-hop long-context QA using our\napproach."
                },
                "authors": [
                    {
                        "name": "Arie Cattan"
                    },
                    {
                        "name": "Alon Jacovi"
                    },
                    {
                        "name": "Alex Fabrikant"
                    },
                    {
                        "name": "Jonathan Herzig"
                    },
                    {
                        "name": "Roee Aharoni"
                    },
                    {
                        "name": "Hannah Rashkin"
                    },
                    {
                        "name": "Dror Marcus"
                    },
                    {
                        "name": "Avinatan Hassidim"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Avi Caciularu"
                    }
                ],
                "author_detail": {
                    "name": "Avi Caciularu"
                },
                "author": "Avi Caciularu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13632v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13632v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12300v2",
                "updated": "2024-10-18T09:07:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    7,
                    19,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-16T06:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    6,
                    59,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "Sparse Causal Effect Estimation using Two-Sample Summary Statistics in\n  the Presence of Unmeasured Confounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Causal Effect Estimation using Two-Sample Summary Statistics in\n  the Presence of Unmeasured Confounding"
                },
                "summary": "Observational genome-wide association studies are now widely used for causal\ninference in genetic epidemiology. To maintain privacy, such data is often only\npublicly available as summary statistics, and often studies for the endogenous\ncovariates and the outcome are available separately. This has necessitated\nmethods tailored to two-sample summary statistics. Current state-of-the-art\nmethods modify linear instrumental variable (IV) regression -- with genetic\nvariants as instruments -- to account for unmeasured confounding. However,\nsince the endogenous covariates can be high dimensional, standard IV\nassumptions are generally insufficient to identify all causal effects\nsimultaneously. We ensure identifiability by assuming the causal effects are\nsparse and propose a sparse causal effect two-sample IV estimator, spaceTSIV,\nadapting the spaceIV estimator by Pfister and Peters (2022) for two-sample\nsummary statistics. We provide two methods, based on L0- and L1-penalization,\nrespectively. We prove identifiability of the sparse causal effects in the\ntwo-sample setting and consistency of spaceTSIV. The performance of spaceTSIV\nis compared with existing two-sample IV methods in simulations. Finally, we\nshowcase our methods using real proteomic and gene-expression data for\ndrug-target discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observational genome-wide association studies are now widely used for causal\ninference in genetic epidemiology. To maintain privacy, such data is often only\npublicly available as summary statistics, and often studies for the endogenous\ncovariates and the outcome are available separately. This has necessitated\nmethods tailored to two-sample summary statistics. Current state-of-the-art\nmethods modify linear instrumental variable (IV) regression -- with genetic\nvariants as instruments -- to account for unmeasured confounding. However,\nsince the endogenous covariates can be high dimensional, standard IV\nassumptions are generally insufficient to identify all causal effects\nsimultaneously. We ensure identifiability by assuming the causal effects are\nsparse and propose a sparse causal effect two-sample IV estimator, spaceTSIV,\nadapting the spaceIV estimator by Pfister and Peters (2022) for two-sample\nsummary statistics. We provide two methods, based on L0- and L1-penalization,\nrespectively. We prove identifiability of the sparse causal effects in the\ntwo-sample setting and consistency of spaceTSIV. The performance of spaceTSIV\nis compared with existing two-sample IV methods in simulations. Finally, we\nshowcase our methods using real proteomic and gene-expression data for\ndrug-target discovery."
                },
                "authors": [
                    {
                        "name": "Shimeng Huang"
                    },
                    {
                        "name": "Niklas Pfister"
                    },
                    {
                        "name": "Jack Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Jack Bowden"
                },
                "author": "Jack Bowden",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09075v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09075v3",
                "updated": "2024-10-18T09:02:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    2,
                    46,
                    4,
                    292,
                    0
                ],
                "published": "2023-12-14T16:10:56Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    16,
                    10,
                    56,
                    3,
                    348,
                    0
                ],
                "title": "Towards Verifiable Text Generation with Evolving Memory and\n  Self-Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Verifiable Text Generation with Evolving Memory and\n  Self-Reflection"
                },
                "summary": "Despite the remarkable ability of large language models (LLMs) in language\ncomprehension and generation, they often suffer from producing factually\nincorrect information, also known as hallucination. A promising solution to\nthis issue is verifiable text generation, which prompts LLMs to generate\ncontent with citations for accuracy verification. However, verifiable text\ngeneration is non-trivial due to the focus-shifting phenomenon, the intricate\nreasoning needed to align the claim with correct citations, and the dilemma\nbetween the precision and breadth of retrieved documents. In this paper, we\npresent VTG, an innovative framework for Verifiable Text Generation with\nevolving memory and self-reflection. VTG introduces evolving long short-term\nmemory to retain both valuable documents and recent documents. A two-tier\nverifier equipped with an evidence finder is proposed to rethink and reflect on\nthe relationship between the claim and citations. Furthermore, active retrieval\nand diverse query generation are utilized to enhance both the precision and\nbreadth of the retrieved documents. We conduct extensive experiments on five\ndatasets across three knowledge-intensive tasks and the results reveal that VTG\nsignificantly outperforms baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable ability of large language models (LLMs) in language\ncomprehension and generation, they often suffer from producing factually\nincorrect information, also known as hallucination. A promising solution to\nthis issue is verifiable text generation, which prompts LLMs to generate\ncontent with citations for accuracy verification. However, verifiable text\ngeneration is non-trivial due to the focus-shifting phenomenon, the intricate\nreasoning needed to align the claim with correct citations, and the dilemma\nbetween the precision and breadth of retrieved documents. In this paper, we\npresent VTG, an innovative framework for Verifiable Text Generation with\nevolving memory and self-reflection. VTG introduces evolving long short-term\nmemory to retain both valuable documents and recent documents. A two-tier\nverifier equipped with an evidence finder is proposed to rethink and reflect on\nthe relationship between the claim and citations. Furthermore, active retrieval\nand diverse query generation are utilized to enhance both the precision and\nbreadth of the retrieved documents. We conduct extensive experiments on five\ndatasets across three knowledge-intensive tasks and the results reveal that VTG\nsignificantly outperforms baselines."
                },
                "authors": [
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Hengyi Cai"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Yingyan Hou"
                    },
                    {
                        "name": "Xiaochi Wei"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "arxiv_comment": "EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.09075v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09075v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13824v2",
                "updated": "2024-10-18T09:01:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    1,
                    1,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-17T17:48:54Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    48,
                    54,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Webpage UIs for Text-Rich Visual Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Webpage UIs for Text-Rich Visual Understanding"
                },
                "summary": "Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios."
                },
                "authors": [
                    {
                        "name": "Junpeng Liu"
                    },
                    {
                        "name": "Tianyue Ou"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Yuxiao Qu"
                    },
                    {
                        "name": "Wai Lam"
                    },
                    {
                        "name": "Chenyan Xiong"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Xiang Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Yue"
                },
                "author": "Xiang Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.14626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.14626v2",
                "updated": "2024-10-18T08:56:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    56,
                    18,
                    4,
                    292,
                    0
                ],
                "published": "2023-10-23T07:00:51Z",
                "published_parsed": [
                    2023,
                    10,
                    23,
                    7,
                    0,
                    51,
                    0,
                    296,
                    0
                ],
                "title": "Conversational Recommender System and Large Language Model Are Made for\n  Each Other in E-commerce Pre-sales Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Recommender System and Large Language Model Are Made for\n  Each Other in E-commerce Pre-sales Dialogue"
                },
                "summary": "E-commerce pre-sales dialogue aims to understand and elicit user needs and\npreferences for the items they are seeking so as to provide appropriate\nrecommendations. Conversational recommender systems (CRSs) learn user\nrepresentation and provide accurate recommendations based on dialogue context,\nbut rely on external knowledge. Large language models (LLMs) generate responses\nthat mimic pre-sales dialogues after fine-tuning, but lack domain-specific\nknowledge for accurate recommendations. Intuitively, the strengths of LLM and\nCRS in E-commerce pre-sales dialogues are complementary, yet no previous work\nhas explored this. This paper investigates the effectiveness of combining LLM\nand CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:\nCRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a\nreal-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of\ntwo collaborative approaches with two CRSs and two LLMs on four tasks of\nEcommerce pre-sales dialogue. We find that collaborations between CRS and LLM\ncan be very effective in some cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-commerce pre-sales dialogue aims to understand and elicit user needs and\npreferences for the items they are seeking so as to provide appropriate\nrecommendations. Conversational recommender systems (CRSs) learn user\nrepresentation and provide accurate recommendations based on dialogue context,\nbut rely on external knowledge. Large language models (LLMs) generate responses\nthat mimic pre-sales dialogues after fine-tuning, but lack domain-specific\nknowledge for accurate recommendations. Intuitively, the strengths of LLM and\nCRS in E-commerce pre-sales dialogues are complementary, yet no previous work\nhas explored this. This paper investigates the effectiveness of combining LLM\nand CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:\nCRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a\nreal-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of\ntwo collaborative approaches with two CRSs and two LLMs on four tasks of\nEcommerce pre-sales dialogue. We find that collaborations between CRS and LLM\ncan be very effective in some cases."
                },
                "authors": [
                    {
                        "name": "Yuanxing Liu"
                    },
                    {
                        "name": "Wei-Nan Zhang"
                    },
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Yuchi Zhang"
                    },
                    {
                        "name": "Haopeng Bai"
                    },
                    {
                        "name": "Fan Feng"
                    },
                    {
                        "name": "Hengbin Cui"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "EMNLP 2023 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.14626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.14626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14296v1",
                "updated": "2024-10-18T08:53:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    53,
                    50,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T08:53:50Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    53,
                    50,
                    4,
                    292,
                    0
                ],
                "title": "Unfolding the Network of Peer Grades: A Latent Variable Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unfolding the Network of Peer Grades: A Latent Variable Approach"
                },
                "summary": "Peer grading is an educational system in which students assess each other's\nwork. It is commonly applied under Massive Open Online Course (MOOC) and\noffline classroom settings. With this system, instructors receive a reduced\ngrading workload, and students enhance their understanding of course materials\nby grading others' work. Peer grading data have a complex dependence structure,\nfor which all the peer grades may be dependent. This complex dependence\nstructure is due to a network structure of peer grading, where each student can\nbe viewed as a vertex of the network, and each peer grade serves as an edge\nconnecting one student as a grader to another student as an examinee. This\npaper introduces a latent variable model framework for analyzing peer grading\ndata and develops a fully Bayesian procedure for its statistical inference.\nThis framework has several advantages. First, when aggregating multiple peer\ngrades, the average score and other simple summary statistics fail to account\nfor grader effects and, thus, can be biased. The proposed approach produces\nmore accurate model parameter estimates and, therefore, more accurate\naggregated grades, by modeling the heterogeneous grading behavior with latent\nvariables. Second, the proposed method provides a way to assess each student's\nperformance as a grader, which may be used to identify a pool of reliable\ngraders or generate feedback to help students improve their grading. Third, our\nmodel may further provide insights into the peer grading system by answering\nquestions such as whether a student who performs better in coursework also\ntends to be a more reliable grader. Finally, thanks to the Bayesian approach,\nuncertainty quantification is straightforward when inferring the\nstudent-specific latent variables as well as the structural parameters of the\nmodel. The proposed method is applied to two real-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer grading is an educational system in which students assess each other's\nwork. It is commonly applied under Massive Open Online Course (MOOC) and\noffline classroom settings. With this system, instructors receive a reduced\ngrading workload, and students enhance their understanding of course materials\nby grading others' work. Peer grading data have a complex dependence structure,\nfor which all the peer grades may be dependent. This complex dependence\nstructure is due to a network structure of peer grading, where each student can\nbe viewed as a vertex of the network, and each peer grade serves as an edge\nconnecting one student as a grader to another student as an examinee. This\npaper introduces a latent variable model framework for analyzing peer grading\ndata and develops a fully Bayesian procedure for its statistical inference.\nThis framework has several advantages. First, when aggregating multiple peer\ngrades, the average score and other simple summary statistics fail to account\nfor grader effects and, thus, can be biased. The proposed approach produces\nmore accurate model parameter estimates and, therefore, more accurate\naggregated grades, by modeling the heterogeneous grading behavior with latent\nvariables. Second, the proposed method provides a way to assess each student's\nperformance as a grader, which may be used to identify a pool of reliable\ngraders or generate feedback to help students improve their grading. Third, our\nmodel may further provide insights into the peer grading system by answering\nquestions such as whether a student who performs better in coursework also\ntends to be a more reliable grader. Finally, thanks to the Bayesian approach,\nuncertainty quantification is straightforward when inferring the\nstudent-specific latent variables as well as the structural parameters of the\nmodel. The proposed method is applied to two real-world datasets."
                },
                "authors": [
                    {
                        "name": "Giuseppe Mignemi"
                    },
                    {
                        "name": "Yunxiao Chen"
                    },
                    {
                        "name": "Irini Moustaki"
                    }
                ],
                "author_detail": {
                    "name": "Irini Moustaki"
                },
                "author": "Irini Moustaki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15053v2",
                "updated": "2024-10-18T08:51:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    51,
                    55,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-21T11:00:38Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    11,
                    0,
                    38,
                    4,
                    173,
                    0
                ],
                "title": "PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement\n  on Multilingual and Multi-Cultural Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement\n  on Multilingual and Multi-Cultural Data"
                },
                "summary": "Evaluation of multilingual Large Language Models (LLMs) is challenging due to\na variety of factors -- the lack of benchmarks with sufficient linguistic\ndiversity, contamination of popular benchmarks into LLM pre-training data and\nthe lack of local, cultural nuances in translated benchmarks. In this work, we\nstudy human and LLM-based evaluation in a multilingual, multi-cultural setting.\nWe evaluate 30 models across 10 Indic languages by conducting 90K human\nevaluations and 30K LLM-based evaluations and find that models such as GPT-4o\nand Llama-3 70B consistently perform best for most Indic languages. We build\nleaderboards for two evaluation settings - pairwise comparison and direct\nassessment and analyze the agreement between humans and LLMs. We find that\nhumans and LLMs agree fairly well in the pairwise setting but the agreement\ndrops for direct assessment evaluation especially for languages such as Bengali\nand Odia. We also check for various biases in human and LLM-based evaluation\nand find evidence of self-bias in the GPT-based evaluator. Our work presents a\nsignificant step towards scaling up multilingual evaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of multilingual Large Language Models (LLMs) is challenging due to\na variety of factors -- the lack of benchmarks with sufficient linguistic\ndiversity, contamination of popular benchmarks into LLM pre-training data and\nthe lack of local, cultural nuances in translated benchmarks. In this work, we\nstudy human and LLM-based evaluation in a multilingual, multi-cultural setting.\nWe evaluate 30 models across 10 Indic languages by conducting 90K human\nevaluations and 30K LLM-based evaluations and find that models such as GPT-4o\nand Llama-3 70B consistently perform best for most Indic languages. We build\nleaderboards for two evaluation settings - pairwise comparison and direct\nassessment and analyze the agreement between humans and LLMs. We find that\nhumans and LLMs agree fairly well in the pairwise setting but the agreement\ndrops for direct assessment evaluation especially for languages such as Bengali\nand Odia. We also check for various biases in human and LLM-based evaluation\nand find evidence of self-bias in the GPT-based evaluator. Our work presents a\nsignificant step towards scaling up multilingual evaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Ishaan Watts"
                    },
                    {
                        "name": "Varun Gumma"
                    },
                    {
                        "name": "Aditya Yadavalli"
                    },
                    {
                        "name": "Vivek Seshadri"
                    },
                    {
                        "name": "Manohar Swaminathan"
                    },
                    {
                        "name": "Sunayana Sitaram"
                    }
                ],
                "author_detail": {
                    "name": "Sunayana Sitaram"
                },
                "author": "Sunayana Sitaram",
                "arxiv_comment": "Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14291v1",
                "updated": "2024-10-18T08:50:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    50,
                    38,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T08:50:38Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    50,
                    38,
                    4,
                    292,
                    0
                ],
                "title": "First results from the JWST Early Release Science Program Q3D: The Fast\n  Outflow in a Red Quasar at z=0.44",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First results from the JWST Early Release Science Program Q3D: The Fast\n  Outflow in a Red Quasar at z=0.44"
                },
                "summary": "Quasar feedback may play a key role in the evolution of massive galaxies. The\ndust-reddened quasar, F2M110648.35$+$480712 at $z = 0.4352$ is one of the few\ncases at its redshift that exhibits powerful quasar feedback through bipolar\noutflows. Our new observation with the integral field unit mode of\nNear-infrared Spectrograph onboard JWST opens a new window to examine this\nspectacular outflow through Pa$\\alpha$ emission line with $\\sim$3$\\times$\nbetter spatial resolution than previous work. The morphology and kinematics of\nthe Pa$\\alpha$ nebula confirm the existence of a bipolar outflow extending on a\nscale of $\\sim$17$\\times$14 kpc and with a velocity reaching $\\sim$1100 km\ns$^{-1}$. The higher spatial resolution of our new observation leads to more\nreliable measurements of outflow kinematics. Considering only the spatially\nresolved outflow and assuming an electron density of 100 cm$^{-2}$, the mass,\nmomentum and kinetic energy outflow rates are $\\sim$50-210 M$_{\\odot}$\nyr$^{-1}$, $\\sim$0.3-1.7$\\times$10$^{36}$ dynes ($\\sim$14-78\\% of the quasar\nphoton momentum flux) and $\\sim$0.16-1.27$\\times$10$^{44}$ erg s$^{-1}$\n($\\sim$0.02-0.20\\% of the quasar bolometric luminosity), respectively. The\nlocal instantaneous outflow rates generally decrease radially. We infer that\nthe quasar is powerful enough to drive the outflow, while stellar processes\ncannot be overlooked as a contributing energy source. The mass outflow rate is\n$\\sim$0.4-1.5 times the star formation rate, and the ratio of kinetic energy\noutflow rate to the quasar bolometric luminosity is comparable to the minimum\nvalue required for negative quasar feedback in simulations. This outflow may\nhelp regulate the star formation activity within the system to some extent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quasar feedback may play a key role in the evolution of massive galaxies. The\ndust-reddened quasar, F2M110648.35$+$480712 at $z = 0.4352$ is one of the few\ncases at its redshift that exhibits powerful quasar feedback through bipolar\noutflows. Our new observation with the integral field unit mode of\nNear-infrared Spectrograph onboard JWST opens a new window to examine this\nspectacular outflow through Pa$\\alpha$ emission line with $\\sim$3$\\times$\nbetter spatial resolution than previous work. The morphology and kinematics of\nthe Pa$\\alpha$ nebula confirm the existence of a bipolar outflow extending on a\nscale of $\\sim$17$\\times$14 kpc and with a velocity reaching $\\sim$1100 km\ns$^{-1}$. The higher spatial resolution of our new observation leads to more\nreliable measurements of outflow kinematics. Considering only the spatially\nresolved outflow and assuming an electron density of 100 cm$^{-2}$, the mass,\nmomentum and kinetic energy outflow rates are $\\sim$50-210 M$_{\\odot}$\nyr$^{-1}$, $\\sim$0.3-1.7$\\times$10$^{36}$ dynes ($\\sim$14-78\\% of the quasar\nphoton momentum flux) and $\\sim$0.16-1.27$\\times$10$^{44}$ erg s$^{-1}$\n($\\sim$0.02-0.20\\% of the quasar bolometric luminosity), respectively. The\nlocal instantaneous outflow rates generally decrease radially. We infer that\nthe quasar is powerful enough to drive the outflow, while stellar processes\ncannot be overlooked as a contributing energy source. The mass outflow rate is\n$\\sim$0.4-1.5 times the star formation rate, and the ratio of kinetic energy\noutflow rate to the quasar bolometric luminosity is comparable to the minimum\nvalue required for negative quasar feedback in simulations. This outflow may\nhelp regulate the star formation activity within the system to some extent."
                },
                "authors": [
                    {
                        "name": "Weizhe Liu"
                    },
                    {
                        "name": "Sylvain Veilleux"
                    },
                    {
                        "name": "Swetha Sankar"
                    },
                    {
                        "name": "David S. N. Rupke"
                    },
                    {
                        "name": "Nadia L. Zakamska"
                    },
                    {
                        "name": "Dominika Wylezalek"
                    },
                    {
                        "name": "Andrey Vayner"
                    },
                    {
                        "name": "Caroline Bertemes"
                    },
                    {
                        "name": "Yu-Ching Chen"
                    },
                    {
                        "name": "Yuzo Ishikawa"
                    },
                    {
                        "name": "Jenny E. Greene"
                    },
                    {
                        "name": "Timothy Heckman"
                    },
                    {
                        "name": "Guilin Liu"
                    },
                    {
                        "name": "Hsiao-Wen Chen"
                    },
                    {
                        "name": "Dieter Lutz"
                    },
                    {
                        "name": "Sean D. Johnson"
                    },
                    {
                        "name": "Nicole P. H. Nesvadba"
                    },
                    {
                        "name": "Patrick Ogle"
                    },
                    {
                        "name": "Nadiia Diachenko"
                    },
                    {
                        "name": "Andy D. Goulding"
                    },
                    {
                        "name": "Kevin N. Hainline"
                    },
                    {
                        "name": "Fred Hamann"
                    },
                    {
                        "name": "Hui Xian Grace Lim"
                    },
                    {
                        "name": "Nora Ltzgendorf"
                    },
                    {
                        "name": "Vincenzo Mainieri"
                    },
                    {
                        "name": "Ryan McCrory"
                    },
                    {
                        "name": "Grey Murphree"
                    },
                    {
                        "name": "Eckhard Sturm"
                    },
                    {
                        "name": "Lillian Whitesell"
                    }
                ],
                "author_detail": {
                    "name": "Lillian Whitesell"
                },
                "author": "Lillian Whitesell",
                "arxiv_comment": "14 pages, 6 figures, 1 table, ApJ in review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14279v1",
                "updated": "2024-10-18T08:35:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    35,
                    57,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T08:35:57Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    35,
                    57,
                    4,
                    292,
                    0
                ],
                "title": "ClearSR: Latent Low-Resolution Image Embeddings Help Diffusion-Based\n  Real-World Super Resolution Models See Clearer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClearSR: Latent Low-Resolution Image Embeddings Help Diffusion-Based\n  Real-World Super Resolution Models See Clearer"
                },
                "summary": "We present ClearSR, a new method that can better take advantage of latent\nlow-resolution image (LR) embeddings for diffusion-based real-world image\nsuper-resolution (Real-ISR). Previous Real-ISR models mostly focus on how to\nactivate more generative priors of text-to-image diffusion models to make the\noutput high-resolution (HR) images look better. However, since these methods\nrely too much on the generative priors, the content of the output images is\noften inconsistent with the input LR ones. To mitigate the above issue, in this\nwork, we explore using latent LR embeddings to constrain the control signals\nfrom ControlNet, and extract LR information at both detail and structure\nlevels. We show that the proper use of latent LR embeddings can produce\nhigher-quality control signals, which enables the super-resolution results to\nbe more consistent with the LR image and leads to clearer visual results. In\naddition, we also show that latent LR embeddings can be used to control the\ninference stage, allowing for the improvement of fidelity and generation\nability simultaneously. Experiments demonstrate that our model can achieve\nbetter performance across multiple metrics on several test sets and generate\nmore consistent SR results with LR images than existing methods. Our code will\nbe made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ClearSR, a new method that can better take advantage of latent\nlow-resolution image (LR) embeddings for diffusion-based real-world image\nsuper-resolution (Real-ISR). Previous Real-ISR models mostly focus on how to\nactivate more generative priors of text-to-image diffusion models to make the\noutput high-resolution (HR) images look better. However, since these methods\nrely too much on the generative priors, the content of the output images is\noften inconsistent with the input LR ones. To mitigate the above issue, in this\nwork, we explore using latent LR embeddings to constrain the control signals\nfrom ControlNet, and extract LR information at both detail and structure\nlevels. We show that the proper use of latent LR embeddings can produce\nhigher-quality control signals, which enables the super-resolution results to\nbe more consistent with the LR image and leads to clearer visual results. In\naddition, we also show that latent LR embeddings can be used to control the\ninference stage, allowing for the improvement of fidelity and generation\nability simultaneously. Experiments demonstrate that our model can achieve\nbetter performance across multiple metrics on several test sets and generate\nmore consistent SR results with LR images than existing methods. Our code will\nbe made publicly available."
                },
                "authors": [
                    {
                        "name": "Yuhao Wan"
                    },
                    {
                        "name": "Peng-Tao Jiang"
                    },
                    {
                        "name": "Qibin Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Jinwei Chen"
                    },
                    {
                        "name": "Ming-Ming Cheng"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14276v1",
                "updated": "2024-10-18T08:31:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    31,
                    22,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T08:31:22Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    31,
                    22,
                    4,
                    292,
                    0
                ],
                "title": "EcomEdit: An Automated E-commerce Knowledge Editing Framework for\n  Enhanced Product and Purchase Intention Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcomEdit: An Automated E-commerce Knowledge Editing Framework for\n  Enhanced Product and Purchase Intention Understanding"
                },
                "summary": "Knowledge Editing (KE) aims to correct and update factual information in\nLarge Language Models (LLMs) to ensure accuracy and relevance without\ncomputationally expensive fine-tuning. Though it has been proven effective in\nseveral domains, limited work has focused on its application within the\ne-commerce sector. However, there are naturally occurring scenarios that make\nKE necessary in this domain, such as the timely updating of product features\nand trending purchase intentions by customers, which necessitate further\nexploration. In this paper, we pioneer the application of KE in the e-commerce\ndomain by presenting ECOMEDIT, an automated e-commerce knowledge editing\nframework tailored for e-commerce-related knowledge and tasks. Our framework\nleverages more powerful LLMs as judges to enable automatic knowledge conflict\ndetection and incorporates conceptualization to enhance the semantic coverage\nof the knowledge to be edited. Through extensive experiments, we demonstrate\nthe effectiveness of ECOMEDIT in improving LLMs' understanding of product\ndescriptions and purchase intentions. We also show that LLMs, after our\nediting, can achieve stronger performance on downstream e-commerce tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing (KE) aims to correct and update factual information in\nLarge Language Models (LLMs) to ensure accuracy and relevance without\ncomputationally expensive fine-tuning. Though it has been proven effective in\nseveral domains, limited work has focused on its application within the\ne-commerce sector. However, there are naturally occurring scenarios that make\nKE necessary in this domain, such as the timely updating of product features\nand trending purchase intentions by customers, which necessitate further\nexploration. In this paper, we pioneer the application of KE in the e-commerce\ndomain by presenting ECOMEDIT, an automated e-commerce knowledge editing\nframework tailored for e-commerce-related knowledge and tasks. Our framework\nleverages more powerful LLMs as judges to enable automatic knowledge conflict\ndetection and incorporates conceptualization to enhance the semantic coverage\nof the knowledge to be edited. Through extensive experiments, we demonstrate\nthe effectiveness of ECOMEDIT in improving LLMs' understanding of product\ndescriptions and purchase intentions. We also show that LLMs, after our\nediting, can achieve stronger performance on downstream e-commerce tasks."
                },
                "authors": [
                    {
                        "name": "Ching Ming Samuel Lau"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Haochen Shi"
                    },
                    {
                        "name": "Baixuan Xu"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14273v1",
                "updated": "2024-10-18T08:27:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    27,
                    2,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T08:27:02Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    27,
                    2,
                    4,
                    292,
                    0
                ],
                "title": "REEF: Representation Encoding Fingerprints for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REEF: Representation Encoding Fingerprints for Large Language Models"
                },
                "summary": "Protecting the intellectual property of open-source Large Language Models\n(LLMs) is very important, because training LLMs costs extensive computational\nresources and data. Therefore, model owners and third parties need to identify\nwhether a suspect model is a subsequent development of the victim model. To\nthis end, we propose a training-free REEF to identify the relationship between\nthe suspect and victim models from the perspective of LLMs' feature\nrepresentations. Specifically, REEF computes and compares the centered kernel\nalignment similarity between the representations of a suspect model and a\nvictim model on the same samples. This training-free REEF does not impair the\nmodel's general capabilities and is robust to sequential fine-tuning, pruning,\nmodel merging, and permutations. In this way, REEF provides a simple and\neffective way for third parties and models' owners to protect LLMs'\nintellectual property together. The code is available at\nhttps://github.com/tmylla/REEF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting the intellectual property of open-source Large Language Models\n(LLMs) is very important, because training LLMs costs extensive computational\nresources and data. Therefore, model owners and third parties need to identify\nwhether a suspect model is a subsequent development of the victim model. To\nthis end, we propose a training-free REEF to identify the relationship between\nthe suspect and victim models from the perspective of LLMs' feature\nrepresentations. Specifically, REEF computes and compares the centered kernel\nalignment similarity between the representations of a suspect model and a\nvictim model on the same samples. This training-free REEF does not impair the\nmodel's general capabilities and is robust to sequential fine-tuning, pruning,\nmodel merging, and permutations. In this way, REEF provides a simple and\neffective way for third parties and models' owners to protect LLMs'\nintellectual property together. The code is available at\nhttps://github.com/tmylla/REEF."
                },
                "authors": [
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03454v2",
                "updated": "2024-10-18T08:24:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    24,
                    49,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-05T16:57:57Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    16,
                    57,
                    57,
                    2,
                    157,
                    0
                ],
                "title": "Mission Design for Unmanned Aerial Vehicles using Hybrid Probabilistic\n  Logic Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mission Design for Unmanned Aerial Vehicles using Hybrid Probabilistic\n  Logic Programs"
                },
                "summary": "Advanced Air Mobility (AAM) is a growing field that demands a deep\nunderstanding of legal, spatial and temporal concepts in navigation. Hence, any\nimplementation of AAM is forced to deal with the inherent uncertainties of\nhuman-inhabited spaces. Enabling growth and innovation requires the creation of\na system for safe and robust mission design, i.e., the way we formalize\nintentions and decide their execution as trajectories for the Unmanned Aerial\nVehicle (UAV). Although legal frameworks have emerged to govern urban air\nspaces, their full integration into the decision process of autonomous agents\nand operators remains an open task. In this work we present ProMis, a system\narchitecture for probabilistic mission design. It links the data available from\nvarious static and dynamic data sources with legal text and operator\nrequirements by following principles of formal verification and probabilistic\nmodeling. Hereby, ProMis enables the combination of low-level perception and\nhigh-level rules in AAM to infer validity over the UAV's state-space. To this\nend, we employ Hybrid Probabilistic Logic Programs (HPLP) as a unifying,\nintermediate representation between perception and action-taking. Furthermore,\nwe present methods to connect ProMis with crowd-sourced map data by generating\nHPLP atoms that represent spatial relations in a probabilistic fashion. Our\nclaims of the utility and generality of ProMis are supported by experiments on\na diverse set of scenarios and a discussion of the computational demands\nassociated with probabilistic missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Air Mobility (AAM) is a growing field that demands a deep\nunderstanding of legal, spatial and temporal concepts in navigation. Hence, any\nimplementation of AAM is forced to deal with the inherent uncertainties of\nhuman-inhabited spaces. Enabling growth and innovation requires the creation of\na system for safe and robust mission design, i.e., the way we formalize\nintentions and decide their execution as trajectories for the Unmanned Aerial\nVehicle (UAV). Although legal frameworks have emerged to govern urban air\nspaces, their full integration into the decision process of autonomous agents\nand operators remains an open task. In this work we present ProMis, a system\narchitecture for probabilistic mission design. It links the data available from\nvarious static and dynamic data sources with legal text and operator\nrequirements by following principles of formal verification and probabilistic\nmodeling. Hereby, ProMis enables the combination of low-level perception and\nhigh-level rules in AAM to infer validity over the UAV's state-space. To this\nend, we employ Hybrid Probabilistic Logic Programs (HPLP) as a unifying,\nintermediate representation between perception and action-taking. Furthermore,\nwe present methods to connect ProMis with crowd-sourced map data by generating\nHPLP atoms that represent spatial relations in a probabilistic fashion. Our\nclaims of the utility and generality of ProMis are supported by experiments on\na diverse set of scenarios and a discussion of the computational demands\nassociated with probabilistic missions."
                },
                "authors": [
                    {
                        "name": "Simon Kohaut"
                    },
                    {
                        "name": "Benedict Flade"
                    },
                    {
                        "name": "Devendra Singh Dhami"
                    },
                    {
                        "name": "Julian Eggert"
                    },
                    {
                        "name": "Kristian Kersting"
                    }
                ],
                "author_detail": {
                    "name": "Kristian Kersting"
                },
                "author": "Kristian Kersting",
                "arxiv_doi": "10.1109/ITSC57777.2023.10422083",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ITSC57777.2023.10422083",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.03454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.14677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14677v1",
                "updated": "2024-10-18T17:59:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    57,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:59:57Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    57,
                    4,
                    292,
                    0
                ],
                "title": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are AI Detectors Good Enough? A Survey on Quality of Datasets With\n  Machine-Generated Texts"
                },
                "summary": "The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of autoregressive Large Language Models (LLMs) has\nsignificantly improved the quality of generated texts, necessitating reliable\nmachine-generated text detectors. A huge number of detectors and collections\nwith AI fragments have emerged, and several detection methods even showed\nrecognition quality up to 99.9% according to the target metrics in such\ncollections. However, the quality of such detectors tends to drop dramatically\nin the wild, posing a question: Are detectors actually highly trustworthy or do\ntheir high benchmark scores come from the poor quality of evaluation datasets?\nIn this paper, we emphasise the need for robust and qualitative methods for\nevaluating generated data to be secure against bias and low generalising\nability of future model. We present a systematic review of datasets from\ncompetitions dedicated to AI-generated content detection and propose methods\nfor evaluating the quality of datasets containing AI-generated fragments. In\naddition, we discuss the possibility of using high-quality generated data to\nachieve two goals: improving the training of detection models and improving the\ntraining datasets themselves. Our contribution aims to facilitate a better\nunderstanding of the dynamics between human and machine text, which will\nultimately support the integrity of information in an increasingly automated\nworld."
                },
                "authors": [
                    {
                        "name": "German Gritsai"
                    },
                    {
                        "name": "Anastasia Voznyuk"
                    },
                    {
                        "name": "Andrey Grabovoy"
                    },
                    {
                        "name": "Yury Chekhovich"
                    }
                ],
                "author_detail": {
                    "name": "Yury Chekhovich"
                },
                "author": "Yury Chekhovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14676v1",
                "updated": "2024-10-18T17:59:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    51,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:59:51Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    51,
                    4,
                    292,
                    0
                ],
                "title": "SudoLM: Learning Access Control of Parametric Knowledge with\n  Authorization Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SudoLM: Learning Access Control of Parametric Knowledge with\n  Authorization Alignment"
                },
                "summary": "Existing preference alignment is a one-size-fits-all alignment mechanism,\nwhere the part of the large language model (LLM) parametric knowledge with\nnon-preferred features is uniformly blocked to all the users. However, this\npart of knowledge can be useful to advanced users whose expertise qualifies\nthem to handle these information. The one-size-fits-all alignment mechanism\nundermines LLM's utility for these qualified users. To address this problem, we\npropose SudoLM, a framework that lets LLMs learn access control over specific\nparametric knowledge for users with different credentials via authorization\nalignment. SudoLM allows authorized users to unlock their access to all the\nparametric knowledge with an assigned SUDO key while blocking access to\nnon-qualified users. Experiments on two application scenarios demonstrate that\nSudoLM effectively controls the user's access to the parametric knowledge and\nmaintains its general utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing preference alignment is a one-size-fits-all alignment mechanism,\nwhere the part of the large language model (LLM) parametric knowledge with\nnon-preferred features is uniformly blocked to all the users. However, this\npart of knowledge can be useful to advanced users whose expertise qualifies\nthem to handle these information. The one-size-fits-all alignment mechanism\nundermines LLM's utility for these qualified users. To address this problem, we\npropose SudoLM, a framework that lets LLMs learn access control over specific\nparametric knowledge for users with different credentials via authorization\nalignment. SudoLM allows authorized users to unlock their access to all the\nparametric knowledge with an assigned SUDO key while blocking access to\nnon-qualified users. Experiments on two application scenarios demonstrate that\nSudoLM effectively controls the user's access to the parametric knowledge and\nmaintains its general utility."
                },
                "authors": [
                    {
                        "name": "Qin Liu"
                    },
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Muhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Muhao Chen"
                },
                "author": "Muhao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14675v1",
                "updated": "2024-10-18T17:59:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    47,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:59:47Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    59,
                    47,
                    4,
                    292,
                    0
                ],
                "title": "Enhancing Large Language Models' Situated Faithfulness to External\n  Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Language Models' Situated Faithfulness to External\n  Contexts"
                },
                "summary": "Large Language Models (LLMs) are often augmented with external information as\ncontexts, but this external information can sometimes be inaccurate or even\nintentionally misleading. We argue that robust LLMs should demonstrate situated\nfaithfulness, dynamically calibrating their trust in external information based\non their confidence in the internal knowledge and the external context. To\nbenchmark this capability, we evaluate LLMs across several QA datasets,\nincluding a newly created dataset called RedditQA featuring in-the-wild\nincorrect contexts sourced from Reddit posts. We show that when provided with\nboth correct and incorrect contexts, both open-source and proprietary models\ntend to overly rely on external information, regardless of its factual\naccuracy. To enhance situated faithfulness, we propose two approaches:\nSelf-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning\n(RCR). SCR enables models to self-access the confidence of external information\nrelative to their own internal knowledge to produce the most accurate answer.\nRCR, in contrast, extracts explicit confidence signals from the LLM and\ndetermines the final answer using predefined rules. Our results show that for\nLLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR\noutperforms RCR, achieving improvements of up to 24.2% over a direct input\naugmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR\noutperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct\nPreference Optimization (CR-DPO) method improves performance on both seen and\nunseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In\naddition to quantitative results, we offer insights into the relative strengths\nof SCR and RCR. Our findings highlight promising avenues for improving situated\nfaithfulness in LLMs. The data and code are released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are often augmented with external information as\ncontexts, but this external information can sometimes be inaccurate or even\nintentionally misleading. We argue that robust LLMs should demonstrate situated\nfaithfulness, dynamically calibrating their trust in external information based\non their confidence in the internal knowledge and the external context. To\nbenchmark this capability, we evaluate LLMs across several QA datasets,\nincluding a newly created dataset called RedditQA featuring in-the-wild\nincorrect contexts sourced from Reddit posts. We show that when provided with\nboth correct and incorrect contexts, both open-source and proprietary models\ntend to overly rely on external information, regardless of its factual\naccuracy. To enhance situated faithfulness, we propose two approaches:\nSelf-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning\n(RCR). SCR enables models to self-access the confidence of external information\nrelative to their own internal knowledge to produce the most accurate answer.\nRCR, in contrast, extracts explicit confidence signals from the LLM and\ndetermines the final answer using predefined rules. Our results show that for\nLLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR\noutperforms RCR, achieving improvements of up to 24.2% over a direct input\naugmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR\noutperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct\nPreference Optimization (CR-DPO) method improves performance on both seen and\nunseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In\naddition to quantitative results, we offer insights into the relative strengths\nof SCR and RCR. Our findings highlight promising avenues for improving situated\nfaithfulness in LLMs. The data and code are released."
                },
                "authors": [
                    {
                        "name": "Yukun Huang"
                    },
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Hongyi Cai"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    }
                ],
                "author_detail": {
                    "name": "Bhuwan Dhingra"
                },
                "author": "Bhuwan Dhingra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06331v2",
                "updated": "2024-10-18T17:53:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    53,
                    46,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-08T20:12:11Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    20,
                    12,
                    11,
                    1,
                    282,
                    0
                ],
                "title": "Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing"
                },
                "summary": "The locate-then-edit paradigm has shown significant promise for knowledge\nediting (KE) in Large Language Models (LLMs). While previous methods perform\nwell on single-hop fact recall tasks, they consistently struggle with multi-hop\nfactual recall tasks involving newly edited knowledge. In this paper,\nleveraging tools in mechanistic interpretability, we first identify that in\nmulti-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper\nMLP layers, unlike single-hop tasks, which rely on earlier layers. This\ndistinction explains the poor performance of current methods in multi-hop\nqueries, as they primarily focus on editing shallow layers, leaving deeper\nlayers unchanged. To address this, we propose IFMET, a novel locate-then-edit\nKE approach designed to edit both shallow and deep MLP layers. IFMET employs\nmulti-hop editing prompts and supplementary sets to locate and modify knowledge\nacross different reasoning stages. Experimental results demonstrate that IFMET\nsignificantly improves performance on multi-hop factual recall tasks,\neffectively overcoming the limitations of previous locate-then-edit methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The locate-then-edit paradigm has shown significant promise for knowledge\nediting (KE) in Large Language Models (LLMs). While previous methods perform\nwell on single-hop fact recall tasks, they consistently struggle with multi-hop\nfactual recall tasks involving newly edited knowledge. In this paper,\nleveraging tools in mechanistic interpretability, we first identify that in\nmulti-hop tasks, LLMs tend to retrieve implicit subject knowledge from deeper\nMLP layers, unlike single-hop tasks, which rely on earlier layers. This\ndistinction explains the poor performance of current methods in multi-hop\nqueries, as they primarily focus on editing shallow layers, leaving deeper\nlayers unchanged. To address this, we propose IFMET, a novel locate-then-edit\nKE approach designed to edit both shallow and deep MLP layers. IFMET employs\nmulti-hop editing prompts and supplementary sets to locate and modify knowledge\nacross different reasoning stages. Experimental results demonstrate that IFMET\nsignificantly improves performance on multi-hop factual recall tasks,\neffectively overcoming the limitations of previous locate-then-edit methods."
                },
                "authors": [
                    {
                        "name": "Zhuoran Zhang"
                    },
                    {
                        "name": "Yongxiang Li"
                    },
                    {
                        "name": "Zijian Kan"
                    },
                    {
                        "name": "Keyuan Cheng"
                    },
                    {
                        "name": "Lijie Hu"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14660v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14660v1",
                "updated": "2024-10-18T17:51:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    51,
                    51,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:51:51Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    51,
                    51,
                    4,
                    292,
                    0
                ],
                "title": "A Large Language Model-Driven Reward Design Framework via Dynamic\n  Feedback for Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Large Language Model-Driven Reward Design Framework via Dynamic\n  Feedback for Reinforcement Learning"
                },
                "summary": "Large Language Models (LLMs) have shown significant potential in designing\nreward functions for Reinforcement Learning (RL) tasks. However, obtaining\nhigh-quality reward code often involves human intervention, numerous LLM\nqueries, or repetitive RL training. To address these issues, we propose CARD, a\nLLM-driven Reward Design framework that iteratively generates and improves\nreward function code. Specifically, CARD includes a Coder that generates and\nverifies the code, while a Evaluator provides dynamic feedback to guide the\nCoder in improving the code, eliminating the need for human feedback. In\naddition to process feedback and trajectory feedback, we introduce Trajectory\nPreference Evaluation (TPE), which evaluates the current reward function based\non trajectory preferences. If the code fails the TPE, the Evaluator provides\npreference feedback, avoiding RL training at every iteration and making the\nreward function better aligned with the task objective. Empirical results on\nMeta-World and ManiSkill2 demonstrate that our method achieves an effective\nbalance between task performance and token efficiency, outperforming or\nmatching the baselines across all tasks. On 10 out of 12 tasks, CARD shows\nbetter or comparable performance to policies trained with expert-designed\nrewards, and our method even surpasses the oracle on 3 tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown significant potential in designing\nreward functions for Reinforcement Learning (RL) tasks. However, obtaining\nhigh-quality reward code often involves human intervention, numerous LLM\nqueries, or repetitive RL training. To address these issues, we propose CARD, a\nLLM-driven Reward Design framework that iteratively generates and improves\nreward function code. Specifically, CARD includes a Coder that generates and\nverifies the code, while a Evaluator provides dynamic feedback to guide the\nCoder in improving the code, eliminating the need for human feedback. In\naddition to process feedback and trajectory feedback, we introduce Trajectory\nPreference Evaluation (TPE), which evaluates the current reward function based\non trajectory preferences. If the code fails the TPE, the Evaluator provides\npreference feedback, avoiding RL training at every iteration and making the\nreward function better aligned with the task objective. Empirical results on\nMeta-World and ManiSkill2 demonstrate that our method achieves an effective\nbalance between task performance and token efficiency, outperforming or\nmatching the baselines across all tasks. On 10 out of 12 tasks, CARD shows\nbetter or comparable performance to policies trained with expert-designed\nrewards, and our method even surpasses the oracle on 3 tasks."
                },
                "authors": [
                    {
                        "name": "Shengjie Sun"
                    },
                    {
                        "name": "Runze Liu"
                    },
                    {
                        "name": "Jiafei Lyu"
                    },
                    {
                        "name": "Jing-Wen Yang"
                    },
                    {
                        "name": "Liangpeng Zhang"
                    },
                    {
                        "name": "Xiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiu Li"
                },
                "author": "Xiu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14660v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14660v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14655v1",
                "updated": "2024-10-18T17:48:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    48,
                    27,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:48:27Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    48,
                    27,
                    4,
                    292,
                    0
                ],
                "title": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated\n  Tokens"
                },
                "summary": "Language models are often trained to maximize the likelihood of the next\ntoken given past tokens in the training dataset. However, during inference\ntime, they are utilized differently, generating text sequentially and\nauto-regressively by using previously generated tokens as input to predict the\nnext one. Marginal differences in predictions at each step can cascade over\nsuccessive steps, resulting in different distributions from what the models\nwere trained for and potentially leading to unpredictable behavior. This paper\nproposes two simple approaches based on model own generation to address this\ndiscrepancy between the training and inference time. Our first approach is\nBatch-Scheduled Sampling, where, during training, we stochastically choose\nbetween the ground-truth token from the dataset and the model's own generated\ntoken as input to predict the next token. This is done in an offline manner,\nmodifying the context window by interleaving ground-truth tokens with those\ngenerated by the model. Our second approach is Reference-Answer-based\nCorrection, where we explicitly incorporate a self-correction capability into\nthe model during training. This enables the model to effectively self-correct\nthe gaps between the generated sequences and the ground truth data without\nrelying on an external oracle model. By incorporating our proposed strategies\nduring training, we have observed an overall improvement in performance\ncompared to baseline methods, as demonstrated by our extensive experiments\nusing summarization, general question-answering, and math question-answering\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models are often trained to maximize the likelihood of the next\ntoken given past tokens in the training dataset. However, during inference\ntime, they are utilized differently, generating text sequentially and\nauto-regressively by using previously generated tokens as input to predict the\nnext one. Marginal differences in predictions at each step can cascade over\nsuccessive steps, resulting in different distributions from what the models\nwere trained for and potentially leading to unpredictable behavior. This paper\nproposes two simple approaches based on model own generation to address this\ndiscrepancy between the training and inference time. Our first approach is\nBatch-Scheduled Sampling, where, during training, we stochastically choose\nbetween the ground-truth token from the dataset and the model's own generated\ntoken as input to predict the next token. This is done in an offline manner,\nmodifying the context window by interleaving ground-truth tokens with those\ngenerated by the model. Our second approach is Reference-Answer-based\nCorrection, where we explicitly incorporate a self-correction capability into\nthe model during training. This enables the model to effectively self-correct\nthe gaps between the generated sequences and the ground truth data without\nrelying on an external oracle model. By incorporating our proposed strategies\nduring training, we have observed an overall improvement in performance\ncompared to baseline methods, as demonstrated by our extensive experiments\nusing summarization, general question-answering, and math question-answering\ntasks."
                },
                "authors": [
                    {
                        "name": "Zhepeng Cen"
                    },
                    {
                        "name": "Yao Liu"
                    },
                    {
                        "name": "Siliang Zeng"
                    },
                    {
                        "name": "Pratik Chaudhar"
                    },
                    {
                        "name": "Huzefa Rangwala"
                    },
                    {
                        "name": "George Karypis"
                    },
                    {
                        "name": "Rasool Fakoor"
                    }
                ],
                "author_detail": {
                    "name": "Rasool Fakoor"
                },
                "author": "Rasool Fakoor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14651v1",
                "updated": "2024-10-18T17:47:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    47,
                    11,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:47:11Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    47,
                    11,
                    4,
                    292,
                    0
                ],
                "title": "Real-time Fake News from Adversarial Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Fake News from Adversarial Feedback"
                },
                "summary": "We show that existing evaluations for fake news detection based on\nconventional sources, such as claims on fact-checking websites, result in an\nincreasing accuracy over time for LLM-based detectors -- even after their\nknowledge cutoffs. This suggests that recent popular political claims, which\nform the majority of fake news on such sources, are easily classified using\nsurface-level shallow patterns. Instead, we argue that a proper fake news\ndetection dataset should test a model's ability to reason factually about the\ncurrent world by retrieving and reading related evidence. To this end, we\ndevelop a novel pipeline that leverages natural language feedback from a\nRAG-based detector to iteratively modify real-time news into deceptive fake\nnews that challenges LLMs. Our iterative rewrite decreases the binary\nclassification AUC by an absolute 17.5 percent for a strong RAG GPT-4o\ndetector. Our experiments reveal the important role of RAG in both detecting\nand generating fake news, as retrieval-free LLM detectors are vulnerable to\nunseen events and adversarial attacks, while feedback from RAG detection helps\ndiscover more deceitful patterns in fake news.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that existing evaluations for fake news detection based on\nconventional sources, such as claims on fact-checking websites, result in an\nincreasing accuracy over time for LLM-based detectors -- even after their\nknowledge cutoffs. This suggests that recent popular political claims, which\nform the majority of fake news on such sources, are easily classified using\nsurface-level shallow patterns. Instead, we argue that a proper fake news\ndetection dataset should test a model's ability to reason factually about the\ncurrent world by retrieving and reading related evidence. To this end, we\ndevelop a novel pipeline that leverages natural language feedback from a\nRAG-based detector to iteratively modify real-time news into deceptive fake\nnews that challenges LLMs. Our iterative rewrite decreases the binary\nclassification AUC by an absolute 17.5 percent for a strong RAG GPT-4o\ndetector. Our experiments reveal the important role of RAG in both detecting\nand generating fake news, as retrieval-free LLM detectors are vulnerable to\nunseen events and adversarial attacks, while feedback from RAG detection helps\ndiscover more deceitful patterns in fake news."
                },
                "authors": [
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Yukun Huang"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    }
                ],
                "author_detail": {
                    "name": "Bhuwan Dhingra"
                },
                "author": "Bhuwan Dhingra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14649v1",
                "updated": "2024-10-18T17:46:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    46,
                    37,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:46:37Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    46,
                    37,
                    4,
                    292,
                    0
                ],
                "title": "EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary\n  Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary\n  Search"
                },
                "summary": "The high computational costs of large language models (LLMs) have led to a\nflurry of research on LLM compression, via methods such as quantization,\nsparsification, or structured pruning. A new frontier in this area is given by\n\\emph{dynamic, non-uniform} compression methods, which adjust the compression\nlevels (e.g., sparsity) per-block or even per-layer in order to minimize\naccuracy loss, while guaranteeing a global compression threshold. Yet, current\nmethods rely on heuristics for identifying the \"importance\" of a given layer\ntowards the loss, based on assumptions such as \\emph{error monotonicity}, i.e.\nthat the end-to-end model compression error is proportional to the sum of\nlayer-wise errors. In this paper, we revisit this area, and propose a new and\ngeneral approach for dynamic compression that is provably optimal in a given\ninput range. We begin from the motivating observation that, in general,\n\\emph{error monotonicity does not hold for LLMs}: compressed models with lower\nsum of per-layer errors can perform \\emph{worse} than models with higher error\nsums. To address this, we propose a new general evolutionary framework for\ndynamic LLM compression called EvoPress, which has provable convergence, and\nlow sample and evaluation complexity. We show that these theoretical guarantees\nlead to highly competitive practical performance for dynamic compression of\nLlama, Mistral and Phi models. Via EvoPress, we set new state-of-the-art\nresults across all compression approaches: structural pruning (block/layer\ndropping), unstructured sparsity, as well as quantization with dynamic\nbitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The high computational costs of large language models (LLMs) have led to a\nflurry of research on LLM compression, via methods such as quantization,\nsparsification, or structured pruning. A new frontier in this area is given by\n\\emph{dynamic, non-uniform} compression methods, which adjust the compression\nlevels (e.g., sparsity) per-block or even per-layer in order to minimize\naccuracy loss, while guaranteeing a global compression threshold. Yet, current\nmethods rely on heuristics for identifying the \"importance\" of a given layer\ntowards the loss, based on assumptions such as \\emph{error monotonicity}, i.e.\nthat the end-to-end model compression error is proportional to the sum of\nlayer-wise errors. In this paper, we revisit this area, and propose a new and\ngeneral approach for dynamic compression that is provably optimal in a given\ninput range. We begin from the motivating observation that, in general,\n\\emph{error monotonicity does not hold for LLMs}: compressed models with lower\nsum of per-layer errors can perform \\emph{worse} than models with higher error\nsums. To address this, we propose a new general evolutionary framework for\ndynamic LLM compression called EvoPress, which has provable convergence, and\nlow sample and evaluation complexity. We show that these theoretical guarantees\nlead to highly competitive practical performance for dynamic compression of\nLlama, Mistral and Phi models. Via EvoPress, we set new state-of-the-art\nresults across all compression approaches: structural pruning (block/layer\ndropping), unstructured sparsity, as well as quantization with dynamic\nbitwidths. Our code is available at https://github.com/IST-DASLab/EvoPress."
                },
                "authors": [
                    {
                        "name": "Oliver Sieberling"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Eldar Kurtic"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14641v1",
                "updated": "2024-10-18T17:41:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    41,
                    19,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:41:19Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    41,
                    19,
                    4,
                    292,
                    0
                ],
                "title": "Distance between Relevant Information Pieces Causes Bias in Long-Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distance between Relevant Information Pieces Causes Bias in Long-Context\n  LLMs"
                },
                "summary": "Positional bias in large language models (LLMs) hinders their ability to\neffectively process long inputs. A prominent example is the \"lost in the\nmiddle\" phenomenon, where LLMs struggle to utilize relevant information\nsituated in the middle of the input. While prior research primarily focuses on\nsingle pieces of relevant information, real-world applications often involve\nmultiple relevant information pieces. To bridge this gap, we present\nLongPiBench, a benchmark designed to assess positional bias involving multiple\npieces of relevant information. Thorough experiments are conducted with five\ncommercial and six open-source models. These experiments reveal that while most\ncurrent models are robust against the \"lost in the middle\" issue, there exist\nsignificant biases related to the spacing of relevant information pieces. These\nfindings highlight the importance of evaluating and reducing positional biases\nto advance LLM's capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positional bias in large language models (LLMs) hinders their ability to\neffectively process long inputs. A prominent example is the \"lost in the\nmiddle\" phenomenon, where LLMs struggle to utilize relevant information\nsituated in the middle of the input. While prior research primarily focuses on\nsingle pieces of relevant information, real-world applications often involve\nmultiple relevant information pieces. To bridge this gap, we present\nLongPiBench, a benchmark designed to assess positional bias involving multiple\npieces of relevant information. Thorough experiments are conducted with five\ncommercial and six open-source models. These experiments reveal that while most\ncurrent models are robust against the \"lost in the middle\" issue, there exist\nsignificant biases related to the spacing of relevant information pieces. These\nfindings highlight the importance of evaluating and reducing positional biases\nto advance LLM's capabilities."
                },
                "authors": [
                    {
                        "name": "Runchu Tian"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Yuepeng Fu"
                    },
                    {
                        "name": "Siyang Deng"
                    },
                    {
                        "name": "Qinyu Luo"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Xin Cong"
                    },
                    {
                        "name": "Zhong Zhang"
                    },
                    {
                        "name": "Yesai Wu"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Huadong Wang"
                    },
                    {
                        "name": "Xiaojiang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojiang Liu"
                },
                "author": "Xiaojiang Liu",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14635v1",
                "updated": "2024-10-18T17:36:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    36,
                    53,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:36:53Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    36,
                    53,
                    4,
                    292,
                    0
                ],
                "title": "GenEOL: Harnessing the Generative Power of LLMs for Training-Free\n  Sentence Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenEOL: Harnessing the Generative Power of LLMs for Training-Free\n  Sentence Embeddings"
                },
                "summary": "Training-free embedding methods directly leverage pretrained large language\nmodels (LLMs) to embed text, bypassing the costly and complex procedure of\ncontrastive learning. Previous training-free embedding methods have mainly\nfocused on optimizing embedding prompts and have overlooked the benefits of\nutilizing the generative abilities of LLMs. We propose a novel method, GenEOL,\nwhich uses LLMs to generate diverse transformations of a sentence that preserve\nits meaning, and aggregates the resulting embeddings of these transformations\nto enhance the overall sentence embedding. GenEOL significantly outperforms the\nexisting training-free embedding methods by an average of 2.85 points across\nseveral LLMs on the sentence semantic text similarity (STS) benchmark. Our\nanalysis shows that GenEOL stabilizes representation quality across LLM layers\nand is robust to perturbations of embedding prompts. GenEOL also achieves\nnotable gains on multiple clustering, reranking and pair-classification tasks\nfrom the MTEB benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free embedding methods directly leverage pretrained large language\nmodels (LLMs) to embed text, bypassing the costly and complex procedure of\ncontrastive learning. Previous training-free embedding methods have mainly\nfocused on optimizing embedding prompts and have overlooked the benefits of\nutilizing the generative abilities of LLMs. We propose a novel method, GenEOL,\nwhich uses LLMs to generate diverse transformations of a sentence that preserve\nits meaning, and aggregates the resulting embeddings of these transformations\nto enhance the overall sentence embedding. GenEOL significantly outperforms the\nexisting training-free embedding methods by an average of 2.85 points across\nseveral LLMs on the sentence semantic text similarity (STS) benchmark. Our\nanalysis shows that GenEOL stabilizes representation quality across LLM layers\nand is robust to perturbations of embedding prompts. GenEOL also achieves\nnotable gains on multiple clustering, reranking and pair-classification tasks\nfrom the MTEB benchmark."
                },
                "authors": [
                    {
                        "name": "Raghuveer Thirukovalluru"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    }
                ],
                "author_detail": {
                    "name": "Bhuwan Dhingra"
                },
                "author": "Bhuwan Dhingra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2109.09889v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2109.09889v3",
                "updated": "2024-10-18T17:32:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    32,
                    27,
                    4,
                    292,
                    0
                ],
                "published": "2021-09-21T00:09:03Z",
                "published_parsed": [
                    2021,
                    9,
                    21,
                    0,
                    9,
                    3,
                    1,
                    264,
                    0
                ],
                "title": "A Distance-based Anomaly Detection Framework for Deep Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distance-based Anomaly Detection Framework for Deep Reinforcement\n  Learning"
                },
                "summary": "In deep reinforcement learning (RL) systems, abnormal states pose significant\nrisks by potentially triggering unpredictable behaviors and unsafe actions,\nthus impeding the deployment of RL systems in real-world scenarios. It is\ncrucial for reliable decision-making systems to have the capability to cast an\nalert whenever they encounter unfamiliar observations that they are not\nequipped to handle. In this paper, we propose a novel Mahalanobis\ndistance-based (MD) anomaly detection framework, called \\textit{MDX}, for deep\nRL algorithms. MDX simultaneously addresses random, adversarial, and\nout-of-distribution (OOD) state outliers in both offline and online settings.\nIt utilizes Mahalanobis distance within class-conditional distributions for\neach action and operates within a statistical hypothesis testing framework\nunder the Gaussian assumption. We further extend it to robust and\ndistribution-free versions by incorporating Robust MD and conformal inference\ntechniques. Through extensive experiments on classical control environments,\nAtari games, and autonomous driving scenarios, we demonstrate the effectiveness\nof our MD-based detection framework. MDX offers a simple, unified, and\npractical anomaly detection tool for enhancing the safety and reliability of RL\nsystems in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In deep reinforcement learning (RL) systems, abnormal states pose significant\nrisks by potentially triggering unpredictable behaviors and unsafe actions,\nthus impeding the deployment of RL systems in real-world scenarios. It is\ncrucial for reliable decision-making systems to have the capability to cast an\nalert whenever they encounter unfamiliar observations that they are not\nequipped to handle. In this paper, we propose a novel Mahalanobis\ndistance-based (MD) anomaly detection framework, called \\textit{MDX}, for deep\nRL algorithms. MDX simultaneously addresses random, adversarial, and\nout-of-distribution (OOD) state outliers in both offline and online settings.\nIt utilizes Mahalanobis distance within class-conditional distributions for\neach action and operates within a statistical hypothesis testing framework\nunder the Gaussian assumption. We further extend it to robust and\ndistribution-free versions by incorporating Robust MD and conformal inference\ntechniques. Through extensive experiments on classical control environments,\nAtari games, and autonomous driving scenarios, we demonstrate the effectiveness\nof our MD-based detection framework. MDX offers a simple, unified, and\npractical anomaly detection tool for enhancing the safety and reliability of RL\nsystems in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Ke Sun"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Linglong Kong"
                    },
                    {
                        "name": "Martin Mller"
                    }
                ],
                "author_detail": {
                    "name": "Martin Mller"
                },
                "author": "Martin Mller",
                "arxiv_comment": "19 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2109.09889v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2109.09889v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14632v1",
                "updated": "2024-10-18T17:32:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    32,
                    22,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:32:22Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    32,
                    22,
                    4,
                    292,
                    0
                ],
                "title": "Diverging Preferences: When do Annotators Disagree and do Models Know?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverging Preferences: When do Annotators Disagree and do Models Know?"
                },
                "summary": "We examine diverging preferences in human-labeled preference datasets. We\ndevelop a taxonomy of disagreement sources spanning 10 categories across four\nhigh-level classes -- task underspecification, response style, refusals, and\nannotation errors. We find that the majority of disagreements are in opposition\nwith standard reward modeling approaches, which are designed with the\nassumption that annotator disagreement is noise. We then explore how these\nfindings impact two areas of LLM development: reward modeling and evaluation.\nIn our experiments, we demonstrate how standard reward modeling methods, like\nthe Bradley-Terry model, fail to differentiate whether a given preference\njudgment is the result of unanimous agreement among annotators or the majority\nopinion among diverging user preferences. We also find that these tendencies\nare also echoed by popular LLM-as-Judge evaluation methods, which consistently\nidentify a winning response in cases of diverging preferences. These findings\nhighlight remaining challenges in LLM evaluations, which are greatly influenced\nby divisive features like response style, and in developing pluralistically\naligned LLMs. To address these issues, we develop methods for identifying\ndiverging preferences to mitigate their influence on evaluation and training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine diverging preferences in human-labeled preference datasets. We\ndevelop a taxonomy of disagreement sources spanning 10 categories across four\nhigh-level classes -- task underspecification, response style, refusals, and\nannotation errors. We find that the majority of disagreements are in opposition\nwith standard reward modeling approaches, which are designed with the\nassumption that annotator disagreement is noise. We then explore how these\nfindings impact two areas of LLM development: reward modeling and evaluation.\nIn our experiments, we demonstrate how standard reward modeling methods, like\nthe Bradley-Terry model, fail to differentiate whether a given preference\njudgment is the result of unanimous agreement among annotators or the majority\nopinion among diverging user preferences. We also find that these tendencies\nare also echoed by popular LLM-as-Judge evaluation methods, which consistently\nidentify a winning response in cases of diverging preferences. These findings\nhighlight remaining challenges in LLM evaluations, which are greatly influenced\nby divisive features like response style, and in developing pluralistically\naligned LLMs. To address these issues, we develop methods for identifying\ndiverging preferences to mitigate their influence on evaluation and training."
                },
                "authors": [
                    {
                        "name": "Michael JQ Zhang"
                    },
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Jena D. Hwang"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Eunsol Choi"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Valentina Pyatkin"
                    }
                ],
                "author_detail": {
                    "name": "Valentina Pyatkin"
                },
                "author": "Valentina Pyatkin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10989v2",
                "updated": "2024-10-18T17:21:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    21,
                    17,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-14T18:17:01Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    18,
                    17,
                    1,
                    0,
                    288,
                    0
                ],
                "title": "Liger Kernel: Efficient Triton Kernels for LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Liger Kernel: Efficient Triton Kernels for LLM Training"
                },
                "summary": "Training Large Language Models (LLMs) efficiently at scale presents a\nformidable challenge, driven by their ever-increasing computational demands and\nthe need for enhanced performance. In this work, we introduce Liger-Kernel, an\nopen-sourced set of Triton kernels developed specifically for LLM training.\nWith kernel optimization techniques like kernel operation fusing and input\nchunking, our kernels achieve on average a 20% increase in training throughput\nand a 60% reduction in GPU memory usage for popular LLMs compared to\nHuggingFace implementations. In addition, Liger-Kernel is designed with\nmodularity, accessibility, and adaptability in mind, catering to both casual\nand expert users. Comprehensive benchmarks and integration tests are built in\nto ensure compatibility, performance, correctness, and convergence across\ndiverse computing environments and model architectures.\n  The source code is available under a permissive license at:\ngithub.com/linkedin/Liger-Kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models (LLMs) efficiently at scale presents a\nformidable challenge, driven by their ever-increasing computational demands and\nthe need for enhanced performance. In this work, we introduce Liger-Kernel, an\nopen-sourced set of Triton kernels developed specifically for LLM training.\nWith kernel optimization techniques like kernel operation fusing and input\nchunking, our kernels achieve on average a 20% increase in training throughput\nand a 60% reduction in GPU memory usage for popular LLMs compared to\nHuggingFace implementations. In addition, Liger-Kernel is designed with\nmodularity, accessibility, and adaptability in mind, catering to both casual\nand expert users. Comprehensive benchmarks and integration tests are built in\nto ensure compatibility, performance, correctness, and convergence across\ndiverse computing environments and model architectures.\n  The source code is available under a permissive license at:\ngithub.com/linkedin/Liger-Kernel."
                },
                "authors": [
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Yun Dai"
                    },
                    {
                        "name": "Vignesh Kothapalli"
                    },
                    {
                        "name": "Qingquan Song"
                    },
                    {
                        "name": "Shao Tang"
                    },
                    {
                        "name": "Siyu Zhu"
                    },
                    {
                        "name": "Steven Shimizu"
                    },
                    {
                        "name": "Shivam Sahni"
                    },
                    {
                        "name": "Haowen Ning"
                    },
                    {
                        "name": "Yanning Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yanning Chen"
                },
                "author": "Yanning Chen",
                "arxiv_comment": "17 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14620v1",
                "updated": "2024-10-18T17:20:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    20,
                    24,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:20:24Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    20,
                    24,
                    4,
                    292,
                    0
                ],
                "title": "Site-Specific Outdoor Propagation Assessment and Ray-Tracing Analysis\n  for Wireless Digital Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Site-Specific Outdoor Propagation Assessment and Ray-Tracing Analysis\n  for Wireless Digital Twins"
                },
                "summary": "Digital twinning is becoming increasingly vital in the design and real-time\ncontrol of future wireless networks by providing precise cost-effective\nsimulations, predictive insights, and real-time data integration. This paper\nexplores the application of digital twinning in optimizing wireless\ncommunication systems within urban environments, where building arrangements\ncan critically impact network performances. We develop a digital twin platform\nto simulate and analyze how factors such as building positioning, base station\nplacement, and antenna design influence wireless propagation. The ray-tracing\nsoftware package of Matlab is compared with Remcom Wireless InSite. Using a\nrealistic radiation pattern of a base transceiver station (BTS) antenna, ray\ntracing simulations for signal propagation and interactions in urban landscapes\nare then extensively examined. By analyzing radio heat maps alongside antenna\npatterns, we gain valuable insights into optimizing wireless deployment\nstrategies. This study highlights the potential of digital twinning as a\ncritical tool for urban planners and network engineers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital twinning is becoming increasingly vital in the design and real-time\ncontrol of future wireless networks by providing precise cost-effective\nsimulations, predictive insights, and real-time data integration. This paper\nexplores the application of digital twinning in optimizing wireless\ncommunication systems within urban environments, where building arrangements\ncan critically impact network performances. We develop a digital twin platform\nto simulate and analyze how factors such as building positioning, base station\nplacement, and antenna design influence wireless propagation. The ray-tracing\nsoftware package of Matlab is compared with Remcom Wireless InSite. Using a\nrealistic radiation pattern of a base transceiver station (BTS) antenna, ray\ntracing simulations for signal propagation and interactions in urban landscapes\nare then extensively examined. By analyzing radio heat maps alongside antenna\npatterns, we gain valuable insights into optimizing wireless deployment\nstrategies. This study highlights the potential of digital twinning as a\ncritical tool for urban planners and network engineers."
                },
                "authors": [
                    {
                        "name": "Morteza Ghaderi Aram"
                    },
                    {
                        "name": "Hao Guo"
                    },
                    {
                        "name": "Mingsheng Yin"
                    },
                    {
                        "name": "Tommy Svensson"
                    }
                ],
                "author_detail": {
                    "name": "Tommy Svensson"
                },
                "author": "Tommy Svensson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14619v1",
                "updated": "2024-10-18T17:17:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    17,
                    4,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:17:04Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    17,
                    4,
                    4,
                    292,
                    0
                ],
                "title": "IoT-Based Water Quality Monitoring System in Philippine Off-Grid\n  Communities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IoT-Based Water Quality Monitoring System in Philippine Off-Grid\n  Communities"
                },
                "summary": "Contaminated and polluted water poses significant threats to human health,\nnecessitating vigilant monitoring of water sources for potential contamination.\nThis paper introduces a low-cost Internet of Things (IoT)-based water quality\nmonitoring system designed to address water quality challenges in rural\ncommunities, as demonstrated through a case study conducted in the Philippines.\nThe system consists of two core components. The hardware component of the\nsystem, built on Arduino technology and featuring real-time data transmission,\nfocuses on monitoring pH levels, turbidity, and temperature via sensors. The\nsystem is equipped to transmit data to a cloud database and send informative\nmessages to mobile numbers, updating users on the status of water supplies. The\napplication component acts as a user interface for accessing and managing data\ncollected by the sensors. The successful deployment of this Water Quality\nMonitoring (WQM) system not only helps community leaders and health workers\nmonitor water sources but also underscores its potential to empower communities\nin safeguarding their water sources, thereby contributing to the advancement of\nclean and safe water access.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contaminated and polluted water poses significant threats to human health,\nnecessitating vigilant monitoring of water sources for potential contamination.\nThis paper introduces a low-cost Internet of Things (IoT)-based water quality\nmonitoring system designed to address water quality challenges in rural\ncommunities, as demonstrated through a case study conducted in the Philippines.\nThe system consists of two core components. The hardware component of the\nsystem, built on Arduino technology and featuring real-time data transmission,\nfocuses on monitoring pH levels, turbidity, and temperature via sensors. The\nsystem is equipped to transmit data to a cloud database and send informative\nmessages to mobile numbers, updating users on the status of water supplies. The\napplication component acts as a user interface for accessing and managing data\ncollected by the sensors. The successful deployment of this Water Quality\nMonitoring (WQM) system not only helps community leaders and health workers\nmonitor water sources but also underscores its potential to empower communities\nin safeguarding their water sources, thereby contributing to the advancement of\nclean and safe water access."
                },
                "authors": [
                    {
                        "name": "Jenny Vi Abrajano"
                    },
                    {
                        "name": "Khavee Agustus Botangen"
                    },
                    {
                        "name": "Jovith Nabua"
                    },
                    {
                        "name": "Jenalyn Apanay"
                    },
                    {
                        "name": "Chezalea Fay Pea"
                    }
                ],
                "author_detail": {
                    "name": "Chezalea Fay Pea"
                },
                "author": "Chezalea Fay Pea",
                "arxiv_comment": "Proceedings of the 2024 9th International Conference on Business and\n  Industrial Research, May 2024, Bangkok, Thailand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.3; H.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06402v2",
                "updated": "2024-10-18T17:10:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    10,
                    5,
                    4,
                    292,
                    0
                ],
                "published": "2024-03-11T03:28:13Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    3,
                    28,
                    13,
                    0,
                    71,
                    0
                ],
                "title": "One size doesn't fit all: Predicting the Number of Examples for\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One size doesn't fit all: Predicting the Number of Examples for\n  In-Context Learning"
                },
                "summary": "In-context learning (ICL) refers to the process of adding a small number of\nlocalized examples (ones that are semantically similar to the input) from a\ntraining set of labelled data to an LLM's prompt with an objective to\neffectively control the generative process seeking to improve the downstream\ntask performance. Existing ICL approaches use an identical number of examples\n(a pre-configured hyper-parameter) for each data instance. Our work alleviates\nthe limitations of this 'one fits all' approach by dynamically predicting the\nnumber of examples for each data instance to be used in few-shot inference with\nLLMs. In particular, we employ a multi-label classifier, the parameters of\nwhich are fitted using a training set, where the label for each instance in the\ntraining set indicates if using a specific value of k (number of most similar\nexamples from 0 up to a maximum value) leads to correct k-shot downstream\npredictions. Our experiments on a number of text classification benchmarks show\nthat AICL substantially outperforms standard ICL by up to 17%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) refers to the process of adding a small number of\nlocalized examples (ones that are semantically similar to the input) from a\ntraining set of labelled data to an LLM's prompt with an objective to\neffectively control the generative process seeking to improve the downstream\ntask performance. Existing ICL approaches use an identical number of examples\n(a pre-configured hyper-parameter) for each data instance. Our work alleviates\nthe limitations of this 'one fits all' approach by dynamically predicting the\nnumber of examples for each data instance to be used in few-shot inference with\nLLMs. In particular, we employ a multi-label classifier, the parameters of\nwhich are fitted using a training set, where the label for each instance in the\ntraining set indicates if using a specific value of k (number of most similar\nexamples from 0 up to a maximum value) leads to correct k-shot downstream\npredictions. Our experiments on a number of text classification benchmarks show\nthat AICL substantially outperforms standard ICL by up to 17%."
                },
                "authors": [
                    {
                        "name": "Manish Chandra"
                    },
                    {
                        "name": "Debasis Ganguly"
                    },
                    {
                        "name": "Iadh Ounis"
                    }
                ],
                "author_detail": {
                    "name": "Iadh Ounis"
                },
                "author": "Iadh Ounis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14609v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14609v1",
                "updated": "2024-10-18T17:03:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    3,
                    17,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T17:03:17Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    17,
                    3,
                    17,
                    4,
                    292,
                    0
                ],
                "title": "DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual\n  Distillation in Conversational Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiSCo Meets LLMs: A Unified Approach for Sparse Retrieval and Contextual\n  Distillation in Conversational Search"
                },
                "summary": "Conversational Search (CS) is the task of retrieving relevant documents from\na corpus within a conversational context, combining retrieval with\nconversational context modeling. With the explosion of Large Language Models\n(LLMs), the CS field has seen major improvements with LLMs rewriting user\nqueries, accounting for conversational context. However, engaging LLMs at\ninference time harms efficiency. Current methods address this by distilling\nembeddings from human-rewritten queries to learn the context modeling task.\nYet, these approaches predominantly focus on context modeling, and only treat\nthe contrastive component of the retrieval task within a\ndistillation-independent loss term. To address these limitations, we propose a\nnew distillation method, as a relaxation of the previous objective, unifying\nretrieval and context modeling. We relax the existing training objectives by\ndistilling similarity scores between conversations and documents, rather than\nrelying solely on representation learning. Our proposed distillation objective\nallows for more freedom in the representation space and leverages the\ncontrastive nature of document relevance. Through experiments on Learned Sparse\nRetrieval (LSR) across 5 CS datasets, our approach demonstrates substantial\nimprovements in both in-domain and out-of-domain retrieval performance,\noutperforming state-of-the-art with gains of up to 6 points in recall for\nout-of-domain datasets. Additionally, through the relaxation of the objective,\nwe propose a multi-teacher distillation, using multiple LLMs as teachers,\nyielding additional gains, and outperforming the teachers themselves in\nin-domain experiments. Finally, analysis of the sparsity of the models reveals\nthat our distillation allows for better control over the sparsity of the\ntrained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Search (CS) is the task of retrieving relevant documents from\na corpus within a conversational context, combining retrieval with\nconversational context modeling. With the explosion of Large Language Models\n(LLMs), the CS field has seen major improvements with LLMs rewriting user\nqueries, accounting for conversational context. However, engaging LLMs at\ninference time harms efficiency. Current methods address this by distilling\nembeddings from human-rewritten queries to learn the context modeling task.\nYet, these approaches predominantly focus on context modeling, and only treat\nthe contrastive component of the retrieval task within a\ndistillation-independent loss term. To address these limitations, we propose a\nnew distillation method, as a relaxation of the previous objective, unifying\nretrieval and context modeling. We relax the existing training objectives by\ndistilling similarity scores between conversations and documents, rather than\nrelying solely on representation learning. Our proposed distillation objective\nallows for more freedom in the representation space and leverages the\ncontrastive nature of document relevance. Through experiments on Learned Sparse\nRetrieval (LSR) across 5 CS datasets, our approach demonstrates substantial\nimprovements in both in-domain and out-of-domain retrieval performance,\noutperforming state-of-the-art with gains of up to 6 points in recall for\nout-of-domain datasets. Additionally, through the relaxation of the objective,\nwe propose a multi-teacher distillation, using multiple LLMs as teachers,\nyielding additional gains, and outperforming the teachers themselves in\nin-domain experiments. Finally, analysis of the sparsity of the models reveals\nthat our distillation allows for better control over the sparsity of the\ntrained models."
                },
                "authors": [
                    {
                        "name": "Simon Lupart"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Kanoulas"
                },
                "author": "Evangelos Kanoulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14609v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14609v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14596v1",
                "updated": "2024-10-18T16:49:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    49,
                    36,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:49:36Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    49,
                    36,
                    4,
                    292,
                    0
                ],
                "title": "Teaching Models to Balance Resisting and Accepting Persuasion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Models to Balance Resisting and Accepting Persuasion"
                },
                "summary": "Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Balanced Training (or PBT),\nwhich leverages multi-agent recursive dialogue trees to create data and trains\nmodels via preference optimization to accept persuasion when appropriate. PBT\nconsistently improves resistance to misinformation and resilience to being\nchallenged while also resulting in the best overall performance on holistic\ndata containing both positive and negative persuasion. Crucially, we show that\nPBT models are better teammates in multi-agent debates. We find that without\nPBT, pairs of stronger and weaker models have unstable performance, with the\norder in which the models present their answers determining whether the team\nobtains the stronger or weaker model's performance. PBT leads to better and\nmore stable results and less order dependence, with the stronger model\nconsistently pulling the weaker one up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to persuasion, which can pose\nrisks when models are faced with an adversarial interlocutor. We take a first\nstep towards defending models against persuasion while also arguing that\ndefense against adversarial (i.e. negative) persuasion is only half of the\nequation: models should also be able to accept beneficial (i.e. positive)\npersuasion to improve their answers. We show that optimizing models for only\none side results in poor performance on the other. In order to balance positive\nand negative persuasion, we introduce Persuasion-Balanced Training (or PBT),\nwhich leverages multi-agent recursive dialogue trees to create data and trains\nmodels via preference optimization to accept persuasion when appropriate. PBT\nconsistently improves resistance to misinformation and resilience to being\nchallenged while also resulting in the best overall performance on holistic\ndata containing both positive and negative persuasion. Crucially, we show that\nPBT models are better teammates in multi-agent debates. We find that without\nPBT, pairs of stronger and weaker models have unstable performance, with the\norder in which the models present their answers determining whether the team\nobtains the stronger or weaker model's performance. PBT leads to better and\nmore stable results and less order dependence, with the stronger model\nconsistently pulling the weaker one up."
                },
                "authors": [
                    {
                        "name": "Elias Stengel-Eskin"
                    },
                    {
                        "name": "Peter Hase"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Code: https://github.com/esteng/persuasion_balanced_training",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14594v1",
                "updated": "2024-10-18T16:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    44,
                    22,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:44:22Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    44,
                    22,
                    4,
                    292,
                    0
                ],
                "title": "Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and\n  Tool Knowledge Bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and\n  Tool Knowledge Bases"
                },
                "summary": "Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks\nlike secure database interactions and multi-agent code development. However,\nscaling tool capacity beyond agent reasoning or model limits remains a\nchallenge. In this paper, we address these challenges by introducing Toolshed\nKnowledge Bases, a tool knowledge base (vector database) designed to store\nenhanced tool representations and optimize tool selection for large-scale\ntool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a\nnovel ensemble of tool-applied advanced retrieval-augmented generation (RAG)\ntechniques across the pre-retrieval, intra-retrieval, and post-retrieval\nphases, without requiring model fine-tuning. During pre-retrieval, tool\ndocuments are enhanced with key information and stored in the Toolshed\nKnowledge Base. Intra-retrieval focuses on query planning and transformation to\nincrease retrieval accuracy. Post-retrieval refines the retrieved tool\ndocuments and enables self-reflection. Furthermore, by varying both the total\nnumber of tools (tool-M) an Agent has access to and the tool selection\nthreshold (top-k), we address trade-offs between retrieval accuracy, agent\nperformance, and token cost. Our approach achieves 46%, 56%, and 47% absolute\nimprovements on the ToolE single-tool, ToolE multi-tool and Seal-Tools\nbenchmark datasets, respectively (Recall@5).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks\nlike secure database interactions and multi-agent code development. However,\nscaling tool capacity beyond agent reasoning or model limits remains a\nchallenge. In this paper, we address these challenges by introducing Toolshed\nKnowledge Bases, a tool knowledge base (vector database) designed to store\nenhanced tool representations and optimize tool selection for large-scale\ntool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a\nnovel ensemble of tool-applied advanced retrieval-augmented generation (RAG)\ntechniques across the pre-retrieval, intra-retrieval, and post-retrieval\nphases, without requiring model fine-tuning. During pre-retrieval, tool\ndocuments are enhanced with key information and stored in the Toolshed\nKnowledge Base. Intra-retrieval focuses on query planning and transformation to\nincrease retrieval accuracy. Post-retrieval refines the retrieved tool\ndocuments and enables self-reflection. Furthermore, by varying both the total\nnumber of tools (tool-M) an Agent has access to and the tool selection\nthreshold (top-k), we address trade-offs between retrieval accuracy, agent\nperformance, and token cost. Our approach achieves 46%, 56%, and 47% absolute\nimprovements on the ToolE single-tool, ToolE multi-tool and Seal-Tools\nbenchmark datasets, respectively (Recall@5)."
                },
                "authors": [
                    {
                        "name": "Elias Lumer"
                    }
                ],
                "author_detail": {
                    "name": "Elias Lumer"
                },
                "author": "Elias Lumer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13191v2",
                "updated": "2024-10-18T16:42:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    42,
                    1,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-17T03:38:29Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    3,
                    38,
                    29,
                    3,
                    291,
                    0
                ],
                "title": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with\n  Iterative Self-Critique, Correction, and Comparison Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCQG-SRefine: Multiple Choice Question Generation and Evaluation with\n  Iterative Self-Critique, Correction, and Comparison Feedback"
                },
                "summary": "Automatic question generation (QG) is essential for AI and NLP, particularly\nin intelligent tutoring, dialogue systems, and fact verification. Generating\nmultiple-choice questions (MCQG) for professional exams, like the United States\nMedical Licensing Examination (USMLE), is particularly challenging, requiring\ndomain expertise and complex multi-hop reasoning for high-quality questions.\nHowever, current large language models (LLMs) like GPT-4 struggle with\nprofessional MCQG due to outdated knowledge, hallucination issues, and prompt\nsensitivity, resulting in unsatisfactory quality and difficulty. To address\nthese challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique\nand Correction) framework for converting medical cases into high-quality\nUSMLE-style questions. By integrating expert-driven prompt engineering with\niterative self-critique and self-correction feedback, MCQG-SRefine\nsignificantly enhances human expert satisfaction regarding both the quality and\ndifficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based\nautomatic metric to replace the complex and costly expert evaluation process,\nensuring reliable and expert-aligned assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic question generation (QG) is essential for AI and NLP, particularly\nin intelligent tutoring, dialogue systems, and fact verification. Generating\nmultiple-choice questions (MCQG) for professional exams, like the United States\nMedical Licensing Examination (USMLE), is particularly challenging, requiring\ndomain expertise and complex multi-hop reasoning for high-quality questions.\nHowever, current large language models (LLMs) like GPT-4 struggle with\nprofessional MCQG due to outdated knowledge, hallucination issues, and prompt\nsensitivity, resulting in unsatisfactory quality and difficulty. To address\nthese challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique\nand Correction) framework for converting medical cases into high-quality\nUSMLE-style questions. By integrating expert-driven prompt engineering with\niterative self-critique and self-correction feedback, MCQG-SRefine\nsignificantly enhances human expert satisfaction regarding both the quality and\ndifficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based\nautomatic metric to replace the complex and costly expert evaluation process,\nensuring reliable and expert-aligned assessments."
                },
                "authors": [
                    {
                        "name": "Zonghai Yao"
                    },
                    {
                        "name": "Aditya Parashar"
                    },
                    {
                        "name": "Huixue Zhou"
                    },
                    {
                        "name": "Won Seok Jang"
                    },
                    {
                        "name": "Feiyun Ouyang"
                    },
                    {
                        "name": "Zhichao Yang"
                    },
                    {
                        "name": "Hong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Yu"
                },
                "author": "Hong Yu",
                "arxiv_comment": "Equal contribution for the first two authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14582v1",
                "updated": "2024-10-18T16:32:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    32,
                    10,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:32:10Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    32,
                    10,
                    4,
                    292,
                    0
                ],
                "title": "Do LLMs estimate uncertainty well in instruction-following?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs estimate uncertainty well in instruction-following?"
                },
                "summary": "Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) could be valuable personal AI agents across\nvarious domains, provided they can precisely follow user instructions. However,\nrecent studies have shown significant limitations in LLMs'\ninstruction-following capabilities, raising concerns about their reliability in\nhigh-stakes applications. Accurately estimating LLMs' uncertainty in adhering\nto instructions is critical to mitigating deployment risks. We present, to our\nknowledge, the first systematic evaluation of the uncertainty estimation\nabilities of LLMs in the context of instruction-following. Our study identifies\nkey challenges with existing instruction-following benchmarks, where multiple\nfactors are entangled with uncertainty stems from instruction-following,\ncomplicating the isolation and comparison across methods and models. To address\nthese issues, we introduce a controlled evaluation setup with two benchmark\nversions of data, enabling a comprehensive comparison of uncertainty estimation\nmethods under various conditions. Our findings show that existing uncertainty\nmethods struggle, particularly when models make subtle errors in instruction\nfollowing. While internal model states provide some improvement, they remain\ninadequate in more complex scenarios. The insights from our controlled\nevaluation setups provide a crucial understanding of LLMs' limitations and\npotential for uncertainty estimation in instruction-following tasks, paving the\nway for more trustworthy AI agents."
                },
                "authors": [
                    {
                        "name": "Juyeon Heo"
                    },
                    {
                        "name": "Miao Xiong"
                    },
                    {
                        "name": "Christina Heinze-Deml"
                    },
                    {
                        "name": "Jaya Narain"
                    }
                ],
                "author_detail": {
                    "name": "Jaya Narain"
                },
                "author": "Jaya Narain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14578v1",
                "updated": "2024-10-18T16:26:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    26,
                    45,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:26:45Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    26,
                    45,
                    4,
                    292,
                    0
                ],
                "title": "Large Language Models Are Overparameterized Text Encoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Overparameterized Text Encoders"
                },
                "summary": "Large language models (LLMs) demonstrate strong performance as text embedding\nmodels when finetuned with supervised contrastive training. However, their\nlarge size balloons inference time and memory requirements. In this paper, we\nshow that by pruning the last $p\\%$ layers of an LLM before supervised training\nfor only 1000 steps, we can achieve a proportional reduction in memory and\ninference time. We evaluate four different state-of-the-art LLMs on text\nembedding tasks and find that our method can prune up to 30\\% of layers with\nnegligible impact on performance and up to 80\\% with only a modest drop. With\nonly three lines of code, our method is easily implemented in any pipeline for\ntransforming LLMs to text encoders. We also propose $\\text{L}^3 \\text{Prune}$,\na novel layer-pruning strategy based on the model's initial loss that provides\ntwo optimal pruning configurations: a large variant with negligible performance\nloss and a small variant for resource-constrained settings. On average, the\nlarge variant prunes 21\\% of the parameters with a $-0.3$ performance drop, and\nthe small variant only suffers from a $-5.1$ decrease while pruning 74\\% of the\nmodel. We consider these results strong evidence that LLMs are\noverparameterized for text embedding tasks, and can be easily pruned.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong performance as text embedding\nmodels when finetuned with supervised contrastive training. However, their\nlarge size balloons inference time and memory requirements. In this paper, we\nshow that by pruning the last $p\\%$ layers of an LLM before supervised training\nfor only 1000 steps, we can achieve a proportional reduction in memory and\ninference time. We evaluate four different state-of-the-art LLMs on text\nembedding tasks and find that our method can prune up to 30\\% of layers with\nnegligible impact on performance and up to 80\\% with only a modest drop. With\nonly three lines of code, our method is easily implemented in any pipeline for\ntransforming LLMs to text encoders. We also propose $\\text{L}^3 \\text{Prune}$,\na novel layer-pruning strategy based on the model's initial loss that provides\ntwo optimal pruning configurations: a large variant with negligible performance\nloss and a small variant for resource-constrained settings. On average, the\nlarge variant prunes 21\\% of the parameters with a $-0.3$ performance drop, and\nthe small variant only suffers from a $-5.1$ decrease while pruning 74\\% of the\nmodel. We consider these results strong evidence that LLMs are\noverparameterized for text embedding tasks, and can be easily pruned."
                },
                "authors": [
                    {
                        "name": "Thennal D K"
                    },
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Chris Biemann"
                    }
                ],
                "author_detail": {
                    "name": "Chris Biemann"
                },
                "author": "Chris Biemann",
                "arxiv_comment": "8 pages of content + 1 for limitations and ethical considerations, 14\n  pages in total including references and appendix, 5+1 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14569v1",
                "updated": "2024-10-18T16:16:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    16,
                    34,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:16:34Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    16,
                    34,
                    4,
                    292,
                    0
                ],
                "title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have established them as\nagentic systems capable of planning and interacting with various tools. These\nLLM agents are often paired with web-based tools, enabling access to diverse\nsources and real-time information. Although these advancements offer\nsignificant benefits across various applications, they also increase the risk\nof malicious use, particularly in cyberattacks involving personal information.\nIn this work, we investigate the risks associated with misuse of LLM agents in\ncyberattacks involving personal data. Specifically, we aim to understand: 1)\nhow potent LLM agents can be when directed to conduct cyberattacks, 2) how\ncyberattacks are enhanced by web-based tools, and 3) how affordable and easy it\nbecomes to launch cyberattacks using LLM agents. We examine three attack\nscenarios: the collection of Personally Identifiable Information (PII), the\ngeneration of impersonation posts, and the creation of spear-phishing emails.\nOur experiments reveal the effectiveness of LLM agents in these attacks: LLM\nagents achieved a precision of up to 95.9% in collecting PII, up to 93.9% of\nimpersonation posts created by LLM agents were evaluated as authentic, and the\nclick rate for links in spear phishing emails created by LLM agents reached up\nto 46.67%. Additionally, our findings underscore the limitations of existing\nsafeguards in contemporary commercial LLMs, emphasizing the urgent need for\nmore robust security measures to prevent the misuse of LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have established them as\nagentic systems capable of planning and interacting with various tools. These\nLLM agents are often paired with web-based tools, enabling access to diverse\nsources and real-time information. Although these advancements offer\nsignificant benefits across various applications, they also increase the risk\nof malicious use, particularly in cyberattacks involving personal information.\nIn this work, we investigate the risks associated with misuse of LLM agents in\ncyberattacks involving personal data. Specifically, we aim to understand: 1)\nhow potent LLM agents can be when directed to conduct cyberattacks, 2) how\ncyberattacks are enhanced by web-based tools, and 3) how affordable and easy it\nbecomes to launch cyberattacks using LLM agents. We examine three attack\nscenarios: the collection of Personally Identifiable Information (PII), the\ngeneration of impersonation posts, and the creation of spear-phishing emails.\nOur experiments reveal the effectiveness of LLM agents in these attacks: LLM\nagents achieved a precision of up to 95.9% in collecting PII, up to 93.9% of\nimpersonation posts created by LLM agents were evaluated as authentic, and the\nclick rate for links in spear phishing emails created by LLM agents reached up\nto 46.67%. Additionally, our findings underscore the limitations of existing\nsafeguards in contemporary commercial LLMs, emphasizing the urgent need for\nmore robust security measures to prevent the misuse of LLM agents."
                },
                "authors": [
                    {
                        "name": "Hanna Kim"
                    },
                    {
                        "name": "Minkyoo Song"
                    },
                    {
                        "name": "Seung Ho Na"
                    },
                    {
                        "name": "Seungwon Shin"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14567v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14567v1",
                "updated": "2024-10-18T16:11:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    11,
                    29,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T16:11:29Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    11,
                    29,
                    4,
                    292,
                    0
                ],
                "title": "RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions"
                },
                "summary": "Conversational AI agents use Retrieval Augmented Generation (RAG) to provide\nverifiable document-grounded responses to user inquiries. However, many natural\nquestions do not have good answers: about 25\\% contain false\nassumptions~\\cite{Yu2023:CREPE}, and over 50\\% are\nambiguous~\\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve\ntheir responses to confusing questions. This paper presents a novel synthetic\ndata generation method to efficiently create a diverse set of context-grounded\nconfusing questions from a given document corpus. We conduct an empirical\ncomparative evaluation of several large language models as RAG agents to\nmeasure the accuracy of confusion detection and appropriate response\ngeneration. We contribute a benchmark dataset to the public domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational AI agents use Retrieval Augmented Generation (RAG) to provide\nverifiable document-grounded responses to user inquiries. However, many natural\nquestions do not have good answers: about 25\\% contain false\nassumptions~\\cite{Yu2023:CREPE}, and over 50\\% are\nambiguous~\\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve\ntheir responses to confusing questions. This paper presents a novel synthetic\ndata generation method to efficiently create a diverse set of context-grounded\nconfusing questions from a given document corpus. We conduct an empirical\ncomparative evaluation of several large language models as RAG agents to\nmeasure the accuracy of confusion detection and appropriate response\ngeneration. We contribute a benchmark dataset to the public domain."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Jinming Nian"
                    },
                    {
                        "name": "Alexandre Evfimievski"
                    },
                    {
                        "name": "Yi Fang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Fang"
                },
                "author": "Yi Fang",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14567v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14567v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14917v2",
                "updated": "2024-10-18T15:42:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    42,
                    6,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-23T11:13:25Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    11,
                    13,
                    25,
                    0,
                    267,
                    0
                ],
                "title": "With Ears to See and Eyes to Hear: Sound Symbolism Experiments with\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Ears to See and Eyes to Hear: Sound Symbolism Experiments with\n  Multimodal Large Language Models"
                },
                "summary": "Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have\ndemonstrated aptitude as potential substitutes for human participants in\nexperiments testing psycholinguistic phenomena. However, an understudied\nquestion is to what extent models that only have access to vision and text\nmodalities are able to implicitly understand sound-based phenomena via abstract\nreasoning from orthography and imagery alone. To investigate this, we analyse\nthe ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise\na non-arbitrary link between sounds and concepts) as well as their ability to\n\"hear\" via the interplay of the language and vision modules of open and\nclosed-source multimodal models. We perform multiple experiments, including\nreplicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism\ntasks, and comparing human judgements of linguistic iconicity with that of\nLLMs. Our results show that VLMs demonstrate varying levels of agreement with\nhuman labels, and more task information may be required for VLMs versus their\nhuman counterparts for in silico experimentation. We additionally see through\nhigher maximum agreement levels that Magnitude Symbolism is an easier pattern\nfor VLMs to identify than Shape Symbolism, and that an understanding of\nlinguistic iconicity is highly dependent on model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) and Vision Language Models (VLMs) have\ndemonstrated aptitude as potential substitutes for human participants in\nexperiments testing psycholinguistic phenomena. However, an understudied\nquestion is to what extent models that only have access to vision and text\nmodalities are able to implicitly understand sound-based phenomena via abstract\nreasoning from orthography and imagery alone. To investigate this, we analyse\nthe ability of VLMs and LLMs to demonstrate sound symbolism (i.e., to recognise\na non-arbitrary link between sounds and concepts) as well as their ability to\n\"hear\" via the interplay of the language and vision modules of open and\nclosed-source multimodal models. We perform multiple experiments, including\nreplicating the classic Kiki-Bouba and Mil-Mal shape and magnitude symbolism\ntasks, and comparing human judgements of linguistic iconicity with that of\nLLMs. Our results show that VLMs demonstrate varying levels of agreement with\nhuman labels, and more task information may be required for VLMs versus their\nhuman counterparts for in silico experimentation. We additionally see through\nhigher maximum agreement levels that Magnitude Symbolism is an easier pattern\nfor VLMs to identify than Shape Symbolism, and that an understanding of\nlinguistic iconicity is highly dependent on model size."
                },
                "authors": [
                    {
                        "name": "Tyler Loakman"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Accepted to EMNLP 2024 (Camera Ready)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14545v1",
                "updated": "2024-10-18T15:40:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    40,
                    48,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T15:40:48Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    40,
                    48,
                    4,
                    292,
                    0
                ],
                "title": "Tell me what I need to know: Exploring LLM-based (Personalized)\n  Abstractive Multi-Source Meeting Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tell me what I need to know: Exploring LLM-based (Personalized)\n  Abstractive Multi-Source Meeting Summarization"
                },
                "summary": "Meeting summarization is crucial in digital communication, but existing\nsolutions struggle with salience identification to generate personalized,\nworkable summaries, and context understanding to fully comprehend the meetings'\ncontent. Previous attempts to address these issues by considering related\nsupplementary resources (e.g., presentation slides) alongside transcripts are\nhindered by models' limited context sizes and handling the additional\ncomplexities of the multi-source tasks, such as identifying relevant\ninformation in additional files and seamlessly aligning it with the meeting\ncontent. This work explores multi-source meeting summarization considering\nsupplementary materials through a three-stage large language model approach:\nidentifying transcript passages needing additional context, inferring relevant\ndetails from supplementary materials and inserting them into the transcript,\nand generating a summary from this enriched transcript. Our multi-source\napproach enhances model understanding, increasing summary relevance by ~9% and\nproducing more content-rich outputs. We introduce a personalization protocol\nthat extracts participant characteristics and tailors summaries accordingly,\nimproving informativeness by ~10%. This work further provides insights on\nperformance-cost trade-offs across four leading model families, including\nedge-device capable options. Our approach can be extended to similar complex\ngenerative tasks benefitting from additional resources and personalization,\nsuch as dialogue systems and action planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meeting summarization is crucial in digital communication, but existing\nsolutions struggle with salience identification to generate personalized,\nworkable summaries, and context understanding to fully comprehend the meetings'\ncontent. Previous attempts to address these issues by considering related\nsupplementary resources (e.g., presentation slides) alongside transcripts are\nhindered by models' limited context sizes and handling the additional\ncomplexities of the multi-source tasks, such as identifying relevant\ninformation in additional files and seamlessly aligning it with the meeting\ncontent. This work explores multi-source meeting summarization considering\nsupplementary materials through a three-stage large language model approach:\nidentifying transcript passages needing additional context, inferring relevant\ndetails from supplementary materials and inserting them into the transcript,\nand generating a summary from this enriched transcript. Our multi-source\napproach enhances model understanding, increasing summary relevance by ~9% and\nproducing more content-rich outputs. We introduce a personalization protocol\nthat extracts participant characteristics and tailors summaries accordingly,\nimproving informativeness by ~10%. This work further provides insights on\nperformance-cost trade-offs across four leading model families, including\nedge-device capable options. Our approach can be extended to similar complex\ngenerative tasks benefitting from additional resources and personalization,\nsuch as dialogue systems and action planning."
                },
                "authors": [
                    {
                        "name": "Frederic Kirstein"
                    },
                    {
                        "name": "Terry Ruas"
                    },
                    {
                        "name": "Robert Kratel"
                    },
                    {
                        "name": "Bela Gipp"
                    }
                ],
                "author_detail": {
                    "name": "Bela Gipp"
                },
                "author": "Bela Gipp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13901v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13901v3",
                "updated": "2024-10-18T15:25:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    25,
                    44,
                    4,
                    292,
                    0
                ],
                "published": "2024-03-20T18:13:17Z",
                "published_parsed": [
                    2024,
                    3,
                    20,
                    18,
                    13,
                    17,
                    2,
                    80,
                    0
                ],
                "title": "Train & Constrain: Phonologically Informed Tongue-Twister Generation\n  from Topics and Paraphrases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Train & Constrain: Phonologically Informed Tongue-Twister Generation\n  from Topics and Paraphrases"
                },
                "summary": "Previous work in phonologically and phonetically grounded language generation\nhas mainly focused on domains such as puns and poetry. In this article, we\npresent new work on the generation of English tongue twisters - a form of\nlanguage that is required to be conditioned on a phoneme level to maximize\nsound overlap, while maintaining semantic consistency with an input topic or\nphrase and still being grammatically correct. We present TwisterLister, a\npipeline for generating phonologically informed tongue twisters from large\nlanguage models (LLMs) that we use to generate TwistList 2.0, the largest\nannotated dataset of tongue twisters to date, consisting of 17K+ examples from\na combination of human and LLM authors. Our generation pipeline involves the\nuse of a phonologically constrained vocabulary alongside LLM prompting to\ngenerate novel, non-derivative tongue twister examples. We additionally present\nthe results of automatic and human evaluation of smaller models trained on our\ngenerated dataset to demonstrate the extent to which phonologically motivated\nlanguage types can be generated without explicit injection of phonological\nknowledge. Additionally, we introduce a phoneme-aware constrained decoding\nmodule (PACD) that can be integrated into an autoregressive language model and\ndemonstrate that this method generates good quality tongue twisters both with\nand without fine-tuning the underlying language model. We also design and\nimplement a range of automatic metrics for the task of tongue twister\ngeneration that is phonologically motivated and captures the unique essence of\ntongue twisters, primarily based on phonemic edit distance (PED)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous work in phonologically and phonetically grounded language generation\nhas mainly focused on domains such as puns and poetry. In this article, we\npresent new work on the generation of English tongue twisters - a form of\nlanguage that is required to be conditioned on a phoneme level to maximize\nsound overlap, while maintaining semantic consistency with an input topic or\nphrase and still being grammatically correct. We present TwisterLister, a\npipeline for generating phonologically informed tongue twisters from large\nlanguage models (LLMs) that we use to generate TwistList 2.0, the largest\nannotated dataset of tongue twisters to date, consisting of 17K+ examples from\na combination of human and LLM authors. Our generation pipeline involves the\nuse of a phonologically constrained vocabulary alongside LLM prompting to\ngenerate novel, non-derivative tongue twister examples. We additionally present\nthe results of automatic and human evaluation of smaller models trained on our\ngenerated dataset to demonstrate the extent to which phonologically motivated\nlanguage types can be generated without explicit injection of phonological\nknowledge. Additionally, we introduce a phoneme-aware constrained decoding\nmodule (PACD) that can be integrated into an autoregressive language model and\ndemonstrate that this method generates good quality tongue twisters both with\nand without fine-tuning the underlying language model. We also design and\nimplement a range of automatic metrics for the task of tongue twister\ngeneration that is phonologically motivated and captures the unique essence of\ntongue twisters, primarily based on phonemic edit distance (PED)"
                },
                "authors": [
                    {
                        "name": "Tyler Loakman"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Accepted Final Version to Computational Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13901v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13901v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v3",
                "updated": "2024-10-18T15:03:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    15,
                    3,
                    8,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Although current Multi-modal Large Language Models (MLLMs) demonstrate\npromising results in video understanding, processing extremely long videos\nremains an ongoing challenge. Typically, MLLMs struggle with handling thousands\nof visual tokens that exceed the maximum context length, and they suffer from\nthe information decay due to token aggregation. Another challenge is the high\ncomputational cost stemming from the large number of video tokens. To tackle\nthese issues, we propose Video-XL, an extra-long vision language model designed\nfor efficient hour-scale video understanding. Specifically, we argue that LLMs\ncan be adapted as effective visual condensers and propose Visual Context Latent\nSummarization which condenses visual contexts into highly compact forms.\nExtensive experiments demonstrate that our model achieves promising results on\npopular long video understanding benchmarks. For example, Video-XL outperforms\nthe current state-of-the-art method on VNBench by nearly 10\\% in accuracy.\nMoreover, Video-XL presents an impressive balance between efficiency and\neffectiveness, processing 2048 frames on a single 80GB GPU while achieving\nnearly 95% accuracy in the Needle-in-a-Haystack evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although current Multi-modal Large Language Models (MLLMs) demonstrate\npromising results in video understanding, processing extremely long videos\nremains an ongoing challenge. Typically, MLLMs struggle with handling thousands\nof visual tokens that exceed the maximum context length, and they suffer from\nthe information decay due to token aggregation. Another challenge is the high\ncomputational cost stemming from the large number of video tokens. To tackle\nthese issues, we propose Video-XL, an extra-long vision language model designed\nfor efficient hour-scale video understanding. Specifically, we argue that LLMs\ncan be adapted as effective visual condensers and propose Visual Context Latent\nSummarization which condenses visual contexts into highly compact forms.\nExtensive experiments demonstrate that our model achieves promising results on\npopular long video understanding benchmarks. For example, Video-XL outperforms\nthe current state-of-the-art method on VNBench by nearly 10\\% in accuracy.\nMoreover, Video-XL presents an impressive balance between efficiency and\neffectiveness, processing 2048 frames on a single 80GB GPU while achieving\nnearly 95% accuracy in the Needle-in-a-Haystack evaluation."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14516v1",
                "updated": "2024-10-18T14:55:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    55,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T14:55:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    55,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "Do LLMs \"know\" internally when they follow instructions?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs \"know\" internally when they follow instructions?"
                },
                "summary": "Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-following is crucial for building AI agents with large language\nmodels (LLMs), as these models must adhere strictly to user-provided\nconstraints and guidelines. However, LLMs often fail to follow even simple and\nclear instructions. To improve instruction-following behavior and prevent\nundesirable outputs, a deeper understanding of how LLMs' internal states relate\nto these outcomes is required. Our analysis of LLM internal states reveal a\ndimension in the input embedding space linked to successful\ninstruction-following. We demonstrate that modifying representations along this\ndimension improves instruction-following success rates compared to random\nchanges, without compromising response quality. Further investigation reveals\nthat this dimension is more closely related to the phrasing of prompts rather\nthan the inherent difficulty of the task or instructions. This discovery also\nsuggests explanations for why LLMs sometimes fail to follow clear instructions\nand why prompt engineering is often effective, even when the content remains\nlargely unchanged. This work provides insight into the internal workings of\nLLMs' instruction-following, paving the way for reliable LLM agents."
                },
                "authors": [
                    {
                        "name": "Juyeon Heo"
                    },
                    {
                        "name": "Christina Heinze-Deml"
                    },
                    {
                        "name": "Oussama Elachqar"
                    },
                    {
                        "name": "Shirley Ren"
                    },
                    {
                        "name": "Udhay Nallasamy"
                    },
                    {
                        "name": "Andy Miller"
                    },
                    {
                        "name": "Kwan Ho Ryan Chan"
                    },
                    {
                        "name": "Jaya Narain"
                    }
                ],
                "author_detail": {
                    "name": "Jaya Narain"
                },
                "author": "Jaya Narain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14481v1",
                "updated": "2024-10-18T14:04:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    4,
                    38,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T14:04:38Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    4,
                    38,
                    4,
                    292,
                    0
                ],
                "title": "DRL Optimization Trajectory Generation via Wireless Network\n  Intent-Guided Diffusion Models for Optimizing Resource Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRL Optimization Trajectory Generation via Wireless Network\n  Intent-Guided Diffusion Models for Optimizing Resource Allocation"
                },
                "summary": "With the rapid advancements in wireless communication fields, including\nlow-altitude economies, 6G, and Wi-Fi, the scale of wireless networks continues\nto expand, accompanied by increasing service quality demands. Traditional deep\nreinforcement learning (DRL)-based optimization models can improve network\nperformance by solving non-convex optimization problems intelligently. However,\nthey heavily rely on online deployment and often require extensive initial\ntraining. Online DRL optimization models typically make accurate decisions\nbased on current channel state distributions. When these distributions change,\ntheir generalization capability diminishes, which hinders the responsiveness\nessential for real-time and high-reliability wireless communication networks.\nFurthermore, different users have varying quality of service (QoS) requirements\nacross diverse scenarios, and conventional online DRL methods struggle to\naccommodate this variability. Consequently, exploring flexible and customized\nAI strategies is critical. We propose a wireless network intent (WNI)-guided\ntrajectory generation model based on a generative diffusion model (GDM). This\nmodel can be generated and fine-tuned in real time to achieve the objective and\nmeet the constraints of target intent networks, significantly reducing state\ninformation exposure during wireless communication. Moreover, The WNI-guided\noptimization trajectory generation can be customized to address differentiated\nQoS requirements, enhancing the overall quality of communication in future\nintelligent networks. Extensive simulation results demonstrate that our\napproach achieves greater stability in spectral efficiency variations and\noutperforms traditional DRL optimization models in dynamic communication\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancements in wireless communication fields, including\nlow-altitude economies, 6G, and Wi-Fi, the scale of wireless networks continues\nto expand, accompanied by increasing service quality demands. Traditional deep\nreinforcement learning (DRL)-based optimization models can improve network\nperformance by solving non-convex optimization problems intelligently. However,\nthey heavily rely on online deployment and often require extensive initial\ntraining. Online DRL optimization models typically make accurate decisions\nbased on current channel state distributions. When these distributions change,\ntheir generalization capability diminishes, which hinders the responsiveness\nessential for real-time and high-reliability wireless communication networks.\nFurthermore, different users have varying quality of service (QoS) requirements\nacross diverse scenarios, and conventional online DRL methods struggle to\naccommodate this variability. Consequently, exploring flexible and customized\nAI strategies is critical. We propose a wireless network intent (WNI)-guided\ntrajectory generation model based on a generative diffusion model (GDM). This\nmodel can be generated and fine-tuned in real time to achieve the objective and\nmeet the constraints of target intent networks, significantly reducing state\ninformation exposure during wireless communication. Moreover, The WNI-guided\noptimization trajectory generation can be customized to address differentiated\nQoS requirements, enhancing the overall quality of communication in future\nintelligent networks. Extensive simulation results demonstrate that our\napproach achieves greater stability in spectral efficiency variations and\noutperforms traditional DRL optimization models in dynamic communication\nsystems."
                },
                "authors": [
                    {
                        "name": "Junjie Wu"
                    },
                    {
                        "name": "Xuming Fang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Jingyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyu Wang"
                },
                "author": "Jingyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14480v1",
                "updated": "2024-10-18T14:03:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    3,
                    52,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T14:03:52Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    3,
                    52,
                    4,
                    292,
                    0
                ],
                "title": "Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of\n  Language Models"
                },
                "summary": "As large language models (LLMs) continue to advance, the need for precise and\nefficient evaluation metrics becomes more pressing. Traditional approaches,\nwhile informative, often face limitations in computational demands and\ninterpretability. In this paper, we introduce a novel hybrid evaluation method\nthat integrates two established techniques: entropy derived from covariance\nmatrices and the Matrix Nuclear Norm (MNN). Our method begins by normalizing\nhidden states from LLMs, then computes the covariance matrix and MNN from these\nrepresentations. We further calculate the entropy of the covariance matrix to\ncapture uncertainty and redundancy in the model's outputs. By combining these\nmetrics into a composite score, we offer a comprehensive evaluation framework\nthat balances accuracy with computational efficiency. Additionally, our\napproach allows for flexibility in adjusting the weightings between entropy and\nMNN, tailoring the evaluation for different objectives. Through a series of\nexperiments on various LLMs, we demonstrate the robustness and efficacy of our\nmethod, offering deeper insights into model performance. This work contributes\nto the ongoing development of LLM evaluation and opens avenues for future\ninnovations in model assessment techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the need for precise and\nefficient evaluation metrics becomes more pressing. Traditional approaches,\nwhile informative, often face limitations in computational demands and\ninterpretability. In this paper, we introduce a novel hybrid evaluation method\nthat integrates two established techniques: entropy derived from covariance\nmatrices and the Matrix Nuclear Norm (MNN). Our method begins by normalizing\nhidden states from LLMs, then computes the covariance matrix and MNN from these\nrepresentations. We further calculate the entropy of the covariance matrix to\ncapture uncertainty and redundancy in the model's outputs. By combining these\nmetrics into a composite score, we offer a comprehensive evaluation framework\nthat balances accuracy with computational efficiency. Additionally, our\napproach allows for flexibility in adjusting the weightings between entropy and\nMNN, tailoring the evaluation for different objectives. Through a series of\nexperiments on various LLMs, we demonstrate the robustness and efficacy of our\nmethod, offering deeper insights into model performance. This work contributes\nto the ongoing development of LLM evaluation and opens avenues for future\ninnovations in model assessment techniques."
                },
                "authors": [
                    {
                        "name": "James Vo"
                    }
                ],
                "author_detail": {
                    "name": "James Vo"
                },
                "author": "James Vo",
                "arxiv_comment": "The method is currently under experimentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09804v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09804v2",
                "updated": "2024-10-18T14:03:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    3,
                    5,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-13T11:15:38Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    11,
                    15,
                    38,
                    6,
                    287,
                    0
                ],
                "title": "BlackDAN: A Black-Box Multi-Objective Approach for Effective and\n  Contextual Jailbreaking of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlackDAN: A Black-Box Multi-Objective Approach for Effective and\n  Contextual Jailbreaking of Large Language Models"
                },
                "summary": "While large language models (LLMs) exhibit remarkable capabilities across\nvarious tasks, they encounter potential security risks such as jailbreak\nattacks, which exploit vulnerabilities to bypass security measures and generate\nharmful outputs. Existing jailbreak strategies mainly focus on maximizing\nattack success rate (ASR), frequently neglecting other critical factors,\nincluding the relevance of the jailbreak response to the query and the level of\nstealthiness. This narrow focus on single objectives can result in ineffective\nattacks that either lack contextual relevance or are easily recognizable. In\nthis work, we introduce BlackDAN, an innovative black-box attack framework with\nmulti-objective optimization, aiming to generate high-quality prompts that\neffectively facilitate jailbreaking while maintaining contextual relevance and\nminimizing detectability. BlackDAN leverages Multiobjective Evolutionary\nAlgorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks\nacross multiple objectives including ASR, stealthiness, and semantic relevance.\nBy integrating mechanisms like mutation, crossover, and Pareto-dominance,\nBlackDAN provides a transparent and interpretable process for generating\njailbreaks. Furthermore, the framework allows customization based on user\npreferences, enabling the selection of prompts that balance harmfulness,\nrelevance, and other factors. Experimental results demonstrate that BlackDAN\noutperforms traditional single-objective methods, yielding higher success rates\nand improved robustness across various LLMs and multimodal LLMs, while ensuring\njailbreak responses are both relevant and less detectable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) exhibit remarkable capabilities across\nvarious tasks, they encounter potential security risks such as jailbreak\nattacks, which exploit vulnerabilities to bypass security measures and generate\nharmful outputs. Existing jailbreak strategies mainly focus on maximizing\nattack success rate (ASR), frequently neglecting other critical factors,\nincluding the relevance of the jailbreak response to the query and the level of\nstealthiness. This narrow focus on single objectives can result in ineffective\nattacks that either lack contextual relevance or are easily recognizable. In\nthis work, we introduce BlackDAN, an innovative black-box attack framework with\nmulti-objective optimization, aiming to generate high-quality prompts that\neffectively facilitate jailbreaking while maintaining contextual relevance and\nminimizing detectability. BlackDAN leverages Multiobjective Evolutionary\nAlgorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks\nacross multiple objectives including ASR, stealthiness, and semantic relevance.\nBy integrating mechanisms like mutation, crossover, and Pareto-dominance,\nBlackDAN provides a transparent and interpretable process for generating\njailbreaks. Furthermore, the framework allows customization based on user\npreferences, enabling the selection of prompts that balance harmfulness,\nrelevance, and other factors. Experimental results demonstrate that BlackDAN\noutperforms traditional single-objective methods, yielding higher success rates\nand improved robustness across various LLMs and multimodal LLMs, while ensuring\njailbreak responses are both relevant and less detectable."
                },
                "authors": [
                    {
                        "name": "Xinyuan Wang"
                    },
                    {
                        "name": "Victor Shea-Jay Huang"
                    },
                    {
                        "name": "Renmiao Chen"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Chengwei Pan"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Minlie Huang"
                    }
                ],
                "author_detail": {
                    "name": "Minlie Huang"
                },
                "author": "Minlie Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09804v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09804v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14479v1",
                "updated": "2024-10-18T14:02:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    2,
                    34,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T14:02:34Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    2,
                    34,
                    4,
                    292,
                    0
                ],
                "title": "Backdoored Retrievers for Prompt Injection Attacks on Retrieval\n  Augmented Generation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoored Retrievers for Prompt Injection Attacks on Retrieval\n  Augmented Generation of Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating coherent text but remain limited by the static nature of their\ntraining data. Retrieval Augmented Generation (RAG) addresses this issue by\ncombining LLMs with up-to-date information retrieval, but also expand the\nattack surface of the system. This paper investigates prompt injection attacks\non RAG, focusing on malicious objectives beyond misinformation, such as\ninserting harmful links, promoting unauthorized services, and initiating\ndenial-of-service behaviors. We build upon existing corpus poisoning techniques\nand propose a novel backdoor attack aimed at the fine-tuning process of the\ndense retriever component. Our experiments reveal that corpus poisoning can\nachieve significant attack success rates through the injection of a small\nnumber of compromised documents into the retriever corpus. In contrast,\nbackdoor attacks demonstrate even higher success rates but necessitate a more\ncomplex setup, as the victim must fine-tune the retriever using the attacker\npoisoned dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating coherent text but remain limited by the static nature of their\ntraining data. Retrieval Augmented Generation (RAG) addresses this issue by\ncombining LLMs with up-to-date information retrieval, but also expand the\nattack surface of the system. This paper investigates prompt injection attacks\non RAG, focusing on malicious objectives beyond misinformation, such as\ninserting harmful links, promoting unauthorized services, and initiating\ndenial-of-service behaviors. We build upon existing corpus poisoning techniques\nand propose a novel backdoor attack aimed at the fine-tuning process of the\ndense retriever component. Our experiments reveal that corpus poisoning can\nachieve significant attack success rates through the injection of a small\nnumber of compromised documents into the retriever corpus. In contrast,\nbackdoor attacks demonstrate even higher success rates but necessitate a more\ncomplex setup, as the victim must fine-tune the retriever using the attacker\npoisoned dataset."
                },
                "authors": [
                    {
                        "name": "Cody Clop"
                    },
                    {
                        "name": "Yannick Teglia"
                    }
                ],
                "author_detail": {
                    "name": "Yannick Teglia"
                },
                "author": "Yannick Teglia",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00552v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00552v3",
                "updated": "2024-10-18T13:49:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    49,
                    19,
                    4,
                    292,
                    0
                ],
                "published": "2024-05-01T14:50:58Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    14,
                    50,
                    58,
                    2,
                    122,
                    0
                ],
                "title": "Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs"
                },
                "summary": "We present a novel approach for long-term human trajectory prediction in\nindoor human-centric environments, which is essential for long-horizon robot\nplanning in these environments. State-of-the-art human trajectory prediction\nmethods are limited by their focus on collision avoidance and short-term\nplanning, and their inability to model complex interactions of humans with the\nenvironment. In contrast, our approach overcomes these limitations by\npredicting sequences of human interactions with the environment and using this\ninformation to guide trajectory predictions over a horizon of up to 60s. We\nleverage Large Language Models (LLMs) to predict interactions with the\nenvironment by conditioning the LLM prediction on rich contextual information\nabout the scene. This information is given as a 3D Dynamic Scene Graph that\nencodes the geometry, semantics, and traversability of the environment into a\nhierarchical representation. We then ground these interaction sequences into\nmulti-modal spatio-temporal distributions over human positions using a\nprobabilistic approach based on continuous-time Markov Chains. To evaluate our\napproach, we introduce a new semi-synthetic dataset of long-term human\ntrajectories in complex indoor environments, which also includes annotations of\nhuman-object interactions. We show in thorough experimental evaluations that\nour approach achieves a 54% lower average negative log-likelihood and a 26.5%\nlower Best-of-20 displacement error compared to the best non-privileged (i.e.,\nevaluated in a zero-shot fashion on the dataset) baselines for a time horizon\nof 60s.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach for long-term human trajectory prediction in\nindoor human-centric environments, which is essential for long-horizon robot\nplanning in these environments. State-of-the-art human trajectory prediction\nmethods are limited by their focus on collision avoidance and short-term\nplanning, and their inability to model complex interactions of humans with the\nenvironment. In contrast, our approach overcomes these limitations by\npredicting sequences of human interactions with the environment and using this\ninformation to guide trajectory predictions over a horizon of up to 60s. We\nleverage Large Language Models (LLMs) to predict interactions with the\nenvironment by conditioning the LLM prediction on rich contextual information\nabout the scene. This information is given as a 3D Dynamic Scene Graph that\nencodes the geometry, semantics, and traversability of the environment into a\nhierarchical representation. We then ground these interaction sequences into\nmulti-modal spatio-temporal distributions over human positions using a\nprobabilistic approach based on continuous-time Markov Chains. To evaluate our\napproach, we introduce a new semi-synthetic dataset of long-term human\ntrajectories in complex indoor environments, which also includes annotations of\nhuman-object interactions. We show in thorough experimental evaluations that\nour approach achieves a 54% lower average negative log-likelihood and a 26.5%\nlower Best-of-20 displacement error compared to the best non-privileged (i.e.,\nevaluated in a zero-shot fashion on the dataset) baselines for a time horizon\nof 60s."
                },
                "authors": [
                    {
                        "name": "Nicolas Gorlo"
                    },
                    {
                        "name": "Lukas Schmid"
                    },
                    {
                        "name": "Luca Carlone"
                    }
                ],
                "author_detail": {
                    "name": "Luca Carlone"
                },
                "author": "Luca Carlone",
                "arxiv_doi": "10.1109/LRA.2024.3482169",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2024.3482169",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.00552v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00552v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages, 6 figures. Accepted at IEEE Robotics and Automation Letters\n  (RA-L). Code released at: https://github.com/MIT-SPARK/LP2",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14464v1",
                "updated": "2024-10-18T13:48:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    48,
                    1,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:48:01Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    48,
                    1,
                    4,
                    292,
                    0
                ],
                "title": "Electrocardiogram-Language Model for Few-Shot Question Answering with\n  Meta Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrocardiogram-Language Model for Few-Shot Question Answering with\n  Meta Learning"
                },
                "summary": "Electrocardiogram (ECG) interpretation requires specialized expertise, often\ninvolving synthesizing insights from ECG signals with complex clinical queries\nposed in natural language. The scarcity of labeled ECG data coupled with the\ndiverse nature of clinical inquiries presents a significant challenge for\ndeveloping robust and adaptable ECG diagnostic systems. This work introduces a\nnovel multimodal meta-learning method for few-shot ECG question answering,\naddressing the challenge of limited labeled data while leveraging the rich\nknowledge encoded within large language models (LLMs). Our LLM-agnostic\napproach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA\nand Gemma) via a trainable fusion module, enabling the language model to reason\nabout ECG data and generate clinically meaningful answers. Extensive\nexperiments demonstrate superior generalization to unseen diagnostic tasks\ncompared to supervised baselines, achieving notable performance even with\nlimited ECG leads. For instance, in a 5-way 5-shot setting, our method using\nLLaMA-3.1-8B achieves accuracy of 84.6%, 77.3%, and 69.6% on single verify,\nchoose and query question types, respectively. These results highlight the\npotential of our method to enhance clinical ECG interpretation by combining\nsignal processing with the nuanced language understanding capabilities of LLMs,\nparticularly in data-constrained scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrocardiogram (ECG) interpretation requires specialized expertise, often\ninvolving synthesizing insights from ECG signals with complex clinical queries\nposed in natural language. The scarcity of labeled ECG data coupled with the\ndiverse nature of clinical inquiries presents a significant challenge for\ndeveloping robust and adaptable ECG diagnostic systems. This work introduces a\nnovel multimodal meta-learning method for few-shot ECG question answering,\naddressing the challenge of limited labeled data while leveraging the rich\nknowledge encoded within large language models (LLMs). Our LLM-agnostic\napproach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA\nand Gemma) via a trainable fusion module, enabling the language model to reason\nabout ECG data and generate clinically meaningful answers. Extensive\nexperiments demonstrate superior generalization to unseen diagnostic tasks\ncompared to supervised baselines, achieving notable performance even with\nlimited ECG leads. For instance, in a 5-way 5-shot setting, our method using\nLLaMA-3.1-8B achieves accuracy of 84.6%, 77.3%, and 69.6% on single verify,\nchoose and query question types, respectively. These results highlight the\npotential of our method to enhance clinical ECG interpretation by combining\nsignal processing with the nuanced language understanding capabilities of LLMs,\nparticularly in data-constrained scenarios."
                },
                "authors": [
                    {
                        "name": "Jialu Tang"
                    },
                    {
                        "name": "Tong Xia"
                    },
                    {
                        "name": "Yuan Lu"
                    },
                    {
                        "name": "Cecilia Mascolo"
                    },
                    {
                        "name": "Aaqib Saeed"
                    }
                ],
                "author_detail": {
                    "name": "Aaqib Saeed"
                },
                "author": "Aaqib Saeed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13663v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13663v4",
                "updated": "2024-10-18T13:16:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    16,
                    57,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-19T16:10:26Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    16,
                    10,
                    26,
                    2,
                    171,
                    0
                ],
                "title": "Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Internals-based Answer Attribution for Trustworthy\n  Retrieval-Augmented Generation"
                },
                "summary": "Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the verifiability of model answers is a fundamental challenge for\nretrieval-augmented generation (RAG) in the question answering (QA) domain.\nRecently, self-citation prompting was proposed to make large language models\n(LLMs) generate citations to supporting documents along with their answers.\nHowever, self-citing LLMs often struggle to match the required format, refer to\nnon-existent sources, and fail to faithfully reflect LLMs' context usage\nthroughout the generation. In this work, we present MIRAGE --Model\nInternals-based RAG Explanations -- a plug-and-play approach using model\ninternals for faithful answer attribution in RAG applications. MIRAGE detects\ncontext-sensitive answer tokens and pairs them with retrieved documents\ncontributing to their prediction via saliency methods. We evaluate our proposed\napproach on a multilingual extractive QA dataset, finding high agreement with\nhuman answer attribution. On open-ended QA, MIRAGE achieves citation quality\nand efficiency comparable to self-citation while also allowing for a\nfiner-grained control of attribution parameters. Our qualitative evaluation\nhighlights the faithfulness of MIRAGE's attributions and underscores the\npromising application of model internals for RAG answer attribution."
                },
                "authors": [
                    {
                        "name": "Jirui Qi"
                    },
                    {
                        "name": "Gabriele Sarti"
                    },
                    {
                        "name": "Raquel Fernndez"
                    },
                    {
                        "name": "Arianna Bisazza"
                    }
                ],
                "author_detail": {
                    "name": "Arianna Bisazza"
                },
                "author": "Arianna Bisazza",
                "arxiv_comment": "Accepted by EMNLP 2024 Main Conference. Code and data released at\n  https://github.com/Betswish/MIRAGE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13663v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13663v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10996v2",
                "updated": "2024-10-18T12:54:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    54,
                    21,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-16T16:17:46Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    16,
                    17,
                    46,
                    6,
                    168,
                    0
                ],
                "title": "Towards Lifelong Dialogue Agents via Relation-aware Memory Construction\n  and Timeline-augmented Response Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Lifelong Dialogue Agents via Relation-aware Memory Construction\n  and Timeline-augmented Response Generation"
                },
                "summary": "To achieve lifelong human-agent interaction, dialogue agents need to\nconstantly memorize perceived information and properly retrieve it for response\ngeneration (RG). While prior work focuses on getting rid of outdated memories\nto improve retrieval quality, we argue that such memories provide rich,\nimportant contextual cues for RG (e.g., changes in user behaviors) in long-term\nconversations. We present Theanine, a framework for LLM-based lifelong dialogue\nagents. Theanine discards memory removal and manages large-scale memories by\nlinking them based on their temporal and cause-effect relation. Enabled by this\nlinking structure, Theanine augments RG with memory timelines - series of\nmemories representing the evolution or causality of relevant past events. Along\nwith Theanine, we introduce TeaFarm, a counterfactual-driven evaluation scheme,\naddressing the limitation of G-Eval and human efforts in measuring\nmemory-augmented dialogue agents. A supplementary video for Theanine and data\nfor TeaFarm are at https://huggingface.co/spaces/ResearcherScholar/Theanine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve lifelong human-agent interaction, dialogue agents need to\nconstantly memorize perceived information and properly retrieve it for response\ngeneration (RG). While prior work focuses on getting rid of outdated memories\nto improve retrieval quality, we argue that such memories provide rich,\nimportant contextual cues for RG (e.g., changes in user behaviors) in long-term\nconversations. We present Theanine, a framework for LLM-based lifelong dialogue\nagents. Theanine discards memory removal and manages large-scale memories by\nlinking them based on their temporal and cause-effect relation. Enabled by this\nlinking structure, Theanine augments RG with memory timelines - series of\nmemories representing the evolution or causality of relevant past events. Along\nwith Theanine, we introduce TeaFarm, a counterfactual-driven evaluation scheme,\naddressing the limitation of G-Eval and human efforts in measuring\nmemory-augmented dialogue agents. A supplementary video for Theanine and data\nfor TeaFarm are at https://huggingface.co/spaces/ResearcherScholar/Theanine."
                },
                "authors": [
                    {
                        "name": "Kai Tzu-iunn Ong"
                    },
                    {
                        "name": "Namyoung Kim"
                    },
                    {
                        "name": "Minju Gwak"
                    },
                    {
                        "name": "Hyungjoo Chae"
                    },
                    {
                        "name": "Taeyoon Kwon"
                    },
                    {
                        "name": "Yohan Jo"
                    },
                    {
                        "name": "Seung-won Hwang"
                    },
                    {
                        "name": "Dongha Lee"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    }
                ],
                "author_detail": {
                    "name": "Jinyoung Yeo"
                },
                "author": "Jinyoung Yeo",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.17819v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.17819v3",
                "updated": "2024-10-18T12:49:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    49,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2023-05-28T22:46:21Z",
                "published_parsed": [
                    2023,
                    5,
                    28,
                    22,
                    46,
                    21,
                    6,
                    148,
                    0
                ],
                "title": "Large Language Models, scientific knowledge and factuality: A framework\n  to streamline human expert evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models, scientific knowledge and factuality: A framework\n  to streamline human expert evaluation"
                },
                "summary": "The paper introduces a framework for the evaluation of the encoding of\nfactual scientific knowledge, designed to streamline the manual evaluation\nprocess typically conducted by domain experts. Inferring over and extracting\ninformation from Large Language Models (LLMs) trained on a large corpus of\nscientific literature can potentially define a step change in biomedical\ndiscovery, reducing the barriers for accessing and integrating existing medical\nevidence. This work explores the potential of LLMs for dialoguing with\nbiomedical background knowledge, using the context of antibiotic discovery. The\nframework involves of three evaluation steps, each assessing different aspects\nsequentially: fluency, prompt alignment, semantic coherence, factual knowledge,\nand specificity of the generated responses. By splitting these tasks between\nnon-experts and experts, the framework reduces the effort required from the\nlatter. The work provides a systematic assessment on the ability of eleven\nstate-of-the-art models LLMs, including ChatGPT, GPT-4 and Llama 2, in two\nprompting-based tasks: chemical compound definition generation and chemical\ncompound-fungus relation determination. Although recent models have improved in\nfluency, factual accuracy is still low and models are biased towards\nover-represented entities. The ability of LLMs to serve as biomedical knowledge\nbases is questioned, and the need for additional systematic evaluation\nframeworks is highlighted. While LLMs are currently not fit for purpose to be\nused as biomedical factual knowledge bases in a zero-shot setting, there is a\npromising emerging property in the direction of factuality as the models become\ndomain specialised, scale-up in size and level of human feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper introduces a framework for the evaluation of the encoding of\nfactual scientific knowledge, designed to streamline the manual evaluation\nprocess typically conducted by domain experts. Inferring over and extracting\ninformation from Large Language Models (LLMs) trained on a large corpus of\nscientific literature can potentially define a step change in biomedical\ndiscovery, reducing the barriers for accessing and integrating existing medical\nevidence. This work explores the potential of LLMs for dialoguing with\nbiomedical background knowledge, using the context of antibiotic discovery. The\nframework involves of three evaluation steps, each assessing different aspects\nsequentially: fluency, prompt alignment, semantic coherence, factual knowledge,\nand specificity of the generated responses. By splitting these tasks between\nnon-experts and experts, the framework reduces the effort required from the\nlatter. The work provides a systematic assessment on the ability of eleven\nstate-of-the-art models LLMs, including ChatGPT, GPT-4 and Llama 2, in two\nprompting-based tasks: chemical compound definition generation and chemical\ncompound-fungus relation determination. Although recent models have improved in\nfluency, factual accuracy is still low and models are biased towards\nover-represented entities. The ability of LLMs to serve as biomedical knowledge\nbases is questioned, and the need for additional systematic evaluation\nframeworks is highlighted. While LLMs are currently not fit for purpose to be\nused as biomedical factual knowledge bases in a zero-shot setting, there is a\npromising emerging property in the direction of factuality as the models become\ndomain specialised, scale-up in size and level of human feedback."
                },
                "authors": [
                    {
                        "name": "Magdalena Wysocka"
                    },
                    {
                        "name": "Oskar Wysocki"
                    },
                    {
                        "name": "Maxime Delmas"
                    },
                    {
                        "name": "Vincent Mutel"
                    },
                    {
                        "name": "Andre Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andre Freitas"
                },
                "author": "Andre Freitas",
                "arxiv_doi": "10.1016/j.jbi.2024.104724",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jbi.2024.104724",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.17819v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.17819v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the Journal of Biomedical Informatics, Volume 158,\n  October 2024, 104724",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14425v1",
                "updated": "2024-10-18T12:39:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    39,
                    32,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T12:39:32Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    39,
                    32,
                    4,
                    292,
                    0
                ],
                "title": "Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge\n  Distillation"
                },
                "summary": "Parameter-efficient fine-tuning (PEFT) can bridge the gap between large\nlanguage models (LLMs) and downstream tasks. However, PEFT has been proven\nvulnerable to malicious attacks. Research indicates that poisoned LLMs, even\nafter PEFT, retain the capability to activate internalized backdoors when input\nsamples contain predefined triggers. In this paper, we introduce a novel\nweak-to-strong unlearning algorithm to defend against backdoor attacks based on\nfeature alignment knowledge distillation, named W2SDefense. Specifically, we\nfirst train a small-scale language model through full-parameter fine-tuning to\nserve as the clean teacher model. Then, this teacher model guides the\nlarge-scale poisoned student model in unlearning the backdoor, leveraging PEFT.\nTheoretical analysis suggests that W2SDefense has the potential to enhance the\nstudent model's ability to unlearn backdoor features, preventing the activation\nof the backdoor. We conduct experiments on text classification tasks involving\nthree state-of-the-art language models and three different backdoor attack\nalgorithms. Our empirical results demonstrate the outstanding performance of\nW2SDefense in defending against backdoor attacks without compromising model\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient fine-tuning (PEFT) can bridge the gap between large\nlanguage models (LLMs) and downstream tasks. However, PEFT has been proven\nvulnerable to malicious attacks. Research indicates that poisoned LLMs, even\nafter PEFT, retain the capability to activate internalized backdoors when input\nsamples contain predefined triggers. In this paper, we introduce a novel\nweak-to-strong unlearning algorithm to defend against backdoor attacks based on\nfeature alignment knowledge distillation, named W2SDefense. Specifically, we\nfirst train a small-scale language model through full-parameter fine-tuning to\nserve as the clean teacher model. Then, this teacher model guides the\nlarge-scale poisoned student model in unlearning the backdoor, leveraging PEFT.\nTheoretical analysis suggests that W2SDefense has the potential to enhance the\nstudent model's ability to unlearn backdoor features, preventing the activation\nof the backdoor. We conduct experiments on text classification tasks involving\nthree state-of-the-art language models and three different backdoor attack\nalgorithms. Our empirical results demonstrate the outstanding performance of\nW2SDefense in defending against backdoor attacks without compromising model\nperformance."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Xiaobao Wu"
                    },
                    {
                        "name": "Cong-Duy Nguyen"
                    },
                    {
                        "name": "Meihuizi Jia"
                    },
                    {
                        "name": "Yichao Feng"
                    },
                    {
                        "name": "Luu Anh Tuan"
                    }
                ],
                "author_detail": {
                    "name": "Luu Anh Tuan"
                },
                "author": "Luu Anh Tuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10918v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10918v5",
                "updated": "2024-10-18T12:27:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    27,
                    7,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-16T12:46:40Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    12,
                    46,
                    40,
                    6,
                    168,
                    0
                ],
                "title": "Multi-LLM QA with Embodied Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-LLM QA with Embodied Exploration"
                },
                "summary": "Large language models (LLMs) have grown in popularity due to their natural\nlanguage interface and pre trained knowledge, leading to rapidly increasing\nsuccess in question-answering (QA) tasks. More recently, multi-agent systems\nwith LLM-based agents (Multi-LLM) have been utilized increasingly more for QA.\nIn these scenarios, the models may each answer the question and reach a\nconsensus or each model is specialized to answer different domain questions.\nHowever, most prior work dealing with Multi-LLM QA has focused on scenarios\nwhere the models are asked in a zero-shot manner or are given information\nsources to extract the answer. For question answering of an unknown\nenvironment, embodied exploration of the environment is first needed to answer\nthe question. This skill is necessary for personalizing embodied AI to\nenvironments such as households. There is a lack of insight into whether a\nMulti-LLM system can handle question-answering based on observations from\nembodied exploration. In this work, we address this gap by investigating the\nuse of Multi-Embodied LLM Explorers (MELE) for QA in an unknown environment.\nMultiple LLM-based agents independently explore and then answer queries about a\nhousehold environment. We analyze different aggregation methods to generate a\nsingle, final answer for each query: debating, majority voting, and training a\ncentral answer module (CAM). Using CAM, we observe a $46\\%$ higher accuracy\ncompared against the other non-learning-based aggregation methods. We provide\ncode and the query dataset for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have grown in popularity due to their natural\nlanguage interface and pre trained knowledge, leading to rapidly increasing\nsuccess in question-answering (QA) tasks. More recently, multi-agent systems\nwith LLM-based agents (Multi-LLM) have been utilized increasingly more for QA.\nIn these scenarios, the models may each answer the question and reach a\nconsensus or each model is specialized to answer different domain questions.\nHowever, most prior work dealing with Multi-LLM QA has focused on scenarios\nwhere the models are asked in a zero-shot manner or are given information\nsources to extract the answer. For question answering of an unknown\nenvironment, embodied exploration of the environment is first needed to answer\nthe question. This skill is necessary for personalizing embodied AI to\nenvironments such as households. There is a lack of insight into whether a\nMulti-LLM system can handle question-answering based on observations from\nembodied exploration. In this work, we address this gap by investigating the\nuse of Multi-Embodied LLM Explorers (MELE) for QA in an unknown environment.\nMultiple LLM-based agents independently explore and then answer queries about a\nhousehold environment. We analyze different aggregation methods to generate a\nsingle, final answer for each query: debating, majority voting, and training a\ncentral answer module (CAM). Using CAM, we observe a $46\\%$ higher accuracy\ncompared against the other non-learning-based aggregation methods. We provide\ncode and the query dataset for further research."
                },
                "authors": [
                    {
                        "name": "Bhrij Patel"
                    },
                    {
                        "name": "Vishnu Sashank Dorbala"
                    },
                    {
                        "name": "Amrit Singh Bedi"
                    },
                    {
                        "name": "Dinesh Manocha"
                    }
                ],
                "author_detail": {
                    "name": "Dinesh Manocha"
                },
                "author": "Dinesh Manocha",
                "arxiv_comment": "16 pages, 9 Figures, 5 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10918v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10918v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12950v2",
                "updated": "2024-10-18T12:19:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    19,
                    41,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-18T12:54:47Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    12,
                    54,
                    47,
                    1,
                    170,
                    0
                ],
                "title": "MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular\n  Property Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular\n  Property Prediction"
                },
                "summary": "Molecular property prediction (MPP) is a fundamental and crucial task in drug\ndiscovery. However, prior methods are limited by the requirement for a large\nnumber of labeled molecules and their restricted ability to generalize for\nunseen and new tasks, both of which are essential for real-world applications.\nTo address these challenges, we present MolecularGPT for few-shot MPP. From a\nperspective on instruction tuning, we fine-tune large language models (LLMs)\nbased on curated molecular instructions spanning over 1000 property prediction\ntasks. This enables building a versatile and specialized LLM that can be\nadapted to novel MPP tasks without any fine-tuning through zero- and few-shot\nin-context learning (ICL). MolecularGPT exhibits competitive in-context\nreasoning capabilities across 10 downstream evaluation datasets, setting new\nbenchmarks for few-shot molecular prediction tasks. More importantly, with just\ntwo-shot examples, MolecularGPT can outperform standard supervised graph neural\nnetwork methods on 4 out of 7 datasets. It also excels state-of-the-art LLM\nbaselines by up to 15.7% increase on classification accuracy and decrease of\n17.9 on regression metrics (e.g., RMSE) under zero-shot. This study\ndemonstrates the potential of LLMs as effective few-shot molecular property\npredictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular property prediction (MPP) is a fundamental and crucial task in drug\ndiscovery. However, prior methods are limited by the requirement for a large\nnumber of labeled molecules and their restricted ability to generalize for\nunseen and new tasks, both of which are essential for real-world applications.\nTo address these challenges, we present MolecularGPT for few-shot MPP. From a\nperspective on instruction tuning, we fine-tune large language models (LLMs)\nbased on curated molecular instructions spanning over 1000 property prediction\ntasks. This enables building a versatile and specialized LLM that can be\nadapted to novel MPP tasks without any fine-tuning through zero- and few-shot\nin-context learning (ICL). MolecularGPT exhibits competitive in-context\nreasoning capabilities across 10 downstream evaluation datasets, setting new\nbenchmarks for few-shot molecular prediction tasks. More importantly, with just\ntwo-shot examples, MolecularGPT can outperform standard supervised graph neural\nnetwork methods on 4 out of 7 datasets. It also excels state-of-the-art LLM\nbaselines by up to 15.7% increase on classification accuracy and decrease of\n17.9 on regression metrics (e.g., RMSE) under zero-shot. This study\ndemonstrates the potential of LLMs as effective few-shot molecular property\npredictors. The code is available at https://github.com/NYUSHCS/MolecularGPT."
                },
                "authors": [
                    {
                        "name": "Yuyan Liu"
                    },
                    {
                        "name": "Sirui Ding"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Qiaoyu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Qiaoyu Tan"
                },
                "author": "Qiaoyu Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14399v1",
                "updated": "2024-10-18T12:02:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    2,
                    41,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T12:02:41Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    12,
                    2,
                    41,
                    4,
                    292,
                    0
                ],
                "title": "SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic\n  Reasoning"
                },
                "summary": "Syllogistic reasoning is crucial for Natural Language Inference (NLI). This\ncapability is particularly significant in specialized domains such as\nbiomedicine, where it can support automatic evidence interpretation and\nscientific discovery. This paper presents SylloBio-NLI, a novel framework that\nleverages external ontologies to systematically instantiate diverse syllogistic\narguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language\nModels (LLMs) on identifying valid conclusions and extracting supporting\nevidence across 28 syllogistic schemes instantiated with human genome pathways.\nExtensive experiments reveal that biomedical syllogistic reasoning is\nparticularly challenging for zero-shot LLMs, which achieve an average accuracy\nbetween 70% on generalized modus ponens and 23% on disjunctive syllogism. At\nthe same time, we found that few-shot prompting can boost the performance of\ndifferent LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper\nanalysis shows that both techniques exhibit high sensitivity to superficial\nlexical variations, highlighting a dependency between reliability, models'\narchitecture, and pre-training regime. Overall, our results indicate that,\nwhile in-context examples have the potential to elicit syllogistic reasoning in\nLLMs, existing models are still far from achieving the robustness and\nconsistency required for safe biomedical NLI applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Syllogistic reasoning is crucial for Natural Language Inference (NLI). This\ncapability is particularly significant in specialized domains such as\nbiomedicine, where it can support automatic evidence interpretation and\nscientific discovery. This paper presents SylloBio-NLI, a novel framework that\nleverages external ontologies to systematically instantiate diverse syllogistic\narguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language\nModels (LLMs) on identifying valid conclusions and extracting supporting\nevidence across 28 syllogistic schemes instantiated with human genome pathways.\nExtensive experiments reveal that biomedical syllogistic reasoning is\nparticularly challenging for zero-shot LLMs, which achieve an average accuracy\nbetween 70% on generalized modus ponens and 23% on disjunctive syllogism. At\nthe same time, we found that few-shot prompting can boost the performance of\ndifferent LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper\nanalysis shows that both techniques exhibit high sensitivity to superficial\nlexical variations, highlighting a dependency between reliability, models'\narchitecture, and pre-training regime. Overall, our results indicate that,\nwhile in-context examples have the potential to elicit syllogistic reasoning in\nLLMs, existing models are still far from achieving the robustness and\nconsistency required for safe biomedical NLI applications."
                },
                "authors": [
                    {
                        "name": "Magdalena Wysocka"
                    },
                    {
                        "name": "Danilo S. Carvalho"
                    },
                    {
                        "name": "Oskar Wysocki"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "Andre Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andre Freitas"
                },
                "author": "Andre Freitas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14391v1",
                "updated": "2024-10-18T11:52:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    52,
                    10,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T11:52:10Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    52,
                    10,
                    4,
                    292,
                    0
                ],
                "title": "Analyzing Context Utilization of LLMs in Document-Level Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Context Utilization of LLMs in Document-Level Translation"
                },
                "summary": "Large language models (LLM) are increasingly strong contenders in machine\ntranslation. We study document-level translation, where some words cannot be\ntranslated without context from outside the sentence. We investigate the\nability of prominent LLMs to utilize context by analyzing models' robustness to\nperturbed and randomized document context. We find that LLMs' improved\ndocument-translation performance is not always reflected in pronoun translation\nperformance. We highlight the need for context-aware finetuning of LLMs with a\nfocus on relevant parts of the context to improve their reliability for\ndocument-level translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) are increasingly strong contenders in machine\ntranslation. We study document-level translation, where some words cannot be\ntranslated without context from outside the sentence. We investigate the\nability of prominent LLMs to utilize context by analyzing models' robustness to\nperturbed and randomized document context. We find that LLMs' improved\ndocument-translation performance is not always reflected in pronoun translation\nperformance. We highlight the need for context-aware finetuning of LLMs with a\nfocus on relevant parts of the context to improve their reliability for\ndocument-level translation."
                },
                "authors": [
                    {
                        "name": "Wafaa Mohammed"
                    },
                    {
                        "name": "Vlad Niculae"
                    }
                ],
                "author_detail": {
                    "name": "Vlad Niculae"
                },
                "author": "Vlad Niculae",
                "arxiv_comment": "4 pages, 2 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14387v1",
                "updated": "2024-10-18T11:39:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    39,
                    34,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T11:39:34Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    11,
                    39,
                    34,
                    4,
                    292,
                    0
                ],
                "title": "How Do Multilingual Models Remember? Investigating Multilingual Factual\n  Recall Mechanisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do Multilingual Models Remember? Investigating Multilingual Factual\n  Recall Mechanisms"
                },
                "summary": "Large Language Models (LLMs) store and retrieve vast amounts of factual\nknowledge acquired during pre-training. Prior research has localized and\nidentified mechanisms behind knowledge recall; however, it has primarily\nfocused on English monolingual models. The question of how these processes\ngeneralize to other languages and multilingual LLMs remains unexplored. In this\npaper, we address this gap by conducting a comprehensive analysis of two highly\nmultilingual LLMs. We assess the extent to which previously identified\ncomponents and mechanisms of factual recall in English apply to a multilingual\ncontext. Then, we examine when language plays a role in the recall process,\nuncovering evidence of language-independent and language-dependent mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) store and retrieve vast amounts of factual\nknowledge acquired during pre-training. Prior research has localized and\nidentified mechanisms behind knowledge recall; however, it has primarily\nfocused on English monolingual models. The question of how these processes\ngeneralize to other languages and multilingual LLMs remains unexplored. In this\npaper, we address this gap by conducting a comprehensive analysis of two highly\nmultilingual LLMs. We assess the extent to which previously identified\ncomponents and mechanisms of factual recall in English apply to a multilingual\ncontext. Then, we examine when language plays a role in the recall process,\nuncovering evidence of language-independent and language-dependent mechanisms."
                },
                "authors": [
                    {
                        "name": "Constanza Fierro"
                    },
                    {
                        "name": "Negar Foroutan"
                    },
                    {
                        "name": "Desmond Elliott"
                    },
                    {
                        "name": "Anders Sgaard"
                    }
                ],
                "author_detail": {
                    "name": "Anders Sgaard"
                },
                "author": "Anders Sgaard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14368v1",
                "updated": "2024-10-18T10:53:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    53,
                    44,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T10:53:44Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    53,
                    44,
                    4,
                    292,
                    0
                ],
                "title": "CoMAL: Collaborative Multi-Agent Large Language Models for\n  Mixed-Autonomy Traffic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoMAL: Collaborative Multi-Agent Large Language Models for\n  Mixed-Autonomy Traffic"
                },
                "summary": "The integration of autonomous vehicles into urban traffic has great potential\nto improve efficiency by reducing congestion and optimizing traffic flow\nsystematically. In this paper, we introduce CoMAL (Collaborative Multi-Agent\nLLMs), a framework designed to address the mixed-autonomy traffic problem by\ncollaboration among autonomous vehicles to optimize traffic flow. CoMAL is\nbuilt upon large language models, operating in an interactive traffic\nsimulation environment. It utilizes a Perception Module to observe surrounding\nagents and a Memory Module to store strategies for each agent. The overall\nworkflow includes a Collaboration Module that encourages autonomous vehicles to\ndiscuss the effective strategy and allocate roles, a reasoning engine to\ndetermine optimal behaviors based on assigned roles, and an Execution Module\nthat controls vehicle actions using a hybrid approach combining rule-based\nmodels. Experimental results demonstrate that CoMAL achieves superior\nperformance on the Flow benchmark. Additionally, we evaluate the impact of\ndifferent language models and compare our framework with reinforcement learning\napproaches. It highlights the strong cooperative capability of LLM agents and\npresents a promising solution to the mixed-autonomy traffic challenge. The code\nis available at https://github.com/Hyan-Yao/CoMAL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of autonomous vehicles into urban traffic has great potential\nto improve efficiency by reducing congestion and optimizing traffic flow\nsystematically. In this paper, we introduce CoMAL (Collaborative Multi-Agent\nLLMs), a framework designed to address the mixed-autonomy traffic problem by\ncollaboration among autonomous vehicles to optimize traffic flow. CoMAL is\nbuilt upon large language models, operating in an interactive traffic\nsimulation environment. It utilizes a Perception Module to observe surrounding\nagents and a Memory Module to store strategies for each agent. The overall\nworkflow includes a Collaboration Module that encourages autonomous vehicles to\ndiscuss the effective strategy and allocate roles, a reasoning engine to\ndetermine optimal behaviors based on assigned roles, and an Execution Module\nthat controls vehicle actions using a hybrid approach combining rule-based\nmodels. Experimental results demonstrate that CoMAL achieves superior\nperformance on the Flow benchmark. Additionally, we evaluate the impact of\ndifferent language models and compare our framework with reinforcement learning\napproaches. It highlights the strong cooperative capability of LLM agents and\npresents a promising solution to the mixed-autonomy traffic challenge. The code\nis available at https://github.com/Hyan-Yao/CoMAL."
                },
                "authors": [
                    {
                        "name": "Huaiyuan Yao"
                    },
                    {
                        "name": "Longchao Da"
                    },
                    {
                        "name": "Vishnu Nandam"
                    },
                    {
                        "name": "Justin Turnau"
                    },
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Linsey Pang"
                    },
                    {
                        "name": "Hua Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wei"
                },
                "author": "Hua Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01129v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01129v4",
                "updated": "2024-10-18T10:46:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    46,
                    43,
                    4,
                    292,
                    0
                ],
                "published": "2024-08-02T09:18:41Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    9,
                    18,
                    41,
                    4,
                    215,
                    0
                ],
                "title": "A Survey of Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Mamba"
                },
                "summary": "As one of the most representative DL techniques, Transformer architecture has\nempowered numerous advanced models, especially the large language models (LLMs)\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space models\n(SSMs), has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering three main aspects:\nthe advancements of Mamba-based models, the techniques of adapting Mamba to\ndiverse data, and the applications where Mamba can excel. Specifically, we\nfirst review the foundational knowledge of various representative deep learning\nmodels and the details of Mamba-1&2 as preliminaries. Then, to showcase the\nsignificance of Mamba for AI, we comprehensively review the related studies\nfocusing on Mamba models' architecture design, data adaptability, and\napplications. Finally, we present a discussion of current limitations and\nexplore various promising research directions to provide deeper insights for\nfuture investigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As one of the most representative DL techniques, Transformer architecture has\nempowered numerous advanced models, especially the large language models (LLMs)\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space models\n(SSMs), has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering three main aspects:\nthe advancements of Mamba-based models, the techniques of adapting Mamba to\ndiverse data, and the applications where Mamba can excel. Specifically, we\nfirst review the foundational knowledge of various representative deep learning\nmodels and the details of Mamba-1&2 as preliminaries. Then, to showcase the\nsignificance of Mamba for AI, we comprehensively review the related studies\nfocusing on Mamba models' architecture design, data adaptability, and\napplications. Finally, we present a discussion of current limitations and\nexplore various promising research directions to provide deeper insights for\nfuture investigations."
                },
                "authors": [
                    {
                        "name": "Haohao Qu"
                    },
                    {
                        "name": "Liangbo Ning"
                    },
                    {
                        "name": "Rui An"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Tyler Derr"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01129v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01129v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.11244v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.11244v4",
                "updated": "2024-10-18T10:21:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    21,
                    31,
                    4,
                    292,
                    0
                ],
                "published": "2023-10-17T13:12:32Z",
                "published_parsed": [
                    2023,
                    10,
                    17,
                    13,
                    12,
                    32,
                    1,
                    290,
                    0
                ],
                "title": "Entity Matching using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity Matching using Large Language Models"
                },
                "summary": "Entity matching is the task of deciding whether two entity descriptions refer\nto the same real-world entity. Entity matching is a central step in most data\nintegration pipelines. Many state-of-the-art entity matching methods rely on\npre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks\nof these models for entity matching are that (i) the models require significant\namounts of task-specific training data and (ii) the fine-tuned models are not\nrobust concerning out-of-distribution entities. This paper investigates using\ngenerative large language models (LLMs) as a less task-specific training\ndata-dependent and more robust alternative to PLM-based matchers. The study\ncovers hosted and open-source LLMs which can be run locally. We evaluate these\nmodels in a zero-shot scenario and a scenario where task-specific training data\nis available. We compare different prompt designs and the prompt sensitivity of\nthe models. We show that there is no single best prompt but that the prompt\nneeds to be tuned for each model/dataset combination. We further investigate\n(i) the selection of in-context demonstrations, (ii) the generation of matching\nrules, as well as (iii) fine-tuning LLMs using the same pool of training data.\nOur experiments show that the best LLMs require no or only a few training\nexamples to perform comparably to PLMs that were fine-tuned using thousands of\nexamples. LLM-based matchers further exhibit higher robustness to unseen\nentities. We show that GPT4 can generate structured explanations for matching\ndecisions and can automatically identify potential causes of matching errors by\nanalyzing explanations of wrong decisions. We demonstrate that the model can\ngenerate meaningful textual descriptions of the identified error classes, which\ncan help data engineers to improve entity matching pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entity matching is the task of deciding whether two entity descriptions refer\nto the same real-world entity. Entity matching is a central step in most data\nintegration pipelines. Many state-of-the-art entity matching methods rely on\npre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks\nof these models for entity matching are that (i) the models require significant\namounts of task-specific training data and (ii) the fine-tuned models are not\nrobust concerning out-of-distribution entities. This paper investigates using\ngenerative large language models (LLMs) as a less task-specific training\ndata-dependent and more robust alternative to PLM-based matchers. The study\ncovers hosted and open-source LLMs which can be run locally. We evaluate these\nmodels in a zero-shot scenario and a scenario where task-specific training data\nis available. We compare different prompt designs and the prompt sensitivity of\nthe models. We show that there is no single best prompt but that the prompt\nneeds to be tuned for each model/dataset combination. We further investigate\n(i) the selection of in-context demonstrations, (ii) the generation of matching\nrules, as well as (iii) fine-tuning LLMs using the same pool of training data.\nOur experiments show that the best LLMs require no or only a few training\nexamples to perform comparably to PLMs that were fine-tuned using thousands of\nexamples. LLM-based matchers further exhibit higher robustness to unseen\nentities. We show that GPT4 can generate structured explanations for matching\ndecisions and can automatically identify potential causes of matching errors by\nanalyzing explanations of wrong decisions. We demonstrate that the model can\ngenerate meaningful textual descriptions of the identified error classes, which\ncan help data engineers to improve entity matching pipelines."
                },
                "authors": [
                    {
                        "name": "Ralph Peeters"
                    },
                    {
                        "name": "Aaron Steiner"
                    },
                    {
                        "name": "Christian Bizer"
                    }
                ],
                "author_detail": {
                    "name": "Christian Bizer"
                },
                "author": "Christian Bizer",
                "arxiv_comment": "Published in Proceedings of the 28th International Conference on\n  Extending Database Technology (EDBT), 25th March-28th March, 2025, ISBN\n  978-3-89318-098-1 on OpenProceedings.org",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.11244v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.11244v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19700v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19700v3",
                "updated": "2024-10-18T10:15:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    15,
                    29,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-29T13:16:37Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    13,
                    16,
                    37,
                    6,
                    273,
                    0
                ],
                "title": "2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding\n  for Large Language Models"
                },
                "summary": "Tables are ubiquitous across various domains for concisely representing\nstructured information. Empowering large language models (LLMs) to reason over\ntabular data represents an actively explored direction. However, since typical\nLLMs only support one-dimensional~(1D) inputs, existing methods often flatten\nthe two-dimensional~(2D) table structure into a sequence of tokens, which can\nseverely disrupt the spatial relationships and result in an inevitable loss of\nvital contextual information. In this paper, we first empirically demonstrate\nthe detrimental impact of such flattening operations on the performance of LLMs\nin capturing the spatial information of tables through two elaborate proxy\ntasks. Subsequently, we introduce a simple yet effective positional encoding\nmethod, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to\naddress this challenge. 2D-TPE enables each attention head to dynamically\nselect a permutation order of tokens within the context for attending to them,\nwhere each permutation represents a distinct traversal mode for the table, such\nas column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of\nlosing essential spatial information while preserving computational efficiency,\nthus better preserving the table structure. Extensive experiments across five\nbenchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring\nthe importance of preserving the table structure for accurate table\ncomprehension. Comprehensive analysis further reveals the substantially better\nscalability of 2D-TPE to large tables than baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables are ubiquitous across various domains for concisely representing\nstructured information. Empowering large language models (LLMs) to reason over\ntabular data represents an actively explored direction. However, since typical\nLLMs only support one-dimensional~(1D) inputs, existing methods often flatten\nthe two-dimensional~(2D) table structure into a sequence of tokens, which can\nseverely disrupt the spatial relationships and result in an inevitable loss of\nvital contextual information. In this paper, we first empirically demonstrate\nthe detrimental impact of such flattening operations on the performance of LLMs\nin capturing the spatial information of tables through two elaborate proxy\ntasks. Subsequently, we introduce a simple yet effective positional encoding\nmethod, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to\naddress this challenge. 2D-TPE enables each attention head to dynamically\nselect a permutation order of tokens within the context for attending to them,\nwhere each permutation represents a distinct traversal mode for the table, such\nas column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of\nlosing essential spatial information while preserving computational efficiency,\nthus better preserving the table structure. Extensive experiments across five\nbenchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring\nthe importance of preserving the table structure for accurate table\ncomprehension. Comprehensive analysis further reveals the substantially better\nscalability of 2D-TPE to large tables than baselines."
                },
                "authors": [
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19700v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19700v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14340v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14340v1",
                "updated": "2024-10-18T09:51:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    51,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T09:51:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    51,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "Zero-shot Action Localization via the Confidence of Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot Action Localization via the Confidence of Large\n  Vision-Language Models"
                },
                "summary": "Precise action localization in untrimmed video is vital for fields such as\nprofessional sports and minimally invasive surgery, where the delineation of\nparticular motions in recordings can dramatically enhance analysis. But in many\ncases, large scale datasets with video-label pairs for localization are\nunavailable, limiting the opportunity to fine-tune video-understanding models.\nRecent developments in large vision-language models (LVLM) address this need\nwith impressive zero-shot capabilities in a variety of video understanding\ntasks. However, the adaptation of image-based LVLMs, with their powerful visual\nquestion answering capabilities, to action localization in long-form video is\nstill relatively unexplored. To this end, we introduce a true ZEro-shot Action\nLocalization method (ZEAL). Specifically, we leverage the built-in action\nknowledge of a large language model (LLM) to inflate actions into\nhighly-detailed descriptions of the archetypal start and end of the action.\nThese descriptions serve as queries to LVLM for generating frame-level\nconfidence scores which can be aggregated to produce localization outputs. The\nsimplicity and flexibility of our method lends it amenable to more capable\nLVLMs as they are developed, and we demonstrate remarkable results in zero-shot\naction localization on a challenging benchmark, without any training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise action localization in untrimmed video is vital for fields such as\nprofessional sports and minimally invasive surgery, where the delineation of\nparticular motions in recordings can dramatically enhance analysis. But in many\ncases, large scale datasets with video-label pairs for localization are\nunavailable, limiting the opportunity to fine-tune video-understanding models.\nRecent developments in large vision-language models (LVLM) address this need\nwith impressive zero-shot capabilities in a variety of video understanding\ntasks. However, the adaptation of image-based LVLMs, with their powerful visual\nquestion answering capabilities, to action localization in long-form video is\nstill relatively unexplored. To this end, we introduce a true ZEro-shot Action\nLocalization method (ZEAL). Specifically, we leverage the built-in action\nknowledge of a large language model (LLM) to inflate actions into\nhighly-detailed descriptions of the archetypal start and end of the action.\nThese descriptions serve as queries to LVLM for generating frame-level\nconfidence scores which can be aggregated to produce localization outputs. The\nsimplicity and flexibility of our method lends it amenable to more capable\nLVLMs as they are developed, and we demonstrate remarkable results in zero-shot\naction localization on a challenging benchmark, without any training."
                },
                "authors": [
                    {
                        "name": "Josiah Aklilu"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14340v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14340v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13281v2",
                "updated": "2024-10-18T09:50:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    50,
                    41,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-17T07:15:15Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    7,
                    15,
                    15,
                    3,
                    291,
                    0
                ],
                "title": "BANTH: A Multi-label Hate Speech Detection Dataset for Transliterated\n  Bangla",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BANTH: A Multi-label Hate Speech Detection Dataset for Transliterated\n  Bangla"
                },
                "summary": "The proliferation of transliterated texts in digital spaces has emphasized\nthe need for detecting and classifying hate speech in languages beyond English,\nparticularly in low-resource languages. As online discourse can perpetuate\ndiscrimination based on target groups, e.g. gender, religion, and origin,\nmulti-label classification of hateful content can help in comprehending hate\nmotivation and enhance content moderation. While previous efforts have focused\non monolingual or binary hate classification tasks, no work has yet addressed\nthe challenge of multi-label hate speech classification in transliterated\nBangla. We introduce BanTH, the first multi-label transliterated Bangla hate\nspeech dataset comprising 37.3k samples. The samples are sourced from YouTube\ncomments, where each instance is labeled with one or more target groups,\nreflecting the regional demographic. We establish novel transformer\nencoder-based baselines by further pre-training on transliterated Bangla\ncorpus. We also propose a novel translation-based LLM prompting strategy for\ntransliterated text. Experiments reveal that our further pre-trained encoders\nare achieving state-of-the-art performance on the BanTH dataset, while our\ntranslation-based prompting outperforms other strategies in the zero-shot\nsetting. The introduction of BanTH not only fills a critical gap in hate speech\nresearch for Bangla but also sets the stage for future exploration into\ncode-mixed and multi-label classification challenges in underrepresented\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of transliterated texts in digital spaces has emphasized\nthe need for detecting and classifying hate speech in languages beyond English,\nparticularly in low-resource languages. As online discourse can perpetuate\ndiscrimination based on target groups, e.g. gender, religion, and origin,\nmulti-label classification of hateful content can help in comprehending hate\nmotivation and enhance content moderation. While previous efforts have focused\non monolingual or binary hate classification tasks, no work has yet addressed\nthe challenge of multi-label hate speech classification in transliterated\nBangla. We introduce BanTH, the first multi-label transliterated Bangla hate\nspeech dataset comprising 37.3k samples. The samples are sourced from YouTube\ncomments, where each instance is labeled with one or more target groups,\nreflecting the regional demographic. We establish novel transformer\nencoder-based baselines by further pre-training on transliterated Bangla\ncorpus. We also propose a novel translation-based LLM prompting strategy for\ntransliterated text. Experiments reveal that our further pre-trained encoders\nare achieving state-of-the-art performance on the BanTH dataset, while our\ntranslation-based prompting outperforms other strategies in the zero-shot\nsetting. The introduction of BanTH not only fills a critical gap in hate speech\nresearch for Bangla but also sets the stage for future exploration into\ncode-mixed and multi-label classification challenges in underrepresented\nlanguages."
                },
                "authors": [
                    {
                        "name": "Fabiha Haider"
                    },
                    {
                        "name": "Fariha Tanjim Shifat"
                    },
                    {
                        "name": "Md Farhan Ishmam"
                    },
                    {
                        "name": "Deeparghya Dutta Barua"
                    },
                    {
                        "name": "Md Sakib Ul Rahman Sourove"
                    },
                    {
                        "name": "Md Fahim"
                    },
                    {
                        "name": "Md Farhad Alam"
                    }
                ],
                "author_detail": {
                    "name": "Md Farhad Alam"
                },
                "author": "Md Farhad Alam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14335v1",
                "updated": "2024-10-18T09:46:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    46,
                    38,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T09:46:38Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    46,
                    38,
                    4,
                    292,
                    0
                ],
                "title": "Critical Questions Generation: Motivation and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical Questions Generation: Motivation and Challenges"
                },
                "summary": "The development of Large Language Models (LLMs) has brought impressive\nperformances on mitigation strategies against misinformation, such as\ncounterargument generation. However, LLMs are still seriously hindered by\noutdated knowledge and by their tendency to generate hallucinated content. In\norder to circumvent these issues, we propose a new task, namely, Critical\nQuestions Generation, consisting of processing an argumentative text to\ngenerate the critical questions (CQs) raised by it. In argumentation theory CQs\nare tools designed to lay bare the blind spots of an argument by pointing at\nthe information it could be missing. Thus, instead of trying to deploy LLMs to\nproduce knowledgeable and relevant counterarguments, we use them to question\narguments, without requiring any external knowledge. Research on CQs Generation\nusing LLMs requires a reference dataset for large scale experimentation. Thus,\nin this work we investigate two complementary methods to create such a\nresource: (i) instantiating CQs templates as defined by Walton's argumentation\ntheory and (ii), using LLMs as CQs generators. By doing so, we contribute with\na procedure to establish what is a valid CQ and conclude that, while LLMs are\nreasonable CQ generators, they still have a wide margin for improvement in this\ntask.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Large Language Models (LLMs) has brought impressive\nperformances on mitigation strategies against misinformation, such as\ncounterargument generation. However, LLMs are still seriously hindered by\noutdated knowledge and by their tendency to generate hallucinated content. In\norder to circumvent these issues, we propose a new task, namely, Critical\nQuestions Generation, consisting of processing an argumentative text to\ngenerate the critical questions (CQs) raised by it. In argumentation theory CQs\nare tools designed to lay bare the blind spots of an argument by pointing at\nthe information it could be missing. Thus, instead of trying to deploy LLMs to\nproduce knowledgeable and relevant counterarguments, we use them to question\narguments, without requiring any external knowledge. Research on CQs Generation\nusing LLMs requires a reference dataset for large scale experimentation. Thus,\nin this work we investigate two complementary methods to create such a\nresource: (i) instantiating CQs templates as defined by Walton's argumentation\ntheory and (ii), using LLMs as CQs generators. By doing so, we contribute with\na procedure to establish what is a valid CQ and conclude that, while LLMs are\nreasonable CQ generators, they still have a wide margin for improvement in this\ntask."
                },
                "authors": [
                    {
                        "name": "Blanca Calvo Figueras"
                    },
                    {
                        "name": "Rodrigo Agerri"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Agerri"
                },
                "author": "Rodrigo Agerri",
                "arxiv_comment": "14 pages, 3 figures, 7 tables, to be published in the 28th Conference\n  on Computational Natural Language Learning (CoNLL 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14332v1",
                "updated": "2024-10-18T09:44:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    44,
                    25,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T09:44:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    44,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have catalyzed the\ndevelopment of Large Multimodal Models (LMMs). However, existing research\nprimarily focuses on tuning language and image instructions, ignoring the\ncritical pretraining phase where models learn to process textual and visual\nmodalities jointly. In this paper, we propose a new pretraining paradigm for\nLMMs to enhance the visual comprehension capabilities of LLMs by introducing a\nnovel cross-modal comprehension stage. Specifically, we design a dynamically\nlearnable prompt token pool and employ the Hungarian algorithm to replace part\nof the original visual tokens with the most relevant prompt tokens. Then, we\nconceptualize visual tokens as analogous to a \"foreign language\" for the LLMs\nand propose a mixed attention mechanism with bidirectional visual attention and\nunidirectional textual attention to comprehensively enhance the understanding\nof visual tokens. Meanwhile, we integrate a detailed caption generation task,\nleveraging rich descriptions to further facilitate LLMs in understanding visual\nsemantic information. After pretraining on 1.5 million publicly accessible\ndata, we present a new foundation model called Croc. Experimental results\ndemonstrate that Croc achieves new state-of-the-art performance on massive\nvision-language benchmarks. To support reproducibility and facilitate further\nresearch, we release the training code and pre-trained model weights at\nhttps://github.com/deepglint/Croc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have catalyzed the\ndevelopment of Large Multimodal Models (LMMs). However, existing research\nprimarily focuses on tuning language and image instructions, ignoring the\ncritical pretraining phase where models learn to process textual and visual\nmodalities jointly. In this paper, we propose a new pretraining paradigm for\nLMMs to enhance the visual comprehension capabilities of LLMs by introducing a\nnovel cross-modal comprehension stage. Specifically, we design a dynamically\nlearnable prompt token pool and employ the Hungarian algorithm to replace part\nof the original visual tokens with the most relevant prompt tokens. Then, we\nconceptualize visual tokens as analogous to a \"foreign language\" for the LLMs\nand propose a mixed attention mechanism with bidirectional visual attention and\nunidirectional textual attention to comprehensively enhance the understanding\nof visual tokens. Meanwhile, we integrate a detailed caption generation task,\nleveraging rich descriptions to further facilitate LLMs in understanding visual\nsemantic information. After pretraining on 1.5 million publicly accessible\ndata, we present a new foundation model called Croc. Experimental results\ndemonstrate that Croc achieves new state-of-the-art performance on massive\nvision-language benchmarks. To support reproducibility and facilitate further\nresearch, we release the training code and pre-trained model weights at\nhttps://github.com/deepglint/Croc."
                },
                "authors": [
                    {
                        "name": "Yin Xie"
                    },
                    {
                        "name": "Kaicheng Yang"
                    },
                    {
                        "name": "Ninghua Yang"
                    },
                    {
                        "name": "Weimo Deng"
                    },
                    {
                        "name": "Xiangzi Dai"
                    },
                    {
                        "name": "Tiancheng Gu"
                    },
                    {
                        "name": "Yumeng Wang"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Yongle Zhao"
                    },
                    {
                        "name": "Ziyong Feng"
                    },
                    {
                        "name": "Jiankang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Jiankang Deng"
                },
                "author": "Jiankang Deng",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14331v1",
                "updated": "2024-10-18T09:43:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    43,
                    30,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T09:43:30Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    43,
                    30,
                    4,
                    292,
                    0
                ],
                "title": "ChartifyText: Automated Chart Generation from Data-Involved Texts via\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChartifyText: Automated Chart Generation from Data-Involved Texts via\n  LLM"
                },
                "summary": "Text documents with numerical values involved are widely used in various\napplications such as scientific research, economy, public health and\njournalism. However, it is difficult for readers to quickly interpret such\ndata-involved texts and gain deep insights. To fill this research gap, this\nwork aims to automatically generate charts to accurately convey the underlying\ndata and ideas to readers, which is essentially a challenging task. The\nchallenges originate from text ambiguities, intrinsic sparsity and uncertainty\nof data in text documents, and subjective sentiment differences. Specifically,\nwe propose ChartifyText, a novel fully-automated approach that leverages Large\nLanguage Models (LLMs) to convert complex data-involved texts to expressive\ncharts. It consists of two major modules: tabular data inference and expressive\nchart generation. The tabular data inference module employs systematic prompt\nengineering to guide the LLM (e.g., GPT-4) to infer table data, where data\nranges, uncertainties, missing data values and corresponding subjective\nsentiments are explicitly considered. The expressive chart generation module\naugments standard charts with intuitive visual encodings and concise texts to\naccurately convey the underlying data and insights. We extensively evaluate the\neffectiveness of ChartifyText on real-world data-involved text documents\nthrough case studies, in-depth interviews with three visualization experts, and\na carefully-designed user study with 15 participants. The results demonstrate\nthe usefulness and effectiveness of ChartifyText in helping readers efficiently\nand effectively make sense of data-involved texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text documents with numerical values involved are widely used in various\napplications such as scientific research, economy, public health and\njournalism. However, it is difficult for readers to quickly interpret such\ndata-involved texts and gain deep insights. To fill this research gap, this\nwork aims to automatically generate charts to accurately convey the underlying\ndata and ideas to readers, which is essentially a challenging task. The\nchallenges originate from text ambiguities, intrinsic sparsity and uncertainty\nof data in text documents, and subjective sentiment differences. Specifically,\nwe propose ChartifyText, a novel fully-automated approach that leverages Large\nLanguage Models (LLMs) to convert complex data-involved texts to expressive\ncharts. It consists of two major modules: tabular data inference and expressive\nchart generation. The tabular data inference module employs systematic prompt\nengineering to guide the LLM (e.g., GPT-4) to infer table data, where data\nranges, uncertainties, missing data values and corresponding subjective\nsentiments are explicitly considered. The expressive chart generation module\naugments standard charts with intuitive visual encodings and concise texts to\naccurately convey the underlying data and insights. We extensively evaluate the\neffectiveness of ChartifyText on real-world data-involved text documents\nthrough case studies, in-depth interviews with three visualization experts, and\na carefully-designed user study with 15 participants. The results demonstrate\nthe usefulness and effectiveness of ChartifyText in helping readers efficiently\nand effectively make sense of data-involved texts."
                },
                "authors": [
                    {
                        "name": "Songheng Zhang"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Toby Jia-Jun Li"
                    },
                    {
                        "name": "Qiaomu Shen"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Yong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wang"
                },
                "author": "Yong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06750v2",
                "updated": "2024-10-18T09:43:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    43,
                    9,
                    4,
                    292,
                    0
                ],
                "published": "2024-04-10T05:34:07Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    5,
                    34,
                    7,
                    2,
                    101,
                    0
                ],
                "title": "Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of\n  Language Model Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of\n  Language Model Agents"
                },
                "summary": "Some have criticised Generative AI Systems for replicating the familiar\npathologies of already widely-deployed AI systems. Other critics highlight how\nthey foreshadow vastly more powerful future systems, which might threaten\nhumanity's survival. The first group says there is nothing new here; the other\nlooks through the present to a perhaps distant horizon. In this paper, I\ninstead pay attention to what makes these particular systems distinctive: both\ntheir remarkable scientific achievement, and the most likely and consequential\nways in which they will change society over the next five to ten years. In\nparticular, I explore the potential societal impacts and normative questions\nraised by the looming prospect of 'Language Model Agents', in which multimodal\nlarge language models (LLMs) form the executive centre of complex, tool-using\nAI systems that can take unsupervised sequences of actions towards some goal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some have criticised Generative AI Systems for replicating the familiar\npathologies of already widely-deployed AI systems. Other critics highlight how\nthey foreshadow vastly more powerful future systems, which might threaten\nhumanity's survival. The first group says there is nothing new here; the other\nlooks through the present to a perhaps distant horizon. In this paper, I\ninstead pay attention to what makes these particular systems distinctive: both\ntheir remarkable scientific achievement, and the most likely and consequential\nways in which they will change society over the next five to ten years. In\nparticular, I explore the potential societal impacts and normative questions\nraised by the looming prospect of 'Language Model Agents', in which multimodal\nlarge language models (LLMs) form the executive centre of complex, tool-using\nAI systems that can take unsupervised sequences of actions towards some goal."
                },
                "authors": [
                    {
                        "name": "Seth Lazar"
                    }
                ],
                "author_detail": {
                    "name": "Seth Lazar"
                },
                "author": "Seth Lazar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14321v1",
                "updated": "2024-10-18T09:32:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    32,
                    8,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T09:32:08Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    32,
                    8,
                    4,
                    292,
                    0
                ],
                "title": "From Solitary Directives to Interactive Encouragement! LLM Secure Code\n  Generation by Natural Language Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Solitary Directives to Interactive Encouragement! LLM Secure Code\n  Generation by Natural Language Prompting"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable potential in code\ngeneration, making them increasingly important in the field. However, the\nsecurity issues of generated code have not been fully addressed, and the\nusability of LLMs in code generation still requires further exploration.\n  This work introduces SecCode, a framework that leverages an innovative\ninteractive encouragement prompting (EP) technique for secure code generation\nwith \\textit{only NL} prompts. This approach ensures that the prompts can be\neasily shared and understood by general users. SecCode functions through three\nstages: 1) Code Generation using NL Prompts; 2) Code Vulnerability Detection\nand Fixing, utilising our proposed encouragement prompting; 3) Vulnerability\nCross-Checking and Code Security Refinement. These stages are executed in\nmultiple interactive iterations to progressively enhance security. By using\nboth proprietary LLMs (i.e., GPT-3.5 Turbo, GPT-4 and GPT-4o) and open-source\nLLMs (i.e., Llama 3.1 8B Instruct, DeepSeek Coder V2 Lite Instruct) evaluated\non three benchmark datasets, extensive experimental results show that our\nproposed SecCode greatly outperforms compared baselines, generating secure code\nwith a high vulnerability correction rate. For example, SecCode exhibits a high\nfix success rate of over 76\\% after running 5 automated EP interactive\niterations and over 89\\% after running 10 automated EP interactive iterations.\nTo the best of our knowledge, this work is the first to formulate secure code\ngeneration with NL prompts only. We have open-sourced our code and encourage\nthe community to focus on secure code generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable potential in code\ngeneration, making them increasingly important in the field. However, the\nsecurity issues of generated code have not been fully addressed, and the\nusability of LLMs in code generation still requires further exploration.\n  This work introduces SecCode, a framework that leverages an innovative\ninteractive encouragement prompting (EP) technique for secure code generation\nwith \\textit{only NL} prompts. This approach ensures that the prompts can be\neasily shared and understood by general users. SecCode functions through three\nstages: 1) Code Generation using NL Prompts; 2) Code Vulnerability Detection\nand Fixing, utilising our proposed encouragement prompting; 3) Vulnerability\nCross-Checking and Code Security Refinement. These stages are executed in\nmultiple interactive iterations to progressively enhance security. By using\nboth proprietary LLMs (i.e., GPT-3.5 Turbo, GPT-4 and GPT-4o) and open-source\nLLMs (i.e., Llama 3.1 8B Instruct, DeepSeek Coder V2 Lite Instruct) evaluated\non three benchmark datasets, extensive experimental results show that our\nproposed SecCode greatly outperforms compared baselines, generating secure code\nwith a high vulnerability correction rate. For example, SecCode exhibits a high\nfix success rate of over 76\\% after running 5 automated EP interactive\niterations and over 89\\% after running 10 automated EP interactive iterations.\nTo the best of our knowledge, this work is the first to formulate secure code\ngeneration with NL prompts only. We have open-sourced our code and encourage\nthe community to focus on secure code generation."
                },
                "authors": [
                    {
                        "name": "Shigang Liu"
                    },
                    {
                        "name": "Bushra Sabir"
                    },
                    {
                        "name": "Seung Ick Jang"
                    },
                    {
                        "name": "Yuval Kansal"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Kristen Moore"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Surya Nepal"
                    }
                ],
                "author_detail": {
                    "name": "Surya Nepal"
                },
                "author": "Surya Nepal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14309v1",
                "updated": "2024-10-18T09:15:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    15,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T09:15:35Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    15,
                    35,
                    4,
                    292,
                    0
                ],
                "title": "LoGU: Long-form Generation with Uncertainty Expressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoGU: Long-form Generation with Uncertainty Expressions"
                },
                "summary": "While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses."
                },
                "authors": [
                    {
                        "name": "Ruihan Yang"
                    },
                    {
                        "name": "Caiqi Zhang"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Sen Yang"
                    },
                    {
                        "name": "Nigel Collier"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13632v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13632v3",
                "updated": "2024-10-18T09:07:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    7,
                    53,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-19T15:28:29Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    15,
                    28,
                    29,
                    2,
                    171,
                    0
                ],
                "title": "Can Few-shot Work in Long-Context? Recycling the Context to Generate\n  Demonstrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Few-shot Work in Long-Context? Recycling the Context to Generate\n  Demonstrations"
                },
                "summary": "Despite recent advancements in Large Language Models (LLMs), their\nperformance on tasks involving long contexts remains sub-optimal. In-Context\nLearning (ICL) with few-shot examples may be an appealing solution to enhance\nLLM performance in this scenario; However, na\\\"ively adding ICL examples with\nlong context introduces challenges, including substantial token overhead added\nfor each few-shot example and context mismatch between the demonstrations and\nthe target query. In this work, we propose to automatically generate few-shot\nexamples for long context QA tasks by recycling contexts. Specifically, given a\nlong input context (1-3k tokens) and a query, we generate additional\nquery-output pairs from the given context as few-shot examples, while\nintroducing the context only once. This ensures that the demonstrations are\nleveraging the same context as the target query while only adding a small\nnumber of tokens to the prompt. We further enhance each demonstration by\ninstructing the model to explicitly identify the relevant paragraphs before the\nanswer, which improves performance while providing fine-grained attribution to\nthe answer source. We apply our method on multiple LLMs and obtain substantial\nimprovements (+16 absolute points on average across models) on various QA\ndatasets with long context, especially when the answer lies within the middle\nof the context. Surprisingly, despite introducing only single-hop ICL examples,\nLLMs also successfully generalize to multi-hop long-context QA using our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite recent advancements in Large Language Models (LLMs), their\nperformance on tasks involving long contexts remains sub-optimal. In-Context\nLearning (ICL) with few-shot examples may be an appealing solution to enhance\nLLM performance in this scenario; However, na\\\"ively adding ICL examples with\nlong context introduces challenges, including substantial token overhead added\nfor each few-shot example and context mismatch between the demonstrations and\nthe target query. In this work, we propose to automatically generate few-shot\nexamples for long context QA tasks by recycling contexts. Specifically, given a\nlong input context (1-3k tokens) and a query, we generate additional\nquery-output pairs from the given context as few-shot examples, while\nintroducing the context only once. This ensures that the demonstrations are\nleveraging the same context as the target query while only adding a small\nnumber of tokens to the prompt. We further enhance each demonstration by\ninstructing the model to explicitly identify the relevant paragraphs before the\nanswer, which improves performance while providing fine-grained attribution to\nthe answer source. We apply our method on multiple LLMs and obtain substantial\nimprovements (+16 absolute points on average across models) on various QA\ndatasets with long context, especially when the answer lies within the middle\nof the context. Surprisingly, despite introducing only single-hop ICL examples,\nLLMs also successfully generalize to multi-hop long-context QA using our\napproach."
                },
                "authors": [
                    {
                        "name": "Arie Cattan"
                    },
                    {
                        "name": "Alon Jacovi"
                    },
                    {
                        "name": "Alex Fabrikant"
                    },
                    {
                        "name": "Jonathan Herzig"
                    },
                    {
                        "name": "Roee Aharoni"
                    },
                    {
                        "name": "Hannah Rashkin"
                    },
                    {
                        "name": "Dror Marcus"
                    },
                    {
                        "name": "Avinatan Hassidim"
                    },
                    {
                        "name": "Yossi Matias"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Avi Caciularu"
                    }
                ],
                "author_detail": {
                    "name": "Avi Caciularu"
                },
                "author": "Avi Caciularu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13632v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13632v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18921v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18921v2",
                "updated": "2024-10-18T09:05:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    5,
                    18,
                    4,
                    292,
                    0
                ],
                "published": "2024-05-29T09:24:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    24,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "GLANCE: Global Actions in a Nutshell for Counterfactual Explainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLANCE: Global Actions in a Nutshell for Counterfactual Explainability"
                },
                "summary": "The widespread deployment of machine learning systems in critical real-world\ndecision-making applications has highlighted the urgent need for counterfactual\nexplainability methods that operate effectively. Global counterfactual\nexplanations, expressed as actions to offer recourse, aim to provide succinct\nexplanations and insights applicable to large population subgroups.\nEffectiveness is measured by the fraction of the population that is provided\nrecourse, ensuring that the actions benefit as many individuals as possible.\nKeeping the cost of actions low ensures the proposed recourse actions remain\npractical and actionable. Limiting the number of actions that provide global\ncounterfactuals is essential to maximize interpretability. The primary\nchallenge, therefore, is balancing these trade-offs, i.e., maximizing\neffectiveness, minimizing cost, while maintaining a small number of actions. We\nintroduce GLANCE, a versatile and adaptive framework, comprising two\nalgorithms, that allows the careful balancing of the trade-offs among the three\nkey objectives, with the size objective functioning as a tunable parameter to\nkeep the actions few and easy to interpret. C-GLANCE employs a clustering\napproach that considers both the feature space and the space of counterfactual\nactions, thereby accounting for the distribution of points in a way that aligns\nwith the structure of the model. T-GLANCE provides additional features to\nenhance flexibility. It employs a tree-based approach, that allows users to\nspecify split features, to build a decision tree with a single counterfactual\naction at each node that can be used as a subgroup policy. Our extensive\nexperimental evaluation demonstrates that our method consistently shows greater\nrobustness and performance compared to existing methods across various datasets\nand models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of machine learning systems in critical real-world\ndecision-making applications has highlighted the urgent need for counterfactual\nexplainability methods that operate effectively. Global counterfactual\nexplanations, expressed as actions to offer recourse, aim to provide succinct\nexplanations and insights applicable to large population subgroups.\nEffectiveness is measured by the fraction of the population that is provided\nrecourse, ensuring that the actions benefit as many individuals as possible.\nKeeping the cost of actions low ensures the proposed recourse actions remain\npractical and actionable. Limiting the number of actions that provide global\ncounterfactuals is essential to maximize interpretability. The primary\nchallenge, therefore, is balancing these trade-offs, i.e., maximizing\neffectiveness, minimizing cost, while maintaining a small number of actions. We\nintroduce GLANCE, a versatile and adaptive framework, comprising two\nalgorithms, that allows the careful balancing of the trade-offs among the three\nkey objectives, with the size objective functioning as a tunable parameter to\nkeep the actions few and easy to interpret. C-GLANCE employs a clustering\napproach that considers both the feature space and the space of counterfactual\nactions, thereby accounting for the distribution of points in a way that aligns\nwith the structure of the model. T-GLANCE provides additional features to\nenhance flexibility. It employs a tree-based approach, that allows users to\nspecify split features, to build a decision tree with a single counterfactual\naction at each node that can be used as a subgroup policy. Our extensive\nexperimental evaluation demonstrates that our method consistently shows greater\nrobustness and performance compared to existing methods across various datasets\nand models."
                },
                "authors": [
                    {
                        "name": "Loukas Kavouras"
                    },
                    {
                        "name": "Eleni Psaroudaki"
                    },
                    {
                        "name": "Konstantinos Tsopelas"
                    },
                    {
                        "name": "Dimitrios Rontogiannis"
                    },
                    {
                        "name": "Nikolaos Theologitis"
                    },
                    {
                        "name": "Dimitris Sacharidis"
                    },
                    {
                        "name": "Giorgos Giannopoulos"
                    },
                    {
                        "name": "Dimitrios Tomaras"
                    },
                    {
                        "name": "Kleopatra Markou"
                    },
                    {
                        "name": "Dimitrios Gunopulos"
                    },
                    {
                        "name": "Dimitris Fotakis"
                    },
                    {
                        "name": "Ioannis Emiris"
                    }
                ],
                "author_detail": {
                    "name": "Ioannis Emiris"
                },
                "author": "Ioannis Emiris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18921v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18921v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09075v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09075v3",
                "updated": "2024-10-18T09:02:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    2,
                    46,
                    4,
                    292,
                    0
                ],
                "published": "2023-12-14T16:10:56Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    16,
                    10,
                    56,
                    3,
                    348,
                    0
                ],
                "title": "Towards Verifiable Text Generation with Evolving Memory and\n  Self-Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Verifiable Text Generation with Evolving Memory and\n  Self-Reflection"
                },
                "summary": "Despite the remarkable ability of large language models (LLMs) in language\ncomprehension and generation, they often suffer from producing factually\nincorrect information, also known as hallucination. A promising solution to\nthis issue is verifiable text generation, which prompts LLMs to generate\ncontent with citations for accuracy verification. However, verifiable text\ngeneration is non-trivial due to the focus-shifting phenomenon, the intricate\nreasoning needed to align the claim with correct citations, and the dilemma\nbetween the precision and breadth of retrieved documents. In this paper, we\npresent VTG, an innovative framework for Verifiable Text Generation with\nevolving memory and self-reflection. VTG introduces evolving long short-term\nmemory to retain both valuable documents and recent documents. A two-tier\nverifier equipped with an evidence finder is proposed to rethink and reflect on\nthe relationship between the claim and citations. Furthermore, active retrieval\nand diverse query generation are utilized to enhance both the precision and\nbreadth of the retrieved documents. We conduct extensive experiments on five\ndatasets across three knowledge-intensive tasks and the results reveal that VTG\nsignificantly outperforms baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable ability of large language models (LLMs) in language\ncomprehension and generation, they often suffer from producing factually\nincorrect information, also known as hallucination. A promising solution to\nthis issue is verifiable text generation, which prompts LLMs to generate\ncontent with citations for accuracy verification. However, verifiable text\ngeneration is non-trivial due to the focus-shifting phenomenon, the intricate\nreasoning needed to align the claim with correct citations, and the dilemma\nbetween the precision and breadth of retrieved documents. In this paper, we\npresent VTG, an innovative framework for Verifiable Text Generation with\nevolving memory and self-reflection. VTG introduces evolving long short-term\nmemory to retain both valuable documents and recent documents. A two-tier\nverifier equipped with an evidence finder is proposed to rethink and reflect on\nthe relationship between the claim and citations. Furthermore, active retrieval\nand diverse query generation are utilized to enhance both the precision and\nbreadth of the retrieved documents. We conduct extensive experiments on five\ndatasets across three knowledge-intensive tasks and the results reveal that VTG\nsignificantly outperforms baselines."
                },
                "authors": [
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Hengyi Cai"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Yingyan Hou"
                    },
                    {
                        "name": "Xiaochi Wei"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "arxiv_comment": "EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.09075v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09075v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13824v2",
                "updated": "2024-10-18T09:01:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    9,
                    1,
                    1,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-17T17:48:54Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    48,
                    54,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Webpage UIs for Text-Rich Visual Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Webpage UIs for Text-Rich Visual Understanding"
                },
                "summary": "Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-rich visual understanding-the ability to process environments where\ndense textual content is integrated with visuals-is crucial for multimodal\nlarge language models (MLLMs) to interact effectively with structured\nenvironments. To enhance this capability, we propose synthesizing general\nmultimodal instructions from webpage UIs using text-based large language models\n(LLMs). Despite lacking direct visual input, text-based LLMs are able to\nprocess structured text representations from webpage accessibility trees. These\ninstructions are then paired with UI screenshots to train multimodal models. We\nintroduce MultiUI, a dataset containing 7.3 million samples from 1 million\nwebsites, covering diverse multimodal tasks and UI layouts. Models trained on\nMultiUI not only excel in web UI tasks-achieving up to a 48% improvement on\nVisualWebBench and a 19.1% boost in element accuracy on a web agent dataset\nMind2Web-but also generalize surprisingly well to non-web UI tasks and even to\nnon-UI domains, such as document understanding, OCR, and chart interpretation.\nThese results highlight the broad applicability of web UI data for advancing\ntext-rich visual understanding across various scenarios."
                },
                "authors": [
                    {
                        "name": "Junpeng Liu"
                    },
                    {
                        "name": "Tianyue Ou"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Yuxiao Qu"
                    },
                    {
                        "name": "Wai Lam"
                    },
                    {
                        "name": "Chenyan Xiong"
                    },
                    {
                        "name": "Wenhu Chen"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Xiang Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiang Yue"
                },
                "author": "Xiang Yue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14298v1",
                "updated": "2024-10-18T08:58:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    58,
                    44,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T08:58:44Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    58,
                    44,
                    4,
                    292,
                    0
                ],
                "title": "Optimizing Collaborative Robotics since Pre-Deployment via\n  Cyber-Physical Systems' Digital Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Collaborative Robotics since Pre-Deployment via\n  Cyber-Physical Systems' Digital Twins"
                },
                "summary": "The collaboration between humans and robots re-quires a paradigm shift not\nonly in robot perception, reasoning, and action, but also in the design of the\nrobotic cell. This paper proposes an optimization framework for designing\ncollaborative robotics cells using a digital twin during the pre-deployment\nphase. This approach mitigates the limitations of experience-based sub-optimal\ndesigns by means of Bayesian optimization to find the optimal layout after a\ncertain number of iterations. By integrating production KPIs into a black-box\noptimization frame-work, the digital twin supports data-driven decision-making,\nreduces the need for costly prototypes, and ensures continuous improvement\nthanks to the learning nature of the algorithm. The paper presents a case study\nwith preliminary results that show how this methodology can be applied to\nobtain safer, more efficient, and adaptable human-robot collaborative\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The collaboration between humans and robots re-quires a paradigm shift not\nonly in robot perception, reasoning, and action, but also in the design of the\nrobotic cell. This paper proposes an optimization framework for designing\ncollaborative robotics cells using a digital twin during the pre-deployment\nphase. This approach mitigates the limitations of experience-based sub-optimal\ndesigns by means of Bayesian optimization to find the optimal layout after a\ncertain number of iterations. By integrating production KPIs into a black-box\noptimization frame-work, the digital twin supports data-driven decision-making,\nreduces the need for costly prototypes, and ensures continuous improvement\nthanks to the learning nature of the algorithm. The paper presents a case study\nwith preliminary results that show how this methodology can be applied to\nobtain safer, more efficient, and adaptable human-robot collaborative\nenvironments."
                },
                "authors": [
                    {
                        "name": "Christian Cella"
                    },
                    {
                        "name": "Marco Faroni"
                    },
                    {
                        "name": "Andrea Zanchettin"
                    },
                    {
                        "name": "Paolo Rocco"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Rocco"
                },
                "author": "Paolo Rocco",
                "arxiv_doi": "10.1109/ETFA61755.2024.10710805",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ETFA61755.2024.10710805",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.14298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE ETFA 2024",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.14626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.14626v2",
                "updated": "2024-10-18T08:56:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    56,
                    18,
                    4,
                    292,
                    0
                ],
                "published": "2023-10-23T07:00:51Z",
                "published_parsed": [
                    2023,
                    10,
                    23,
                    7,
                    0,
                    51,
                    0,
                    296,
                    0
                ],
                "title": "Conversational Recommender System and Large Language Model Are Made for\n  Each Other in E-commerce Pre-sales Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Recommender System and Large Language Model Are Made for\n  Each Other in E-commerce Pre-sales Dialogue"
                },
                "summary": "E-commerce pre-sales dialogue aims to understand and elicit user needs and\npreferences for the items they are seeking so as to provide appropriate\nrecommendations. Conversational recommender systems (CRSs) learn user\nrepresentation and provide accurate recommendations based on dialogue context,\nbut rely on external knowledge. Large language models (LLMs) generate responses\nthat mimic pre-sales dialogues after fine-tuning, but lack domain-specific\nknowledge for accurate recommendations. Intuitively, the strengths of LLM and\nCRS in E-commerce pre-sales dialogues are complementary, yet no previous work\nhas explored this. This paper investigates the effectiveness of combining LLM\nand CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:\nCRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a\nreal-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of\ntwo collaborative approaches with two CRSs and two LLMs on four tasks of\nEcommerce pre-sales dialogue. We find that collaborations between CRS and LLM\ncan be very effective in some cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-commerce pre-sales dialogue aims to understand and elicit user needs and\npreferences for the items they are seeking so as to provide appropriate\nrecommendations. Conversational recommender systems (CRSs) learn user\nrepresentation and provide accurate recommendations based on dialogue context,\nbut rely on external knowledge. Large language models (LLMs) generate responses\nthat mimic pre-sales dialogues after fine-tuning, but lack domain-specific\nknowledge for accurate recommendations. Intuitively, the strengths of LLM and\nCRS in E-commerce pre-sales dialogues are complementary, yet no previous work\nhas explored this. This paper investigates the effectiveness of combining LLM\nand CRS in E-commerce pre-sales dialogues, proposing two collaboration methods:\nCRS assisting LLM and LLM assisting CRS. We conduct extensive experiments on a\nreal-world dataset of Ecommerce pre-sales dialogues. We analyze the impact of\ntwo collaborative approaches with two CRSs and two LLMs on four tasks of\nEcommerce pre-sales dialogue. We find that collaborations between CRS and LLM\ncan be very effective in some cases."
                },
                "authors": [
                    {
                        "name": "Yuanxing Liu"
                    },
                    {
                        "name": "Wei-Nan Zhang"
                    },
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Yuchi Zhang"
                    },
                    {
                        "name": "Haopeng Bai"
                    },
                    {
                        "name": "Fan Feng"
                    },
                    {
                        "name": "Hengbin Cui"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "EMNLP 2023 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.14626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.14626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15053v2",
                "updated": "2024-10-18T08:51:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    51,
                    55,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-21T11:00:38Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    11,
                    0,
                    38,
                    4,
                    173,
                    0
                ],
                "title": "PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement\n  on Multilingual and Multi-Cultural Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement\n  on Multilingual and Multi-Cultural Data"
                },
                "summary": "Evaluation of multilingual Large Language Models (LLMs) is challenging due to\na variety of factors -- the lack of benchmarks with sufficient linguistic\ndiversity, contamination of popular benchmarks into LLM pre-training data and\nthe lack of local, cultural nuances in translated benchmarks. In this work, we\nstudy human and LLM-based evaluation in a multilingual, multi-cultural setting.\nWe evaluate 30 models across 10 Indic languages by conducting 90K human\nevaluations and 30K LLM-based evaluations and find that models such as GPT-4o\nand Llama-3 70B consistently perform best for most Indic languages. We build\nleaderboards for two evaluation settings - pairwise comparison and direct\nassessment and analyze the agreement between humans and LLMs. We find that\nhumans and LLMs agree fairly well in the pairwise setting but the agreement\ndrops for direct assessment evaluation especially for languages such as Bengali\nand Odia. We also check for various biases in human and LLM-based evaluation\nand find evidence of self-bias in the GPT-based evaluator. Our work presents a\nsignificant step towards scaling up multilingual evaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of multilingual Large Language Models (LLMs) is challenging due to\na variety of factors -- the lack of benchmarks with sufficient linguistic\ndiversity, contamination of popular benchmarks into LLM pre-training data and\nthe lack of local, cultural nuances in translated benchmarks. In this work, we\nstudy human and LLM-based evaluation in a multilingual, multi-cultural setting.\nWe evaluate 30 models across 10 Indic languages by conducting 90K human\nevaluations and 30K LLM-based evaluations and find that models such as GPT-4o\nand Llama-3 70B consistently perform best for most Indic languages. We build\nleaderboards for two evaluation settings - pairwise comparison and direct\nassessment and analyze the agreement between humans and LLMs. We find that\nhumans and LLMs agree fairly well in the pairwise setting but the agreement\ndrops for direct assessment evaluation especially for languages such as Bengali\nand Odia. We also check for various biases in human and LLM-based evaluation\nand find evidence of self-bias in the GPT-based evaluator. Our work presents a\nsignificant step towards scaling up multilingual evaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Ishaan Watts"
                    },
                    {
                        "name": "Varun Gumma"
                    },
                    {
                        "name": "Aditya Yadavalli"
                    },
                    {
                        "name": "Vivek Seshadri"
                    },
                    {
                        "name": "Manohar Swaminathan"
                    },
                    {
                        "name": "Sunayana Sitaram"
                    }
                ],
                "author_detail": {
                    "name": "Sunayana Sitaram"
                },
                "author": "Sunayana Sitaram",
                "arxiv_comment": "Accepted to EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14276v1",
                "updated": "2024-10-18T08:31:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    31,
                    22,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T08:31:22Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    31,
                    22,
                    4,
                    292,
                    0
                ],
                "title": "EcomEdit: An Automated E-commerce Knowledge Editing Framework for\n  Enhanced Product and Purchase Intention Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EcomEdit: An Automated E-commerce Knowledge Editing Framework for\n  Enhanced Product and Purchase Intention Understanding"
                },
                "summary": "Knowledge Editing (KE) aims to correct and update factual information in\nLarge Language Models (LLMs) to ensure accuracy and relevance without\ncomputationally expensive fine-tuning. Though it has been proven effective in\nseveral domains, limited work has focused on its application within the\ne-commerce sector. However, there are naturally occurring scenarios that make\nKE necessary in this domain, such as the timely updating of product features\nand trending purchase intentions by customers, which necessitate further\nexploration. In this paper, we pioneer the application of KE in the e-commerce\ndomain by presenting ECOMEDIT, an automated e-commerce knowledge editing\nframework tailored for e-commerce-related knowledge and tasks. Our framework\nleverages more powerful LLMs as judges to enable automatic knowledge conflict\ndetection and incorporates conceptualization to enhance the semantic coverage\nof the knowledge to be edited. Through extensive experiments, we demonstrate\nthe effectiveness of ECOMEDIT in improving LLMs' understanding of product\ndescriptions and purchase intentions. We also show that LLMs, after our\nediting, can achieve stronger performance on downstream e-commerce tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Editing (KE) aims to correct and update factual information in\nLarge Language Models (LLMs) to ensure accuracy and relevance without\ncomputationally expensive fine-tuning. Though it has been proven effective in\nseveral domains, limited work has focused on its application within the\ne-commerce sector. However, there are naturally occurring scenarios that make\nKE necessary in this domain, such as the timely updating of product features\nand trending purchase intentions by customers, which necessitate further\nexploration. In this paper, we pioneer the application of KE in the e-commerce\ndomain by presenting ECOMEDIT, an automated e-commerce knowledge editing\nframework tailored for e-commerce-related knowledge and tasks. Our framework\nleverages more powerful LLMs as judges to enable automatic knowledge conflict\ndetection and incorporates conceptualization to enhance the semantic coverage\nof the knowledge to be edited. Through extensive experiments, we demonstrate\nthe effectiveness of ECOMEDIT in improving LLMs' understanding of product\ndescriptions and purchase intentions. We also show that LLMs, after our\nediting, can achieve stronger performance on downstream e-commerce tasks."
                },
                "authors": [
                    {
                        "name": "Ching Ming Samuel Lau"
                    },
                    {
                        "name": "Weiqi Wang"
                    },
                    {
                        "name": "Haochen Shi"
                    },
                    {
                        "name": "Baixuan Xu"
                    },
                    {
                        "name": "Jiaxin Bai"
                    },
                    {
                        "name": "Yangqiu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yangqiu Song"
                },
                "author": "Yangqiu Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14273v1",
                "updated": "2024-10-18T08:27:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    27,
                    2,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T08:27:02Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    27,
                    2,
                    4,
                    292,
                    0
                ],
                "title": "REEF: Representation Encoding Fingerprints for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REEF: Representation Encoding Fingerprints for Large Language Models"
                },
                "summary": "Protecting the intellectual property of open-source Large Language Models\n(LLMs) is very important, because training LLMs costs extensive computational\nresources and data. Therefore, model owners and third parties need to identify\nwhether a suspect model is a subsequent development of the victim model. To\nthis end, we propose a training-free REEF to identify the relationship between\nthe suspect and victim models from the perspective of LLMs' feature\nrepresentations. Specifically, REEF computes and compares the centered kernel\nalignment similarity between the representations of a suspect model and a\nvictim model on the same samples. This training-free REEF does not impair the\nmodel's general capabilities and is robust to sequential fine-tuning, pruning,\nmodel merging, and permutations. In this way, REEF provides a simple and\neffective way for third parties and models' owners to protect LLMs'\nintellectual property together. The code is available at\nhttps://github.com/tmylla/REEF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting the intellectual property of open-source Large Language Models\n(LLMs) is very important, because training LLMs costs extensive computational\nresources and data. Therefore, model owners and third parties need to identify\nwhether a suspect model is a subsequent development of the victim model. To\nthis end, we propose a training-free REEF to identify the relationship between\nthe suspect and victim models from the perspective of LLMs' feature\nrepresentations. Specifically, REEF computes and compares the centered kernel\nalignment similarity between the representations of a suspect model and a\nvictim model on the same samples. This training-free REEF does not impair the\nmodel's general capabilities and is robust to sequential fine-tuning, pruning,\nmodel merging, and permutations. In this way, REEF provides a simple and\neffective way for third parties and models' owners to protect LLMs'\nintellectual property together. The code is available at\nhttps://github.com/tmylla/REEF."
                },
                "authors": [
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Jing Shao"
                    }
                ],
                "author_detail": {
                    "name": "Jing Shao"
                },
                "author": "Jing Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14268v1",
                "updated": "2024-10-18T08:22:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    22,
                    7,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T08:22:07Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    22,
                    7,
                    4,
                    292,
                    0
                ],
                "title": "MoDification: Mixture of Depths Made Easy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDification: Mixture of Depths Made Easy"
                },
                "summary": "Long-context efficiency has recently become a trending topic in serving large\nlanguage models (LLMs). And mixture of depths (MoD) is proposed as a perfect\nfit to bring down both latency and memory. In this paper, however, we discover\nthat MoD can barely transform existing LLMs without costly training over an\nextensive number of tokens. To enable the transformations from any LLMs to MoD\nones, we showcase top-k operator in MoD should be promoted to threshold-p\noperator, and refinement to architecture and data should also be crafted along.\nAll these designs form our method termed MoDification. Through a comprehensive\nset of experiments covering model scales from 3B to 70B, we exhibit\nMoDification strikes an excellent balance between efficiency and effectiveness.\nMoDification can achieve up to ~1.2x speedup in latency and ~1.8x reduction in\nmemory compared to original LLMs especially in long-context applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context efficiency has recently become a trending topic in serving large\nlanguage models (LLMs). And mixture of depths (MoD) is proposed as a perfect\nfit to bring down both latency and memory. In this paper, however, we discover\nthat MoD can barely transform existing LLMs without costly training over an\nextensive number of tokens. To enable the transformations from any LLMs to MoD\nones, we showcase top-k operator in MoD should be promoted to threshold-p\noperator, and refinement to architecture and data should also be crafted along.\nAll these designs form our method termed MoDification. Through a comprehensive\nset of experiments covering model scales from 3B to 70B, we exhibit\nMoDification strikes an excellent balance between efficiency and effectiveness.\nMoDification can achieve up to ~1.2x speedup in latency and ~1.8x reduction in\nmemory compared to original LLMs especially in long-context applications."
                },
                "authors": [
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Qimeng Wang"
                    },
                    {
                        "name": "Xuantao Lu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Chengqiang Lu"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "arxiv_comment": "12 pages, 9 figures, 5 tables, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06572v2",
                "updated": "2024-10-18T08:20:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    20,
                    38,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-03T17:07:46Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    17,
                    7,
                    46,
                    0,
                    155,
                    0
                ],
                "title": "Graph Neural Network Enhanced Retrieval for Question Answering of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Network Enhanced Retrieval for Question Answering of LLMs"
                },
                "summary": "Retrieval augmented generation has revolutionized large language model (LLM)\noutputs by providing factual supports. Nevertheless, it struggles to capture\nall the necessary knowledge for complex reasoning questions. Existing retrieval\nmethods typically divide reference documents into passages, treating them in\nisolation. These passages, however, are often interrelated, such as passages\nthat are contiguous or share the same keywords. Therefore, it is crucial to\nrecognize such relatedness for enhancing the retrieval process. In this paper,\nwe propose a novel retrieval method, called GNN-Ret, which leverages graph\nneural networks (GNNs) to enhance retrieval by exploiting the relatedness\nbetween passages. Specifically, we first construct a graph of passages by\nconnecting passages that are structure-related or keyword-related. A graph\nneural network (GNN) is then leveraged to exploit the relationships between\npassages and improve the retrieval of supporting passages. Furthermore, we\nextend our method to handle multi-hop reasoning questions using a recurrent\ngraph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates\nthe graphs of passages from previous steps, thereby enhancing the retrieval of\nsupporting passages. Extensive experiments on benchmark datasets demonstrate\nthat GNN-Ret achieves higher accuracy for question answering with a single\nquery of LLMs than strong baselines that require multiple queries, and RGNN-Ret\nfurther improves accuracy and achieves state-of-the-art performance, with up to\n10.4% accuracy improvement on the 2WikiMQA dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation has revolutionized large language model (LLM)\noutputs by providing factual supports. Nevertheless, it struggles to capture\nall the necessary knowledge for complex reasoning questions. Existing retrieval\nmethods typically divide reference documents into passages, treating them in\nisolation. These passages, however, are often interrelated, such as passages\nthat are contiguous or share the same keywords. Therefore, it is crucial to\nrecognize such relatedness for enhancing the retrieval process. In this paper,\nwe propose a novel retrieval method, called GNN-Ret, which leverages graph\nneural networks (GNNs) to enhance retrieval by exploiting the relatedness\nbetween passages. Specifically, we first construct a graph of passages by\nconnecting passages that are structure-related or keyword-related. A graph\nneural network (GNN) is then leveraged to exploit the relationships between\npassages and improve the retrieval of supporting passages. Furthermore, we\nextend our method to handle multi-hop reasoning questions using a recurrent\ngraph neural network (RGNN), named RGNN-Ret. At each step, RGNN-Ret integrates\nthe graphs of passages from previous steps, thereby enhancing the retrieval of\nsupporting passages. Extensive experiments on benchmark datasets demonstrate\nthat GNN-Ret achieves higher accuracy for question answering with a single\nquery of LLMs than strong baselines that require multiple queries, and RGNN-Ret\nfurther improves accuracy and achieves state-of-the-art performance, with up to\n10.4% accuracy improvement on the 2WikiMQA dataset."
                },
                "authors": [
                    {
                        "name": "Zijian Li"
                    },
                    {
                        "name": "Qingyan Guo"
                    },
                    {
                        "name": "Jiawei Shao"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14262v1",
                "updated": "2024-10-18T08:18:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    18,
                    18,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T08:18:18Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    18,
                    18,
                    4,
                    292,
                    0
                ],
                "title": "Good Parenting is all you need -- Multi-agentic LLM Hallucination\n  Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Good Parenting is all you need -- Multi-agentic LLM Hallucination\n  Mitigation"
                },
                "summary": "This study explores the ability of Large Language Model (LLM) agents to\ndetect and correct hallucinations in AI-generated content. A primary agent was\ntasked with creating a blog about a fictional Danish artist named Flipfloppidy,\nwhich was then reviewed by another agent for factual inaccuracies. Most LLMs\nhallucinated the existence of this artist. Across 4,900 test runs involving\nvarious combinations of primary and reviewing agents, advanced AI models such\nas Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in\nidentifying hallucinations and successfully revised outputs in 85% to 100% of\ncases following feedback. These findings underscore the potential of advanced\nAI models to significantly enhance the accuracy and reliability of generated\ncontent, providing a promising approach to improving AI workflow orchestration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the ability of Large Language Model (LLM) agents to\ndetect and correct hallucinations in AI-generated content. A primary agent was\ntasked with creating a blog about a fictional Danish artist named Flipfloppidy,\nwhich was then reviewed by another agent for factual inaccuracies. Most LLMs\nhallucinated the existence of this artist. Across 4,900 test runs involving\nvarious combinations of primary and reviewing agents, advanced AI models such\nas Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in\nidentifying hallucinations and successfully revised outputs in 85% to 100% of\ncases following feedback. These findings underscore the potential of advanced\nAI models to significantly enhance the accuracy and reliability of generated\ncontent, providing a promising approach to improving AI workflow orchestration."
                },
                "authors": [
                    {
                        "name": "Edward"
                    },
                    {
                        "name": "Kwartler"
                    },
                    {
                        "name": "Matthew Berman"
                    },
                    {
                        "name": "Alan Aqrawi"
                    }
                ],
                "author_detail": {
                    "name": "Alan Aqrawi"
                },
                "arxiv_affiliation": "Ted",
                "author": "Alan Aqrawi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14259v1",
                "updated": "2024-10-18T08:14:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    14,
                    10,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T08:14:10Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    14,
                    10,
                    4,
                    292,
                    0
                ],
                "title": "Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via\n  Role Recognition and Involvement Measurement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via\n  Role Recognition and Involvement Measurement"
                },
                "summary": "The rapid development of large language models (LLMs), like ChatGPT, has\nresulted in the widespread presence of LLM-generated content on social media\nplatforms, raising concerns about misinformation, data biases, and privacy\nviolations, which can undermine trust in online discourse. While detecting\nLLM-generated content is crucial for mitigating these risks, current methods\noften focus on binary classification, failing to address the complexities of\nreal-world scenarios like human-AI collaboration. To move beyond binary\nclassification and address these challenges, we propose a new paradigm for\ndetecting LLM-generated content. This approach introduces two novel tasks: LLM\nRole Recognition (LLM-RR), a multi-class classification task that identifies\nspecific roles of LLM in content generation, and LLM Influence Measurement\n(LLM-IM), a regression task that quantifies the extent of LLM involvement in\ncontent creation. To support these tasks, we propose LLMDetect, a benchmark\ndesigned to evaluate detectors' performance on these new tasks. LLMDetect\nincludes the Hybrid News Detection Corpus (HNDC) for training detectors, as\nwell as DetectEval, a comprehensive evaluation suite that considers five\ndistinct cross-context variations and multi-intensity variations within the\nsame LLM role. This allows for a thorough assessment of detectors'\ngeneralization and robustness across diverse contexts. Our empirical validation\nof 10 baseline detection methods demonstrates that fine-tuned PLM-based models\nconsistently outperform others on both tasks, while advanced LLMs face\nchallenges in accurately detecting their own generated content. Our\nexperimental results and analysis offer insights for developing more effective\ndetection models for LLM-generated content. This research enhances the\nunderstanding of LLM-generated content and establishes a foundation for more\nnuanced detection methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs), like ChatGPT, has\nresulted in the widespread presence of LLM-generated content on social media\nplatforms, raising concerns about misinformation, data biases, and privacy\nviolations, which can undermine trust in online discourse. While detecting\nLLM-generated content is crucial for mitigating these risks, current methods\noften focus on binary classification, failing to address the complexities of\nreal-world scenarios like human-AI collaboration. To move beyond binary\nclassification and address these challenges, we propose a new paradigm for\ndetecting LLM-generated content. This approach introduces two novel tasks: LLM\nRole Recognition (LLM-RR), a multi-class classification task that identifies\nspecific roles of LLM in content generation, and LLM Influence Measurement\n(LLM-IM), a regression task that quantifies the extent of LLM involvement in\ncontent creation. To support these tasks, we propose LLMDetect, a benchmark\ndesigned to evaluate detectors' performance on these new tasks. LLMDetect\nincludes the Hybrid News Detection Corpus (HNDC) for training detectors, as\nwell as DetectEval, a comprehensive evaluation suite that considers five\ndistinct cross-context variations and multi-intensity variations within the\nsame LLM role. This allows for a thorough assessment of detectors'\ngeneralization and robustness across diverse contexts. Our empirical validation\nof 10 baseline detection methods demonstrates that fine-tuned PLM-based models\nconsistently outperform others on both tasks, while advanced LLMs face\nchallenges in accurately detecting their own generated content. Our\nexperimental results and analysis offer insights for developing more effective\ndetection models for LLM-generated content. This research enhances the\nunderstanding of LLM-generated content and establishes a foundation for more\nnuanced detection methodologies."
                },
                "authors": [
                    {
                        "name": "Zihao Cheng"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Feng Jiang"
                    },
                    {
                        "name": "Benyou Wang"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "Social Media, Large Language Models, LLM-generated Text Detection,\n  AI-assisted News Detection",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14257v1",
                "updated": "2024-10-18T08:05:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    5,
                    37,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T08:05:37Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    5,
                    37,
                    4,
                    292,
                    0
                ],
                "title": "Revisiting SLO and Goodput Metrics in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting SLO and Goodput Metrics in LLM Serving"
                },
                "summary": "Large language models (LLMs) have achieved remarkable performance and are\nwidely deployed in various applications, while the serving of LLM inference has\nraised concerns about user experience and serving throughput. Accordingly,\nservice level objectives (SLOs) and goodput-the number of requests that meet\nSLOs per second-are introduced to evaluate the performance of LLM serving.\nHowever, existing metrics fail to capture the nature of user experience. We\nobserve two ridiculous phenomena in existing metrics: 1) delaying token\ndelivery can smooth the tail time between tokens (tail TBT) of a request and 2)\ndropping the request that fails to meet the SLOs midway can improve goodput.\n  In this paper, we revisit SLO and goodput metrics in LLM serving and propose\na unified metric framework smooth goodput including SLOs and goodput to reflect\nthe nature of user experience in LLM serving. The framework can adapt to\nspecific goals of different tasks by setting parameters. We re-evaluate the\nperformance of different LLM serving systems under multiple workloads based on\nthis unified framework and provide possible directions for future optimization\nof existing strategies. We hope that this framework can provide a unified\nstandard for evaluating LLM serving and foster researches in the field of LLM\nserving optimization to move in a cohesive direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable performance and are\nwidely deployed in various applications, while the serving of LLM inference has\nraised concerns about user experience and serving throughput. Accordingly,\nservice level objectives (SLOs) and goodput-the number of requests that meet\nSLOs per second-are introduced to evaluate the performance of LLM serving.\nHowever, existing metrics fail to capture the nature of user experience. We\nobserve two ridiculous phenomena in existing metrics: 1) delaying token\ndelivery can smooth the tail time between tokens (tail TBT) of a request and 2)\ndropping the request that fails to meet the SLOs midway can improve goodput.\n  In this paper, we revisit SLO and goodput metrics in LLM serving and propose\na unified metric framework smooth goodput including SLOs and goodput to reflect\nthe nature of user experience in LLM serving. The framework can adapt to\nspecific goals of different tasks by setting parameters. We re-evaluate the\nperformance of different LLM serving systems under multiple workloads based on\nthis unified framework and provide possible directions for future optimization\nof existing strategies. We hope that this framework can provide a unified\nstandard for evaluating LLM serving and foster researches in the field of LLM\nserving optimization to move in a cohesive direction."
                },
                "authors": [
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Shipeng Li"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Nguyen Cam-Tu"
                    },
                    {
                        "name": "Chen Tian"
                    },
                    {
                        "name": "Sheng Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhong"
                },
                "author": "Sheng Zhong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14255v1",
                "updated": "2024-10-18T08:04:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    4,
                    36,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T08:04:36Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    4,
                    36,
                    4,
                    292,
                    0
                ],
                "title": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and\n  Diversity of LLM Generated Ideas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nova: An Iterative Planning and Search Approach to Enhance Novelty and\n  Diversity of LLM Generated Ideas"
                },
                "summary": "Scientific innovation is pivotal for humanity, and harnessing large language\nmodels (LLMs) to generate research ideas could transform discovery. However,\nexisting LLMs often produce simplistic and repetitive suggestions due to their\nlimited ability in acquiring external knowledge for innovation. To address this\nproblem, we introduce an enhanced planning and search methodology designed to\nboost the creative potential of LLM-based systems. Our approach involves an\niterative process to purposely plan the retrieval of external knowledge,\nprogressively enriching the idea generation with broader and deeper insights.\nValidation through automated and human assessments indicates that our framework\nsubstantially elevates the quality of generated ideas, particularly in novelty\nand diversity. The number of unique novel ideas produced by our framework is\n3.4 times higher than without it. Moreover, our method outperforms the current\nstate-of-the-art, generating at least 2.5 times more top-rated ideas based on\n170 seed papers in a Swiss Tournament evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific innovation is pivotal for humanity, and harnessing large language\nmodels (LLMs) to generate research ideas could transform discovery. However,\nexisting LLMs often produce simplistic and repetitive suggestions due to their\nlimited ability in acquiring external knowledge for innovation. To address this\nproblem, we introduce an enhanced planning and search methodology designed to\nboost the creative potential of LLM-based systems. Our approach involves an\niterative process to purposely plan the retrieval of external knowledge,\nprogressively enriching the idea generation with broader and deeper insights.\nValidation through automated and human assessments indicates that our framework\nsubstantially elevates the quality of generated ideas, particularly in novelty\nand diversity. The number of unique novel ideas produced by our framework is\n3.4 times higher than without it. Moreover, our method outperforms the current\nstate-of-the-art, generating at least 2.5 times more top-rated ideas based on\n170 seed papers in a Swiss Tournament evaluation."
                },
                "authors": [
                    {
                        "name": "Xiang Hu"
                    },
                    {
                        "name": "Hongyu Fu"
                    },
                    {
                        "name": "Jinge Wang"
                    },
                    {
                        "name": "Yifeng Wang"
                    },
                    {
                        "name": "Zhikun Li"
                    },
                    {
                        "name": "Renjun Xu"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Yaochu Jin"
                    },
                    {
                        "name": "Lili Pan"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17524v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17524v4",
                "updated": "2024-10-18T08:03:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    3,
                    2,
                    4,
                    292,
                    0
                ],
                "published": "2024-04-26T16:41:00Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    16,
                    41,
                    0,
                    4,
                    117,
                    0
                ],
                "title": "On the Use of Large Language Models to Generate Capability Ontologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Use of Large Language Models to Generate Capability Ontologies"
                },
                "summary": "Capability ontologies are increasingly used to model functionalities of\nsystems or machines. The creation of such ontological models with all\nproperties and constraints of capabilities is very complex and can only be done\nby ontology experts. However, Large Language Models (LLMs) have shown that they\ncan generate machine-interpretable models from natural language text input and\nthus support engineers / ontology experts. Therefore, this paper investigates\nhow LLMs can be used to create capability ontologies. We present a study with a\nseries of experiments in which capabilities with varying complexities are\ngenerated using different prompting techniques and with different LLMs. Errors\nin the generated ontologies are recorded and compared. To analyze the quality\nof the generated ontologies, a semi-automated approach based on RDF syntax\nchecking, OWL reasoning, and SHACL constraints is used. The results of this\nstudy are very promising because even for complex capabilities, the generated\nontologies are almost free of errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capability ontologies are increasingly used to model functionalities of\nsystems or machines. The creation of such ontological models with all\nproperties and constraints of capabilities is very complex and can only be done\nby ontology experts. However, Large Language Models (LLMs) have shown that they\ncan generate machine-interpretable models from natural language text input and\nthus support engineers / ontology experts. Therefore, this paper investigates\nhow LLMs can be used to create capability ontologies. We present a study with a\nseries of experiments in which capabilities with varying complexities are\ngenerated using different prompting techniques and with different LLMs. Errors\nin the generated ontologies are recorded and compared. To analyze the quality\nof the generated ontologies, a semi-automated approach based on RDF syntax\nchecking, OWL reasoning, and SHACL constraints is used. The results of this\nstudy are very promising because even for complex capabilities, the generated\nontologies are almost free of errors."
                },
                "authors": [
                    {
                        "name": "Luis Miguel Vieira da Silva"
                    },
                    {
                        "name": "Aljosha Kcher"
                    },
                    {
                        "name": "Felix Gehlhoff"
                    },
                    {
                        "name": "Alexander Fay"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Fay"
                },
                "author": "Alexander Fay",
                "arxiv_doi": "10.1109/ETFA61755.2024.10710775",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ETFA61755.2024.10710775",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.17524v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17524v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14252v1",
                "updated": "2024-10-18T08:02:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    2,
                    36,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T08:02:36Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    2,
                    36,
                    4,
                    292,
                    0
                ],
                "title": "Harmony: A Home Agent for Responsive Management and Action Optimization\n  with a Locally Deployed Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harmony: A Home Agent for Responsive Management and Action Optimization\n  with a Locally Deployed Large Language Model"
                },
                "summary": "Since the launch of GPT-3.5, intelligent home assistant technology based on\nlarge language models (LLMs) has made significant progress. These intelligent\nhome assistant frameworks, such as those based on high-performance LLMs like\nGPT-4, have greatly expanded their functional range and application scenarios\nby computing on the cloud, enriching user experience and diversification. In\norder to optimize the privacy and economy of data processing while maintaining\nthe powerful functions of LLMs, we propose Harmony, a smart home assistant\nframework that uses a locally deployable small-scale LLM. Based on Llama3-8b,\nan open LLM that can be easily deployed on a consumer-grade PC, Harmony does\nnot send any data to the internet during operation, ensuring local computation\nand privacy secured. Harmony based on Llama3-8b achieved competitive\nperformance on our benchmark tests with the framework used in related work with\nGPT-4. In addition to solving the issues mentioned above, Harmony can also take\nactions according to the user and home status, even if the user does not issue\na command. For example, when the user wants to wake up later than normal on the\nweekend, Harmony would open the curtains only when the user gets up or prepare\nthe room when the user comes home without requiring user commands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the launch of GPT-3.5, intelligent home assistant technology based on\nlarge language models (LLMs) has made significant progress. These intelligent\nhome assistant frameworks, such as those based on high-performance LLMs like\nGPT-4, have greatly expanded their functional range and application scenarios\nby computing on the cloud, enriching user experience and diversification. In\norder to optimize the privacy and economy of data processing while maintaining\nthe powerful functions of LLMs, we propose Harmony, a smart home assistant\nframework that uses a locally deployable small-scale LLM. Based on Llama3-8b,\nan open LLM that can be easily deployed on a consumer-grade PC, Harmony does\nnot send any data to the internet during operation, ensuring local computation\nand privacy secured. Harmony based on Llama3-8b achieved competitive\nperformance on our benchmark tests with the framework used in related work with\nGPT-4. In addition to solving the issues mentioned above, Harmony can also take\nactions according to the user and home status, even if the user does not issue\na command. For example, when the user wants to wake up later than normal on the\nweekend, Harmony would open the curtains only when the user gets up or prepare\nthe room when the user comes home without requiring user commands."
                },
                "authors": [
                    {
                        "name": "Ziqi Yin"
                    },
                    {
                        "name": "Mingxin Zhang"
                    },
                    {
                        "name": "Daisuke Kawahara"
                    }
                ],
                "author_detail": {
                    "name": "Daisuke Kawahara"
                },
                "author": "Daisuke Kawahara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14251v1",
                "updated": "2024-10-18T08:01:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    1,
                    39,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T08:01:39Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    8,
                    1,
                    39,
                    4,
                    292,
                    0
                ],
                "title": "Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation"
                },
                "summary": "Post-training is essential for enabling large language models (LLMs) to\nfollow human instructions. Inspired by the recent success of using LLMs to\nsimulate human society, we leverage multi-agent simulation to automatically\ngenerate diverse text-based scenarios, capturing a wide range of real-world\nhuman needs. We propose MATRIX, a multi-agent simulator that creates realistic\nand scalable scenarios. Leveraging these outputs, we introduce a novel\nscenario-driven instruction generator MATRIX-Gen for controllable and highly\nrealistic data synthesis. Extensive experiments demonstrate that our framework\neffectively generates both general and domain-specific data. Notably, on\nAlpacaEval 2 and Arena-Hard benchmarks, Llama-3-8B-Base, post-trained on\ndatasets synthesized by MATRIX-Gen with just 20K instruction-response pairs,\noutperforms Meta's Llama-3-8B-Instruct model, which was trained on over 10M\npairs; see our project at https://github.com/ShuoTang123/MATRIX-Gen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training is essential for enabling large language models (LLMs) to\nfollow human instructions. Inspired by the recent success of using LLMs to\nsimulate human society, we leverage multi-agent simulation to automatically\ngenerate diverse text-based scenarios, capturing a wide range of real-world\nhuman needs. We propose MATRIX, a multi-agent simulator that creates realistic\nand scalable scenarios. Leveraging these outputs, we introduce a novel\nscenario-driven instruction generator MATRIX-Gen for controllable and highly\nrealistic data synthesis. Extensive experiments demonstrate that our framework\neffectively generates both general and domain-specific data. Notably, on\nAlpacaEval 2 and Arena-Hard benchmarks, Llama-3-8B-Base, post-trained on\ndatasets synthesized by MATRIX-Gen with just 20K instruction-response pairs,\noutperforms Meta's Llama-3-8B-Instruct model, which was trained on over 10M\npairs; see our project at https://github.com/ShuoTang123/MATRIX-Gen."
                },
                "authors": [
                    {
                        "name": "Shuo Tang"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Zexi Liu"
                    },
                    {
                        "name": "Bohan Tang"
                    },
                    {
                        "name": "Rui Ye"
                    },
                    {
                        "name": "Xiaowen Dong"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14248v1",
                "updated": "2024-10-18T07:52:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    7,
                    52,
                    22,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T07:52:22Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    7,
                    52,
                    22,
                    4,
                    292,
                    0
                ],
                "title": "Addressing Blind Guessing: Calibration of Selection Bias in\n  Multiple-Choice Question Answering by Video Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing Blind Guessing: Calibration of Selection Bias in\n  Multiple-Choice Question Answering by Video Language Models"
                },
                "summary": "Evaluating Video Language Models (VLMs) is a challenging task. Due to its\ntransparency, Multiple-Choice Question Answering (MCQA) is widely used to\nmeasure the performance of these models through accuracy. However, existing\nMCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to\nselection bias, when models disproportionately favor certain answer options\nbased on positional patterns observed during training. In this work, we conduct\na comprehensive empirical analysis of several VLM architectures across major\ndatasets designed to assess complex video-focused reasoning. We identify where\nthe bias is most pronounced and demonstrate to what extent model responses\nreflect genuine understanding of video content and related questions, as\nopposed to reliance on arbitrary patterns or superficial cues, such as answer\nposition. By decomposing the MCQA task and adapting fairness bias metrics to\nVLMs, we introduce a post-processing calibration technique BOLD to balance this\nbias. Our results show that reducing selection bias improves not only debiasing\nmetrics but also overall model performance, including Accuracy and F1 Mean\nscore. Our method, by suppressing \"blind guessing\", offers a more cost- and\ntime-effective approach to mitigating selection bias compared to existing\ntechniques. This study represents the first focused investigation of selection\nbias in video-to-text LLM-powered models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Video Language Models (VLMs) is a challenging task. Due to its\ntransparency, Multiple-Choice Question Answering (MCQA) is widely used to\nmeasure the performance of these models through accuracy. However, existing\nMCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to\nselection bias, when models disproportionately favor certain answer options\nbased on positional patterns observed during training. In this work, we conduct\na comprehensive empirical analysis of several VLM architectures across major\ndatasets designed to assess complex video-focused reasoning. We identify where\nthe bias is most pronounced and demonstrate to what extent model responses\nreflect genuine understanding of video content and related questions, as\nopposed to reliance on arbitrary patterns or superficial cues, such as answer\nposition. By decomposing the MCQA task and adapting fairness bias metrics to\nVLMs, we introduce a post-processing calibration technique BOLD to balance this\nbias. Our results show that reducing selection bias improves not only debiasing\nmetrics but also overall model performance, including Accuracy and F1 Mean\nscore. Our method, by suppressing \"blind guessing\", offers a more cost- and\ntime-effective approach to mitigating selection bias compared to existing\ntechniques. This study represents the first focused investigation of selection\nbias in video-to-text LLM-powered models."
                },
                "authors": [
                    {
                        "name": "Olga Loginova"
                    },
                    {
                        "name": "Oleksandr Bezrukov"
                    },
                    {
                        "name": "Alexey Kravets"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Kravets"
                },
                "author": "Alexey Kravets",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07962v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07962v2",
                "updated": "2024-10-18T07:34:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    7,
                    34,
                    39,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-12T07:41:44Z",
                "published_parsed": [
                    2024,
                    6,
                    12,
                    7,
                    41,
                    44,
                    2,
                    164,
                    0
                ],
                "title": "Toward a Method to Generate Capability Ontologies from Natural Language\n  Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Method to Generate Capability Ontologies from Natural Language\n  Descriptions"
                },
                "summary": "To achieve a flexible and adaptable system, capability ontologies are\nincreasingly leveraged to describe functions in a machine-interpretable way.\nHowever, modeling such complex ontological descriptions is still a manual and\nerror-prone task that requires a significant amount of effort and ontology\nexpertise. This contribution presents an innovative method to automate\ncapability ontology modeling using Large Language Models (LLMs), which have\nproven to be well suited for such tasks. Our approach requires only a natural\nlanguage description of a capability, which is then automatically inserted into\na predefined prompt using a few-shot prompting technique. After prompting an\nLLM, the resulting capability ontology is automatically verified through\nvarious steps in a loop with the LLM to check the overall correctness of the\ncapability ontology. First, a syntax check is performed, then a check for\ncontradictions, and finally a check for hallucinations and missing ontology\nelements. Our method greatly reduces manual effort, as only the initial natural\nlanguage description and a final human review and possible correction are\nnecessary, thereby streamlining the capability ontology generation process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To achieve a flexible and adaptable system, capability ontologies are\nincreasingly leveraged to describe functions in a machine-interpretable way.\nHowever, modeling such complex ontological descriptions is still a manual and\nerror-prone task that requires a significant amount of effort and ontology\nexpertise. This contribution presents an innovative method to automate\ncapability ontology modeling using Large Language Models (LLMs), which have\nproven to be well suited for such tasks. Our approach requires only a natural\nlanguage description of a capability, which is then automatically inserted into\na predefined prompt using a few-shot prompting technique. After prompting an\nLLM, the resulting capability ontology is automatically verified through\nvarious steps in a loop with the LLM to check the overall correctness of the\ncapability ontology. First, a syntax check is performed, then a check for\ncontradictions, and finally a check for hallucinations and missing ontology\nelements. Our method greatly reduces manual effort, as only the initial natural\nlanguage description and a final human review and possible correction are\nnecessary, thereby streamlining the capability ontology generation process."
                },
                "authors": [
                    {
                        "name": "Luis Miguel Vieira da Silva"
                    },
                    {
                        "name": "Aljosha Kcher"
                    },
                    {
                        "name": "Felix Gehlhoff"
                    },
                    {
                        "name": "Alexander Fay"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Fay"
                },
                "author": "Alexander Fay",
                "arxiv_doi": "10.1109/ETFA61755.2024.10710783",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ETFA61755.2024.10710783",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.07962v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07962v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14235v1",
                "updated": "2024-10-18T07:34:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    7,
                    34,
                    21,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T07:34:21Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    7,
                    34,
                    21,
                    4,
                    292,
                    0
                ],
                "title": "Towards Robust Knowledge Representations in Multilingual LLMs for\n  Equivalence and Inheritance based Consistent Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Robust Knowledge Representations in Multilingual LLMs for\n  Equivalence and Inheritance based Consistent Reasoning"
                },
                "summary": "Reasoning and linguistic skills form the cornerstone of human intelligence,\nfacilitating problem-solving and decision-making. Recent advances in Large\nLanguage Models (LLMs) have led to impressive linguistic capabilities and\nemergent reasoning behaviors, fueling widespread adoption across application\ndomains. However, LLMs still struggle with complex reasoning tasks,\nhighlighting their systemic limitations. In this work, we focus on evaluating\nwhether LLMs have the requisite representations to reason using two\nfoundational relationships: \"equivalence\" and \"inheritance\". We introduce novel\ntasks and benchmarks spanning six languages and observe that current SOTA LLMs\noften produce conflicting answers to the same questions across languages in\n17.3-57.5% of cases and violate inheritance constraints in up to 37.2% cases.\nTo enhance consistency across languages, we propose novel \"Compositional\nRepresentations\" where tokens are represented as composition of equivalent\ntokens across languages, with resulting conflict reduction (up to -4.7%)\nindicating benefits of shared LLM representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning and linguistic skills form the cornerstone of human intelligence,\nfacilitating problem-solving and decision-making. Recent advances in Large\nLanguage Models (LLMs) have led to impressive linguistic capabilities and\nemergent reasoning behaviors, fueling widespread adoption across application\ndomains. However, LLMs still struggle with complex reasoning tasks,\nhighlighting their systemic limitations. In this work, we focus on evaluating\nwhether LLMs have the requisite representations to reason using two\nfoundational relationships: \"equivalence\" and \"inheritance\". We introduce novel\ntasks and benchmarks spanning six languages and observe that current SOTA LLMs\noften produce conflicting answers to the same questions across languages in\n17.3-57.5% of cases and violate inheritance constraints in up to 37.2% cases.\nTo enhance consistency across languages, we propose novel \"Compositional\nRepresentations\" where tokens are represented as composition of equivalent\ntokens across languages, with resulting conflict reduction (up to -4.7%)\nindicating benefits of shared LLM representations."
                },
                "authors": [
                    {
                        "name": "Gaurav Arora"
                    },
                    {
                        "name": "Srujana Merugu"
                    },
                    {
                        "name": "Shreya Jain"
                    },
                    {
                        "name": "Vaibhav Saxena"
                    }
                ],
                "author_detail": {
                    "name": "Vaibhav Saxena"
                },
                "author": "Vaibhav Saxena",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14231v1",
                "updated": "2024-10-18T07:25:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    7,
                    25,
                    0,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T07:25:00Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    7,
                    25,
                    0,
                    4,
                    292,
                    0
                ],
                "title": "Unveiling Large Language Models Generated Texts: A Multi-Level\n  Fine-Grained Detection Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Large Language Models Generated Texts: A Multi-Level\n  Fine-Grained Detection Framework"
                },
                "summary": "Large language models (LLMs) have transformed human writing by enhancing\ngrammar correction, content expansion, and stylistic refinement. However, their\nwidespread use raises concerns about authorship, originality, and ethics, even\npotentially threatening scholarly integrity. Existing detection methods, which\nmainly rely on single-feature analysis and binary classification, often fail to\neffectively identify LLM-generated text in academic contexts. To address these\nchallenges, we propose a novel Multi-level Fine-grained Detection (MFD)\nframework that detects LLM-generated text by integrating low-level structural,\nhigh-level semantic, and deep-level linguistic features, while conducting\nsentence-level evaluations of lexicon, grammar, and syntax for comprehensive\nanalysis. To improve detection of subtle differences in LLM-generated text and\nenhance robustness against paraphrasing, we apply two mainstream evasion\ntechniques to rewrite the text. These variations, along with original texts,\nare used to train a text encoder via contrastive learning, extracting\nhigh-level semantic features of sentence to boost detection generalization.\nFurthermore, we leverage advanced LLM to analyze the entire text and extract\ndeep-level linguistic features, enhancing the model's ability to capture\ncomplex patterns and nuances while effectively incorporating contextual\ninformation. Extensive experiments on public datasets show that the MFD model\noutperforms existing methods, achieving an MAE of 0.1346 and an accuracy of\n88.56%. Our research provides institutions and publishers with an effective\nmechanism to detect LLM-generated text, mitigating risks of compromised\nauthorship. Educators and editors can use the model's predictions to refine\nverification and plagiarism prevention protocols, ensuring adherence to\nstandards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have transformed human writing by enhancing\ngrammar correction, content expansion, and stylistic refinement. However, their\nwidespread use raises concerns about authorship, originality, and ethics, even\npotentially threatening scholarly integrity. Existing detection methods, which\nmainly rely on single-feature analysis and binary classification, often fail to\neffectively identify LLM-generated text in academic contexts. To address these\nchallenges, we propose a novel Multi-level Fine-grained Detection (MFD)\nframework that detects LLM-generated text by integrating low-level structural,\nhigh-level semantic, and deep-level linguistic features, while conducting\nsentence-level evaluations of lexicon, grammar, and syntax for comprehensive\nanalysis. To improve detection of subtle differences in LLM-generated text and\nenhance robustness against paraphrasing, we apply two mainstream evasion\ntechniques to rewrite the text. These variations, along with original texts,\nare used to train a text encoder via contrastive learning, extracting\nhigh-level semantic features of sentence to boost detection generalization.\nFurthermore, we leverage advanced LLM to analyze the entire text and extract\ndeep-level linguistic features, enhancing the model's ability to capture\ncomplex patterns and nuances while effectively incorporating contextual\ninformation. Extensive experiments on public datasets show that the MFD model\noutperforms existing methods, achieving an MAE of 0.1346 and an accuracy of\n88.56%. Our research provides institutions and publishers with an effective\nmechanism to detect LLM-generated text, mitigating risks of compromised\nauthorship. Educators and editors can use the model's predictions to refine\nverification and plagiarism prevention protocols, ensuring adherence to\nstandards."
                },
                "authors": [
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Zhiyu Li"
                    },
                    {
                        "name": "Runyu Chen"
                    },
                    {
                        "name": "Dinghao Xi"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09785v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09785v3",
                "updated": "2024-10-18T07:11:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    7,
                    11,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-15T16:32:49Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    16,
                    32,
                    49,
                    6,
                    259,
                    0
                ],
                "title": "Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Based Generative Error Correction: A Challenge and\n  Baselines for Speech Recognition, Speaker Tagging, and Emotion Recognition"
                },
                "summary": "Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given recent advances in generative AI technology, a key question is how\nlarge language models (LLMs) can enhance acoustic modeling tasks using text\ndecoding results from a frozen, pretrained automatic speech recognition (ASR)\nmodel. To explore new capabilities in language modeling for speech processing,\nwe introduce the generative speech transcription error correction (GenSEC)\nchallenge. This challenge comprises three post-ASR language modeling tasks: (i)\npost-ASR transcription correction, (ii) speaker tagging, and (iii) emotion\nrecognition. These tasks aim to emulate future LLM-based agents handling\nvoice-based interfaces while remaining accessible to a broad audience by\nutilizing open pretrained language models or agent-based APIs. We also discuss\ninsights from baseline evaluations, as well as lessons learned for designing\nfuture evaluations."
                },
                "authors": [
                    {
                        "name": "Chao-Han Huck Yang"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Yuan Gong"
                    },
                    {
                        "name": "Yuanchao Li"
                    },
                    {
                        "name": "Zhehuai Chen"
                    },
                    {
                        "name": "Yen-Ting Lin"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yuchen Hu"
                    },
                    {
                        "name": "Kunal Dhawan"
                    },
                    {
                        "name": "Piotr elasko"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yun-Nung Chen"
                    },
                    {
                        "name": "Yu Tsao"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    },
                    {
                        "name": "Sabato Marco Siniscalchi"
                    },
                    {
                        "name": "Eng Siong Chng"
                    },
                    {
                        "name": "Peter Bell"
                    },
                    {
                        "name": "Catherine Lai"
                    },
                    {
                        "name": "Shinji Watanabe"
                    },
                    {
                        "name": "Andreas Stolcke"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Stolcke"
                },
                "author": "Andreas Stolcke",
                "arxiv_comment": "IEEE SLT 2024. The initial draft version has been done in December\n  2023. Post-ASR Text Processing and Understanding Community and LlaMA-7B\n  pre-training correction model:\n  https://huggingface.co/GenSEC-LLM/SLT-Task1-Llama2-7b-HyPo-baseline",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09785v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09785v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.04808v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.04808v3",
                "updated": "2024-10-18T07:05:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    7,
                    5,
                    57,
                    4,
                    292,
                    0
                ],
                "published": "2024-03-06T10:55:30Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    10,
                    55,
                    30,
                    2,
                    66,
                    0
                ],
                "title": "WaterMax: breaking the LLM watermark detectability-robustness-quality\n  trade-off",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaterMax: breaking the LLM watermark detectability-robustness-quality\n  trade-off"
                },
                "summary": "Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking is a technical means to dissuade malfeasant usage of Large\nLanguage Models. This paper proposes a novel watermarking scheme, so-called\nWaterMax, that enjoys high detectability while sustaining the quality of the\ngenerated text of the original LLM. Its new design leaves the LLM untouched (no\nmodification of the weights, logits, temperature, or sampling technique).\nWaterMax balances robustness and complexity contrary to the watermarking\ntechniques of the literature inherently provoking a trade-off between quality\nand robustness. Its performance is both theoretically proven and experimentally\nvalidated. It outperforms all the SotA techniques under the most complete\nbenchmark suite. Code available at https://github.com/eva-giboulot/WaterMax."
                },
                "authors": [
                    {
                        "name": "Eva Giboulot"
                    },
                    {
                        "name": "Teddy Furon"
                    }
                ],
                "author_detail": {
                    "name": "Teddy Furon"
                },
                "author": "Teddy Furon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.04808v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.04808v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14024v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14024v4",
                "updated": "2024-10-18T06:59:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    59,
                    24,
                    4,
                    292,
                    0
                ],
                "published": "2024-06-20T06:42:27Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    6,
                    42,
                    27,
                    3,
                    172,
                    0
                ],
                "title": "LLM Critics Help Catch Bugs in Mathematics: Towards a Better\n  Mathematical Verifier with Natural Language Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Critics Help Catch Bugs in Mathematics: Towards a Better\n  Mathematical Verifier with Natural Language Feedback"
                },
                "summary": "In recent progress, mathematical verifiers have achieved success in\nmathematical reasoning tasks by validating the correctness of solutions\ngenerated by policy models. However, existing verifiers are trained with binary\nclassification labels, which are not informative enough for the model to\naccurately assess the solutions. To mitigate the aforementioned insufficiency\nof binary labels, we introduce step-wise natural language feedback as rationale\nlabels, that is, the correctness of each step and the detailed explanations. In\nthis paper, we propose Math-Minos, a natural language feedback-enhanced\nverifier by constructing automatically generated training data and a two-stage\ntraining paradigm for effective training and efficient inference. Our\nexperiments reveal that a small set of natural language feedback can\nsignificantly boost the performance of the verifier in both verification and\nreinforcement learning. We have released the code and data for further\nexploration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent progress, mathematical verifiers have achieved success in\nmathematical reasoning tasks by validating the correctness of solutions\ngenerated by policy models. However, existing verifiers are trained with binary\nclassification labels, which are not informative enough for the model to\naccurately assess the solutions. To mitigate the aforementioned insufficiency\nof binary labels, we introduce step-wise natural language feedback as rationale\nlabels, that is, the correctness of each step and the detailed explanations. In\nthis paper, we propose Math-Minos, a natural language feedback-enhanced\nverifier by constructing automatically generated training data and a two-stage\ntraining paradigm for effective training and efficient inference. Our\nexperiments reveal that a small set of natural language feedback can\nsignificantly boost the performance of the verifier in both verification and\nreinforcement learning. We have released the code and data for further\nexploration."
                },
                "authors": [
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Runji Lin"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14024v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14024v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14211v1",
                "updated": "2024-10-18T06:57:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    57,
                    19,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T06:57:19Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    57,
                    19,
                    4,
                    292,
                    0
                ],
                "title": "Paths-over-Graph: Knowledge Graph Enpowered Large Language Model\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Paths-over-Graph: Knowledge Graph Enpowered Large Language Model\n  Reasoning"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive results in various\ntasks but struggle with hallucination problems and lack of relevant knowledge,\nespecially in deep complex reasoning and knowledge-intensive tasks. Knowledge\nGraphs (KGs), which capture vast amounts of facts in a structured format, offer\na reliable source of knowledge for reasoning. However, existing KG-based LLM\nreasoning methods face challenges like handling multi-hop reasoning,\nmulti-entity questions, and effectively utilizing graph structures. To address\nthese issues, we propose Paths-over-Graph (PoG), a novel method that enhances\nLLM reasoning by integrating knowledge reasoning paths from KGs, improving the\ninterpretability and faithfulness of LLM outputs. PoG tackles multi-hop and\nmulti-entity questions through a three-phase dynamic multi-hop path\nexploration, which combines the inherent knowledge of LLMs with factual\nknowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant\ninformation from the graph exploration first and introduces efficient\nthree-step pruning techniques that incorporate graph structures, LLM prompting,\nand a pre-trained language model (e.g., SBERT) to effectively narrow down the\nexplored candidate paths. This ensures all reasoning paths contain highly\nrelevant information captured from KGs, making the reasoning faithful and\ninterpretable in problem-solving. PoG innovatively utilizes graph structure to\nprune the irrelevant noise and represents the first method to implement\nmulti-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive\nexperiments on five benchmark KGQA datasets demonstrate PoG outperforms the\nstate-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an\naverage accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo\nsurpasses ToG with GPT-4 by up to 23.9%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive results in various\ntasks but struggle with hallucination problems and lack of relevant knowledge,\nespecially in deep complex reasoning and knowledge-intensive tasks. Knowledge\nGraphs (KGs), which capture vast amounts of facts in a structured format, offer\na reliable source of knowledge for reasoning. However, existing KG-based LLM\nreasoning methods face challenges like handling multi-hop reasoning,\nmulti-entity questions, and effectively utilizing graph structures. To address\nthese issues, we propose Paths-over-Graph (PoG), a novel method that enhances\nLLM reasoning by integrating knowledge reasoning paths from KGs, improving the\ninterpretability and faithfulness of LLM outputs. PoG tackles multi-hop and\nmulti-entity questions through a three-phase dynamic multi-hop path\nexploration, which combines the inherent knowledge of LLMs with factual\nknowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant\ninformation from the graph exploration first and introduces efficient\nthree-step pruning techniques that incorporate graph structures, LLM prompting,\nand a pre-trained language model (e.g., SBERT) to effectively narrow down the\nexplored candidate paths. This ensures all reasoning paths contain highly\nrelevant information captured from KGs, making the reasoning faithful and\ninterpretable in problem-solving. PoG innovatively utilizes graph structure to\nprune the irrelevant noise and represents the first method to implement\nmulti-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive\nexperiments on five benchmark KGQA datasets demonstrate PoG outperforms the\nstate-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an\naverage accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo\nsurpasses ToG with GPT-4 by up to 23.9%."
                },
                "authors": [
                    {
                        "name": "Xingyu Tan"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Qing Liu"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Wenjie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Zhang"
                },
                "author": "Wenjie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19740v2",
                "updated": "2024-10-18T06:57:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    57,
                    8,
                    4,
                    292,
                    0
                ],
                "published": "2024-05-30T06:38:32Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    6,
                    38,
                    32,
                    3,
                    151,
                    0
                ],
                "title": "PertEval: Unveiling Real Knowledge Capacity of LLMs with\n  Knowledge-Invariant Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PertEval: Unveiling Real Knowledge Capacity of LLMs with\n  Knowledge-Invariant Perturbations"
                },
                "summary": "Expert-designed close-ended benchmarks are indispensable in assessing the\nknowledge capacity of large language models (LLMs). Despite their widespread\nuse, concerns have mounted regarding their reliability due to limited test\nscenarios and an unavoidable risk of data contamination. To rectify this, we\npresent PertEval, a toolkit devised for in-depth probing of LLMs' knowledge\ncapacity through \\textbf{knowledge-invariant perturbations}. These\nperturbations employ human-like restatement techniques to generate on-the-fly\ntest samples from static benchmarks, meticulously retaining knowledge-critical\ncontent while altering irrelevant details. Our toolkit further includes a suite\nof \\textbf{response consistency analyses} that compare performance on raw vs.\nperturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six\nrepresentative LLMs are re-evaluated using PertEval. Results reveal\nsignificantly inflated performance of the LLMs on raw benchmarks, including an\nabsolute 25.8% overestimation for GPT-4. Additionally, through a nuanced\nresponse pattern analysis, we discover that PertEval retains LLMs' uncertainty\nto specious knowledge, and reveals their potential rote memorization to correct\noptions which leads to overestimated performance. We also find that the\ndetailed response consistency analyses by PertEval could illuminate various\nweaknesses in existing LLMs' knowledge mastery and guide the development of\nrefinement. Our findings provide insights for advancing more robust and\ngenuinely knowledgeable LLMs. Our code is available at\n\\url{https://github.com/aigc-apps/PertEval}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-designed close-ended benchmarks are indispensable in assessing the\nknowledge capacity of large language models (LLMs). Despite their widespread\nuse, concerns have mounted regarding their reliability due to limited test\nscenarios and an unavoidable risk of data contamination. To rectify this, we\npresent PertEval, a toolkit devised for in-depth probing of LLMs' knowledge\ncapacity through \\textbf{knowledge-invariant perturbations}. These\nperturbations employ human-like restatement techniques to generate on-the-fly\ntest samples from static benchmarks, meticulously retaining knowledge-critical\ncontent while altering irrelevant details. Our toolkit further includes a suite\nof \\textbf{response consistency analyses} that compare performance on raw vs.\nperturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six\nrepresentative LLMs are re-evaluated using PertEval. Results reveal\nsignificantly inflated performance of the LLMs on raw benchmarks, including an\nabsolute 25.8% overestimation for GPT-4. Additionally, through a nuanced\nresponse pattern analysis, we discover that PertEval retains LLMs' uncertainty\nto specious knowledge, and reveals their potential rote memorization to correct\noptions which leads to overestimated performance. We also find that the\ndetailed response consistency analyses by PertEval could illuminate various\nweaknesses in existing LLMs' knowledge mastery and guide the development of\nrefinement. Our findings provide insights for advancing more robust and\ngenuinely knowledgeable LLMs. Our code is available at\n\\url{https://github.com/aigc-apps/PertEval}."
                },
                "authors": [
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Renjun Hu"
                    },
                    {
                        "name": "Kunzhe Huang"
                    },
                    {
                        "name": "Yan Zhuang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Mengxiao Zhu"
                    },
                    {
                        "name": "Xing Shi"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "Accepted by NeurIPS '24 D&B Spotlight; 28 pages, 15 figures, 14\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.01976v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.01976v5",
                "updated": "2024-10-18T06:52:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    52,
                    17,
                    4,
                    292,
                    0
                ],
                "published": "2024-03-04T12:19:28Z",
                "published_parsed": [
                    2024,
                    3,
                    4,
                    12,
                    19,
                    28,
                    0,
                    64,
                    0
                ],
                "title": "SciAssess: Benchmarking LLM Proficiency in Scientific Literature\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciAssess: Benchmarking LLM Proficiency in Scientific Literature\n  Analysis"
                },
                "summary": "Recent breakthroughs in Large Language Models (LLMs) have revolutionized\nscientific literature analysis. However, existing benchmarks fail to adequately\nevaluate the proficiency of LLMs in this domain, particularly in scenarios\nrequiring higher-level abilities beyond mere memorization and the handling of\nmultimodal data. In response to this gap, we introduce SciAssess, a benchmark\nspecifically designed for the comprehensive evaluation of LLMs in scientific\nliterature analysis. It aims to thoroughly assess the efficacy of LLMs by\nevaluating their capabilities in Memorization (L1), Comprehension (L2), and\nAnalysis \\& Reasoning (L3). It encompasses a variety of tasks drawn from\ndiverse scientific fields, including biology, chemistry, material, and\nmedicine. To ensure the reliability of SciAssess, rigorous quality control\nmeasures have been implemented, ensuring accuracy, anonymization, and\ncompliance with copyright standards. SciAssess evaluates 11 LLMs, highlighting\ntheir strengths and areas for improvement. We hope this evaluation supports the\nongoing development of LLM applications in scientific literature analysis.\nSciAssess and its resources are available at\n\\url{https://github.com/sci-assess/SciAssess}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent breakthroughs in Large Language Models (LLMs) have revolutionized\nscientific literature analysis. However, existing benchmarks fail to adequately\nevaluate the proficiency of LLMs in this domain, particularly in scenarios\nrequiring higher-level abilities beyond mere memorization and the handling of\nmultimodal data. In response to this gap, we introduce SciAssess, a benchmark\nspecifically designed for the comprehensive evaluation of LLMs in scientific\nliterature analysis. It aims to thoroughly assess the efficacy of LLMs by\nevaluating their capabilities in Memorization (L1), Comprehension (L2), and\nAnalysis \\& Reasoning (L3). It encompasses a variety of tasks drawn from\ndiverse scientific fields, including biology, chemistry, material, and\nmedicine. To ensure the reliability of SciAssess, rigorous quality control\nmeasures have been implemented, ensuring accuracy, anonymization, and\ncompliance with copyright standards. SciAssess evaluates 11 LLMs, highlighting\ntheir strengths and areas for improvement. We hope this evaluation supports the\nongoing development of LLM applications in scientific literature analysis.\nSciAssess and its resources are available at\n\\url{https://github.com/sci-assess/SciAssess}."
                },
                "authors": [
                    {
                        "name": "Hengxing Cai"
                    },
                    {
                        "name": "Xiaochen Cai"
                    },
                    {
                        "name": "Junhan Chang"
                    },
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Lin Yao"
                    },
                    {
                        "name": "Changxin Wang"
                    },
                    {
                        "name": "Zhifeng Gao"
                    },
                    {
                        "name": "Hongshuai Wang"
                    },
                    {
                        "name": "Yongge Li"
                    },
                    {
                        "name": "Mujie Lin"
                    },
                    {
                        "name": "Shuwen Yang"
                    },
                    {
                        "name": "Jiankun Wang"
                    },
                    {
                        "name": "Mingjun Xu"
                    },
                    {
                        "name": "Jin Huang"
                    },
                    {
                        "name": "Xi Fang"
                    },
                    {
                        "name": "Jiaxi Zhuang"
                    },
                    {
                        "name": "Yuqi Yin"
                    },
                    {
                        "name": "Yaqi Li"
                    },
                    {
                        "name": "Changhong Chen"
                    },
                    {
                        "name": "Zheng Cheng"
                    },
                    {
                        "name": "Zifeng Zhao"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Guolin Ke"
                    }
                ],
                "author_detail": {
                    "name": "Guolin Ke"
                },
                "author": "Guolin Ke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.01976v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.01976v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14209v1",
                "updated": "2024-10-18T06:51:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    51,
                    13,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T06:51:13Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    51,
                    13,
                    4,
                    292,
                    0
                ],
                "title": "Agents4PLC: Automating Closed-loop PLC Code Generation and Verification\n  in Industrial Control Systems using LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents4PLC: Automating Closed-loop PLC Code Generation and Verification\n  in Industrial Control Systems using LLM-based Agents"
                },
                "summary": "In industrial control systems, the generation and verification of\nProgrammable Logic Controller (PLC) code are critical for ensuring operational\nefficiency and safety. While Large Language Models (LLMs) have made strides in\nautomated code generation, they often fall short in providing correctness\nguarantees and specialized support for PLC programming. To address these\nchallenges, this paper introduces Agents4PLC, a novel framework that not only\nautomates PLC code generation but also includes code-level verification through\nan LLM-based multi-agent system. We first establish a comprehensive benchmark\nfor verifiable PLC code generation area, transitioning from natural language\nrequirements to human-written-verified formal specifications and reference PLC\ncode. We further enhance our `agents' specifically for industrial control\nsystems by incorporating Retrieval-Augmented Generation (RAG), advanced prompt\nengineering techniques, and Chain-of-Thought strategies. Evaluation against the\nbenchmark demonstrates that Agents4PLC significantly outperforms previous\nmethods, achieving superior results across a series of increasingly rigorous\nmetrics. This research not only addresses the critical challenges in PLC\nprogramming but also highlights the potential of our framework to generate\nverifiable code applicable to real-world industrial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In industrial control systems, the generation and verification of\nProgrammable Logic Controller (PLC) code are critical for ensuring operational\nefficiency and safety. While Large Language Models (LLMs) have made strides in\nautomated code generation, they often fall short in providing correctness\nguarantees and specialized support for PLC programming. To address these\nchallenges, this paper introduces Agents4PLC, a novel framework that not only\nautomates PLC code generation but also includes code-level verification through\nan LLM-based multi-agent system. We first establish a comprehensive benchmark\nfor verifiable PLC code generation area, transitioning from natural language\nrequirements to human-written-verified formal specifications and reference PLC\ncode. We further enhance our `agents' specifically for industrial control\nsystems by incorporating Retrieval-Augmented Generation (RAG), advanced prompt\nengineering techniques, and Chain-of-Thought strategies. Evaluation against the\nbenchmark demonstrates that Agents4PLC significantly outperforms previous\nmethods, achieving superior results across a series of increasingly rigorous\nmetrics. This research not only addresses the critical challenges in PLC\nprogramming but also highlights the potential of our framework to generate\nverifiable code applicable to real-world industrial applications."
                },
                "authors": [
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Ruinan Zeng"
                    },
                    {
                        "name": "Dongxia Wang"
                    },
                    {
                        "name": "Gengyun Peng"
                    },
                    {
                        "name": "Jingyi Wang"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Wenhai Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhai Wang"
                },
                "author": "Wenhai Wang",
                "arxiv_comment": "12 pages (references included), 6 figures and 3 tables. ICSE-SEIP at\n  review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14202v1",
                "updated": "2024-10-18T06:35:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    35,
                    17,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T06:35:17Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    35,
                    17,
                    4,
                    292,
                    0
                ],
                "title": "Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay\n  Scoring with Rationale Generated by LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay\n  Scoring with Rationale Generated by LLMs"
                },
                "summary": "Existing automated essay scoring (AES) has solely relied on essay text\nwithout using explanatory rationales for the scores, thereby forgoing an\nopportunity to capture the specific aspects evaluated by rubric indicators in a\nfine-grained manner. This paper introduces Rationale-based Multiple Trait\nScoring (RMTS), a novel approach for multi-trait essay scoring that integrates\nprompt-engineering-based large language models (LLMs) with a fine-tuning-based\nessay scoring model using a smaller large language model (S-LLM). RMTS uses an\nLLM-based trait-wise rationale generation system where a separate LLM agent\ngenerates trait-specific rationales based on rubric guidelines, which the\nscoring model uses to accurately predict multi-trait scores. Extensive\nexperiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize,\nshow that RMTS significantly outperforms state-of-the-art models and vanilla\nS-LLMs in trait-specific scoring. By assisting quantitative assessment with\nfine-grained qualitative rationales, RMTS enhances the trait-wise reliability,\nproviding partial explanations about essays.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing automated essay scoring (AES) has solely relied on essay text\nwithout using explanatory rationales for the scores, thereby forgoing an\nopportunity to capture the specific aspects evaluated by rubric indicators in a\nfine-grained manner. This paper introduces Rationale-based Multiple Trait\nScoring (RMTS), a novel approach for multi-trait essay scoring that integrates\nprompt-engineering-based large language models (LLMs) with a fine-tuning-based\nessay scoring model using a smaller large language model (S-LLM). RMTS uses an\nLLM-based trait-wise rationale generation system where a separate LLM agent\ngenerates trait-specific rationales based on rubric guidelines, which the\nscoring model uses to accurately predict multi-trait scores. Extensive\nexperiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize,\nshow that RMTS significantly outperforms state-of-the-art models and vanilla\nS-LLMs in trait-specific scoring. By assisting quantitative assessment with\nfine-grained qualitative rationales, RMTS enhances the trait-wise reliability,\nproviding partial explanations about essays."
                },
                "authors": [
                    {
                        "name": "SeongYeub Chu"
                    },
                    {
                        "name": "JongWoo Kim"
                    },
                    {
                        "name": "Bryan Wong"
                    },
                    {
                        "name": "MunYong Yi"
                    }
                ],
                "author_detail": {
                    "name": "MunYong Yi"
                },
                "author": "MunYong Yi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14198v1",
                "updated": "2024-10-18T06:25:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    25,
                    27,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T06:25:27Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    25,
                    27,
                    4,
                    292,
                    0
                ],
                "title": "Supervised Chain of Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Chain of Thought"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nand hold immense potential for advancing Artificial Intelligence. However, the\ncore architecture of most mainstream LLMs -- the Transformer -- has inherent\nlimitations in computational depth, rendering them theoretically incapable of\nsolving many reasoning tasks that demand increasingly deep computations. Chain\nof Thought (CoT) prompting has emerged as a technique to address these\narchitectural limitations, as evidenced by several theoretical studies. It\noffers a promising approach to solving complex reasoning tasks that were\npreviously beyond the capabilities of these models. Despite its successes, CoT\nand its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a\n\"one-prompt-for-all\" approach, using a single prompt structure (e.g., \"think\nstep by step\") for a wide range of tasks -- from counting and sorting to\nsolving mathematical and algorithmic problems. This approach poses significant\nchallenges for models to generate the correct reasoning steps, as the model\nmust navigate through a vast prompt template space to find the appropriate\ntemplate for each task. In this work, we build upon previous theoretical\nanalyses of CoT to demonstrate how the one-prompt-for-all approach can\nnegatively affect the computability of LLMs. We partition the solution search\nspace into two: the prompt space and the answer space. Our findings show that\ntask-specific supervision is essential for navigating the prompt space\naccurately and achieving optimal performance. Through experiments with\nstate-of-the-art LLMs, we reveal a gap in reasoning performance when\nsupervision is applied versus when it is not.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nand hold immense potential for advancing Artificial Intelligence. However, the\ncore architecture of most mainstream LLMs -- the Transformer -- has inherent\nlimitations in computational depth, rendering them theoretically incapable of\nsolving many reasoning tasks that demand increasingly deep computations. Chain\nof Thought (CoT) prompting has emerged as a technique to address these\narchitectural limitations, as evidenced by several theoretical studies. It\noffers a promising approach to solving complex reasoning tasks that were\npreviously beyond the capabilities of these models. Despite its successes, CoT\nand its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a\n\"one-prompt-for-all\" approach, using a single prompt structure (e.g., \"think\nstep by step\") for a wide range of tasks -- from counting and sorting to\nsolving mathematical and algorithmic problems. This approach poses significant\nchallenges for models to generate the correct reasoning steps, as the model\nmust navigate through a vast prompt template space to find the appropriate\ntemplate for each task. In this work, we build upon previous theoretical\nanalyses of CoT to demonstrate how the one-prompt-for-all approach can\nnegatively affect the computability of LLMs. We partition the solution search\nspace into two: the prompt space and the answer space. Our findings show that\ntask-specific supervision is essential for navigating the prompt space\naccurately and achieving optimal performance. Through experiments with\nstate-of-the-art LLMs, we reveal a gap in reasoning performance when\nsupervision is applied versus when it is not."
                },
                "authors": [
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Dujian Ding"
                    }
                ],
                "author_detail": {
                    "name": "Dujian Ding"
                },
                "author": "Dujian Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06833v2",
                "updated": "2024-10-18T06:19:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    19,
                    22,
                    4,
                    292,
                    0
                ],
                "published": "2024-04-10T08:49:27Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    8,
                    49,
                    27,
                    2,
                    101,
                    0
                ],
                "title": "Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural\n  Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural\n  Knowledge"
                },
                "summary": "Recent studies have highlighted the presence of cultural biases in Large\nLanguage Models (LLMs), yet often lack a robust methodology to dissect these\nphenomena comprehensively. Our work aims to bridge this gap by delving into the\nFood domain, a universally relevant yet culturally diverse aspect of human\nlife. We introduce FmLAMA, a multilingual dataset centered on food-related\ncultural facts and variations in food practices. We analyze LLMs across various\narchitectures and configurations, evaluating their performance in both\nmonolingual and multilingual settings. By leveraging templates in six different\nlanguages, we investigate how LLMs interact with language-specific and cultural\nknowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias\ntowards food knowledge prevalent in the United States; (2) Incorporating\nrelevant cultural context significantly improves LLMs' ability to access\ncultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is\nhighly dependent on the interplay between the probing language, the specific\nmodel architecture, and the cultural context in question. This research\nunderscores the complexity of integrating cultural understanding into LLMs and\nemphasizes the importance of culturally diverse datasets to mitigate biases and\nenhance model performance across different cultural domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have highlighted the presence of cultural biases in Large\nLanguage Models (LLMs), yet often lack a robust methodology to dissect these\nphenomena comprehensively. Our work aims to bridge this gap by delving into the\nFood domain, a universally relevant yet culturally diverse aspect of human\nlife. We introduce FmLAMA, a multilingual dataset centered on food-related\ncultural facts and variations in food practices. We analyze LLMs across various\narchitectures and configurations, evaluating their performance in both\nmonolingual and multilingual settings. By leveraging templates in six different\nlanguages, we investigate how LLMs interact with language-specific and cultural\nknowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias\ntowards food knowledge prevalent in the United States; (2) Incorporating\nrelevant cultural context significantly improves LLMs' ability to access\ncultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is\nhighly dependent on the interplay between the probing language, the specific\nmodel architecture, and the cultural context in question. This research\nunderscores the complexity of integrating cultural understanding into LLMs and\nemphasizes the importance of culturally diverse datasets to mitigate biases and\nenhance model performance across different cultural domains."
                },
                "authors": [
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Taelin Karidi"
                    },
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Nicolas Garneau"
                    },
                    {
                        "name": "Yong Cao"
                    },
                    {
                        "name": "Wenyu Chen"
                    },
                    {
                        "name": "Haizhou Li"
                    },
                    {
                        "name": "Daniel Hershcovich"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Hershcovich"
                },
                "author": "Daniel Hershcovich",
                "arxiv_comment": "cultural bias analysis, cultural knowledge probing, large language\n  models, cultural NLP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15585v3",
                "updated": "2024-10-18T06:14:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    14,
                    50,
                    4,
                    292,
                    0
                ],
                "published": "2024-05-24T14:13:54Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    14,
                    13,
                    54,
                    4,
                    145,
                    0
                ],
                "title": "Synergizing In-context Learning with Hints for End-to-end Task-oriented\n  Dialog Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergizing In-context Learning with Hints for End-to-end Task-oriented\n  Dialog Systems"
                },
                "summary": "End-to-end Task-Oriented Dialog (TOD) systems typically require extensive\ntraining datasets to perform well. In contrast, large language model (LLM)\nbased TOD systems can excel even with limited data due to their ability to\nlearn tasks through in-context exemplars. However, these models lack alignment\nwith the style of responses in training data and often generate comprehensive\nresponses, making it difficult for users to grasp the information quickly. In\nresponse, we propose SyncTOD that synergizes LLMs with task-specific hints to\nimprove alignment in low-data settings. SyncTOD employs small auxiliary models\nto provide hints and select exemplars for in-context prompts. With ChatGPT,\nSyncTOD achieves superior performance compared to LLM-based baselines and SoTA\nmodels in low-data settings, while retaining competitive performance in\nfull-data settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end Task-Oriented Dialog (TOD) systems typically require extensive\ntraining datasets to perform well. In contrast, large language model (LLM)\nbased TOD systems can excel even with limited data due to their ability to\nlearn tasks through in-context exemplars. However, these models lack alignment\nwith the style of responses in training data and often generate comprehensive\nresponses, making it difficult for users to grasp the information quickly. In\nresponse, we propose SyncTOD that synergizes LLMs with task-specific hints to\nimprove alignment in low-data settings. SyncTOD employs small auxiliary models\nto provide hints and select exemplars for in-context prompts. With ChatGPT,\nSyncTOD achieves superior performance compared to LLM-based baselines and SoTA\nmodels in low-data settings, while retaining competitive performance in\nfull-data settings."
                },
                "authors": [
                    {
                        "name": "Vishal Vivek Saley"
                    },
                    {
                        "name": "Rocktim Jyoti Das"
                    },
                    {
                        "name": "Dinesh Raghu"
                    },
                    {
                        "name": "Mausam"
                    }
                ],
                "author_detail": {
                    "name": "Mausam"
                },
                "author": "Mausam",
                "arxiv_comment": "EMNLP2024 Camera-Ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04422v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04422v4",
                "updated": "2024-10-18T06:09:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    6,
                    9,
                    31,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-06T09:29:19Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    9,
                    29,
                    19,
                    6,
                    280,
                    0
                ],
                "title": "Hyper-multi-step: The Truth Behind Difficult Long-context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyper-multi-step: The Truth Behind Difficult Long-context Tasks"
                },
                "summary": "Long-context language models (LCLM), characterized by their extensive context\nwindow, is becoming increasingly popular. Meanwhile, many long-context\nbenchmarks present challenging tasks that even the most advanced LCLMs struggle\nto complete. However, the underlying sources of various challenging\nlong-context tasks have seldom been studied. To bridge this gap, we conduct\nexperiments to indicate their difficulty stems primarily from two basic issues:\n\"multi-matching retrieval,\" which requires the simultaneous retrieval of\nmultiple items, and \"logic-based retrieval,\" which necessitates logical\njudgment within retrieval criteria. These two problems, while seemingly\nstraightforward, actually exceed the capabilities of LCLMs because they are\nproven to be hyper-multi-step (demanding numerous steps to solve) in nature.\nThis finding could explain why LLMs struggle with more advanced long-context\ntasks, providing a more accurate perspective for rethinking solutions for them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context language models (LCLM), characterized by their extensive context\nwindow, is becoming increasingly popular. Meanwhile, many long-context\nbenchmarks present challenging tasks that even the most advanced LCLMs struggle\nto complete. However, the underlying sources of various challenging\nlong-context tasks have seldom been studied. To bridge this gap, we conduct\nexperiments to indicate their difficulty stems primarily from two basic issues:\n\"multi-matching retrieval,\" which requires the simultaneous retrieval of\nmultiple items, and \"logic-based retrieval,\" which necessitates logical\njudgment within retrieval criteria. These two problems, while seemingly\nstraightforward, actually exceed the capabilities of LCLMs because they are\nproven to be hyper-multi-step (demanding numerous steps to solve) in nature.\nThis finding could explain why LLMs struggle with more advanced long-context\ntasks, providing a more accurate perspective for rethinking solutions for them."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Ma Xiufa"
                    },
                    {
                        "name": "Fang Jianwei"
                    },
                    {
                        "name": "Zhi Xu"
                    },
                    {
                        "name": "Su Guangyao"
                    },
                    {
                        "name": "Wang Jiancheng"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Zhixiao Qi"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Weifeng Liu"
                    },
                    {
                        "name": "Ran Chen"
                    },
                    {
                        "name": "Ji Pei"
                    }
                ],
                "author_detail": {
                    "name": "Ji Pei"
                },
                "author": "Ji Pei",
                "arxiv_comment": "Our code is publicly available at\n  https://github.com/yuyijiong/hard_retrieval_for_llm and the datasets is at\n  https://huggingface.co/datasets/yuyijiong/difficult_retrieval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04422v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04422v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14191v1",
                "updated": "2024-10-18T05:55:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    5,
                    55,
                    28,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T05:55:28Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    5,
                    55,
                    28,
                    4,
                    292,
                    0
                ],
                "title": "A Probabilistic Model for Skill Acquisition with Switching Latent\n  Feedback Controllers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Probabilistic Model for Skill Acquisition with Switching Latent\n  Feedback Controllers"
                },
                "summary": "Manipulation tasks often consist of subtasks, each representing a distinct\nskill. Mastering these skills is essential for robots, as it enhances their\nautonomy, efficiency, adaptability, and ability to work in their environment.\nLearning from demonstrations allows robots to rapidly acquire new skills\nwithout starting from scratch, with demonstrations typically sequencing skills\nto achieve tasks. Behaviour cloning approaches to learning from demonstration\ncommonly rely on mixture density network output heads to predict robot actions.\nIn this work, we first reinterpret the mixture density network as a library of\nfeedback controllers (or skills) conditioned on latent states. This arises from\nthe observation that a one-layer linear network is functionally equivalent to a\nclassical feedback controller, with network weights corresponding to controller\ngains. We use this insight to derive a probabilistic graphical model that\ncombines these elements, describing the skill acquisition process as\nsegmentation in a latent space, where each skill policy functions as a feedback\ncontrol law in this latent space. Our approach significantly improves not only\ntask success rate, but also robustness to observation noise when trained with\nhuman demonstrations. Our physical robot experiments further show that the\ninduced robustness improves model deployment on robots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manipulation tasks often consist of subtasks, each representing a distinct\nskill. Mastering these skills is essential for robots, as it enhances their\nautonomy, efficiency, adaptability, and ability to work in their environment.\nLearning from demonstrations allows robots to rapidly acquire new skills\nwithout starting from scratch, with demonstrations typically sequencing skills\nto achieve tasks. Behaviour cloning approaches to learning from demonstration\ncommonly rely on mixture density network output heads to predict robot actions.\nIn this work, we first reinterpret the mixture density network as a library of\nfeedback controllers (or skills) conditioned on latent states. This arises from\nthe observation that a one-layer linear network is functionally equivalent to a\nclassical feedback controller, with network weights corresponding to controller\ngains. We use this insight to derive a probabilistic graphical model that\ncombines these elements, describing the skill acquisition process as\nsegmentation in a latent space, where each skill policy functions as a feedback\ncontrol law in this latent space. Our approach significantly improves not only\ntask success rate, but also robustness to observation noise when trained with\nhuman demonstrations. Our physical robot experiments further show that the\ninduced robustness improves model deployment on robots."
                },
                "authors": [
                    {
                        "name": "Juyan Zhang"
                    },
                    {
                        "name": "Dana Kulic"
                    },
                    {
                        "name": "Michael Burke"
                    }
                ],
                "author_detail": {
                    "name": "Michael Burke"
                },
                "author": "Michael Burke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14583v2",
                "updated": "2024-10-18T05:41:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    5,
                    41,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-22T20:21:20Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    20,
                    21,
                    20,
                    6,
                    266,
                    0
                ],
                "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A\n  Comparative Analysis of Occupational and Crime Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A\n  Comparative Analysis of Occupational and Crime Scenarios"
                },
                "summary": "Recent advancements in Large Language Models(LLMs) have been notable, yet\nwidespread enterprise adoption remains limited due to various constraints. This\npaper examines bias in LLMs-a crucial issue affecting their usability,\nreliability, and fairness. Researchers are developing strategies to mitigate\nbias, including debiasing layers, specialized reference datasets like\nWinogender and Winobias, and reinforcement learning with human feedback (RLHF).\nThese techniques have been integrated into the latest LLMs. Our study evaluates\ngender bias in occupational scenarios and gender, age, and racial bias in crime\nscenarios across four leading LLMs released in 2024: Gemini 1.5 Pro, Llama 3\n70B, Claude 3 Opus, and GPT-4o. Findings reveal that LLMs often depict female\ncharacters more frequently than male ones in various occupations, showing a 37%\ndeviation from US BLS data. In crime scenarios, deviations from US FBI data are\n54% for gender, 28% for race, and 17% for age. We observe that efforts to\nreduce gender and racial bias often lead to outcomes that may over-index one\nsub-class, potentially exacerbating the issue. These results highlight the\nlimitations of current bias mitigation techniques and underscore the need for\nmore effective approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models(LLMs) have been notable, yet\nwidespread enterprise adoption remains limited due to various constraints. This\npaper examines bias in LLMs-a crucial issue affecting their usability,\nreliability, and fairness. Researchers are developing strategies to mitigate\nbias, including debiasing layers, specialized reference datasets like\nWinogender and Winobias, and reinforcement learning with human feedback (RLHF).\nThese techniques have been integrated into the latest LLMs. Our study evaluates\ngender bias in occupational scenarios and gender, age, and racial bias in crime\nscenarios across four leading LLMs released in 2024: Gemini 1.5 Pro, Llama 3\n70B, Claude 3 Opus, and GPT-4o. Findings reveal that LLMs often depict female\ncharacters more frequently than male ones in various occupations, showing a 37%\ndeviation from US BLS data. In crime scenarios, deviations from US FBI data are\n54% for gender, 28% for race, and 17% for age. We observe that efforts to\nreduce gender and racial bias often lead to outcomes that may over-index one\nsub-class, potentially exacerbating the issue. These results highlight the\nlimitations of current bias mitigation techniques and underscore the need for\nmore effective approaches."
                },
                "authors": [
                    {
                        "name": "Vishal Mirza"
                    },
                    {
                        "name": "Rahul Kulkarni"
                    },
                    {
                        "name": "Aakanksha Jadhav"
                    }
                ],
                "author_detail": {
                    "name": "Aakanksha Jadhav"
                },
                "author": "Aakanksha Jadhav",
                "arxiv_comment": "11 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14184v1",
                "updated": "2024-10-18T05:31:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    5,
                    31,
                    13,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T05:31:13Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    5,
                    31,
                    13,
                    4,
                    292,
                    0
                ],
                "title": "MetaAlign: Align Large Language Models with Diverse Preferences during\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaAlign: Align Large Language Models with Diverse Preferences during\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) acquire extensive knowledge and remarkable\nabilities from extensive text corpora, making them powerful tools for various\napplications. To make LLMs more usable, aligning them with human preferences is\nessential. Existing alignment techniques, such as Reinforcement Learning from\nHuman Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed\npredefined preferences directly within the model's parameters. These methods,\nhowever, often result in a static alignment that can not account for the\ndiversity of human preferences in practical applications. In response to this\nchallenge, we propose an effective method, \\textbf{MetaAlign}, which aims to\nhelp LLMs dynamically align with various explicit or implicit preferences\nspecified at inference time. Experimental results show that LLMs optimized on\nour meticulously constructed MetaAlign Dataset can effectively align with any\npreferences specified at the inference stage, validating the feasibility of\nMetaAlign. We hope that our work can provide some insights into the alignment\nof language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) acquire extensive knowledge and remarkable\nabilities from extensive text corpora, making them powerful tools for various\napplications. To make LLMs more usable, aligning them with human preferences is\nessential. Existing alignment techniques, such as Reinforcement Learning from\nHuman Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed\npredefined preferences directly within the model's parameters. These methods,\nhowever, often result in a static alignment that can not account for the\ndiversity of human preferences in practical applications. In response to this\nchallenge, we propose an effective method, \\textbf{MetaAlign}, which aims to\nhelp LLMs dynamically align with various explicit or implicit preferences\nspecified at inference time. Experimental results show that LLMs optimized on\nour meticulously constructed MetaAlign Dataset can effectively align with any\npreferences specified at the inference stage, validating the feasibility of\nMetaAlign. We hope that our work can provide some insights into the alignment\nof language models."
                },
                "authors": [
                    {
                        "name": "Mozhi Zhang"
                    },
                    {
                        "name": "Pengyu Wang"
                    },
                    {
                        "name": "Chenkun Tan"
                    },
                    {
                        "name": "Mianqiu Huang"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Yaqian Zhou"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "19 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00131v2",
                "updated": "2024-10-18T05:22:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    5,
                    22,
                    2,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-30T18:12:18Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    12,
                    18,
                    0,
                    274,
                    0
                ],
                "title": "Fisher Information-based Efficient Curriculum Federated Learning with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fisher Information-based Efficient Curriculum Federated Learning with\n  Large Language Models"
                },
                "summary": "As a promising paradigm to collaboratively train models with decentralized\ndata, Federated Learning (FL) can be exploited to fine-tune Large Language\nModels (LLMs). While LLMs correspond to huge size, the scale of the training\ndata significantly increases, which leads to tremendous amounts of computation\nand communication costs. The training data is generally non-Independent and\nIdentically Distributed (non-IID), which requires adaptive data processing\nwithin each device. Although Low Rank Adaptation (LoRA) can significantly\nreduce the scale of parameters to update in the fine-tuning process, it still\ntakes unaffordable time to transfer the low-rank parameters of all the layers\nin LLMs. In this paper, we propose a Fisher Information-based Efficient\nCurriculum Federated Learning framework (FibecFed) with two novel methods,\ni.e., adaptive federated curriculum learning and efficient sparse parameter\nupdate. First, we propose a fisher information-based method to adaptively\nsample data within each device to improve the effectiveness of the FL\nfine-tuning process. Second, we dynamically select the proper layers for global\naggregation and sparse parameters for local update with LoRA so as to improve\nthe efficiency of the FL fine-tuning process. Extensive experimental results\nbased on 10 datasets demonstrate that FibecFed yields excellent performance (up\nto 45.35% in terms of accuracy) and superb fine-tuning speed (up to 98.61%\nfaster) compared with 17 baseline approaches).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a promising paradigm to collaboratively train models with decentralized\ndata, Federated Learning (FL) can be exploited to fine-tune Large Language\nModels (LLMs). While LLMs correspond to huge size, the scale of the training\ndata significantly increases, which leads to tremendous amounts of computation\nand communication costs. The training data is generally non-Independent and\nIdentically Distributed (non-IID), which requires adaptive data processing\nwithin each device. Although Low Rank Adaptation (LoRA) can significantly\nreduce the scale of parameters to update in the fine-tuning process, it still\ntakes unaffordable time to transfer the low-rank parameters of all the layers\nin LLMs. In this paper, we propose a Fisher Information-based Efficient\nCurriculum Federated Learning framework (FibecFed) with two novel methods,\ni.e., adaptive federated curriculum learning and efficient sparse parameter\nupdate. First, we propose a fisher information-based method to adaptively\nsample data within each device to improve the effectiveness of the FL\nfine-tuning process. Second, we dynamically select the proper layers for global\naggregation and sparse parameters for local update with LoRA so as to improve\nthe efficiency of the FL fine-tuning process. Extensive experimental results\nbased on 10 datasets demonstrate that FibecFed yields excellent performance (up\nto 45.35% in terms of accuracy) and superb fine-tuning speed (up to 98.61%\nfaster) compared with 17 baseline approaches)."
                },
                "authors": [
                    {
                        "name": "Ji Liu"
                    },
                    {
                        "name": "Jiaxiang Ren"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Zijie Zhang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Patrick Valduriez"
                    },
                    {
                        "name": "Dejing Dou"
                    }
                ],
                "author_detail": {
                    "name": "Dejing Dou"
                },
                "author": "Dejing Dou",
                "arxiv_comment": "27 pages, 8 figures, 14 tables, to appear in EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14182v1",
                "updated": "2024-10-18T05:21:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    5,
                    21,
                    5,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T05:21:05Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    5,
                    21,
                    5,
                    4,
                    292,
                    0
                ],
                "title": "LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs"
                },
                "summary": "Laboratory accidents pose significant risks to human life and property,\nunderscoring the importance of robust safety protocols. Despite advancements in\nsafety training, laboratory personnel may still unknowingly engage in unsafe\npractices. With the increasing reliance on large language models (LLMs) for\nguidance in various fields, including laboratory settings, there is a growing\nconcern about their reliability in critical safety-related decision-making.\nUnlike trained human researchers, LLMs lack formal lab safety education,\nraising questions about their ability to provide safe and accurate guidance.\nExisting research on LLM trustworthiness primarily focuses on issues such as\nethical compliance, truthfulness, and fairness but fails to fully cover\nsafety-critical real-world applications, like lab safety. To address this gap,\nwe propose the Laboratory Safety Benchmark (LabSafety Bench), a comprehensive\nevaluation framework based on a new taxonomy aligned with Occupational Safety\nand Health Administration (OSHA) protocols. This benchmark includes 765\nmultiple-choice questions verified by human experts, assessing LLMs and vision\nlanguage models (VLMs) performance in lab safety contexts. Our evaluations\ndemonstrate that while GPT-4o outperforms human participants, it is still prone\nto critical errors, highlighting the risks of relying on LLMs in\nsafety-critical environments. Our findings emphasize the need for specialized\nbenchmarks to accurately assess the trustworthiness of LLMs in real-world\nsafety applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laboratory accidents pose significant risks to human life and property,\nunderscoring the importance of robust safety protocols. Despite advancements in\nsafety training, laboratory personnel may still unknowingly engage in unsafe\npractices. With the increasing reliance on large language models (LLMs) for\nguidance in various fields, including laboratory settings, there is a growing\nconcern about their reliability in critical safety-related decision-making.\nUnlike trained human researchers, LLMs lack formal lab safety education,\nraising questions about their ability to provide safe and accurate guidance.\nExisting research on LLM trustworthiness primarily focuses on issues such as\nethical compliance, truthfulness, and fairness but fails to fully cover\nsafety-critical real-world applications, like lab safety. To address this gap,\nwe propose the Laboratory Safety Benchmark (LabSafety Bench), a comprehensive\nevaluation framework based on a new taxonomy aligned with Occupational Safety\nand Health Administration (OSHA) protocols. This benchmark includes 765\nmultiple-choice questions verified by human experts, assessing LLMs and vision\nlanguage models (VLMs) performance in lab safety contexts. Our evaluations\ndemonstrate that while GPT-4o outperforms human participants, it is still prone\nto critical errors, highlighting the risks of relying on LLMs in\nsafety-critical environments. Our findings emphasize the need for specialized\nbenchmarks to accurately assess the trustworthiness of LLMs in real-world\nsafety applications."
                },
                "authors": [
                    {
                        "name": "Yujun Zhou"
                    },
                    {
                        "name": "Jingdong Yang"
                    },
                    {
                        "name": "Kehan Guo"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Tian Gao"
                    },
                    {
                        "name": "Werner Geyer"
                    },
                    {
                        "name": "Nuno Moniz"
                    },
                    {
                        "name": "Nitesh V Chawla"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "arxiv_comment": "50 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14180v1",
                "updated": "2024-10-18T05:16:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    5,
                    16,
                    39,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T05:16:39Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    5,
                    16,
                    39,
                    4,
                    292,
                    0
                ],
                "title": "XForecast: Evaluating Natural Language Explanations for Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XForecast: Evaluating Natural Language Explanations for Time Series\n  Forecasting"
                },
                "summary": "Time series forecasting aids decision-making, especially for stakeholders who\nrely on accurate predictions, making it very important to understand and\nexplain these models to ensure informed decisions. Traditional explainable AI\n(XAI) methods, which underline feature or temporal importance, often require\nexpert knowledge. In contrast, natural language explanations (NLEs) are more\naccessible to laypeople. However, evaluating forecast NLEs is difficult due to\nthe complex causal relationships in time series data. To address this, we\nintroduce two new performance metrics based on simulatability, assessing how\nwell a human surrogate can predict model forecasts using the explanations.\nExperiments show these metrics differentiate good from poor explanations and\nalign with human judgments. Utilizing these metrics, we further evaluate the\nability of state-of-the-art large language models (LLMs) to generate\nexplanations for time series data, finding that numerical reasoning, rather\nthan model size, is the main factor influencing explanation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series forecasting aids decision-making, especially for stakeholders who\nrely on accurate predictions, making it very important to understand and\nexplain these models to ensure informed decisions. Traditional explainable AI\n(XAI) methods, which underline feature or temporal importance, often require\nexpert knowledge. In contrast, natural language explanations (NLEs) are more\naccessible to laypeople. However, evaluating forecast NLEs is difficult due to\nthe complex causal relationships in time series data. To address this, we\nintroduce two new performance metrics based on simulatability, assessing how\nwell a human surrogate can predict model forecasts using the explanations.\nExperiments show these metrics differentiate good from poor explanations and\nalign with human judgments. Utilizing these metrics, we further evaluate the\nability of state-of-the-art large language models (LLMs) to generate\nexplanations for time series data, finding that numerical reasoning, rather\nthan model size, is the main factor influencing explanation quality."
                },
                "authors": [
                    {
                        "name": "Taha Aksu"
                    },
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Sarah Tan"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14177v1",
                "updated": "2024-10-18T05:09:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    5,
                    9,
                    7,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T05:09:07Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    5,
                    9,
                    7,
                    4,
                    292,
                    0
                ],
                "title": "Learning autonomous driving from aerial imagery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning autonomous driving from aerial imagery"
                },
                "summary": "In this work, we consider the problem of learning end to end perception to\ncontrol for ground vehicles solely from aerial imagery. Photogrammetric\nsimulators allow the synthesis of novel views through the transformation of\npre-generated assets into novel views.However, they have a large setup cost,\nrequire careful collection of data and often human effort to create usable\nsimulators. We use a Neural Radiance Field (NeRF) as an intermediate\nrepresentation to synthesize novel views from the point of view of a ground\nvehicle. These novel viewpoints can then be used for several downstream\nautonomous navigation applications. In this work, we demonstrate the utility of\nnovel view synthesis though the application of training a policy for end to end\nlearning from images and depth data. In a traditional real to sim to real\nframework, the collected data would be transformed into a visual simulator\nwhich could then be used to generate novel views. In contrast, using a NeRF\nallows a compact representation and the ability to optimize over the parameters\nof the visual simulator as more data is gathered in the environment. We\ndemonstrate the efficacy of our method in a custom built mini-city environment\nthrough the deployment of imitation policies on robotic cars. We additionally\nconsider the task of place localization and demonstrate that our method is able\nto relocalize the car in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the problem of learning end to end perception to\ncontrol for ground vehicles solely from aerial imagery. Photogrammetric\nsimulators allow the synthesis of novel views through the transformation of\npre-generated assets into novel views.However, they have a large setup cost,\nrequire careful collection of data and often human effort to create usable\nsimulators. We use a Neural Radiance Field (NeRF) as an intermediate\nrepresentation to synthesize novel views from the point of view of a ground\nvehicle. These novel viewpoints can then be used for several downstream\nautonomous navigation applications. In this work, we demonstrate the utility of\nnovel view synthesis though the application of training a policy for end to end\nlearning from images and depth data. In a traditional real to sim to real\nframework, the collected data would be transformed into a visual simulator\nwhich could then be used to generate novel views. In contrast, using a NeRF\nallows a compact representation and the ability to optimize over the parameters\nof the visual simulator as more data is gathered in the environment. We\ndemonstrate the efficacy of our method in a custom built mini-city environment\nthrough the deployment of imitation policies on robotic cars. We additionally\nconsider the task of place localization and demonstrate that our method is able\nto relocalize the car in the real world."
                },
                "authors": [
                    {
                        "name": "Varun Murali"
                    },
                    {
                        "name": "Guy Rosman"
                    },
                    {
                        "name": "Sertac Karaman"
                    },
                    {
                        "name": "Daniela Rus"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Rus"
                },
                "author": "Daniela Rus",
                "arxiv_comment": "Presented at IROS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15545v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15545v3",
                "updated": "2024-10-18T05:04:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    5,
                    4,
                    53,
                    4,
                    292,
                    0
                ],
                "published": "2024-08-28T05:41:52Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    5,
                    41,
                    52,
                    2,
                    241,
                    0
                ],
                "title": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding"
                },
                "summary": "Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific literature understanding is crucial for extracting targeted\ninformation and garnering insights, thereby significantly advancing scientific\ndiscovery. Despite the remarkable success of Large Language Models (LLMs), they\nface challenges in scientific literature understanding, primarily due to (1) a\nlack of scientific knowledge and (2) unfamiliarity with specialized scientific\ntasks.\n  To develop an LLM specialized in scientific literature understanding, we\npropose a hybrid strategy that integrates continual pre-training (CPT) and\nsupervised fine-tuning (SFT), to simultaneously infuse scientific domain\nknowledge and enhance instruction-following capabilities for domain-specific\ntasks.cIn this process, we identify two key challenges: (1) constructing\nhigh-quality CPT corpora, and (2) generating diverse SFT instructions. We\naddress these challenges through a meticulous pipeline, including PDF text\nextraction, parsing content error correction, quality filtering, and synthetic\ninstruction creation. Applying this strategy, we present a suite of LLMs:\nSciLitLLM, specialized in scientific literature understanding. These models\ndemonstrate promising performance on scientific literature understanding\nbenchmarks.\n  Our contributions are threefold: (1) We present an effective framework that\nintegrates CPT and SFT to adapt LLMs to scientific literature understanding,\nwhich can also be easily adapted to other domains. (2) We propose an LLM-based\nsynthesis method to generate diverse and high-quality scientific instructions,\nresulting in a new instruction set -- SciLitIns -- for supervised fine-tuning\nin less-represented scientific domains. (3) SciLitLLM achieves promising\nperformance improvements on scientific literature understanding benchmarks."
                },
                "authors": [
                    {
                        "name": "Sihang Li"
                    },
                    {
                        "name": "Jin Huang"
                    },
                    {
                        "name": "Jiaxi Zhuang"
                    },
                    {
                        "name": "Yaorui Shi"
                    },
                    {
                        "name": "Xiaochen Cai"
                    },
                    {
                        "name": "Mingjun Xu"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Guolin Ke"
                    },
                    {
                        "name": "Hengxing Cai"
                    }
                ],
                "author_detail": {
                    "name": "Hengxing Cai"
                },
                "author": "Hengxing Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15545v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15545v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]